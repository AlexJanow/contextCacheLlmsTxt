title: The GenAI evaluation and observability platform

===
- [Full Context of Maxim AI](https://www.getmaxim.ai/llms-full.txt): *   **One-line description:** Contains the full length context of the contents of the website for LLMs.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai): *   **One-line description:** This page introduces Maxim, an end-to-end evaluation and observability platform designed to accelerate the development and deployment of AI agents.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai#experimentation): *   **One-line description:** This page introduces Maxim, an end-to-end platform for evaluating, simulating, and observing AI agents, enabling faster and more reliable AI development.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/contact): Contact Page for Maxim AI

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/demo): Demo Page

- [Maxim AI Blog](https://www.getmaxim.ai/blog): *   **One-line description:** The page is the blog section of Maxim AI, featuring articles on AI agent evaluation, prompt engineering, updates on Maxim AI features, and research paper summaries.

- [Ensuring Responsible AI: DeepMind’s FACTS Framework Explained](https://www.getmaxim.ai/blog/deepmind-facts-framework-responsible-ai): *   **One-line description:** This page describes DeepMind's FACTS Grounding framework, a benchmark for evaluating the factual accuracy of language models generating long-form responses grounded in provided documents.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/privacy-policy): **One-line description:** This page is the Privacy Policy for Maxim AI, a GenAI evaluation and observability platform, detailing the collection, use, and protection of user data in compliance with GDPR and HIPAA.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/pricing): *   **One-line description:** This page outlines the pricing plans and features of a GenAI evaluation and observability platform.

- [Coconut by Meta AI: Efficient LLM Reasoning Improvement](https://www.getmaxim.ai/blog/llms-continuous-latent-spaces): *   **One-line description:** This webpage discusses Coconut, a novel approach by Meta AI to improve Large Language Model (LLM) reasoning by using a continuous latent space instead of traditional language-based methods.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/careers): *   **One-line description:** This page is about Maxim, an AI evaluation and observability platform company, highlighting its mission, values, team, and open job positions.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/demo?utm_campaign=blog_post&utm_content=red-teaming&utm_medium=social&utm_source=content): **One-line description:** This page promotes an AI evaluation and observability platform called Maxim, offering a demo and contact information.

- [Guide to Master Prompt Engineering for Better AI Outcomes](https://www.getmaxim.ai/blog/mastering-prompt-engineering): *   **One-line description:** This webpage is a guide to mastering prompt engineering techniques for improving the output of AI models, emphasizing practical techniques and the use of Maxim AI.

- [Overview | Maxim](https://www.getmaxim.ai/docs/introduction/overview): *   **One-line description:** This page introduces Maxim, a platform designed to streamline AI application development and deployment through experimentation, evaluation, observability, and data management.

- [Maxim AI - Product Updates, December 2024 ✨](https://www.getmaxim.ai/blog/maxim-ai-december-2024-updates): **One-line description:**

- [Maxim Blog (Page 2)](https://www.getmaxim.ai/blog/page/2): *   **One-line description:** This is the second page of the Maxim AI blog, featuring articles about AI agents, prompt engineering, agent evaluation, and related topics.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/demo?utm_campaign=blog_post&utm_content=chain-of-thought-prompting&utm_medium=social&utm_source=content): **One-line description:** This page is a landing page for a GenAI evaluation and observability platform, encouraging users to schedule a demo.

- [Overview | Maxim](https://www.getmaxim.ai/docs/library/overview): *   **One-line description:** This page provides an overview of Maxim's features and components for AI testing, including evaluators, datasets, context sources, prompt tools, and custom models.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/jobs/full-stack-engineer): *   **One-line description:** This page is a job posting for a Full-stack Engineer position at Maxim, an AI evaluation and observability platform company, located in Bangalore, India.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/jobs/head-of-engineering): *   **One-line description:** This page is a job posting for a Head of Engineering at Maxim, a company building an AI evaluation and observability platform.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/jobs/full-stack-engineer#apply-now): *   **One-line description:** This page is a job posting for a Full-stack Engineer at Maxim in Bangalore, India, a company building an AI evaluation and observability platform.

- [Overview | Maxim](https://www.getmaxim.ai/docs/self-hosting/overview): *   **One-line description:** This page describes Maxim's self-hosting and enterprise deployment options, focusing on Zero Touch and Data Plane deployments, infrastructure components, and security measures.

- [Sameer Gupta - Maxim Blog](https://www.getmaxim.ai/blog/author/sameer): *   **One-line description:** This page is a blog featuring articles written by Sameer Gupta and others, focusing on AI topics such as prompt engineering, LLM tool use, and agent evaluation.

- [BrowseComp by OpenAI: Benchmarking for Web-Browsing Agent](https://www.getmaxim.ai/blog/openai-browsecomp-web-browsing-agent-benchmark): *   **One-line description:** OpenAI's BrowseComp is a new benchmark designed to rigorously evaluate the web-browsing and reasoning capabilities of AI agents.

- [Overview | Maxim](https://www.getmaxim.ai/docs/introduction/overview#2-evaluate): *   **One-line description:** Maxim is a platform that streamlines AI application development and deployment by providing tools for experimentation, evaluation, observability, and data management.

- [Building Trustworthy AI: Thoughtful’s Journey with Maxim AI](https://www.getmaxim.ai/blog/building-smarter-ai-thoughtfuls-journey-with-maxim-ai): **One-line description:** This page describes how Thoughtful, an AI-companion company, uses Maxim AI to streamline their AI development and improve the quality of their AI interactions.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/jobs/applied-ai-engineer): *   **One-line description:** This page is a job posting for an Applied AI Engineer at Maxim, an AI evaluation and observability platform company, located in Bangalore, India.

- [Test your AI application using an API endpoint | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-your-ai-outputs-using-application-endpoint): *   **One-line description:** This page explains how to test an AI application using its existing API endpoint within the Maxim platform.

- [Transform API data with Workflow scripts | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/scripting-to-configure-response-structures): *   **One-line description:** This page explains how to use Workflow scripts to transform API data by customizing API requests and responses within the Maxim platform.

- [Node level evaluation | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/node-level-evaluation): *   **One-line description:** This page explains how to use Maxim's Node level evaluation feature to gain insights into the behavior of individual components within AI agent workflows.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/library/concepts#evaluator-grading): **One-line description:**

- [Overview | Maxim](https://www.getmaxim.ai/docs/introduction/overview#4-data-engine): *   **One-line description:** This page introduces Maxim, a platform designed to streamline AI application development and deployment by offering tools for experimentation, evaluation, observation, and data management.

- [Experiment in the Prompt playground | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/experiment-in-prompt-playground): *   **One-line description:** This page explains how to experiment with and refine prompts using the Maxim playground, including model selection, prompt engineering, parameter configuration, tool integration, variable usage, and testing workflows.

- [Overview | Maxim](https://www.getmaxim.ai/docs/self-hosting/overview#maxim-infrastructure): *   **One-line description:** This page provides an overview of Maxim's self-hosting and enterprise deployment options, including Zero Touch and Data Plane deployments, infrastructure components, and security measures.

- [Test your AI application using an API endpoint | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-your-ai-outputs-using-application-endpoint#map-the-output-for-evaluation): *   **One-line description:** This page explains how to test AI applications using an existing API endpoint within the Maxim platform by creating workflows, configuring API requests, and mapping outputs for evaluation.

- [MiniCheck-FT5: Cost-effective GPT-4 Accuracy in LLMs](https://www.getmaxim.ai/blog/minicheck-llm-fact-check): *   **One-line description:** This webpage presents MiniCheck-FT5, a cost-effective model for fact-checking LLM outputs that achieves GPT-4 level accuracy.

- [Transform API data with Workflow scripts | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/scripting-to-configure-response-structures#post-response-script): *   **One-line description:** This page explains how to use workflow scripts within the Maxim platform to transform API request and response data, and how to use pre/post simulation scripts.

- [Set up human evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/human-evaluation): *   **One-line description:** This page explains how to set up and use human evaluation on logs within the Maxim platform to assess the quality of interactions between LLMs and users.

- [Evaluating AI Agent Performance with Dynamic Metrics](https://www.getmaxim.ai/blog/ai-agent-evaluation-metrics): *   **One-line description:** This page discusses the importance of dynamic metrics for evaluating AI agent performance in complex, multi-step tasks, going beyond traditional static benchmarks.

- [Curate data from production | Maxim](https://www.getmaxim.ai/docs/library/how-to/datasets/curate-data-from-production): *   **One-line description:** This page provides instructions on how to curate data from production logs into structured Datasets within a model training and evaluation platform.

- [Node level evaluation | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/node-level-evaluation#understanding-how-the-maxim-sdk-logger-evaluates): *   **One-line description:** This page explains how to use Maxim SDK's node level evaluation to analyze AI agent performance by evaluating individual components of a trace or log.

- [Node level evaluation | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/node-level-evaluation#best-practices): *   **One-line description:** This page explains how to use Node level evaluation in the Maxim SDK to evaluate individual components of a trace or log in an AI application.

- [Maxim AI January 2025 Updates ✨](https://www.getmaxim.ai/blog/maxim-ai-january-2025-updates): *   **One-line description:** This page provides updates on Maxim AI's features and capabilities, specifically focusing on the January 2025 release.

- [Setting up your workspace | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/setting-up-workspace): *   **One-line description:** This page guides users through setting up their workspace in Maxim, including adding model API keys, creating Maxim API keys, inviting team members, and managing roles using RBAC.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk): *   **One-line description:** This page explains how to trigger AI application test runs programmatically using the Maxim SDK, including data configuration, custom output functions, evaluators, and advanced settings.

- [Use Prompt partials in your Prompts | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/use-prompt-partials): *   **One-line description:** This page explains how to use Prompt partials in Maxim's platform to enhance and modularize Prompts.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#example): *   **One-line description:** This page explains how to trigger test runs for AI applications programmatically using the Maxim SDK, including data configuration, output handling, and evaluators.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#trigger-a-test-on-a-workflow-stored-on-maxim-platform): *   **One-line description:** This page explains how to trigger test runs for AI applications programmatically using Maxim's SDK, including configuring datasets, outputs, evaluators, and advanced options.

- [Quickstart | Maxim](https://www.getmaxim.ai/docs/observe/quickstart): *   **One-line description:** This page provides a quickstart guide for setting up distributed tracing for GenAI applications using Maxim to monitor performance and debug issues.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/library/concepts#evaluator-store): **One-line description:** This page explains key concepts of AI evaluation within the Maxim platform, focusing on evaluators, datasets, and custom tools for assessing AI model performance.

- [Create a Dataset using templates | Maxim](https://www.getmaxim.ai/docs/library/how-to/datasets/use-dataset-templates): **One-line description:** This page explains how to create datasets using templates within a platform for AI model training, testing, and evaluation.

- [Upgrading to v3 | Maxim](https://www.getmaxim.ai/docs/sdk/upgrading-to-v3): **One-line description:**

- [Public APIs, xAI support, MCP — Maxim's March updates](https://www.getmaxim.ai/blog/maxim-ai-march-2025-updates): *   **One-line description:** This webpage is a blog post announcing the March 2025 updates to Maxim AI, including new features, API improvements, model support, and customer stories.

- [Overview | Maxim](https://www.getmaxim.ai/docs/self-hosting/overview#cloud-provider-support): *   **One-line description:** This page provides an overview of Maxim's self-hosting and enterprise deployment options, focusing on Zero Touch and Data Plane deployments, infrastructure, and security measures.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#3-platform-datasets): *   **One-line description:** This page explains how to trigger test runs using the Maxim SDK for AI applications, covering data structures, data sources, custom output functions, evaluators, and advanced configurations.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/jobs/applied-ai-engineer#apply-now): **One-line description:** This page describes Maxim, an AI evaluation and observability platform, and advertises an Applied AI Engineer position in Bangalore, India.

- [Test your AI application using an API endpoint | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-your-ai-outputs-using-application-endpoint#test-your-workflow): *   **One-line description:** This page describes how to test an AI application by connecting it to Maxim using an existing API endpoint and evaluating the results.

- [From Zero to OTel: Architecting a Stateless Tracing SDK for GenAI](https://www.getmaxim.ai/blog/from-zero-to-otel-architecting-a-stateless-tracing-sdk-for-genai-part-1): *   **One-line description:** This page describes Maxim's stateless, OpenTelemetry-compatible tracing SDK for GenAI applications, highlighting its architecture, advantages, and high-scale production capabilities.

- [Overview | Maxim](https://www.getmaxim.ai/docs/self-hosting/overview#components-1): *   **One-line description:** This page outlines Maxim's self-hosting and enterprise deployment options, including Zero Touch and Data Plane deployments, infrastructure components, and security measures.

- [Simulate multi-turn conversations | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/simulate-multi-turn-conversations): *   **One-line description:** This page explains how to simulate multi-turn conversations to test and evaluate AI conversational abilities within the Maxim platform.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/library/concepts#evaluators): **One-line description:** This page explains the core concepts of AI evaluation within the Maxim platform, focusing on evaluators, datasets, and data curation techniques.

- [Setting up your workspace | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/setting-up-workspace#create-roles): **One-line description:** This page guides users on setting up their workspace in Maxim, including adding model API keys, creating Maxim API keys, inviting team members, and creating roles.

- [Bring your RAG via an API endpoint | Maxim](https://www.getmaxim.ai/docs/library/how-to/context-sources/bring-your-rag-via-an-api-endpoint): **One-line description:** This page explains how to integrate Retrieval-Augmented Generation (RAG) functionality into Maxim by creating a context source that connects to a RAG API endpoint.

- [Overview | Maxim](https://www.getmaxim.ai/docs/introduction/overview#3-observe): *   **One-line description:** Maxim is a platform that streamlines AI application development and deployment by providing tools for experimentation, evaluation, observability, and data management.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#working-with-data-sources): *   **One-line description:** This page explains how to use Maxim's SDK to programmatically trigger test runs for AI applications with custom datasets, output functions, and evaluators.

- [Overview | Maxim](https://www.getmaxim.ai/docs/evaluate/overview): * **One-line description:** This page provides an overview of Maxim, a platform for evaluating AI application performance through prompt testing, workflow automation, and continuous log monitoring.

- [Simulate multi-turn conversations | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/simulate-multi-turn-conversations#example-simulation): *   **One-line description:** This page explains how to simulate multi-turn conversations to test and evaluate AI agents in realistic scenarios.

- [Use pre-built Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/use-pre-built-evaluators): *   **One-line description:** This page explains how to use pre-built evaluators in Maxim for AI evaluation, including installation and usage.

- [Setting up your workspace | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/setting-up-workspace#add-model-api-keys): *   **One-line description:** This page provides a quickstart guide on setting up your workspace in Maxim, including adding model API keys, creating Maxim API keys, inviting team members, and creating roles with role-based access control.

- [Create a Slack integration | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/create-a-slack-integration): **One-line description:**

- [Unbiased LLM Evaluation with PoLL Models](https://www.getmaxim.ai/blog/llm-as-a-jury): *   **One-line description:** This page discusses using a Panel of LLM evaluators (PoLL) with smaller models for more accurate, unbiased, and cost-effective LLM evaluation compared to using a single large model.

- [Overview | Maxim](https://www.getmaxim.ai/docs/self-hosting/overview#control-plane): *   **One-line description:** This page describes Maxim's self-hosting and enterprise deployment options, including Zero Touch Deployment and Data Plane Deployment, focusing on infrastructure, security, and cloud provider support.

- [Setting up your workspace | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/setting-up-workspace#invite-team-members): *   **One-line description:** This page provides a quickstart guide on setting up a workspace in Maxim, including adding API keys, inviting team members, and managing role-based access control.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#human-evaluation): *   **One-line description:** This page explains how to trigger test runs programmatically using the Maxim SDK, covering data structures, data sources, output functions, evaluators, and advanced configurations.

- [Inside OpenAI's o1: Highlights from the System Card (Part 1)](https://www.getmaxim.ai/blog/inside-openai-o1): *   **One-line description:** This webpage is a blog post discussing the first part of OpenAI's o1 model family, focusing on its system card and evaluations regarding disallowed content, jailbreaks, hallucinations, fairness, bias, instruction hierarchy adherence, chain-of-thought safety, and potentially deceptive behavior, along with comparisons to GPT-4o.

- [Set up human evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/human-evaluation#viewing-annotations): *   **One-line description:** This page guides users on how to set up and utilize human evaluation for logs within the Maxim platform.

- [Create Prompt Partials | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-partials/create-prompt-partial): **One-line description:** This page explains how to create and use prompt partials, reusable snippets of prompt elements, to maintain consistency and reduce repetition in AI application development.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/library/concepts#prompt-partials): **One-line description:** This page explains key concepts related to AI evaluation within the Maxim platform, focusing on evaluators, datasets, and related features.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/library/concepts#ai-evaluators): **One-line description:** This page explains key concepts in AI evaluation within the Maxim platform, including evaluators, datasets, and tools for assessing model performance.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/library/concepts#programmatic-evaluators): **One-line description:** This page explains the core concepts of AI evaluation within the Maxim platform, focusing on evaluators, datasets, and related tools for assessing AI model performance.

- [Compare Prompts in the playground | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/compare-prompts-playground): *   **One-line description:** This page explains how to use the Prompt comparison feature in a playground environment to evaluate and optimize prompts by comparing different models and prompt versions side-by-side.

- [Create Prompt Partials | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-partials/create-prompt-partial#publish-your-partial): **One-line description:**

- [Compare Prompts in the playground | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/compare-prompts-playground#run-your-comparison): *   **One-line description:** This page explains how to use the Prompt comparison feature in the playground to compare different prompts, models, and configurations side-by-side for optimization and evaluation.

- [Measure the quality of your RAG pipeline | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/rag-quality): **One-line description:**

- [Quickstart | Maxim](https://www.getmaxim.ai/docs/observe/quickstart#3-install-sdk): *   **One-line description:** This page provides a quickstart guide on setting up distributed tracing for GenAI applications using the Maxim platform, focusing on monitoring performance and debugging issues across services.

- [Create custom AI Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-custom-ai-evaluator): *   **One-line description:** This page provides a guide on how to create custom AI evaluators within the Maxim platform for specific evaluation needs beyond the pre-built options.

- [Scheduled test runs | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/scheduled-test-runs): *   **One-line description:** This page explains how to schedule automated test runs for prompts, prompt chains, and workflows within the Maxim platform.

- [Node level evaluation | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/node-level-evaluation#attaching-evaluators-via-maxim-sdk): *   **One-line description:** This page explains how to use Maxim's Node level evaluation feature with the Maxim SDK to evaluate specific components of AI application traces for debugging and optimization.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#adding-evaluators): *   **One-line description:** This page explains how to use the Maxim SDK to programmatically trigger and configure AI application test runs with custom datasets, output functions, and evaluators.

- [Overview | Maxim](https://www.getmaxim.ai/docs/library/evaluators/concept#ai-evaluators): *   **One-line description:** This page provides an overview of Maxim, a platform designed to streamline AI application development and deployment with tools for experimentation, evaluation, observation, and data management.

- [Automate Prompt evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd): *   **One-line description:** This page describes how to automate prompt evaluation in CI/CD pipelines using Maxim, ensuring quality with every deployment by triggering test runs via CLI or GitHub Actions.

- [Quickstart | Maxim](https://www.getmaxim.ai/docs/observe/quickstart#5-create-trace-in-api-gateway): *   **One-line description:** This page provides a quickstart guide for setting up distributed tracing for GenAI applications using the Maxim platform, focusing on monitoring performance and debugging issues in a chatbot example.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/library/concepts#api-based-evaluators): **One-line description:** This page explains key concepts in AI evaluation within the Maxim platform, including evaluators, datasets, and custom tools for assessing model performance.

- [Scheduled test runs | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/scheduled-test-runs#select-prompt-or-workflow-or-prompt-chain-to-schedule-runs-for): *   **One-line description:** This page describes how to schedule automated test runs for prompts, prompt chains, and workflows in the Maxim platform.

- [Simulate multi-turn conversations | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/simulate-multi-turn-conversations#why-simulate-conversations): *   **One-line description:** This page explains how to simulate multi-turn conversations to test and evaluate the performance of AI conversational agents, including scenario creation, user persona definition, and advanced settings.

- [Set up human evaluation | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-human-evaluators): **One-line description:** This page provides instructions on how to set up human evaluation for AI model outputs, including creating evaluators, defining instructions for raters, selecting evaluation types, and setting pass criteria.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#complete-example): *   **One-line description:** This page provides a guide on how to programmatically trigger test runs using the Maxim SDK for AI application testing.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observe/overview): *   **One-line description:** This page provides an overview of Maxim's LLM observability platform for monitoring, improving, and optimizing AI applications.

- [Query Prompt Chains via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains): *   **One-line description:** This page explains how to use the Maxim SDK to query and retrieve prompt chains, folders, and configure caching.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai#features): *   **One-line description:** This page introduces Maxim, an end-to-end evaluation and observability platform for AI agents, aiming to accelerate development and improve reliability.

- [Overview | Maxim](https://www.getmaxim.ai/docs/self-hosting/overview#pillars-of-maxims-infrastructure): *   **One-line description:** This page describes Maxim's self-hosting and enterprise deployment options, including Zero Touch and Data Plane deployments, infrastructure components, and security measures.

- [Zero Touch Deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/zerotouch): *   **One-line description:** This page describes Maxim's Zero Touch Deployment option, which allows organizations to deploy Maxim's services within their own infrastructure for enhanced security and privacy.

- [Automate workflow evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/automate-via-ci-cd): *   **One-line description:** This page explains how to automate the evaluation of workflows in CI/CD pipelines using Maxim's CLI and GitHub Action.

- [Experiment in the Prompt playground | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/experiment-in-prompt-playground#next-steps): *   **One-line description:** This page explains how to use Maxim's prompt playground to create, refine, experiment with, and deploy prompts, including configuring models, adding prompts, experimenting with tools, setting parameters, and using variables.

- [Measure the quality of your RAG pipeline | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/rag-quality#evaluate-retrieval-at-scale): *   **One-line description:** This page explains how to measure and evaluate the quality of your RAG (Retrieval-Augmented Generation) pipeline using Maxim, focusing on context retrieval and iterative testing.

- [Enhance LLM Reasoning with Chain-of-Thought Prompting](https://www.getmaxim.ai/blog/chain-of-thought-prompting): *   **One-line description:** This page is a guide explaining Chain-of-Thought (CoT) prompting, a technique to enhance the reasoning abilities of Large Language Models (LLMs).

- [Automate Prompt evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd#ensuring-quality-with-every-deployment): * **One-line description:** This page explains how to automate prompt evaluation within a CI/CD pipeline using Maxim, ensuring AI application quality with every deployment.

- [Inside OpenAI's o1: Highlights from the System Card (Part 2)](https://www.getmaxim.ai/blog/inside-openai-o1-part-2): *   **One-line description:** This page discusses the capabilities of OpenAI's o1 model family, focusing on its performance in cybersecurity, handling chemical and biological threats, multimodal troubleshooting, persuasion, coding, and multilingual tasks, as detailed in its System Card Part 2.

- [Automate Prompt evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd#outputs): *   **One-line description:** This page explains how to automate prompt evaluation in CI/CD pipelines using Maxim's CLI tool or GitHub Action to ensure consistent quality with every deployment.

- [Query Prompt Chains via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains#interface-for-custom-cache): *   **One-line description:** This page explains how to use the Maxim SDK to query and retrieve prompt chains, including filtering by deployment variables, folders, and tags, and customizing caching and fallback algorithms.

- [Overview | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/concepts#prompts): *   **One-line description:** This page provides an overview of Maxim, a platform for streamlining AI application development and deployment with tools for experimentation, evaluation, observability, and data management.

- [Create Prompt Partials | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-partials/create-prompt-partial#write-your-partial-content): **One-line description:**

- [Manav Singhal - Maxim Blog](https://www.getmaxim.ai/blog/author/manav): *   **One-line description:** This webpage is Manav Singhal's blog on Maxim, focusing on AI agent evaluation, benchmarking, and related topics.

- [Create a Dataset using templates | Maxim](https://www.getmaxim.ai/docs/library/how-to/datasets/use-dataset-templates#prompt-or-workflow-testing): **One-line description:** This page explains how to create datasets using templates for AI model training, testing, and evaluation, with a focus on Prompt/Workflow testing, Agent simulation, and Dataset testing.

- [Create custom AI Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-custom-ai-evaluator#define-evaluation-logic): *   **One-line description:** This page explains how to create custom AI evaluators within the Maxim platform when pre-built options don't suffice.

- [Overview | Maxim](https://www.getmaxim.ai/docs/library/overview#evaluators): *   **One-line description:** This page provides an overview of the supporting components within Maxim for ensuring high-quality AI through testing, including Evaluators, Datasets, Context Sources, Prompt Tools, and Custom Models.

- [Query Prompt Chains via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains#overriding-fallback-algorithm): *   **One-line description:** This page explains how to use the Maxim SDK to query and retrieve prompt chains, including filtering by deployment variables and folders, and how to implement custom caching.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#understanding-the-data-structure): *   **One-line description:** This page explains how to use the Maxim SDK to programmatically trigger and configure test runs for AI applications, covering data structures, data sources, custom output functions, evaluators, and advanced configurations.

- [Simulate multi-turn conversations | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/simulate-multi-turn-conversations#3-advanced-settings-optional): *   **One-line description:** This page explains how to simulate multi-turn conversations to test and evaluate AI conversational abilities within a platform, using scenarios, personas, and advanced settings.

- [Upgrading to v3 | Maxim](https://www.getmaxim.ai/docs/sdk/upgrading-to-v3#maxim-sdk-initialization-changes): **One-line description:**

- [Evaluate Tool Call Accuracy | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-tools/evaluate-tool-call-accuracy): **One-line description:** This page provides documentation and guidance on evaluating the accuracy of tool calls, creating schema-based prompt tools, and using prompt partials within the MaximPricingCareersBlogDocs platform.

- [Share test run report | Maxim](https://www.getmaxim.ai/docs/api/test-runs/share-report/post): *   **One-line description:** This page describes how to share a test run report using the Maxim API.

- [Query Prompt Chains via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains#query-deployed-prompt-chains-using-folder): *   **One-line description:** This page explains how to use the Maxim SDK to query and retrieve prompt chains, including filtering by deployment variables, folder, and tags, along with information on caching and the matching algorithm.

- [Experiment in the Prompt playground | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/experiment-in-prompt-playground#configuring-parameters): *   **One-line description:** This page explains how to use Maxim's prompt playground to create, refine, experiment with, and deploy prompts, including selecting models, adding system/user prompts, configuring parameters, using variables, and integrating with tools.

- [Automate workflow evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/automate-via-ci-cd#quick-start): **One-line description:** This page explains how to automate workflow evaluation in CI/CD pipelines using Maxim's CLI tool and GitHub Action.

- [Compare Prompt versions | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/compare-prompt-versions): *   **One-line description:** This page explains how to compare different versions of prompts using Maxim to track changes and understand the impact on prompt quality.

- [Delete test runs | Maxim](https://www.getmaxim.ai/docs/api/test-runs/delete): *   **One-line description:** This page documents how to delete test runs from a workspace using the Maxim API.

- [Test multi-turn conversations manually | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-multi-turn-conversations-manually): *   **One-line description:** This page explains how to manually test and simulate multi-turn conversations with an AI endpoint using Maxim's interactive workflows.

- [Curate a golden Dataset from Human Annotation | Maxim](https://www.getmaxim.ai/docs/library/how-to/datasets/curate-golden-dataset-for-human-annotation): **One-line description:** This page explains how to create high-quality, "golden" datasets from human annotations within the Maxim platform.

- [Create Prompt Partials | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-partials/create-prompt-partial#next-steps): **One-line description:**

- [Create custom AI Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-custom-ai-evaluator#normalize-score-optional): *   **One-line description:** This page provides a guide on how to create custom AI evaluators within the Maxim platform to assess AI model outputs based on specific requirements.

- [Share test run report | Maxim](https://www.getmaxim.ai/docs/api/test-runs/share-report/post#authorization): *   **One-line description:** This page documents the API endpoint for sharing a test run report using the Maxim platform, detailing request parameters and response codes.

- [Chain-of-Tools: Future of LLM Tool Use for Complex Reasoning](https://www.getmaxim.ai/blog/chain-of-tools-llm-framework): *   **One-line description:** This page introduces Chain-of-Tools (CoTools), a framework that allows Large Language Models (LLMs) to dynamically use external tools for complex reasoning without retraining.

- [Quickstart | Maxim](https://www.getmaxim.ai/docs/observe/quickstart#1-create-maxim-repository): *   **One-line description:** This page provides a quickstart guide for setting up distributed tracing for GenAI applications using Maxim, focusing on monitoring performance and debugging issues within a microservices architecture.

- [Test multi-turn conversations manually | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-multi-turn-conversations-manually#why-do-we-need-to-test-multi-turn-conversations): *   **One-line description:** This page explains how to manually test multi-turn conversations with an AI endpoint using Maxim's interactive Workflows, allowing simulation, manipulation, and debugging of conversations in real-time.

- [Raising an incident | Maxim](https://www.getmaxim.ai/docs/support/raising-an-incident): *   **One-line description:** This page provides instructions on how to report bugs or incidents on the Maxim platform.

- [Query Prompt Chains via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains#prompt-chain-structure): *   **One-line description:** This page explains how to query and retrieve prompt chains using the Maxim SDK in Python and JavaScript/TypeScript, including filtering by deployment variables, folders, and tags, and customizing the caching mechanism.

- [Overview | Maxim](https://www.getmaxim.ai/docs/self-hosting/overview#data-plane-deployment): *   **One-line description:** This page describes Maxim's self-hosting and enterprise deployment options, including Zero Touch and Data Plane deployments, infrastructure components, and security measures.

- [Setting up alerts for performance metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-performance-metrics): *   **One-line description:** This page provides instructions on how to set up alerts for performance metrics like latency, token usage, and cost in the Maxim platform.

- [Raising an incident | Maxim](https://www.getmaxim.ai/docs/support/raising-an-incident#submit-your-report): **One-line description:** This page provides a guide on how to report bugs and issues on the Maxim platform.

- [Test multi-turn conversations manually | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-multi-turn-conversations-manually#configure-your-endpoint): *   **One-line description:** This page provides a guide on how to manually test and simulate multi-turn conversations with an AI endpoint using Maxim's interactive workflows.

- [Use pre-built Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/use-pre-built-evaluators#install-evaluators): *   **One-line description:** This page explains how to use pre-built evaluators from the Maxim platform's Evaluator Store for AI evaluation.

- [Test multi-turn conversations manually | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-multi-turn-conversations-manually#example-usage): *   **One-line description:** This page explains how to manually test and simulate multi-turn conversations with an AI endpoint using Maxim's interactive Workflows.

- [Automate Prompt evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd#test-runs-via-github-action): *   **One-line description:** This page explains how to automate prompt evaluation using CI/CD pipelines with Maxim, utilizing CLI tools or GitHub Actions.

- [Scheduled test runs | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/scheduled-test-runs#pick-version): *   **One-line description:** This page explains how to schedule automated test runs for prompts, prompt chains, and workflows within the Maxim platform.

- [Automate Prompt evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd#env-variables): *   **One-line description:** This page explains how to automate prompt evaluation in CI/CD pipelines using Maxim's CLI tool and GitHub Action to ensure the quality of AI applications with every deployment.

- [Set up human evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/human-evaluation#setting-up-human-evaluation): *   **One-line description:** This page explains how to set up human evaluation on logs within the Maxim platform to assess the quality of LLM interactions, including configuration, queue management, annotation, and viewing results.

- [Use API nodes within chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/use-api-nodes-within-chains): *   **One-line description:** This page explains how to use API nodes within Prompt Chains to integrate with external services using Maxim.

- [Export logs and evaluation results as CSV | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/export-logs): *   **One-line description:** This page explains how to export logs and evaluation results as a CSV file from the Maxim platform for AI application performance analysis.

- [Create Programmatic Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-programmatic-evaluator): *   **One-line description:** This page guides users on creating custom code-based evaluators using Javascript or Python within the Maxim platform.

- [Data plane deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/dataplane): *   **One-line description:** This page describes Maxim's data plane deployment process for securely processing data within a customer's cloud environment, emphasizing security, control, and data residency.

- [Set up auto evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/auto-evaluation#making-sense-of-evaluations-on-logs): *   **One-line description:** This page explains how to set up and interpret automated evaluations on logs within the Maxim platform, which is crucial for monitoring and improving the performance of LLMs in a live system.

- [Compare Prompt versions | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/compare-prompt-versions#enter-comparison-mode): **One-line description:** This page explains how to compare different versions of prompts within the Maxim platform to track changes and understand quality improvements.

- [Log multi-turn interactions as a session | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/log-multiturn-interactions-as-session): *   **One-line description:** This page explains how to log multi-turn interactions as sessions for tracking user engagement with GenAI systems.

- [Data plane deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/dataplane#security-measures): *   **One-line description:** This page describes Maxim's data plane deployment process, emphasizing security, control, and data residency within a customer's cloud environment.

- [Web agent automation with BrowserGym: Key insights](https://www.getmaxim.ai/blog/browsergym-web-agent-automation): **One-line description:** This webpage provides a technical overview of BrowserGym, a framework for web agent automation, including its architecture, features, and experimental results.

- [Introduction | Maxim](https://www.getmaxim.ai/docs/sdk/overview): * **One-line description:** This page introduces the Maxim SDK, a platform designed to streamline AI application evaluation and observability.

- [Zero Touch Deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/zerotouch#deployment-process): *   **One-line description:** This page describes Maxim's zero-touch deployment process, a self-hosting solution for organizations requiring maximum security and data privacy.

- [Share test run report | Maxim](https://www.getmaxim.ai/docs/api/test-runs/share-report/post#query-parameters): *   **One-line description:** This page documents the API endpoint for sharing a test run report using the Maxim platform.

- [Create custom AI Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-custom-ai-evaluator#test-your-evaluator): *   **One-line description:** This page explains how to create custom AI evaluators within the Maxim platform to assess AI model outputs based on specific requirements and logic.

- [Set up custom token pricing | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/use-custom-pricing): *   **One-line description:** This page explains how to configure custom token pricing in Maxim to ensure accurate cost reporting for AI evaluations and logs.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/library/concepts#custom-evaluators): *   **One-line description:** This page explains key concepts in AI evaluation within the Maxim platform, focusing on evaluators, datasets, and related tools for assessing AI model performance.

- [Overview | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/concepts#4-data-engine): **One-line description:** This page introduces Maxim, a platform for streamlining AI application development and deployment through evaluation, observability, and data management tools.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observe/overview#3-open-source-compatibility): *   **One-line description:** This page introduces Maxim's AI observability platform for monitoring and improving the performance of AI applications.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/demo?utm_campaign=blog_post&utm_content=minicheck&utm_medium=social&utm_source=content): **One-line description:** This page promotes a GenAI evaluation and observability platform, offering to save development time and includes a lead capture form.

- [Send feedback for AI application traces | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/send-user-feedback): *   **One-line description:** This page explains how to send user feedback for AI application traces using Maxim's Feedback entity.
*   **Concise Summary:**

- [Raising an incident | Maxim](https://www.getmaxim.ai/docs/support/raising-an-incident#title): **One-line description:** This page guides users on how to report bugs or incidents on the Maxim platform.

- [Data plane deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/dataplane#support): *   **One-line description:** This page describes Maxim's Data Plane Deployment solution, which allows secure data processing within a customer's cloud environment while leveraging Maxim's cloud-hosted services.

- [Transform API data with Workflow scripts | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/scripting-to-configure-response-structures#supported-models): *   **One-line description:** This page explains how to use Maxim Workflow scripts to transform API data by customizing requests and responses, including pre-request, post-response, and simulation scripts.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/evaluate/concepts): *   **One-line description:** This page introduces and explains key concepts within the Maxim platform related to prompting, testing, and evaluating AI applications.

- [Get test runs | Maxim](https://www.getmaxim.ai/docs/api/test-runs/get): **One-line description:**

- [Set up auto evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/auto-evaluation#individual-evaluators-tab): *   **One-line description:** This page explains how to set up and interpret automatic evaluation of logs using Maxim, allowing users to assess LLM performance in a live system.

- [Create a code-based Prompt Tool | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-tools/create-a-code-tool): *   **One-line description:** This page guides users on how to create and test code-based prompt tools within the Maxim platform using JavaScript, providing an example of a travel price calculator.

- [Run a Prompt with tool calls | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/run-prompt-tool-calls): *   **One-line description:** This page explains how to use Maxim's playground to test and ensure accurate tool call selection in prompts for building reliable AI workflows.

- [Quickstart | Maxim](https://www.getmaxim.ai/docs/observe/quickstart#7-log-llm-calls): *   **One-line description:** This page provides a quickstart guide for setting up distributed tracing for GenAI applications using Maxim's AI observability platform, demonstrated through an enterprise search chatbot example.

- [Zero Touch Deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/zerotouch#observability): *   **One-line description:** This page outlines Maxim's zero-touch deployment process, designed for organizations requiring maximum security and privacy by hosting all data within their own infrastructure.

- [Run bulk comparisons across test cases | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases): *   **One-line description:** This page explains how to run bulk comparisons across test cases for different prompt versions to improve prompt engineering decisions.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observe/overview#data-curation): *   **One-line description:** This page introduces Maxim's LLM observability platform for monitoring AI applications in real-time, improving performance, and reducing costs.

- [Enhance LLM with Retrieval-Augmented Generation (RAG)](https://www.getmaxim.ai/blog/rag-in-ai): *   **One-line description:** This webpage is an article explaining Retrieval-Augmented Generation (RAG), its importance, how it works, its drawbacks, and advancements.

- [Use pre-built Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/use-pre-built-evaluators#use-installed-evaluators): *   **One-line description:** This page explains how to use pre-built AI evaluators from Maxim's Evaluator Store for AI evaluation scenarios.

- [Connect your logs to external platforms | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/use-data-connectors): *   **One-line description:** This page explains how to connect Maxim logs to external observability platforms like New Relic, Snowflake, and any OpenTelemetry (OTLP) collector.

- [Automate workflow evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/automate-via-ci-cd#outputs): *   **One-line description:** This page explains how to automate workflow evaluation using Maxim's CI/CD integration, covering CLI and GitHub Actions.

- [Use variable columns in Datasets | Maxim](https://www.getmaxim.ai/docs/library/how-to/datasets/use-variable-columns-in-datasets): *   **One-line description:** This page explains how to use variable columns within datasets in Maxim to dynamically insert values into entities at runtime.

- [Overview | Maxim](https://www.getmaxim.ai/docs/self-hosting/overview#data-plane): *   **One-line description:** This page provides an overview of Maxim's self-hosting and enterprise deployment options, including Zero Touch and Data Plane deployments, infrastructure components, security measures, and cloud provider support.

- [Overview | Maxim](https://www.getmaxim.ai/docs/api/overview): **One-line Description:**

- [research paper - Maxim Blog](https://www.getmaxim.ai/blog/tag/research-paper): *   **One-line description:** This page from Maxim Blog highlights research papers focusing on AI, including DeepMind's FACTS framework and Meta AI's innovative LLM training in continuous latent spaces.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/observe/concepts): **One-line description:** This page explains the key concepts of Maxim's AI Observability platform, focusing on Log Repositories and the components of logs (Sessions, Traces, Spans, Events, Generations, Retrievals, and Tool Calls).

- [Model Context Protocol: A Detailed Guide](https://www.getmaxim.ai/blog/model-context-protocol-guide-mcp): *   **One-line description:** This page provides a detailed guide to the Model Context Protocol (MCP), explaining its core components, transport mechanisms, server capabilities, and how to integrate it with Claude Desktop.

- [Create a Slack integration | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/create-a-slack-integration#using-the-integration): *   **One-line description:** This page explains how to integrate Maxim with Slack to send alert notifications to Slack channels.

- [Create custom AI Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-custom-ai-evaluator#configure-model-and-parameters): *   **One-line description:** This page provides instructions on how to create custom AI evaluators within the Maxim platform.

- [Introduction | Maxim](https://www.getmaxim.ai/docs/sdk/overview#whats-next): *   **One-line description:** This page introduces the Maxim SDK, a platform designed to streamline AI application evaluation and observability by providing tools and services to integrate traditional software best practices into AI workflows.

- [Export logs and evaluation results as CSV | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/export-logs#export-logs-and-evaluation-data): *   **One-line description:** This page describes how to export logs and evaluation results from Maxim for AI applications as a CSV file for analysis and reporting.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#example-of-triggering-test-runs-using-the-sdk): *   **One-line description:** This page explains how to use Maxim's SDK to programmatically trigger AI application test runs with custom datasets, output functions, and evaluators.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/evaluate/concepts#datasets): *   **One-line description:** This page explains the key concepts within MaximPrompts, a platform for building, testing, and evaluating AI applications using prompts, workflows, datasets, and other related tools.

- [Quickstart | Maxim](https://www.getmaxim.ai/docs/observe/quickstart#view-traces): *   **One-line description:** This page is a quickstart guide for setting up distributed tracing for GenAI applications using Maxim to monitor performance and debug issues across services, using an enterprise search chatbot example.

- [Organize Prompts | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/organize-prompts): *   **One-line description:** This page describes how to organize, tag, version, and deploy prompts using Maxim AI for collaborative AI application development.

- [Quickstart | Maxim](https://www.getmaxim.ai/docs/observe/quickstart#setting-up-the-maxim-dashboard): *   **One-line description:** This page provides a quickstart guide for setting up distributed tracing for GenAI applications using Maxim's platform, focusing on an enterprise search chatbot example.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/evaluate/concepts#context-sources): *   **One-line description:** This page introduces key concepts related to prompts, workflows, and AI application evaluation within the Maxim platform.

- [Set up auto evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/auto-evaluation): *   **One-line description:** This page explains how to set up automated evaluation of logs within a system, focusing on its benefits, configuration steps, and how to interpret the evaluation results.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#custom-output-function): *   **One-line description:** This page explains how to trigger AI application test runs programmatically using the Maxim SDK, covering data structures, data sources, custom outputs, evaluators, and advanced configurations.

- [Running your first test | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/running-first-test): **One-line description:** This page provides a step-by-step guide on how to run your first test using the Maxim platform, covering environment setup, prompt/workflow creation, dataset preparation, evaluator addition, test execution, and results analysis.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/library/concepts): **One-line description:** This page explains key concepts in AI evaluation within the Maxim platform, focusing on evaluators, datasets, and tools for assessing AI model performance.

- [Export logs and evaluation results as CSV | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/export-logs#your-csv-export-contains): *   **One-line description:** This page explains how to export logs and evaluation results as CSV files from the Maxim platform for analysis and reporting of AI application performance.

- [Overview | Maxim](https://www.getmaxim.ai/docs/self-hosting/overview#infra-as-code): *   **One-line description:** This page describes Maxim's self-hosting and enterprise deployment options, including Zero Touch and Data Plane deployments, along with infrastructure details and security measures.

- [Set up human evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/human-evaluation#the-need-for-human-evaluation): *   **One-line description:** This page explains how to set up and use human evaluation to assess the quality of logs within the Maxim platform.

- [maxim updates - Maxim Blog](https://www.getmaxim.ai/blog/tag/maxim-updates): **One-line description:**

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#1-callable): *   **One-line description:** This page describes how to programmatically trigger test runs for AI applications using Maxim's SDK, including custom datasets, output functions, and evaluators.

- [Use variable columns in Datasets | Maxim](https://www.getmaxim.ai/docs/library/how-to/datasets/use-variable-columns-in-datasets#variable-usage-in-prompt): *   **One-line description:** This page explains how to use variable columns in Maxim's Datasets to insert dynamic values into entities at runtime using Jinja template syntax.

- [Set up custom token pricing | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/use-custom-pricing#set-up-pricing-for-log-repository): *   **One-line description:** This page provides instructions on how to set up custom token pricing within the Maxim platform for accurate cost reporting in AI evaluations and logs.

- [Evaluate simulated sessions for agents | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/evaluate-simulated-sessions-for-agents): **One-line description:**
This page explains how to evaluate the performance of AI agents using simulated conversations and test runs.

- [Query Prompt Chains via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains#matching-algorithm): *   **One-line description:** This page explains how to use the Maxim SDK to query and retrieve prompt chains, including filtering by deployment variables, folder, and tags, and how to use custom caching and override the default matching algorithm.

- [Set up human evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/human-evaluation#the-logs-table): **One-line description:** This page explains how to set up and utilize human evaluation for logs within the Maxim platform, allowing for nuanced quality assessment beyond machine learning capabilities.

- [Evaluate simulated sessions for agents | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/evaluate-simulated-sessions-for-agents#set-up-the-test-run): **One-line description:** This page provides a guide on how to evaluate the performance of AI agents using simulated conversations and datasets.

- [Overview | Maxim](https://www.getmaxim.ai/docs/library/overview#prompt-tools): *   **One-line description:** This page provides an overview of the supporting components and features within Maxim for AI testing, including evaluators, datasets, context sources, prompt tools, and custom models.

- [Automate Prompt evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd#test-runs-via-cli): *   **One-line description:** This page explains how to automate prompt evaluation using CI/CD pipelines with Maxim, using either a CLI tool or a GitHub Action.

- [Overview | Maxim](https://www.getmaxim.ai/docs/library/overview#custom-models): *   **One-line description:** This page provides an overview of Maxim's features for AI testing, including evaluators, datasets, context sources, prompt tools, and custom models.

- [Set up automated email summaries to monitor your logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/summary-emails): *   **One-line description:** This page describes how to set up and manage automated weekly email summaries for log repositories to monitor performance, user feedback, and identify potential issues.

- [Compare Prompt versions | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/compare-prompt-versions#access-prompt-versions): **One-line description:**

- [Create Programmatic Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-programmatic-evaluator#evaluator-console): *   **One-line description:** This page explains how to create custom code-based evaluators within the Maxim platform using Javascript or Python.

- [Set up human evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/human-evaluation#the-annotation-queue-page): *   **One-line description:** This page explains how to set up and utilize human evaluation for logs within the Maxim platform.

- [Quickstart | Maxim](https://www.getmaxim.ai/docs/observe/quickstart#4-initialize-logger): *   **One-line description:** This page provides a quickstart guide for setting up distributed tracing for GenAI applications using Maxim to monitor performance and debug issues, using an enterprise search chatbot example.

- [Automate workflow evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/automate-via-ci-cd#installation): *   **One-line description:** This page explains how to automate workflow evaluation in CI/CD pipelines using Maxim's CLI and GitHub Actions.

- [Raising an incident | Maxim](https://www.getmaxim.ai/docs/support/raising-an-incident#what-happens-next): *   **One-line description:** This page provides a guide on how to report issues or bugs on the Maxim platform.

- [Zero Touch Deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/zerotouch#setup-requirements): *   **One-line description:** This page describes Maxim's Zero Touch Deployment option, which enables organizations to host the entire Maxim platform within their own infrastructure for maximum security and privacy.

- [Raising an incident | Maxim](https://www.getmaxim.ai/docs/support/raising-an-incident#locate-the-report-a-bug-tab): *   **One-line description:** This page provides a guide on how to report bugs and incidents on the Maxim platform.

- [Overview | Maxim](https://www.getmaxim.ai/docs/sdk/test-runs-via-sdk/js-ts): Error processing content

- [Run bulk comparisons across test cases | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases#next-steps): Error processing content

- [Connect your logs to external platforms | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/use-data-connectors#how-we-protect-your-data): Error processing content

- [Set up human evaluation | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-human-evaluators#add-evaluation-instructions): **One-line description:** This page provides instructions on setting up human evaluation within the Maxim platform for quality control and oversight of AI system outputs.

- [Overview | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/concepts#1-experiment): *   **One-line description:** This page provides an overview of Maxim, a platform designed to streamline AI application development and deployment through experimentation, evaluation, observability, and data management.

- [Zero Touch Deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/zerotouch#infrastructure-requirements): *   **One-line description:** This page describes Maxim's zero-touch deployment option, which provides maximum security and privacy by keeping all data within the customer's infrastructure.

- [Experiment in the Prompt playground | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/experiment-in-prompt-playground#selecting-a-model): *   **One-line description:** This page describes how to experiment with prompts using Maxim's playground, including selecting models, adding prompts, configuring parameters, and using variables.

- [Simulate multi-turn conversations | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/simulate-multi-turn-conversations#1-create-a-realistic-scenario-and-be-specific-about-the-situation-you-want-to-test): *   **One-line description:** This page explains how to simulate multi-turn conversations to test and evaluate AI agents.

- [Set up alerts for quality metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-quality-metrics): *   **One-line description:** This page guides users on setting up alerts in Maxim to monitor AI application quality using evaluation scores and quality checks.

- [Zero Touch Deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/zerotouch#services-we-deploy): Error processing content

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/careers#jobs): Error processing content

- [Overview | Maxim](https://www.getmaxim.ai/docs/observe/overview#key-features): Error processing content

- [Overview | Maxim](https://www.getmaxim.ai/docs/self-hosting/overview#security-measures): *   **One-line description:** This page describes Maxim's self-hosting and enterprise deployment options, including Zero Touch and Data Plane deployments, emphasizing security and infrastructure details.

- [Query Prompt Chains via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains#querying-prompts-chains): *   **One-line description:** This page explains how to use the Maxim SDK to query and retrieve prompt chains, folders and configure custom caching.

- [Test multi-turn conversations manually | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-multi-turn-conversations-manually#edit-and-modify-conversations): *   **One-line description:** This page explains how to manually test multi-turn conversations with an AI endpoint using Maxim's interactive Workflows, allowing for real-time simulation, manipulation, and debugging.

- [Set up auto evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/auto-evaluation#why-evaluate-logs): **One-line description:** This page explains how to set up and interpret automated evaluation of logs within the Maxim platform, focusing on evaluating LLM performance based on captured interactions.

- [Create a PagerDuty integration | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/create-a-pagerduty-integration): *   **One-line description:** This page describes how to integrate Maxim with PagerDuty to send alert notifications.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/evaluate/concepts#prompts): *   **One-line description:** This page introduces key concepts related to MaximPrompts, including prompts, prompt comparisons, prompt chains, workflows, test runs, evaluators, datasets, context sources, prompt tools, and simulation for evaluating AI model performance.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/observe/concepts#logs-tab): *   **One-line description:** This page explains the core concepts of Maxim's AI Observability platform, focusing on log repositories, traces, spans, events, generations, retrievals, and tool calls.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#available-types): *   **One-line description:** This page provides a guide on how to programmatically trigger AI application test runs using the Maxim SDK, covering data structures, data sources, output functions, evaluators, and advanced configurations.

- [Evaluate Datasets | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-datasets): *   **One-line description:** This page explains how to use Maxim's tools to evaluate AI model performance directly using datasets by comparing AI outputs against expected results.

- [Partha Sarathi Roy - Maxim Blog](https://www.getmaxim.ai/blog/author/parth): **One-line description:** This page is a blog featuring articles by Partha Sarathi Roy on various topics related to Large Language Models (LLMs), including RAG architectures, Chain-of-Thought prompting, hallucination detection, synthetic data generation, DSPy framework, and agent workflow memory.

- [Get test runs | Maxim](https://www.getmaxim.ai/docs/api/test-runs/get#authorization): *   **One-line description:** This page documents the API endpoint for retrieving test runs from a workspace using the Maxim AI platform.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/observe/concepts#log-repository): *   **One-line description:** This page explains the key concepts of Maxim's AI Observability platform, focusing on Log Repositories and their components (Sessions, Traces, Spans, Events, Generations, Retrievals, Tool Calls).

- [Concepts | Maxim](https://www.getmaxim.ai/docs/library/concepts#evaluator-reasoning): **One-line description:**

- [Running your first test | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/running-first-test#7-analyze-test-results): *   **One-line description:** This page is a quickstart guide for running your first test on the Maxim platform, covering environment setup, prompt/workflow creation, dataset preparation, evaluator addition, test execution, and result analysis.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/library/concepts#human-evaluators): **One-line description:** This page explains key concepts in AI evaluation, focusing on evaluators, datasets, and tools within the Maxim platform for assessing AI model performance.

- [Overview | Maxim](https://www.getmaxim.ai/docs/sdk/test-runs-via-sdk/js-ts#4-data-engine): *   **One-line description:** This page provides an overview of Maxim, a platform that streamlines AI application development and deployment with tools for experimentation, evaluation, observability, and data management.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#basic-structure): *   **One-line description:** This page describes how to trigger AI application test runs programmatically using Maxim's SDK, including data structure definition, data source options, custom output functions, evaluators, and advanced configurations.

- [Raising an incident | Maxim](https://www.getmaxim.ai/docs/support/raising-an-incident#how-to-report-an-issue): *   **One-line description:** This page provides a guide on how to report bugs or incidents on the Maxim platform.

- [Build complex AI workflows with Prompt Chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/experiment-with-prompt-chains): *   **One-line description:** This page introduces Prompt Chains, a feature for building complex AI workflows by connecting prompts, code, and APIs.

- [Overview | Maxim](https://www.getmaxim.ai/docs/api/overview#available-apis): *   **One-line description:** This page provides an overview and documentation for the Maxim API, including available APIs, authentication methods, and access to specific features.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai?utm_campaign=llm_as_jury&utm_medium=content&utm_source=blog): **One-line description:** Maxim is an end-to-end GenAI evaluation and observability platform designed to help teams build and deploy AI agents reliably and faster.

- [Data plane deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/dataplane#slas): *   **One-line description:** This page describes Maxim's data plane deployment process, which offers enhanced security and data residency by deploying data processing infrastructure within a customer's cloud environment.

- [Bring your existing Evaluators via API | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-api-evaluators): *   **One-line description:** This page explains how to integrate existing evaluation systems with Maxim using API endpoints to reuse custom evaluators without rebuilding them.

- [AI Red-Teaming: Automated Adversarial Testing](https://www.getmaxim.ai/blog/ai-red-teaming): *   **One-line description:** This webpage presents an automated AI red-teaming framework using multi-step reinforcement learning to generate diverse and effective adversarial attacks for testing the robustness of AI systems like LLMs.

- [Overview | Maxim](https://www.getmaxim.ai/docs/library/evaluators/concept#1-experiment): *   **One-line description:** This page provides an overview of Maxim, a platform that streamlines AI application development and deployment through evaluation, observability, and data management tools.

- [Quickstart | Maxim](https://www.getmaxim.ai/docs/observe/quickstart#system-architecture): *   **One-line description:** This page provides a quickstart guide to setting up distributed tracing for GenAI applications using Maxim's platform, focusing on monitoring performance and debugging issues through a sample enterprise search chatbot application.

- [Automate Prompt evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd#triggering-a-test-run): *   **One-line description:** This page explains how to automate prompt evaluation using CI/CD pipelines with Maxim's CLI tool and GitHub Action to ensure quality with every deployment.

- [Zero Touch Deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/zerotouch#slas): *   **One-line description:** This page describes Maxim's Zero Touch Deployment option, a secure deployment process ensuring data remains within the customer's infrastructure.

- [Upgrading to v3 | Maxim](https://www.getmaxim.ai/docs/sdk/upgrading-to-v3#import-changes): Here's the requested information about the webpage content:

- [Compare Prompts in the playground | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/compare-prompts-playground#create-a-new-comparison): *   **One-line description:** This page explains how to use the Prompt comparison feature in a playground environment to evaluate and optimize prompts by comparing their performance side-by-side.

- [Set up human evaluation | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-human-evaluators#create-evaluator): **One-line description:** This page provides a guide on setting up human evaluation for AI model outputs, including creating evaluators, defining instructions, and setting pass criteria.

- [Running your first test | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/running-first-test#prompt): *   **One-line description:** This page provides a step-by-step guide on how to run your first test in Maxim, covering environment setup, prompt creation, dataset preparation, evaluator setup, test execution, and results analysis.

- [Set up alerts for quality metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-quality-metrics#create-a-quality-alert): **One-line description:** This page provides a guide on setting up alerts in Maxim to monitor AI application quality metrics like bias, toxicity, clarity, and factual accuracy.

- [Create a Slack integration | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/create-a-slack-integration#editing-or-deleting-the-integration): *   **One-line description:** This page explains how to integrate Maxim with Slack to send alert notifications to specific Slack channels.

- [Create Programmatic Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-programmatic-evaluator#set-pass-criteria): *   **One-line description:** This page explains how to create custom code-based evaluators using Javascript or Python within the Maxim platform for specific evaluation needs.

- [Build complex AI workflows with Prompt Chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/experiment-with-prompt-chains#available-nodes): *   **One-line description:** This page introduces Prompt Chains, a tool for building complex AI workflows by connecting prompts, code, and APIs in a visual editor.

- [Create Prompt Partials | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-partials/create-prompt-partial#create-a-prompt-partial): **One-line description:** This page explains how to create and use prompt partials, which are reusable snippets of prompt text, in the Maxim AI platform.

- [Use API nodes within chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/use-api-nodes-within-chains#configuring-api-nodes): *   **One-line description:** This page explains how to use API nodes within prompt chains to integrate with external services using HTTP requests.

- [Set up automated email summaries to monitor your logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/summary-emails#manage-your-email-preferences): *   **One-line description:** This page explains how to set up and manage automated weekly email summaries to monitor log repository performance.

- [Run a Prompt with tool calls | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/run-prompt-tool-calls#measure-tool-call-accuracy-across-your-test-cases): *   **One-line description:** This page explains how to use Maxim's playground to test and ensure accurate tool call selection in prompts for building reliable AI workflows.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/evaluate/concepts#test-runs): **One-line description:** This page explains key concepts within the Maxim platform, focusing on prompts, prompt comparisons, chains, workflows, test runs, evaluators, datasets, context sources, prompt tools and simulations used in AI model evaluation and testing.

- [Create a code-based Prompt Tool | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-tools/create-a-code-tool#example-travel-price-calculator): *   **One-line description:** This page provides a guide on how to create and test code-based Prompt Tools within the Maxim platform, including an example of a travel price calculator.

- [Measure the quality of your RAG pipeline | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/rag-quality#fetch-retrieved-context-while-running-prompts): *   **One-line description:** This page explains how to measure the quality of your Retrieval-Augmented Generation (RAG) pipeline using Maxim by fetching retrieved context in prompts and evaluating retrieval at scale with specific metrics.

- [Set up human evaluation | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-human-evaluators#set-pass-criteria): **One-line description:** This page provides a guide on how to set up human evaluation for AI outputs, focusing on quality control and oversight.

- [Set up automated email summaries to monitor your logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/summary-emails#email-content-overview): *   **One-line description:** This page explains how to set up and manage automated weekly email summaries for log repositories to monitor performance, track feedback, and identify potential issues.

- [Setting up alerts for performance metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-performance-metrics#best-practices): *   **One-line description:** This page explains how to set up alerts for performance metrics like latency, token usage, and cost in the Maxim application.

- [Customize and share reports | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/optimize-evaluation-processes/customize-share-reports): *   **One-line description:** This page describes how to customize, filter, and share evaluation reports in the Maxim AI observability platform.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/observe/concepts#overview-tab): **One-line description:** This page describes the key concepts of Maxim's AI Observability platform, focusing on the Log Repository and its components for monitoring AI applications.

- [Set up auto evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/auto-evaluation#overview-tab): *   **One-line description:** This page explains how to set up and understand automated log evaluation in Maxim, focusing on evaluating LLM performance in live systems.

- [Zero Touch Deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/zerotouch#release-cadence): *   **One-line description:** This page outlines Maxim's Zero Touch Deployment option, focusing on infrastructure requirements, security protocols, and the deployment process for organizations requiring the highest level of data security and privacy by keeping their data within their own infrastructure.

- [Agent Workflow Memory: Enhancing AI with Long-Horizon Tasks](https://www.getmaxim.ai/blog/agent-workflow-memory): **One-line description:** This page introduces "Agent Workflow Memory (AWM)," a new approach to enhance AI agents' ability to perform complex, long-horizon tasks by equipping them with a memory mechanism.

- [Track errors in traces | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/track-llm-errors): *   **One-line description:** This page explains how to track and log errors from LLM results and tool calls within AI application traces using Maxim.

- [Create Programmatic Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-programmatic-evaluator#write-evaluation-logic): **One-line description:** This page explains how to create custom code-based evaluators using Javascript or Python within the Maxim platform.

- [Build an AI-powered customer support email agent | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/create-customer-support-agent): *   **One-line description:** This page provides a guide on building an AI-powered customer support email agent using Prompt Chains, covering email classification, priority scoring, help desk ticket creation, personalized response generation, and sending email responses.

- [Overview | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/concepts#2-evaluate): *   **One-line description:** Maxim is a platform that streamlines AI application development and deployment by providing tools for experimentation, evaluation, observation, and data management.

- [Delete test runs | Maxim](https://www.getmaxim.ai/docs/api/test-runs/delete#authorization): *   **One-line description:** This page documents the API endpoint for deleting test runs in a Maxim workspace.

- [Evaluate Datasets | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-datasets#configure-the-test-run): **One-line description:** This page provides instructions on how to evaluate AI outputs using Maxim's dataset evaluation tools.

- [Test your agentic workflows using chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/test-prompt-chains): *   **One-line description:** This page explains how to evaluate and test Prompt Chains using datasets and evaluators to ensure consistent performance before deployment.

- [Automate workflow evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/automate-via-ci-cd#triggering-a-test-run): *   **One-line description:** This page explains how to automate workflow evaluation using Maxim's CI/CD integration, including CLI commands and GitHub Actions.

- [Compare Prompt versions | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/compare-prompt-versions#review-differences): *   **One-line description:** This page explains how to use Maxim to compare different versions of your prompts to track changes and understand their impact on quality.

- [Query Prompt Chains via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains#initializing-the-sdk): *   **One-line description:** This page provides instructions and code examples for querying and retrieving prompt chains using the Maxim SDK in JavaScript/TypeScript and Python, including filtering by deployment variables, folders, and tags, as well as customizing the caching mechanism and overriding the fallback matching algorithm.

- [Use Events to send point-in-time information | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/use-events): **One-line description:** This page explains how to use events in application traces to track milestones, state changes, and other point-in-time information using Maxim.

- [Track errors in traces | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/track-llm-errors#track-errors-from-tool-calls): *   **One-line description:** This page explains how to track and log errors from LLM results and tool calls within AI application traces using Maxim.

- [Set up custom token pricing | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/use-custom-pricing#configure-model-pricing): *   **One-line description:** This page explains how to set up custom token pricing in Maxim to ensure accurate cost reporting for AI evaluations and logs, reflecting negotiated rates for different models and log repositories.

- [Setting up alerts for performance metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-performance-metrics#setting-up-token-consumption-alerts): **One-line description:** This page guides users on how to set up alerts for performance metrics like latency, token usage, and cost in Maxim.

- [Organize Prompts | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/organize-prompts#organize-using-folders): *   **One-line description:** This page explains how to organize and manage prompts within the Maxim AI platform using folders, tags, versions, and SDK queries for collaborative AI application development.

- [Set up custom token pricing | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/use-custom-pricing#create-custom-pricing-structures): *   **One-line description:** This page explains how to set up custom token pricing in Maxim to accurately track and report AI model costs based on negotiated rates.

- [Configure filters and saved views | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/filters-and-saved-views): **One-line description:** This page explains how to configure filters and saved views in Maxim to efficiently manage and analyze logs, as well as track token usage and set custom token pricing for cost management.

- [Create Programmatic Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-programmatic-evaluator#test-your-evaluator): **One-line description:** This page guides users on how to create custom, code-based evaluators in Maxim using JavaScript or Python for specific evaluation needs.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai?utm_campaign=blog_post&utm_content=RAG_overview&utm_medium=social&utm_source=content): **One-line description:** This page introduces Maxim, an AI evaluation and observability platform that helps teams develop and deploy AI agents faster and more reliably.

- [Create a code-based Prompt Tool | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-tools/create-a-code-tool#code-editor-interface): *   **One-line description:** This page provides a guide on creating and testing code-based prompt tools within the Maxim platform.

- [Build complex AI workflows with Prompt Chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/experiment-with-prompt-chains#optimize-connections): *   **One-line description:** This page explains Prompt Chains, a visual tool for building complex AI workflows by connecting prompts, code, and APIs.

- [Add Dataset entries using SDK | Maxim](https://www.getmaxim.ai/docs/library/how-to/datasets/add-new-entries-using-sdk): *   **One-line description:** This page explains how to programmatically add new entries to a dataset using the Maxim SDK.

- [Create custom AI Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-custom-ai-evaluator#set-pass-criteria): *   **One-line description:** This page explains how to create custom AI evaluators in Maxim for evaluating AI model outputs using LLMs, custom instructions, and scoring logic.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/observe/concepts#components-of-a-log): **One-line description:** This page explains the key concepts of Maxim's AI Observability platform, focusing on the Log Repository and its components.

- [Query Prompt Chains via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains#using-your-own-cache-for-prompts): *   **One-line description:** This page explains how to use the Maxim SDK to query and retrieve prompt chains, including filtering by deployment variables, folder, and tags, as well as how to customize caching and override the fallback matching algorithm.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#concurrency-control): *   **One-line description:** This page provides a guide on how to programmatically trigger test runs using the Maxim SDK for AI applications, covering data structures, data sources, output functions, evaluators, and advanced configurations.

- [Automate Prompt evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd#inputs): *   **One-line description:** This page explains how to automate prompt evaluation using CI/CD pipelines with Maxim's CLI tool or GitHub Action to ensure quality with every deployment.

- [Connect your logs to external platforms | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/use-data-connectors#connect-to-new-relic): *   **One-line description:** This page explains how to connect Maxim logs to external observability platforms like New Relic, Snowflake, and any OpenTelemetry (OTLP) collector.

- [Bring your existing Evaluators via API | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-api-evaluators#test-your-evaluator): *   **One-line description:** This page explains how to integrate existing evaluation systems into Maxim using an API to reuse evaluators without rebuilding them.

- [Data plane deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/dataplane#deployment-process): *   **One-line description:** This page describes Maxim's Data Plane Deployment, which enables secure data processing within the customer's cloud environment while leveraging Maxim's cloud-hosted services.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/evaluate/concepts#prompt-tools): **One-line description:** This page explains the key concepts within MaximPrompts, including prompts, prompt comparisons, prompt chains, workflows, test runs, evaluators, datasets, context sources, prompt tools and simulations.

- [Connect your logs to external platforms | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/use-data-connectors#connect-to-an-otlp-collector): *   **One-line description:** This page explains how to connect Maxim logs to external platforms like New Relic, Snowflake, and OpenTelemetry (OTLP) collectors for enhanced analysis and monitoring.

- [Customize and share reports | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/optimize-evaluation-processes/customize-share-reports#share-links): *   **One-line description:** This page explains how to customize, filter, and share reports generated by the Maxim AI observability platform for understanding AI system performance during experiments or pre-release testing.

- [Set up human evaluation | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-human-evaluators#select-evaluation-type): **One-line description:** This page provides a guide on setting up human evaluation for AI outputs, including creating evaluators, providing instructions, selecting evaluation types, and setting pass criteria.

- [Raising an incident | Maxim](https://www.getmaxim.ai/docs/support/raising-an-incident#description-optional): *   **One-line description:** This page provides a guide on how to report bugs or incidents on the Maxim platform, outlining the steps to submit a report and what to expect after submission.

- [Node level evaluation | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/node-level-evaluation#code-example-for-agentic-evaluation): *   **One-line description:** This page explains how to use Node level evaluation with the Maxim SDK to evaluate specific components of AI application traces or logs, enabling granular insight into agent behavior.

- [Compare Prompt versions | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/compare-prompt-versions#choose-versions): **One-line description:** This page explains how to compare different versions of prompts within the Maxim platform to track changes and analyze their impact on quality.

- [Automate workflow evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/automate-via-ci-cd#pre-requisites): *   **One-line description:** This page explains how to automate workflow evaluation using Maxim's CI/CD integration via CLI and GitHub Actions.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observe/overview#improve-your-ai-application-outcomes): *   **One-line description:** This page introduces Maxim, an enterprise-grade LLM observability platform designed to monitor AI applications in real-time and improve their performance.

- [Connect your logs to external platforms | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/use-data-connectors#connect-to-snowflake): *   **One-line description:** This page explains how to connect Maxim logs to external observability platforms like New Relic, Snowflake, and OTLP collectors for enhanced log analysis and monitoring.

- [Introduction | Maxim](https://www.getmaxim.ai/docs/sdk/overview#language-and-framework-support): **One-line description:** This page introduces the Maxim SDK, a tool for streamlining AI application evaluation and observability, and provides instructions for installation and usage in various programming languages.

- [Evaluate simulated sessions for agents | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/evaluate-simulated-sessions-for-agents#create-a-dataset-for-testing): *   **One-line description:** This page explains how to evaluate the performance of AI agents using simulated multi-turn conversations and automated testing.

- [Overview | Maxim](https://www.getmaxim.ai/docs/library/evaluators/concept#4-data-engine): *   **One-line description:** Maxim is an AI application development and deployment platform that streamlines workflows with tools for experimentation, evaluation, observability, and data management.

- [Automate Prompt evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd#supported-os--arch): *   **One-line description:** This page explains how to automate prompt evaluation in CI/CD pipelines using Maxim, ensuring quality with every deployment through CLI tools or GitHub Actions.

- [Overview | Maxim](https://www.getmaxim.ai/docs/analyze/overview): *   **One-line description:** This page provides an overview of Maxim's data analysis tools, including comparison reports and upcoming live dashboards, for gaining insights from AI application data.

- [Query Prompt Chains via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains#get-all-deployed-prompts-from-a-folder): *   **One-line description:** This page provides a guide on how to query and retrieve prompt chains using the Maxim SDK in JavaScript/TypeScript and Python, including filtering by deployment variables, folders, and tags, and customizing cache implementation.

- [Create Prompt Partials | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-partials/create-prompt-partial#add-a-new-partial): *   **One-line description:** This page explains how to create and manage reusable prompt partials within a prompt engineering platform.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#2-manual-data-array): *   **One-line description:** This page explains how to trigger and configure AI application test runs programmatically using the Maxim SDK, including data structures, data sources, custom output functions, evaluators, and advanced configurations.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#custom-evaluators): *   **One-line description:** This page explains how to trigger and configure AI model test runs programmatically using Maxim's SDK, covering data structures, data sources, output functions, evaluators, and advanced options like concurrency and timeouts.

- [Create a Slack integration | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/create-a-slack-integration#creating-the-integration-in-maxim): **One-line description:**

- [Setting up your workspace | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/setting-up-workspace#create-maxim-api-key): **One-line description:** This page provides a quickstart guide on setting up your workspace in Maxim, including adding model API keys, creating Maxim API keys, inviting team members, and creating roles.

- [Upgrading to v3 | Maxim](https://www.getmaxim.ai/docs/sdk/upgrading-to-v3#prompt-management-changes): **One-line description:**

- [Log LLM generations in your AI application traces | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/adding-llm-call): *   **One-line description:** This page provides instructions on how to log LLM (Large Language Model) generations within AI application traces, including recording requests, responses, and integrating with spans and RAG pipelines.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/library/concepts#data-curation): **One-line description:** This page explains key concepts in AI evaluation within the Maxim platform, focusing on evaluators, datasets, and custom tools.

- [A Survey of Agent Evaluation Frameworks: Benchmarking the Benchmarks](https://www.getmaxim.ai/blog/llm-agent-evaluation-framework-comparison): *   **One-line description:** This page is a blog post summarizing a research paper that surveys and benchmarks various frameworks used for evaluating AI agents, highlighting their strengths, limitations, and future directions.

- [Maxim Blog (Page 3)](https://www.getmaxim.ai/blog/page/3): *   **One-line description:** This page is the third page of the Maxim AI blog, featuring articles and updates on AI agent development, monitoring, and responsible AI practices.

- [Raising an incident | Maxim](https://www.getmaxim.ai/docs/support/raising-an-incident#fill-out-the-report-form): *   **One-line description:** This page provides a guide on how to report bugs or issues encountered while using the Maxim platform.

- [DSPy: Self-Improving Framework for LLM Pipelines](https://www.getmaxim.ai/blog/dspy-framework): **One-line description:** This webpage introduces DSPy, a self-improving framework for simplifying and optimizing LLM pipelines.

- [Transform API data with Workflow scripts | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/scripting-to-configure-response-structures#pre-request-script): *   **One-line description:** This page explains how to use Workflow scripts in Maxim to transform API data by modifying requests and responses, and how to use simulation scripts for multi-turn conversations.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#using-custom-evaluators): *   **One-line description:** This page explains how to trigger test runs for AI applications programmatically using Maxim's SDK, covering data structure definition, various data sources, custom output functions, evaluators, and advanced configurations.

- [Build complex AI workflows with Prompt Chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/experiment-with-prompt-chains#version-and-deploy): *   **One-line description:** This page introduces Prompt Chains, a visual tool for building complex AI workflows by connecting prompts, code, and APIs.

- [Quickstart | Maxim](https://www.getmaxim.ai/docs/observe/quickstart#6-add-spans-in-services): *   **One-line description:** This page provides a quickstart guide on setting up distributed tracing for GenAI applications using Maxim, focusing on monitoring performance and debugging issues in a chatbot example.

- [Build a RAG application using MongoDB and Maxim AI](https://www.getmaxim.ai/blog/build-rag-app-mongodb-maxim): *   **One-line description:** This page provides a guide on building a Retrieval-Augmented Generation (RAG) application using MongoDB as a vector database and Maxim AI for logging and monitoring.

- [Set up a human annotation pipeline | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline): **One-line description:** This page provides a guide on setting up a human annotation pipeline within the Maxim platform to improve AI quality through human feedback and evaluation.

- [Overview | Maxim](https://www.getmaxim.ai/docs/self-hosting/overview#components): *   **One-line description:** This page provides an overview of Maxim's self-hosting and enterprise deployment options, including Zero Touch and Data Plane deployments, infrastructure components, and security measures.

- [Test your AI application using an API endpoint | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-your-ai-outputs-using-application-endpoint#create-a-workflow-for-a-public-endpoint): *   **One-line description:** This page explains how to test your AI application by connecting it to Maxim using an existing API endpoint and evaluating the responses.

- [](https://www.getmaxim.ai/docs): You did not provide any webpage content. Please provide the content so I can give you a one-line description and a concise summary.

- [Overview | Maxim](https://www.getmaxim.ai/docs/sdk/test-runs-via-sdk/js-ts#3-observe): *   **One-line description:** This page provides an overview of Maxim, a platform for streamlining AI application development and deployment through tools for experimentation, evaluation, observation, and data management.

- [Use splits within a Dataset | Maxim](https://www.getmaxim.ai/docs/library/how-to/datasets/use-splits-within-a-dataset): **One-line description:**

- [Receive notifications for Test Run status | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/optimize-evaluation-processes/receive-notifications-test-runs): **One-line description:** This page explains how to set up notifications for test run statuses using Slack and PagerDuty, reuse test configurations with presets, and customize and share reports.

- [Compare Prompts in the playground | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/compare-prompts-playground#why-use-prompt-comparison): *   **One-line description:** This page explains how to use the prompt comparison feature in a playground environment to evaluate and optimize prompts by comparing their performance across different models and inputs.

- [Running your first test | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/running-first-test#5-add-evaluators): *   **One-line description:** This page provides a quickstart guide on running your first test in Maxim, covering environment setup, prompt or workflow creation, dataset preparation, evaluator addition, test execution, and results analysis.

- [Running your first test | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/running-first-test#workflow): *   **One-line description:** This page guides users through running their first test on the Maxim platform, covering environment setup, prompt/workflow creation, dataset preparation, evaluator setup, test execution, and results analysis.

- [Zero Touch Deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/zerotouch#security-measures): *   **One-line description:** This page outlines Maxim's Zero Touch Deployment process, focusing on infrastructure, security, and support for organizations requiring the highest data privacy.

- [Set up human evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/human-evaluation#annotating-the-logs): *   **One-line description:** This page explains how to set up and use human evaluation to assess the quality of logs within the Maxim application.

- [Raising an incident | Maxim](https://www.getmaxim.ai/docs/support/raising-an-incident#navigate-to-settings): *   **One-line description:** This page provides a guide on how to report bugs and incidents on the Maxim platform.

- [Mindtickle x Maxim- Ensuring AI Quality in Production](https://www.getmaxim.ai/blog/mindtickle-ai-quality-evaluation-using-maxim): *   **One-line description:** This page discusses how Mindtickle partnered with Maxim to improve their AI quality in production, leading to increased productivity and faster time to market.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai?utm_campaign=blog_post&utm_content=minicheck&utm_medium=social&utm_source=content): *   **One-line description:** Maxim is a GenAI evaluation and observability platform that helps teams ship AI agents reliably and faster.

- [Create and manage Prompt versions | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/create-prompt-versions): *   **One-line description:** This page describes how to create, manage, and compare prompt versions in Maxim for AI application development, enabling collaboration and organized changes.

- [Understanding AI Agents and Evaluating their Quality](https://www.getmaxim.ai/blog/ai-agent-quality-evaluation): *   **One-line description:** This webpage is an article discussing the nature of AI agents, their types, real-world applications, and the importance of evaluating their quality.

- [Automate workflow evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/automate-via-ci-cd#evaluating-prompts): *   **One-line description:** This page describes how to automate workflow evaluation using Maxim's CI/CD tools, including CLI commands and GitHub Actions.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/jobs/platform-engineer): **One-line description:** This page is a job posting for a Platform Engineer at Maxim, an AI evaluation and observability platform company, located in Bangalore, India.

- [Setting up alerts for performance metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-performance-metrics#monitor-response-time): *   **One-line description:** This page explains how to set up alerts in Maxim for monitoring application performance metrics like latency, token usage, and cost, and provides guidance on configuration and best practices.

- [Zero Touch Deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/zerotouch#support): **One-line description:** This page describes Maxim's zero-touch deployment process, which allows organizations to host data and applications within their own infrastructure for maximum security and privacy.

- [Create a Dataset with images | Maxim](https://www.getmaxim.ai/docs/library/how-to/datasets/create-dataset-with-files-and-images): *   **One-line description:** This page provides instructions on how to create a dataset with images using the Maxim platform.

- [Evaluate simulated sessions for agents | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/evaluate-simulated-sessions-for-agents#review-results): *   **One-line description:** This page provides a guide on evaluating the performance of AI agents using simulated conversations and automated testing.

- [Set up automated email summaries to monitor your logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/summary-emails#benefits-of-summary-emails): *   **One-line description:** This page explains how to set up and manage automated weekly email summaries to monitor log repository performance, track user feedback, and identify potential issues.

- [Transform API data with Workflow scripts | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/scripting-to-configure-response-structures#simulation-scripts): **One-line description:** This page explains how to use Workflow scripts to transform API data within the Maxim platform, including pre-request, post-response, and simulation scripts.

- [Test your agentic workflows using chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/test-prompt-chains#review-results): *   **One-line description:** This page explains how to test and evaluate prompt chains, particularly for generating and translating product descriptions, using datasets and evaluators on the Maxim platform.

- [Set up auto evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/auto-evaluation#tree-view-on-the-left-panel): *   **One-line description:** This page explains how to set up and understand auto-evaluation of logs for LLMs, enabling continuous monitoring of LLM performance in live systems.

- [Overview | Maxim](https://www.getmaxim.ai/docs/library/overview#context-sources): *   **One-line description:** This page provides an overview of the supporting components within Maxim, specifically focusing on tools for AI testing such as Evaluators, Datasets, Context Sources, Prompt Tools, and Custom Models.

- [Deploy Prompts | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/deploy-prompts): **One-line description:** This page explains how to deploy prompts using Maxim, allowing for quick iterations, conditional deployments, and organized prompt management.

- [Create a PagerDuty integration | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/create-a-pagerduty-integration#prerequisites): *   **One-line description:** This page guides users on how to integrate Maxim with PagerDuty to send alert notifications.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#trigger-a-test-on-a-prompt-version-stored-on-maxim-platform): *   **One-line description:** This page explains how to use the Maxim SDK to programmatically trigger and configure test runs for AI applications, including custom datasets, output functions, and evaluators.

- [Capture your RAG pipeline | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/logging-rag-pipeline): *   **One-line description:** This page explains how to log Retrieval-Augmented Generation (RAG) pipelines to capture and monitor the retrieval stage, including example code snippets.

- [Automate workflow evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/automate-via-ci-cd#test-runs-via-cli): *   **One-line description:** This page explains how to automate the evaluation of workflows in CI/CD pipelines using the Maxim platform, utilizing both CLI and GitHub Actions.

- [Use spans to group units of work | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/add-spans-to-traces): *   **One-line description:** This page explains how to use spans to organize and track requests across microservices within traces using the Maxim platform.

- [Scheduled test runs | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/scheduled-test-runs#select-evaluators): **One-line description:** This page describes how to schedule test runs for prompts, prompt chains, and workflows using Maxim, including selecting the version, schedule, and evaluators.

- [Save and track Prompt experiments with sessions | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/save-prompt-session): *   **One-line description:** This page describes how to use sessions within a prompt playground to save, track, and organize prompt experiments.

- [Building Robust AI Agent Evaluation Workflows](https://www.getmaxim.ai/blog/evaluation-workflows-for-ai-agents): *   **One-line description:** This page discusses best practices for building robust AI agent evaluation workflows, covering both pre-release and post-release phases for continuous refinement and improved user experiences.

- [Create custom AI Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-custom-ai-evaluator#create-new-evaluator): *   **One-line description:** This page provides a guide on creating custom AI evaluators within the Maxim platform for evaluating AI model outputs, allowing users to define evaluation logic, set pass criteria, and test their evaluators.

- [Use API nodes within chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/use-api-nodes-within-chains#create-an-api-node): *   **One-line description:** This page explains how to use API nodes within Prompt Chains to integrate with external services in Maxim.

- [Build complex AI workflows with Prompt Chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/experiment-with-prompt-chains#run-a-test): *   **One-line description:** This page introduces Prompt Chains, a tool for building complex AI workflows by connecting prompts, code, and APIs.

- [Overview | Maxim](https://www.getmaxim.ai/docs/self-hosting/overview#zero-touch-deployment): *   **One-line description:** This page provides an overview of Maxim's self-hosting and enterprise deployment options, including Zero Touch and Data Plane deployments, infrastructure components, and security measures.

- [Data plane deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/dataplane#setup-requirements): *   **One-line description:** This page describes Maxim's data plane deployment process, emphasizing security, control, and data residency within the user's cloud environment.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observe/overview#2-zero-state-sdk-architecture): *   **One-line description:** This page provides an overview of Maxim's LLM observability platform, which helps monitor, debug, and improve AI applications in real-time.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/library/concepts#splits): **One-line description:**

- [Re-use your test configurations using presets | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/optimize-evaluation-processes/re-use-configuration-via-presets): *   **One-line description:** This page explains how to use presets to reuse test configurations, trigger test runs using SDK, and receive notifications for test run status within the Maxim platform.

- [Build complex AI workflows with Prompt Chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/experiment-with-prompt-chains#what-are-prompt-chains): *   **One-line description:** This page explains Prompt Chains, a visual tool for building complex AI workflows by connecting prompts, code, and APIs.

- [Automate workflow evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/automate-via-ci-cd#test-runs-via-github-action): *   **One-line description:** This page explains how to automate workflow evaluation using CI/CD pipelines with the Maxim platform, covering CLI and GitHub Actions methods.

- [Agent simulation, Ollama, 2FA, Sonnet 3.7 - Maxim's Feb updates](https://www.getmaxim.ai/blog/maxim-february-2025-update): *   **One-line description:** This webpage is a blog post from Maxim AI detailing their February 2025 updates, including agent simulation, 2FA, Ollama model support, and Claude 3.7 Sonnet integration.

- [LLM Jailbreaking: Threats & Mitigation Strategies](https://www.getmaxim.ai/blog/jailbreaking-prompt-injection): **One-line description:** This page discusses the threats of LLM jailbreaking and prompt injection, along with mitigation strategies.

- [Automate Prompt evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd#before-we-start): *   **One-line description:** This page explains how to automate prompt evaluation in CI/CD pipelines using Maxim's platform, covering CLI and GitHub Actions integration.

- [Vaibhavi Gangwar - Maxim Blog](https://www.getmaxim.ai/blog/author/vaibhavi): **One-line description:** This is the blog page of Vaibhavi Gangwar, featuring articles about AI agents, evaluation workflows, and related topics, published on the Maxim platform.

- [Debug AI agent errors step by step | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/debug-errors-at-every-node): *   **One-line description:** This page describes how to debug errors in Prompt Chains, a visual tool for building AI workflows.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observe/overview#maxims-solution): *   **One-line description:** This page introduces Maxim's AI observability platform for monitoring and improving the performance of AI applications in real-time.

- [Node level evaluation | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/node-level-evaluation#viewing-evaluation-results-on-evaluations-tab): *   **One-line description:** This page explains how to use Node level evaluation within the Maxim platform to evaluate specific components of AI application traces and logs using the Maxim SDK.

- [Setting up alerts for performance metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-performance-metrics#monitor-daily-costs): *   **One-line description:** This page explains how to set up alerts in Maxim to monitor application performance metrics like latency, token usage, and cost, including configuration examples and management tips.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/observe/concepts#session): *   **One-line description:** This page explains the key concepts and components of Maxim's AI observability platform, focusing on Log Repositories and their contents (Sessions, Traces, Spans, Events, Generations, Retrievals, and Tool Calls).

- [Utsav Khandelwal - Maxim Blog](https://www.getmaxim.ai/blog/author/utsav): *   **One-line description:** This webpage is a blog by Utsav Khandelwal featuring updates and articles about Maxim AI.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/observe/concepts#trace): *   **One-line description:** This page explains the key concepts behind Maxim's AI Observability platform, focusing on the Log Repository and its components for monitoring and troubleshooting AI applications.

- [Scheduled test runs | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/scheduled-test-runs#decide-schedule-of-the-run): *   **One-line description:** This page explains how to schedule automated test runs for prompts, prompt chains, and workflows within the Maxim platform.

- [Set up alerts for quality metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-quality-metrics#manage-quality-alerts): *   **One-line description:** This page provides instructions on setting up alerts for quality metrics within the Maxim platform to monitor AI application responses.

- [Query Prompts via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts): *   **One-line description:** This page explains how to query and manage prompts within the Maxim AI platform using the SDK, focusing on deployment variables, tags, folders, caching, and matching algorithms.

- [Experiment in the Prompt playground | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/experiment-in-prompt-playground#using-variables): *   **One-line description:** This page explains how to experiment with prompts in Maxim's playground, including selecting models, adding prompts, configuring parameters, using variables, and integrating tools for building and testing AI workflows.

- [Astute RAG: Enhancing LLMs with Advanced Retrieval Techniques](https://www.getmaxim.ai/blog/advanced-rag-techniques): *   **One-line description:** This webpage introduces Astute RAG, an advanced retrieval-augmented generation technique designed to improve LLM performance by mitigating the problems of imperfect information retrieval.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observe/overview#online-evaluation): *   **One-line description:** This page introduces Maxim's LLM observability platform, which helps monitor and improve AI applications in real-time.

- [Create a Dataset using templates | Maxim](https://www.getmaxim.ai/docs/library/how-to/datasets/use-dataset-templates#agent-simulation): **One-line description:** This page explains how to create datasets for AI model testing and evaluation using pre-defined templates in a platform.

- [Enhance LLMs with Source2Synth for Better Synthetic Data](https://www.getmaxim.ai/blog/synthetic-data-generation): *   **One-line description:** This page discusses Source2Synth, a framework for generating high-quality synthetic data grounded in real-world sources to improve the performance of LLMs on complex tasks.

- [Node level evaluation | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/node-level-evaluation#what-is-node-level-evaluation-or-agentic-evaluation): *   **One-line description:** This page explains how to use Maxim's Node level evaluation feature to evaluate specific components of AI agent traces or logs for granular insights into agent performance.

- [Create realistic multi-turn training data for AI Agents with APIGen-MT](https://www.getmaxim.ai/blog/apigen-mt-structured-multi-turn-training-data-for-agents): *   **One-line description:** This webpage introduces APIGen-MT, a framework for generating realistic, verifiable multi-turn training data for AI agents through simulation, enabling better performance in real-world, interactive scenarios.

- [Raising an incident | Maxim](https://www.getmaxim.ai/docs/support/raising-an-incident#priority-level): *   **One-line description:** This page provides a guide on how to report issues and bugs on the Maxim platform.

- [Data plane deployment | Maxim](https://www.getmaxim.ai/docs/self-hosting/dataplane#release-cadence): *   **One-line description:** This page describes Maxim's data plane deployment process, which allows organizations to process data within their own infrastructure while leveraging Maxim's cloud-hosted application services.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/terms-of-service): *   **One-line description:** This page outlines the terms of service for Maxim, a GenAI evaluation and observability platform.

- [Experiment in the Prompt playground | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/experiment-in-prompt-playground#adding-system-and-user-prompts): *   **One-line description:** This page describes how to use the Maxim Prompt playground to create, refine, experiment with, and deploy prompts by selecting models, adding system/user prompts, configuring parameters, using variables, and integrating tools.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/library/concepts#multimodal-datasets): **One-line description:** This page explains key concepts in AI evaluation within the Maxim platform, focusing on evaluators, datasets, and custom tools for assessing model performance.

- [Setting up your first trace | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/setting-up-trace): *   **One-line description:** This page guides users on setting up their first trace using the Maxim platform to monitor and evaluate the performance of their AI applications.

- [Ingest files as a context source | Maxim](https://www.getmaxim.ai/docs/library/how-to/context-sources/ingest-files-as-a-context-source): *   **One-line description:** This page describes how to ingest files of various formats into Maxim to create context sources for use in Retrieval Augmented Generation (RAG) applications.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai?utm_campaign=blog_post&utm_content=chain-of-thought-prompting&utm_medium=social&utm_source=content): *   **One-line description:** This page promotes Maxim, an AI evaluation and observability platform designed to help teams build and deploy AI agents reliably and quickly.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/evaluate/concepts#prompt-comparisons): **One-line description:** This page explains the key concepts within the Maxim platform, focusing on prompts, prompt comparisons, prompt chains, workflows, test runs, evaluators, datasets, context sources, prompt tools and simulations for AI application evaluation.

- [Query Prompt Chains via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains#first-capture-folder-id): *   **One-line description:** This page explains how to use the Maxim SDK to efficiently query and retrieve prompt chains, folders, and configure caching for AI workflows.

- [Social Updates](https://www.getmaxim.ai/blog/maxim-social-updates): **One-line description:** This page provides social updates about Maxim AI, highlighting partnerships, platform listings, and product launches.

- [Evaluate Datasets | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-datasets#prepare-your-dataset): *   **One-line description:** This page explains how to use Maxim to evaluate AI model outputs against expected results using datasets.

- [Introduction | Maxim](https://www.getmaxim.ai/docs/sdk/overview#initializing-sdk): **One-line description:** This page introduces the Maxim SDK, a tool designed to streamline AI application development, evaluation, and observability.

- [Overview | Maxim](https://www.getmaxim.ai/docs/sdk/test-runs-via-sdk/js-ts#2-evaluate): *   **One-line description:** This page introduces Maxim, a platform designed to streamline AI application development and deployment through experimentation, evaluation, observability, and data management.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observe/overview#saved-views): *   **One-line description:** This page provides an overview of Maxim's LLM observability platform, designed to monitor AI applications in real-time and improve their performance.

- [Generate and share comparison reports | Maxim](https://www.getmaxim.ai/docs/analyze/how-to/comparison-reports): **One-line description:** This page explains how to generate, understand, update, and share comparison reports in Maxim for analyzing AI application data.

- [Build complex AI workflows with Prompt Chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/experiment-with-prompt-chains#create-your-first-chain): *   **One-line description:** This page introduces Prompt Chains, a visual tool for building complex AI workflows by connecting prompts, code, and APIs.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/jobs/developer-relations-engineer): *   **One-line description:** This page is a job posting for a Founding Developer Relations Engineer at Maxim, an AI application evaluation and observability platform.

- [Overview | Maxim](https://www.getmaxim.ai/docs/library/overview#datasets): *   **One-line description:** This page provides an overview of the testing components available in Maxim for ensuring high-quality AI applications, including evaluators, datasets, context sources, prompt tools, and custom models.

- [Overview | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/concepts#3-observe): *   **One-line description:** Maxim provides a platform for streamlining AI application development and deployment by offering tools for experimentation, evaluation, observation, and data management.

- [Overview | Maxim](https://www.getmaxim.ai/docs/library/evaluators/concept#3-observe): *   **One-line description:** This page provides an overview of Maxim, a platform that streamlines AI application development and deployment through experiment management, evaluation, observability, and data management tools.

- [Query Prompts via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts#querying-prompts): *   **One-line description:** This page explains how to query and retrieve prompts from Maxim AI using their SDK, focusing on deployment variables, tags, and folder-based organization, along with caching and fallback algorithm customization.

- [Create Programmatic Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-programmatic-evaluator#create-evaluator): *   **One-line description:** This page explains how to create custom evaluators using Javascript or Python within the Maxim platform.

- [Advanced Hallucination Detection in LLMs: Meet FAVA](https://www.getmaxim.ai/blog/llm-hallucination-detection): *   **One-line description:** This page introduces FAVA, a new technique for fine-grained hallucination detection in Large Language Models (LLMs), which categorizes and corrects specific types of factual inaccuracies.

- [Use API nodes within chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/use-api-nodes-within-chains#edit-api-node): *   **One-line description:** This page explains how to use API nodes within Prompt Chains to integrate with external services by configuring HTTP requests and scripts within the Maxim platform.

- [Set up custom token pricing | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/use-custom-pricing#how-it-works): *   **One-line description:** This page explains how to set up custom token pricing in Maxim for accurate cost reporting in AI evaluations and logs.

- [Set up a human annotation pipeline | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline#collect-ratings-via-test-report-columns): **One-line description:** This page describes how to set up and use a human annotation pipeline within the Maxim platform for evaluating and improving AI outputs.

- [Overview | Maxim](https://www.getmaxim.ai/docs/analyze/overview#generate-comparison-reports): *   **One-line description:** This page provides an overview of the analysis tools available in Maxim for gaining insights from AI application data, including comparison reports and upcoming live dashboards.

- [Set up human evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/human-evaluation#the-trace-details-sheet-under-the-evaluation-tab): *   **One-line description:** This page explains how to set up and use human evaluation for assessing the quality of logs within the Maxim platform.

- [Quickstart | Maxim](https://www.getmaxim.ai/docs/observe/quickstart#2-generate-api-key): *   **One-line description:** This page provides a quickstart guide on setting up distributed tracing for GenAI applications using the Maxim dashboard, focusing on monitoring performance and debugging issues in a microservice-based chatbot example.

- [Use API nodes within chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/use-api-nodes-within-chains#api-node-editor): **One-line description:** This page provides instructions on how to use API nodes within Prompt Chains in Maxim to integrate with third-party services by configuring HTTP requests and handling responses.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/evaluate/concepts#evaluators): *   **One-line description:** This page defines and explains key concepts within the Maxim platform, including Prompts, Prompt Comparisons, Prompt Chains, Workflows, Test Runs, Evaluators, Datasets, Context Sources, Prompt Tools and Simulations.

- [Use Events to send point-in-time information | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/use-events#attach-an-event-to-your-trace): **One-line description:** This page explains how to use events in Maxim to send point-in-time information within your application traces for tracking milestones and state changes.

- [Running your first test | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/running-first-test#next-steps): **One-line description:** This page provides a quickstart guide for running your first test on the Maxim platform, covering environment setup, prompt/workflow creation, dataset preparation, evaluator addition, test execution, and results analysis.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/jobs/head-of-engineering#apply-now): *   **One-line description:** This page is a job posting for a Head of Engineering position at Maxim, an AI evaluation and observability platform company based in Bangalore, India.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observe/overview#1-comprehensive-distributed-tracing): **One-line description:** This page introduces Maxim, an enterprise-grade LLM observability platform designed to monitor AI applications in real-time, improve outcomes, and address the challenges of LLM monitoring.

- [Bring your existing Evaluators via API | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-api-evaluators#create-evaluator): *   **One-line description:** This page explains how to integrate existing evaluation systems with Maxim using API endpoints.

- [Test your first Prompt Chain | Maxim](https://www.getmaxim.ai/docs/evaluate/quickstart/run-your-first-test-on-prompt-chains): **One-line description:** This page guides users on testing AI agentic workflows using Prompt Chains with Datasets and Evaluators.

- [Overview | Maxim](https://www.getmaxim.ai/docs/introduction/overview#1-experiment): *   **One-line description:** This webpage provides an overview of Maxim, a platform for streamlining AI application development and deployment through tools for experimentation, evaluation, observability, and data management.

- [Query Prompts via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts#first-capture-folder-id): *   **One-line description:** This page explains how to query and retrieve prompts, manage deployments, and use custom caching with Maxim AI's SDK, including filtering by deployment variables, tags, and folders.

- [Query Prompts via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts#prompt-structure): *   **One-line description:** This page explains how to use the Maxim AI SDK to query and retrieve prompts, including filtering by deployment variables, tags, and folders, as well as customizing caching and overriding the fallback algorithm.

- [Setting up alerts for performance metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-performance-metrics#available-metrics): *   **One-line description:** This page explains how to set up alerts in Maxim to monitor application performance metrics like latency, token usage, and cost.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/demo?utm_campaign=blog_post&utm_content=RAG_overview&utm_medium=social&utm_source=content): Here's the requested information:

- [Overview | Maxim](https://www.getmaxim.ai/docs/observability/concepts#trace): *   **One-line description:** This webpage introduces Maxim, a platform for streamlining AI application development and deployment with tools for experimentation, evaluation, observation, and data management.

- [Create a PagerDuty integration | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/create-a-pagerduty-integration#editing-or-deleting-the-integration): *   **One-line description:** This page guides users on how to create a PagerDuty integration within Maxim for sending alert notifications.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#timeout-configuration): *   **One-line description:** This page provides a guide on how to trigger AI application test runs programmatically using Maxim's SDK, covering data structures, data sources, custom output functions, evaluators, and advanced configurations.

- [Deploy Prompts | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/deploy-prompts#fetching-prompts-via-sdk): **One-line description:** This page explains how to deploy, manage, and query prompts using the Maxim platform for efficient prompt engineering and A/B testing.

- [Organize Prompts | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/organize-prompts#tag-prompts): *   **One-line description:** This page explains how to organize, tag, version, and deploy prompts within the Maxim AI platform for collaborative AI application development.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/jobs/platform-engineer#apply-now): *   **One-line description:** This page is a job posting for a Platform Engineer at Maxim, an AI evaluation and observability platform company, located in Bangalore, India.

- [Node level evaluation | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/node-level-evaluation#providing-variables-to-evaluators): *   **One-line description:** This page explains how to use Node level evaluation within the Maxim SDK to evaluate specific components of AI agent traces and logs.

- [Organize Prompts | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/organize-prompts#prompt-versions-and-sessions): *   **One-line description:** This page explains how to organize, tag, version, and deploy prompts effectively within the Maxim AI platform for collaborative AI application development.

- [Automate Prompt evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd#evaluating-workflows): *   **One-line description:** This page explains how to automate prompt evaluation using CI/CD pipelines with Maxim, using either CLI or GitHub Actions.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/evaluate/concepts#workflows): *   **One-line description:** This page explains key concepts within the Maxim platform for working with and evaluating AI models, including prompts, prompt comparisons, prompt chains, workflows, test runs, evaluators, datasets, context sources, prompt tools, and simulations.

- [Customize and share reports | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/optimize-evaluation-processes/customize-share-reports#search-and-filter): *   **One-line description:** This page explains how to customize and share reports generated by the Maxim AI observability platform, including toggling columns, pinning/reordering columns, searching/filtering, and sharing links.

- [Setting up alerts for performance metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-performance-metrics#create-an-alert): *   **One-line description:** This page explains how to set up alerts for performance metrics (latency, token usage, and cost) in the Maxim application.

- [Akshay Deo - Maxim Blog](https://www.getmaxim.ai/blog/author/akshay): *   **One-line description:** This webpage is a blog post by Akshay Deo on Maxim Blog, discussing the challenges of observability for GenAI systems and introducing a stateless tracing SDK.

- [Evaluate your context | Maxim](https://www.getmaxim.ai/docs/library/how-to/context-sources/evaluate-your-context): **One-line description:**

- [Query Prompts via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts#using-your-own-cache-for-prompts): *   **One-line description:** This page explains how to use the Maxim AI SDK to query and retrieve prompts, including filtering by deployments, tags, and folders, along with advanced features like custom caching and overriding the fallback algorithm.

- [Compare Prompts in the playground | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/compare-prompts-playground#configure-prompts): *   **One-line description:** This page explains how to use the Prompt comparison playground to evaluate different prompts and models side-by-side.

- [Bring your existing Evaluators via API | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-api-evaluators#map-response-fields): *   **One-line description:** This page explains how to integrate existing evaluation systems with Maxim by exposing them via API endpoints, allowing users to reuse their evaluators without rebuilding them.

- [Trigger Test Runs using SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk#advanced-configuration): *   **One-line description:** This page provides a guide on how to programmatically trigger test runs in Maxim using the SDK for AI application evaluation, covering data structures, data sources, output functions, evaluators, and advanced configurations.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/observe/concepts#alerts-tab): *   **One-line description:** This page explains the core concepts of Maxim's AI Observability platform, focusing on the Log Repository and its components for monitoring GenAI applications.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observe/overview#understanding-llm-observability-challenges): *   **One-line description:** This page introduces Maxim's AI observability platform for monitoring, troubleshooting, and improving the performance of LLM applications.

- [Get test run entries | Maxim](https://www.getmaxim.ai/docs/api/test-runs/entries/get): *   **One-line description:** This page documents the API endpoint for retrieving test run entries from Maxim.

- [Build complex AI workflows with Prompt Chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/experiment-with-prompt-chains#choose-node-functionality): *   **One-line description:** This page introduces Prompt Chains, a visual tool for building complex AI workflows by connecting prompts, code, and APIs.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observability/concepts#3-observe): *   **One-line description:** This page provides an overview of Maxim, a platform for streamlining AI application development and deployment through advanced evaluation and observability tools.

- [Create a Dataset using templates | Maxim](https://www.getmaxim.ai/docs/library/how-to/datasets/use-dataset-templates#dataset-testing): *   **One-line description:** This page describes how to create datasets using templates for testing and evaluating AI models, prompts, workflows, and agents.

- [Create Programmatic Evaluators | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-programmatic-evaluator#configure-evaluation-settings): **One-line description:** This page explains how to create custom evaluators using Javascript or Python within the Maxim platform.

- [Build an AI-powered customer support email agent | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/create-customer-support-agent#add-priority-scoring): *   **One-line description:** This page describes how to build an AI-powered customer support email agent using Prompt Chains.

- [Create a PagerDuty integration | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/create-a-pagerduty-integration#creating-the-integration-in-maxim): *   **One-line description:** This page explains how to integrate Maxim with PagerDuty to send alert notifications to a specified PagerDuty service.

- [Simulate multi-turn conversations | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/simulate-multi-turn-conversations#2-define-the-user-persona): *   **One-line description:** This page explains how to simulate multi-turn conversations to test and evaluate AI agents, focusing on scenario creation, user persona definition, and advanced settings.

- [Log multi-turn interactions as a session | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/log-multiturn-interactions-as-session#linking-traces-using-session-ids): *   **One-line description:** This page explains how to use a logging tool to group related traces into sessions for tracking user interactions with Generative AI systems.

- [Set up alerts for quality metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-quality-metrics#bias-check-alert): *   **One-line description:** This page provides instructions on how to set up alerts in Maxim to monitor the quality of AI application responses using evaluation scores and quality checks.

- [Query Prompts via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts#deployments): *   **One-line description:** This page explains how to query and retrieve prompts using the Maxim AI SDK, focusing on deployment-specific queries, tag-based queries, folder queries, and caching mechanisms.

- [Use variable columns in Datasets | Maxim](https://www.getmaxim.ai/docs/library/how-to/datasets/use-variable-columns-in-datasets#variable-usage-in-evaluators): *   **One-line description:** This page explains how to use variable columns in Maxim datasets to insert dynamic values into entities at runtime, using Jinja template syntax and different population methods.

- [Send feedback for AI application traces | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/send-user-feedback#add-feedback-to-traces): *   **One-line description:** This page guides users on how to send user feedback within application traces using Maxim's Feedback entity for AI applications.

- [Overview | Maxim](https://www.getmaxim.ai/docs/library/evaluators/concept#2-evaluate): *   **One-line description:** This page provides an overview of Maxim, a platform designed to streamline AI application development and deployment through experimentation, evaluation, observability, and data management.

- [Use tags on nodes | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/use-tags-on-nodes): **One-line description:** This page explains how to use tags on nodes within a tracing system to effectively group and filter workflow data, particularly in the context of LLM applications.

- [Create Prompt Partials | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-partials/create-prompt-partial#name-your-partial): *   **One-line description:** This page provides a guide on how to create and manage reusable prompt partials in Maxim, including naming, content creation, organization, and publishing.

- [Log multi-turn interactions as a session | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/log-multiturn-interactions-as-session#add-a-trace-to-the-session): *   **One-line description:** This page explains how to use a logging library to group related traces into sessions, enabling tracking of multi-turn interactions with GenAI systems.

- [Set up alerts for quality metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-quality-metrics#choose-evaluation-metrics-for-alerts): Error processing content

- [Concepts | Maxim](https://www.getmaxim.ai/docs/observe/concepts#span): Error processing content

- [Track tool calls | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/track-tool-calls): *   **One-line description:** This page provides instructions on how to track tool calls, which are external system calls triggered by LLM responses, within agentic workflows.

- [Set up alerts for quality metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-quality-metrics#follow-these-best-practices): *   **One-line description:** This page provides a guide on setting up alerts in Maxim to monitor the quality of AI application responses using evaluation scores and quality checks.

- [Track errors in traces | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/track-llm-errors#track-llm-errors-in-your-workflow): *   **One-line description:** This page explains how to track and log errors from LLM results and tool calls within AI application traces using Maxim.

- [Create a Slack integration | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/create-a-slack-integration#prerequisites): **One-line description:** This page provides a guide on setting up a Slack integration in Maxim to send alert notifications to Slack channels.

- [Create a Schema-based Prompt Tool | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-tools/create-a-tool-schema): *   **One-line description:** This page explains how to create and test schema-based prompt tools within the Maxim platform to ensure LLM responses adhere to a specific format.

- [Run a Prompt with tool calls | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/run-prompt-tool-calls#attach-and-run-your-tools-in-playground): *   **One-line description:** This page explains how to use Maxim's playground to test and measure tool call accuracy in AI workflows, both individually and at scale.

- [Query Prompts via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts#initializing-the-sdk): *   **One-line description:** This page explains how to use the Maxim AI SDK to efficiently query and retrieve prompts based on deployments, tags, and folder structures.

- [Query Prompt Chains via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains#for-a-prompt-chain-with-specific-deployment-variables): *   **One-line description:** This page explains how to use the Maxim SDK to query and retrieve prompt chains, folders, and customize caching.

- [Query Prompt Chains via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains#example): *   **One-line description:** This page explains how to use the Maxim SDK to query and retrieve prompt chains, folders, and customize caching.

- [Log multi-turn interactions as a session | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/log-multiturn-interactions-as-session#create-a-new-session): *   **One-line description:** This page explains how to use a logging tool to group related traces into sessions for tracking user interactions with GenAI systems.

- [Automate Prompt evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd#quick-start): *   **One-line description:** This page describes how to automate prompt evaluation in CI/CD pipelines using Maxim, using either a CLI tool or a GitHub Action, to ensure quality with every deployment.

- [Save and track Prompt experiments with sessions | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/save-prompt-session#next-steps): *   **One-line description:** This page explains how to use sessions to save and track prompt experiments in a prompt playground environment.

- [Compare Prompts in the playground | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/compare-prompts-playground#next-steps): *   **One-line description:** This page explains how to use the Prompt comparison playground to evaluate different prompt versions, models, and their performance side-by-side for prompt optimization and model selection.

- [Generate and translate product descriptions with AI | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/create-product-description-generator): *   **One-line description:** This page provides a step-by-step guide on how to use Maxim Prompt Chains to generate and translate product descriptions using AI, including adding translation support, data processing with code blocks, and integrating with external services.

- [Run your first Prompt test | Maxim](https://www.getmaxim.ai/docs/evaluate/quickstart/run-your-first-test-on-prompt): **One-line description:** This page is a quickstart guide on how to run your first prompt test using Maxim, a platform for evaluating and improving AI prompt performance.

- [Create an API-based Prompt Tool | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-tools/create-an-api-tool): *   **One-line description:** This page explains how to create API-based prompt tools within the Maxim platform, which involves automatically generating function schemas from API payloads.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observability/concepts#4-data-engine): *   **One-line description:** This page provides an overview of Maxim, a platform for streamlining AI application development and deployment through tools for experimentation, evaluation, observability, and data management.

- [Generate and translate product descriptions with AI | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/create-product-description-generator#connect-logging-service): *   **One-line description:** This page explains how to use Maxim Prompt Chains to generate and translate product descriptions using AI workflows.

- [Run your first Prompt test | Maxim](https://www.getmaxim.ai/docs/evaluate/quickstart/run-your-first-test-on-prompt#review-test-results): *   **One-line description:** This page provides a quickstart guide to testing AI prompts using Maxim, including creating prompts, configuring tests with datasets and evaluators, and reviewing results.

- [Automate workflow evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/automate-via-ci-cd#inputs): *   **One-line description:** This page explains how to automate workflow evaluation using Maxim's CI/CD integration, including CLI and GitHub Actions.

- [Deploy Prompt Chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/deploy-prompt-chains): *   **One-line description:** This page explains how to deploy prompt chains on Maxim for easier experimentation, deployment without code changes, and conditional deployments using custom variables.

- [Query Prompts via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts#query-deployed-prompts-using-folder): *   **One-line description:** This page provides a guide on how to query and retrieve prompts from Maxim AI using their SDK, including filtering by deployments, tags, and folders, as well as customizing caching and overriding the fallback algorithm.

- [Generate and share comparison reports | Maxim](https://www.getmaxim.ai/docs/analyze/how-to/comparison-reports#understand-your-comparison-report): *   **One-line description:** This page explains how to generate, analyze, update, and share comparison reports in Maxim for tracking improvements and making data-driven decisions across AI application test runs.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observe/overview#real-time-monitoring-and-alerting): *   **One-line description:** This page provides an overview of Maxim, an LLM observability platform designed to monitor and improve AI application performance in real-time.

- [Query Prompt Chains via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains#folder-structure): **One-line description:** This page explains how to use the Maxim SDK to query and retrieve prompt chains, folders, and configure caching, including examples in JavaScript/TypeScript and Python.

- [Set up auto evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/auto-evaluation#evaluation-summary): **One-line description:**
This page explains how to set up and understand automated evaluation of logs in Maxim, enabling users to monitor and improve the performance of their LLMs.

- [Automate Prompt evaluation via CI/CD | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd#installation): * **One-line description:** This page explains how to automate prompt evaluation using CI/CD pipelines with Maxim, using either the CLI tool or GitHub Actions.

- [Use variable columns in Datasets | Maxim](https://www.getmaxim.ai/docs/library/how-to/datasets/use-variable-columns-in-datasets#variable-usage-in-api-workflow): *   **One-line description:** This page explains how to use variable columns in Maxim's Datasets to insert dynamic values into entities at runtime using Jinja template syntax.

- [Delete Alert | Maxim](https://www.getmaxim.ai/docs/api/alerts/delete): *   **One-line description:** This page documents how to delete an alert using the Maxim API.

- [Create a prompt version | Maxim](https://www.getmaxim.ai/docs/api/prompts/versions/post): *   **One-line description:** This page documents how to create a prompt version using the Maxim API, detailing the required request parameters and the structure of the successful response.

- [Setting up alerts for performance metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-performance-metrics#common-alert-configurations-or-alert-configuration-examples): *   **One-line description:** This page provides instructions on how to set up alerts for performance metrics like latency, token usage, and cost within the Maxim platform.

- [Create a code-based Prompt Tool | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-tools/create-a-code-tool#creating-a-code-based-tool): *   **One-line description:** This page explains how to create and test code-based prompt tools in Maxim using a JavaScript code editor.

- [Use API nodes within chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/use-api-nodes-within-chains#select-output-field-optional): *   **One-line description:** This page explains how to use API nodes within Prompt Chains in Maxim to integrate with external services.

- [Evaluate Datasets | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-datasets#get-started-with-dataset-evaluation): *   **One-line description:** This page explains how to evaluate AI model outputs against expected results using Maxim's dataset evaluation tools.

- [Track tool calls | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/track-tool-calls#how-to-log-tool-calls): *   **One-line description:** This page explains how to track tool calls in AI application traces, specifically focusing on logging external system calls triggered by LLM responses using JS/TS and Python.

- [Setting up alerts for performance metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-performance-metrics#manage-alerts): *   **One-line description:** This page explains how to set up alerts in Maxim to monitor application performance metrics like latency, token usage, and cost, and provides best practices for alert configuration.

- [Test multi-turn conversations manually | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-multi-turn-conversations-manually#start-a-conversation): *   **One-line description:** This page explains how to manually test multi-turn conversations with an AI endpoint using Maxim's interactive Workflows.

- [Track token usage and costs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/track-token-usage-and-cost): *   **One-line description:** This page explains how to track token usage and costs in LLM applications using Maxim's logging capabilities, along with guidance on error tracking, custom pricing, and log filtering.

- [Running your first test | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/running-first-test#1-set-up-your-environment): *   **One-line description:** This page provides a quickstart guide on running your first test in Maxim, an AI evaluation platform, covering environment setup, prompt/workflow creation, dataset preparation, evaluator addition, test execution, and results analysis.

- [Customize and share reports | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/optimize-evaluation-processes/customize-share-reports#pinning-columns): *   **One-line description:** This page describes how to customize and share reports generated within the Maxim AI observability platform to analyze and understand AI system performance.

- [Delete test runs | Maxim](https://www.getmaxim.ai/docs/api/test-runs/delete#request-body): *   **One-line description:** This page documents the API endpoint for deleting test runs in the Maxim platform.

- [Run bulk comparisons across test cases | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases#why-run-comparison-experiments): *   **One-line description:** This page explains how to run bulk comparisons across test cases for prompt versions using the Maxim platform, enabling informed decisions and preventing regressions.

- [Overview | Maxim](https://www.getmaxim.ai/docs/sdk/test-runs-via-sdk/js-ts#1-experiment): *   **One-line description:** This page provides an overview of Maxim, a platform for streamlining AI application development and deployment by offering tools for experimentation, evaluation, observability, and data management.

- [Bring your existing Evaluators via API | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-api-evaluators#set-pass-criteria): *   **One-line description:** This page explains how to connect an existing evaluation system to Maxim using an API to reuse evaluators.

- [Query Prompts via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts#overriding-fallback-algorithm): *   **One-line description:** This page explains how to query and retrieve prompts, including filtering by deployment variables, tags, and folders, using Maxim AI's SDK, and it also covers caching mechanisms and matching algorithms.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/evaluate/concepts#prompt-chains): *   **One-line description:** This page defines key concepts in Maxim, a platform for evaluating AI application performance, including prompts, prompt comparisons, prompt chains, workflows, test runs, evaluators, datasets, context sources, prompt tools, and simulation.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai?utm_campaign=blog_post&utm_content=jailbreak&utm_medium=social&utm_source=content): *   **One-line description:** This page introduces Maxim, an AI evaluation and observability platform that helps teams build and ship AI agents reliably and faster.

- [Test multi-turn AI conversations | Maxim](https://www.getmaxim.ai/docs/evaluate/quickstart/simulate-and-evaluate-multi-turn-conversations): **One-line description:** This page describes how to test multi-turn AI conversations using Maxim Workflows through simulation, configuration, and result analysis.

- [Get Alerts | Maxim](https://www.getmaxim.ai/docs/api/alerts/get): *   **One-line description:** This page documents the API endpoint for retrieving alerts in a workspace using the Maxim API.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/observe/concepts#event): **One-line description:** This page describes key concepts of Maxim's AI Observability platform, focusing on Log Repositories and their components (Sessions, Traces, Spans, Events, Generations, Retrievals, and Tool Calls) for monitoring and troubleshooting AI applications.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/observe/concepts#structure): **One-line description:** This page explains the key concepts of Maxim's AI Observability platform, focusing on Log Repositories and their components (Overview, Logs, Alerts tabs) and the structure of logs (Session, Trace, Span, Event, Generation, Retrieval, Tool Call).

- [RAG Architecture Analysis: Optimize Retrieval & Generation](https://www.getmaxim.ai/blog/rag-evaluation-metrics): *   **One-line description:** This page discusses the evaluation of Retrieval-Augmented Generation (RAG) systems, focusing on metrics, benchmarks, and dataset management using platforms like Maxim.

- [Overview | Maxim](https://www.getmaxim.ai/docs/api/overview#authentication): *   **One-line description:** This page provides an overview and documentation for the Maxim API, including available APIs, authentication methods, and related topics.

- [Get test runs | Maxim](https://www.getmaxim.ai/docs/api/test-runs/get#query-parameters): *   **One-line description:** This page documents the API endpoint for retrieving test runs within a workspace using the Maxim AI platform.

- [Build complex AI workflows with Prompt Chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/experiment-with-prompt-chains#initialize-your-chain): Error processing content

- [Test your agentic workflows using chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/test-prompt-chains#build-your-prompt-chain): *   **One-line description:** This page explains how to evaluate and test Prompt Chains, particularly for agentic workflows and product description generation, using datasets and evaluators within the Maxim platform.

- [Enhancing RAG Systems: Document Repacking & Summarization](https://www.getmaxim.ai/blog/rag-generation-component): **One-line description:** This page discusses techniques to improve the generation component of RAG systems, including document repacking, summarization, and fine-tuning LLMs.

- [Set up auto evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/auto-evaluation#trace-evaluation-card): *   **One-line description:** This page explains how to set up automatic evaluation of logs in Maxim to monitor the performance of LLMs in live systems.

- [Log LLM generations in your AI application traces | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/adding-llm-call#record-llm-response): *   **One-line description:** This page explains how to log LLM generations within AI application traces, including recording requests, responses, and integrating with RAG pipelines.

- [Update Prompt | Maxim](https://www.getmaxim.ai/docs/api/prompts/put): *   **One-line description:** This page documents the API endpoint for updating an existing prompt using a PUT request to `/v1/prompts`.

- [Update Prompt | Maxim](https://www.getmaxim.ai/docs/api/prompts/put#request-body): *   **One-line description:** This page documents the API endpoint for updating an existing prompt in the Maxim AI platform.

- [Generate and share comparison reports | Maxim](https://www.getmaxim.ai/docs/analyze/how-to/comparison-reports#update-your-report): **One-line description:** This page explains how to generate, understand, update, and share comparison reports in Maxim for analyzing AI application data.

- [Set up alerts for quality metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-quality-metrics#example-configurations): *   **One-line description:** This page explains how to set up alerts in Maxim for monitoring the quality of AI application responses based on evaluation scores.

- [Set up auto evaluation on logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/evaluate-logs/auto-evaluation#setting-up-auto-evaluation): *   **One-line description:** This page explains how to set up automatic evaluation of logs in Maxim to monitor and improve LLM performance.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/demo?utm_campaign=blog_post&utm_content=rag-evaluation-metrics&utm_medium=social&utm_source=content): ## Analysis of Webpage Content

- [Use spans to group units of work | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/add-spans-to-traces#create-spans-with-logger-object): *   **One-line description:** This page explains how to use spans in the Maxim platform to organize and track requests across microservices within traces.

- [Update Alert | Maxim](https://www.getmaxim.ai/docs/api/alerts/put): *   **One-line description:** This page documents the API endpoint for updating an alert in the Maxim AI platform using a PUT request.

- [Setting up your first trace | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/setting-up-trace#install-maxim-sdk): **One-line description:** This page guides users on setting up tracing for their AI applications using the Maxim platform, covering repository creation, SDK installation, initialization, and basic trace implementation.

- [Query Prompts via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts#query-deployed-prompts-using-tags): **One-line description:** This page explains how to query and retrieve prompts, folders, and manage caching using Maxim AI's SDK, including filtering by deployments, tags, and folder IDs.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/observe/concepts#retrieval): **One-line description:** This page describes the key concepts of Maxim's AI Observability platform, focusing on the Log Repository and its components (Sessions, Traces, Spans, Events, Generations, Retrievals, and Tool Calls).

- [Create a PagerDuty integration | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/create-a-pagerduty-integration#using-the-integration): *   **One-line description:** This page provides instructions on how to integrate Maxim with PagerDuty to send alert notifications to a PagerDuty service.

- [Query Prompts via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts#matching-algorithm): **One-line description:** This page explains how to use the Maxim AI SDK to query and retrieve prompts, including filtering by deployment variables, tags, and folders, and customizing the caching and matching algorithms.

- [Dhwanil Pithva - Maxim Blog](https://www.getmaxim.ai/blog/author/dhwanil): **One-line description:**

- [Run bulk comparisons across test cases | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases#run-a-comparison-report): *   **One-line description:** This page explains how to run bulk comparisons of different prompt versions across test cases in the Maxim platform to optimize prompt performance and quality.

- [Maxim Blog (Page 4)](https://www.getmaxim.ai/blog/page/4): *   **One-line description:** This is the blog page (page 4) of Maxim AI, featuring articles on AI, LLMs, and related topics.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/evaluate/concepts#simulation): *   **One-line description:** This page introduces key concepts within the Maxim platform related to AI application development and testing, including prompts, prompt comparisons, workflows, evaluators, and datasets.

- [Concepts | Maxim](https://www.getmaxim.ai/docs/observe/concepts#generation): **One-line description:** This page describes Maxim's AI Observability platform concepts, focusing on the Log Repository, its components, and the structure of logs (sessions, traces, spans, events, generations, retrievals, and tool calls).

- [Concepts | Maxim](https://www.getmaxim.ai/docs/observe/concepts#tool-call): **One-line description:**

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai?utm_campaign=blog_post&utm_content=dspy&utm_medium=social&utm_source=content): **One-line description:** Maxim is an end-to-end evaluation and observability platform for AI agents, enabling teams to ship them reliably and faster.

- [Running your first test | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/running-first-test#3-prepare-your-dataset): *   **One-line description:** This page provides a quickstart guide on how to run your first test using the Maxim platform for evaluating AI application performance.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/demo?utm_campaign=web&utm_content=knowhalu&utm_medium=blogs&utm_source=content): *   **One-line description:** This page is a landing page for a GenAI evaluation and observability platform, offering a demo and a contact option for urgent needs.

- [Test your first Prompt Chain | Maxim](https://www.getmaxim.ai/docs/evaluate/quickstart/run-your-first-test-on-prompt-chains#review-results): **One-line description:** This page guides users on testing agentic workflows using Prompt Chains with datasets and evaluators, offering a quickstart to build, configure, and review test results.

- [Add Dataset entries using SDK | Maxim](https://www.getmaxim.ai/docs/library/how-to/datasets/add-new-entries-using-sdk#adding-entries-to-a-dataset): *   **One-line description:** This page provides a guide on how to add entries to a Maxim Dataset programmatically using the Maxim SDK.

- [Overview | Maxim](https://www.getmaxim.ai/docs/sdk/observability/concepts): *   **One-line description:** This page introduces Maxim, a platform for streamlining AI application development and deployment through experiment tracking, evaluation, observability, and data management.

- [Running your first test | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/running-first-test#2-create-your-first-prompt-or-workflow): *   **One-line description:** This page is a quickstart guide for running your first test on the Maxim platform for evaluating AI applications.

- [Set up a human annotation pipeline | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline#select-human-evaluators-while-triggering-a-test-run): **One-line description:** This page explains how to set up a human annotation pipeline in Maxim to improve AI quality through human feedback on AI outputs.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/demo?utm_campaign=blog_post&utm_content=llm_hallucination&utm_medium=social&utm_source=content): **One-line description:** This page promotes GenAI evaluation and observability platform, featuring a demo schedule option and a form for contact/inquiry.

- [Query Prompts via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts#get-all-deployed-prompts-from-a-folder): *   **One-line description:** This page explains how to use the Maxim AI SDK to efficiently query and retrieve prompts, including filtering by deployment variables, tags, and folders.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/demo?utm_campaign=blog_post&utm_content=jailbreak&utm_medium=social&utm_source=content): Here's a breakdown of the provided webpage content:

- [Set up alerts for quality metrics | Maxim](https://www.getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-quality-metrics#toxicity-alert): *   **One-line description:** This page explains how to set up alerts in Maxim to monitor the quality of AI application responses based on evaluation scores and quality checks.

- [Evaluate simulated sessions for agents | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/evaluate-simulated-sessions-for-agents#execute-test-run): **One-line description:** This page explains how to evaluate the performance of AI agents using simulated multi-turn conversations and automated testing within the MaximPricing platform.

- [Running your first test | Maxim](https://www.getmaxim.ai/docs/introduction/quickstart/running-first-test#6-run-your-first-test): *   **One-line description:** This page provides a quickstart guide on running your first test in the Maxim AI evaluation platform, covering environment setup, prompt/workflow creation, dataset preparation, evaluator setup, test execution, and result analysis.

- [Deploy Prompt Chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/deploy-prompt-chains#why-deploy-prompt-chains-via-maxim): *   **One-line description:** This page explains how to deploy prompt chains in Maxim, allowing for easy A/B testing, conditional deployments based on variables, and querying via SDK.

- [Query Prompts via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts#for-a-prompt-with-specific-deployment-variables): *   **One-line description:** This page explains how to query and retrieve prompts from Maxim AI's platform using the SDK, including filtering by deployment variables, tags, and folders.

- [Use tags on nodes | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/use-tags-on-nodes#add-tags-to-a-trace): **One-line description:** This page provides instructions on how to use tags on nodes (spans, generations, retrievals, events, etc.) within traces to group and filter workflow data effectively, using JavaScript/TypeScript, Python, Go and Java code snippets.

- [Set up automated email summaries to monitor your logs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/summary-emails#set-up-summary-emails): *   **One-line description:** This page explains how to set up and manage automated weekly email summaries to monitor log repository performance and user feedback.

- [Save and track Prompt experiments with sessions | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/save-prompt-session#create-prompt-sessions): **One-line description:** This page explains how to use sessions to save and track prompt experiments within a prompt engineering platform.

- [Query Prompts via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts#example): *   **One-line description:** This page explains how to query and retrieve prompts from Maxim AI using the SDK, including filtering by deployment variables, tags, and folders.

- [Create and manage Prompt versions | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/create-prompt-versions#create-prompt-versions): *   **One-line description:** This page explains how to create, manage, and compare prompt versions within the Maxim platform for AI application development.

- [Build an AI-powered customer support email agent | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/create-customer-support-agent#set-up-email-classification): *   **One-line description:** This page explains how to build an AI-powered customer support email agent using prompt chains to classify emails, prioritize responses, create help desk tickets, and generate personalized replies.

- [Build an AI-powered customer support email agent | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/create-customer-support-agent#send-email-response): *   **One-line description:** This page provides a guide on building an AI-powered customer support email agent using Prompt Chains, covering email classification, priority scoring, ticket creation, personalized response generation, and automated email sending.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observability/concepts): *   **One-line description:** This page introduces Maxim, a platform for streamlining AI application development and deployment through tools for experimentation, evaluation, observability, and data management.

- [Generate and translate product descriptions with AI | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/create-product-description-generator#add-the-prompt-to-a-chain): *   **One-line description:** This page provides a guide on how to generate and translate product descriptions using AI-powered Prompt Chains, including adding translation support, processing translations with code, and connecting to external services.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/demo?utm_campaign=blog_post&utm_content=astute_RAG&utm_medium=social&utm_source=content): Here's the breakdown of the webpage content:

- [Build complex AI workflows with Prompt Chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/experiment-with-prompt-chains#expand-the-workflow): *   **One-line description:** This page introduces Prompt Chains, a visual tool for building complex AI workflows by connecting prompts, code, and APIs.

- [Overview | Maxim](https://www.getmaxim.ai/docs/analyze/overview#analyze-your-data): *   **One-line description:** This page introduces the "Analyze" section of Maxim, highlighting features for generating comparison reports and upcoming live dashboards for AI application data analysis.

- [Delete Prompt | Maxim](https://www.getmaxim.ai/docs/api/prompts/delete): **One-line description:**

- [Create Prompt | Maxim](https://www.getmaxim.ai/docs/api/prompts/post): *   **One-line description:** This page documents the API endpoint for creating a new prompt using the Maxim AI platform.

- [Test your agentic workflows using chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/test-prompt-chains#configure-your-test): *   **One-line description:** This page explains how to test and evaluate Prompt Chains, specifically focusing on agentic workflows for tasks like generating and translating product descriptions.

- [Get test run entries | Maxim](https://www.getmaxim.ai/docs/api/test-runs/entries/get#query-parameters): *   **One-line description:** This page documents the API endpoint for retrieving test run entries from the Maxim AI platform.

- [OpenAI Agents SDK | Maxim](https://www.getmaxim.ai/docs/observe/integrations/openai-agents-sdk): *   **One-line description:** This page explains how to integrate Maxim's observability and real-time evaluation capabilities with the OpenAI Agents SDK.

- [Bring your existing Evaluators via API | Maxim](https://www.getmaxim.ai/docs/library/how-to/evaluators/create-api-evaluators#configure-endpoint): *   **One-line description:** This page explains how to connect an existing evaluation system to Maxim using an API, allowing reuse of evaluators without rebuilding them.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai?utm_campaign=blog_post&utm_content=synth_data&utm_medium=social&utm_source=content): *   **One-line description:** This page introduces Maxim, an end-to-end evaluation and observability platform designed to help teams ship AI agents reliably and faster.

- [Build an AI-powered customer support email agent | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/create-customer-support-agent#generate-personalized-response): *   **One-line description:** This page guides you on building an AI-powered customer support email agent using Prompt Chains for email classification, priority scoring, ticket creation, and personalized response generation.

- [Update Alert | Maxim](https://www.getmaxim.ai/docs/api/alerts/put#request-body): *   **One-line description:** This page documents the API endpoint for updating an alert in the Maxim platform, including request parameters, example code, and response formats.

- [Build an AI-powered customer support email agent | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/create-customer-support-agent#create-help-desk-ticket): *   **One-line description:** This page provides a guide on building an AI-powered customer support email agent using Prompt Chains.

- [Maxim Blog (Page 5)](https://www.getmaxim.ai/blog/page/5): *   **One-line description:** This is the fifth page of the Maxim Blog, featuring articles about Large Language Models (LLMs), AI agents, and related technologies.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai?utm_campaign=blog_post&utm_content=rag-generation-component&utm_medium=social&utm_source=content): *   **One-line description:** This page promotes Maxim, an AI evaluation and observability platform that helps teams develop and deploy AI agents faster and more reliably.

- [Test your first Prompt Chain | Maxim](https://www.getmaxim.ai/docs/evaluate/quickstart/run-your-first-test-on-prompt-chains#configure-your-test): *   **One-line description:** This page provides a quickstart guide on testing AI agentic workflows using Prompt Chains with Datasets and Evaluators.

- [Create Alert | Maxim](https://www.getmaxim.ai/docs/api/alerts/post): *   **One-line description:** This page describes how to create a new alert using the Maxim API.

- [Track token usage and costs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/track-token-usage-and-cost#custom-pricing): *   **One-line description:** This page explains how to track token usage, costs, and errors in LLM applications using Maxim's logging capabilities, including setting custom pricing and configuring filters.

- [Get Prompt Versions | Maxim](https://www.getmaxim.ai/docs/api/prompts/versions/get): **One-line description:** This page documents the API endpoint for retrieving versions of a prompt using the Maxim API.

- [Setting up your first trace | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/setting-up-trace#create-a-new-repository-on-maxim-dashboard): *   **One-line description:** This page guides users on setting up their first trace using the Maxim platform for monitoring and evaluating AI application performance.

- [Configure filters and saved views | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/filters-and-saved-views#create-filters-and-saved-views): *   **One-line description:** This page explains how to configure filters and saved views in Maxim for efficient log analysis and debugging, as well as how to track token usage and set custom token pricing for AI applications.

- [Create Prompt | Maxim](https://www.getmaxim.ai/docs/api/prompts/post#request-body): *   **One-line description:** This page documents the API endpoint for creating a new prompt using the Maxim AI platform.

- [Test your agentic workflows using chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/test-prompt-chains#create-a-dataset): *   **One-line description:** This page explains how to test and evaluate Prompt Chains using datasets and evaluators within the Maxim platform to ensure consistent performance and facilitate easy deployment.

- [Set up a human annotation pipeline | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline#create-human-evaluators): **One-line description:** This page describes how to set up and utilize a human annotation pipeline within the Maxim platform to improve AI quality through human feedback.

- [Create a Schema-based Prompt Tool | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-tools/create-a-tool-schema#creating-a-schema-tool): *   **One-line description:** This page explains how to create and test schema-based prompt tools within the Maxim platform, ensuring structured and schema-compliant outputs from LLMs.

- [Test your agentic workflows using chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/test-prompt-chains#start-a-test-run): *   **One-line description:** This page explains how to test and evaluate Prompt Chains, especially for agentic workflows like generating and translating product descriptions.

- [Test your AI application via an API endpoint | Maxim](https://www.getmaxim.ai/docs/evaluate/quickstart/run-your-first-test-on-api-workflow): *   **One-line description:** This page explains how to test AI applications through an API endpoint using a testing platform.

- [Maxim Blog (Page 6)](https://www.getmaxim.ai/blog/page/6): *   **One-line description:** This is page 6 of the Maxim Blog, featuring articles about AI agents, LLMs, RAG systems, and related research.

- [Maxim AI November 2024 Updates](https://www.getmaxim.ai/blog/maxim-ai-november-2024-updates): **One-line description:** This webpage details the November 2024 updates to Maxim AI, focusing on new features and improvements to its platform.

- [Create a prompt version | Maxim](https://www.getmaxim.ai/docs/api/prompts/versions/post#request-body): *   **One-line description:** This page describes how to create a prompt version using the Maxim API, detailing the necessary request parameters and expected response.

- [Create Alert | Maxim](https://www.getmaxim.ai/docs/api/alerts/post#request-body): *   **One-line description:** This page provides documentation for creating alerts using the Maxim API, including request parameters, example code, and response formats.

- [Query Prompts via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts#interface-for-custom-cache): *   **One-line description:** This page explains how to query and retrieve prompts, folders and leverage caching mechanisms within the Maxim AI platform using the SDK, including filtering by deployments, tags, and folder IDs.

- [Get test run entries | Maxim](https://www.getmaxim.ai/docs/api/test-runs/entries/get#authorization): *   **One-line description:** This page documents the API endpoint for retrieving test run entries from the Maxim AI platform.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/jobs/developer-relations-engineer#apply-now): *   **One-line description:** This page is a job posting for a Founding Developer Relations Engineer at Maxim, an AI application development platform company.

- [Customize and share reports | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/optimize-evaluation-processes/customize-share-reports#toggle-columns): **One-line description:**

- [Overview | Maxim](https://www.getmaxim.ai/docs/observability/concepts#1-experiment): *   **One-line description:** This page introduces Maxim, a platform for streamlining AI application development, evaluation, and deployment.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai?utm_campaign=blog_post&utm_content=llm_hallucination&utm_medium=social&utm_source=content): *   **One-line description:** This page promotes Maxim, an AI evaluation and observability platform that helps teams build and deploy AI agents faster and more reliably.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/demo?utm_campaign=blog_post&utm_content=rag-generation-component&utm_medium=social&utm_source=content): **One-line description:** This page promotes a GenAI evaluation and observability platform and includes a form to request a demo.

- [OpenAI Agents SDK | Maxim](https://www.getmaxim.ai/docs/observe/integrations/openai-agents-sdk#integrating-with-maxim): *   **One-line description:** This page describes how to integrate Maxim's observability and real-time evaluation capabilities with OpenAI Agents SDK.

- [Create and manage Prompt versions | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/create-prompt-versions#next-steps): *   **One-line description:** This page explains how to create and manage prompt versions in Maxim for collaborative AI application development, testing, and deployment.

- [Run your first Prompt test | Maxim](https://www.getmaxim.ai/docs/evaluate/quickstart/run-your-first-test-on-prompt#create-and-publish-a-prompt): **One-line description:** This page provides a quickstart guide on how to run your first prompt test using Maxim, a platform for testing AI applications and prompts.

- [Overview | Maxim](https://www.getmaxim.ai/docs/library/evaluators/concept): *   **One-line description:** This page introduces Maxim, a platform designed to streamline AI application development and deployment by providing tools for experimentation, evaluation, observability, and data management.

- [Customize and share reports | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/optimize-evaluation-processes/customize-share-reports#re-ordering-columns): *   **One-line description:** This page explains how to customize, filter, and share reports generated by the Maxim AI observability platform to analyze and understand AI system performance.

- [Deploy Prompts | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/deploy-prompts#why-deploy-prompts-via-maxim): **One-line description:** This page explains how to deploy, query, and organize prompts using Maxim, a platform that facilitates prompt engineering and management.

- [Deploy Prompt Chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/deploy-prompt-chains#deploying-a-prompt-chain): **One-line description:**

- [Overview | Maxim](https://www.getmaxim.ai/docs/evaluation/workflow/workflow-overview): *   **One-line description:** Maxim is a platform that helps streamline AI application development and deployment by offering tools for experimentation, evaluation, observation, and data management.

- [Add Dataset entries using SDK | Maxim](https://www.getmaxim.ai/docs/library/how-to/datasets/add-new-entries-using-sdk#getting-your-dataset-id): *   **One-line description:** This page provides a guide on how to add entries to a Dataset using the Maxim SDK.

- [Delete Dataset | Maxim](https://www.getmaxim.ai/docs/api/datasets/delete): *   **One-line description:** This page describes how to delete a dataset using the Maxim API.

- [Set up a human annotation pipeline | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline#set-up-human-evaluation-for-this-run): **One-line description:** This page explains how to set up a human annotation pipeline within the Maxim platform to improve AI quality through human feedback on AI outputs.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observability/evaluating-logs/auto-evaluation): *   **One-line description:** This page provides an overview of Maxim, a platform designed to streamline AI application development and deployment through tools for experimentation, evaluation, observability, and data management.

- [Set up a human annotation pipeline | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline#analyze-human-ratings): **One-line description:** This page describes how to set up and utilize a human annotation pipeline within the Maxim platform for evaluating AI outputs and improving AI quality.

- [Generate and translate product descriptions with AI | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/create-product-description-generator#process-translations-with-code-block-node): **One-line description:** This page explains how to use Maxim Prompt Chains to generate and translate product descriptions using AI workflows.

- [Enhance RAG with Advanced Retrieval & Chunking Techniques](https://www.getmaxim.ai/blog/rag-retrieval): *   **One-line description:** This page discusses techniques to improve the retrieval component of Retrieval-Augmented Generation (RAG) systems, focusing on document chunking and query enhancement methods.

- [Overview | Maxim](https://www.getmaxim.ai/docs/analyze/overview#create-live-dashboards-coming-soon): *   **One-line description:** This page provides an overview of the analysis tools available in Maxim for generating comparison reports and creating live dashboards to gain actionable insights from AI application data.

- [Generate and share comparison reports | Maxim](https://www.getmaxim.ai/docs/analyze/how-to/comparison-reports#share-your-report): **One-line description:** This page provides instructions on how to generate, understand, update, and share comparison reports within the Maxim platform for analyzing AI application test runs.

- [Get Alerts | Maxim](https://www.getmaxim.ai/docs/api/alerts/get#authorization): *   **One-line description:** This page documents the API endpoint for retrieving alerts in a Maxim workspace.

- [Log LLM generations in your AI application traces | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/adding-llm-call#send-and-record-llm-request): *   **One-line description:** This page explains how to log LLM generations in AI application traces to monitor and analyze LLM calls within the application.

- [Generate and share comparison reports | Maxim](https://www.getmaxim.ai/docs/analyze/how-to/comparison-reports#create-a-comparison-report): **One-line description:**

- [Delete Alert | Maxim](https://www.getmaxim.ai/docs/api/alerts/delete#authorization): *   **One-line description:** This page documents how to delete an alert using the Maxim API's DELETE /v1/alerts endpoint.

- [Get Alerts | Maxim](https://www.getmaxim.ai/docs/api/alerts/get#query-parameters): *   **One-line description:** This page documents the API endpoint for retrieving alerts in the Maxim platform.

- [Test your AI application via an API endpoint | Maxim](https://www.getmaxim.ai/docs/evaluate/quickstart/run-your-first-test-on-api-workflow#configure-your-http-endpoint): Error processing content

- [Deploy Prompts | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/deploy-prompts#deploying-a-prompt): Error processing content

- [Run your first Prompt test | Maxim](https://www.getmaxim.ai/docs/evaluate/quickstart/run-your-first-test-on-prompt#configure-and-trigger-a-test): **One-line description:** This page provides a quickstart guide to running your first prompt test using Maxim to evaluate and improve AI prompt performance.

- [Query Prompts via SDK | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts#folder-structure): **One-line description:** This page explains how to use the Maxim AI SDK to query and retrieve prompts, folders, and configure caching, including filtering by deployment variables and tags.

- [Delete Prompt | Maxim](https://www.getmaxim.ai/docs/api/prompts/delete#query-parameters): **One-line description:** This page documents the API endpoint for deleting a prompt in the Maxim AI platform.

- [Test your AI application via an API endpoint | Maxim](https://www.getmaxim.ai/docs/evaluate/quickstart/run-your-first-test-on-api-workflow#analyze-test-results): *   **One-line description:** This page provides a quickstart guide on how to test AI applications using an API endpoint.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observability/concepts#2-evaluate): *   **One-line description:** This page provides an overview of Maxim, a platform that helps streamline AI application development and deployment by providing tools for experimentation, evaluation, observation, and data management.

- [Test multi-turn AI conversations | Maxim](https://www.getmaxim.ai/docs/evaluate/quickstart/simulate-and-evaluate-multi-turn-conversations#configure-test-settings): **One-line description:**

- [Use spans to group units of work | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/add-spans-to-traces#create-spans-with-trace-object): *   **One-line description:** This page explains how to use spans to organize and track requests across microservices within traces using the Maxim platform, with code examples in JS/TS, Python, Go, and Java.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/demo?utm_campaign=blog_post&utm_content=synth_data&utm_medium=social&utm_source=content): Here's a breakdown of the webpage content:

- [Set up a human annotation pipeline | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline#collect-ratings-via-email): **One-line description:**

- [Setting up your first trace | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/setting-up-trace#initialize-maxim-sdk): *   **One-line description:** This page guides users on setting up their first trace within the Maxim platform to monitor and evaluate AI application performance.

- [Setting up your first trace | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/setting-up-trace#start-tracing-your-application): *   **One-line description:** This page provides a guide on setting up tracing in an AI application using the Maxim platform.

- [OpenAI Agents SDK | Maxim](https://www.getmaxim.ai/docs/observe/integrations/openai-agents-sdk#cookbooks): *   **One-line description:** This page explains how to integrate Maxim's observability and real-time evaluation capabilities with the OpenAI Agents SDK.

- [Delete Prompt | Maxim](https://www.getmaxim.ai/docs/api/prompts/delete#authorization): *   **One-line description:** This page documents the API endpoint for deleting a prompt using the Maxim AI platform.

- [Overview | Maxim](https://www.getmaxim.ai/docs/sdk/observability/concepts#2-evaluate): *   **One-line description:** This page introduces Maxim, a platform designed to streamline AI application development and deployment by providing tools for experimentation, evaluation, observability, and data management.

- [Test multi-turn AI conversations | Maxim](https://www.getmaxim.ai/docs/evaluate/quickstart/simulate-and-evaluate-multi-turn-conversations#review-simulation-results): **One-line description:** This page describes how to use Maxim Workflows to automatically evaluate AI chat interactions and test multi-turn conversations by simulating user interactions with an AI agent.

- [Test your first Prompt Chain | Maxim](https://www.getmaxim.ai/docs/evaluate/quickstart/run-your-first-test-on-prompt-chains#build-your-chain): *   **One-line description:** This page provides a quickstart guide on testing Prompt Chains using Datasets and Evaluators to analyze the performance and quality of agentic workflows.

- [ConTAM: Tackling Data Contamination in LLM Benchmarks](https://www.getmaxim.ai/blog/llm-data-quality): *   **One-line description:** This page discusses data contamination in LLM benchmarks, introduces a new method called ConTAM for assessing contamination, and highlights the impact of contamination on model performance.

- [Get Prompts | Maxim](https://www.getmaxim.ai/docs/api/prompts/get): *   **One-line description:** This page documents the API endpoint for retrieving prompts within a workspace using the Maxim AI platform.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai?utm_campaign=blog_post&utm_content=rag-evaluation-metrics&utm_medium=social&utm_source=content): **One-line description:** Maxim is an end-to-end GenAI evaluation and observability platform that helps teams ship AI agents reliably and faster.

- [Generate and translate product descriptions with AI | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/create-product-description-generator#add-translation-support): *   **One-line description:** This page provides a tutorial on how to use Maxim Prompt Chains to generate and translate product descriptions from images using AI, including adding translation support, processing translations with code, and connecting to external services.

- [Test your AI application via an API endpoint | Maxim](https://www.getmaxim.ai/docs/evaluate/quickstart/run-your-first-test-on-api-workflow#set-up-test-parameters): *   **One-line description:** This page provides a guide on testing AI applications via an HTTP API endpoint using a no-code approach.

- [Create a prompt version | Maxim](https://www.getmaxim.ai/docs/api/prompts/versions/post#authorization): *   **One-line description:** This page provides documentation for the Maxim API endpoint to create a new version of a prompt.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observability/evaluating-logs/auto-evaluation#2-evaluate): *   **One-line description:** Maxim is a platform that streamlines AI application development and deployment by providing tools for experimentation, evaluation, observation, and data management.

- [Overview | Maxim](https://www.getmaxim.ai/docs/evaluation/workflow/workflow-overview#2-evaluate): *   **One-line description:** This webpage introduces Maxim, a platform that streamlines AI application development and deployment by providing tools for experimentation, evaluation, observation, and data management.

- [Exploring LLM Approaches: Long-Context vs. RAG Models](https://www.getmaxim.ai/blog/llm-rag-compare): *   **One-line description:** This page discusses and compares long-context LLMs and RAG models for handling extensive text and complex queries, highlighting their strengths, weaknesses, and applications.

- [Create a Schema-based Prompt Tool | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-tools/create-a-tool-schema#testing-your-schema-tool): **One-line description:** This page describes how to create and test schema-based prompt tools using Maxim, which enforce specific output formats from LLMs.

- [Get Prompt Versions | Maxim](https://www.getmaxim.ai/docs/api/prompts/versions/get#authorization): *   **One-line description:** This page documents the API endpoint for retrieving versions of a prompt using the Maxim AI platform.

- [RAGEval: Evaluating RAG Systems in Finance, Healthcare & Legal](https://www.getmaxim.ai/blog/rageval-rag-eval): *   **One-line description:** This webpage describes RAGEval, a framework for automatically generating domain-specific evaluation datasets for Retrieval-Augmented Generation (RAG) systems, particularly in finance, healthcare, and legal domains.

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai/demo?utm_campaign=blog_post&utm_content=RAGEval&utm_medium=social&utm_source=content): **One-line description:** This page is a landing page for a GenAI evaluation and observability platform, offering a demo and contact information for urgent inquiries.

- [Optimize RAG Framework: Best Practices & Techniques](https://www.getmaxim.ai/blog/rag-best-practices): *   **One-line description:** This page discusses best practices and techniques for optimizing Retrieval-Augmented Generation (RAG) frameworks, based on the research paper "Searching for Best Practices in Retrieval-Augmented Generation."

- [The GenAI evaluation and observability platform](https://www.getmaxim.ai?utm_campaign=blog_post&utm_content=rag-retrieval&utm_medium=social&utm_source=content): *   **One-line description:** This page is about Maxim, an AI evaluation and observability platform that helps teams build and deploy AI agents faster and more reliably.

- [Maxim Blog (Page 7)](https://www.getmaxim.ai/blog/page/7): *   **One-line description:** This webpage is the seventh page of the Maxim Blog, featuring articles related to Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs).

- [Get Prompt Versions | Maxim](https://www.getmaxim.ai/docs/api/prompts/versions/get#query-parameters): *   **One-line description:** This page documents the API endpoint for retrieving versions of a prompt using the Maxim AI platform.

- [Overview | Maxim](https://www.getmaxim.ai/docs/sdk/observability/concepts#3-observe): *   **One-line description:** This page provides an overview of Maxim, a platform designed to streamline AI application development and deployment through experimentation, evaluation, observability, and data management tools.

- [Create an API-based Prompt Tool | Maxim](https://www.getmaxim.ai/docs/library/how-to/prompt-tools/create-an-api-tool#example): *   **One-line description:** This page explains how to create API-based Prompt Tools in Maxim, converting API payloads into function schemas for use with LLMs.

- [Graph RAG: Enhanced Retrieval for Complex Queries](https://www.getmaxim.ai/blog/graph-rag): *   **One-line description:** This page discusses Graph RAG, a novel approach to Retrieval-Augmented Generation that uses knowledge graphs to improve performance on complex queries requiring global summarization.

- [Overview | Maxim](https://www.getmaxim.ai/docs/sdk/observability/concepts#4-data-engine): *   **One-line description:** This page provides an overview of Maxim, a platform designed to streamline AI application development and deployment with tools for experimentation, evaluation, observability, and data management.

- [Overview | Maxim](https://www.getmaxim.ai/docs/evaluation/workflow/workflow-overview#3-observe): **One-line description:** This page provides an overview of Maxim, a platform designed to streamline AI application development and deployment by offering tools for experimentation, evaluation, observation, and data management.

- [Deploy Prompt Chains | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/deploy-prompt-chains#fetching-prompt-chains-via-sdk): *   **One-line description:** This page explains how to deploy and manage prompt chains in Maxim, allowing for easy experimentation, deployment without code changes, and conditional updates based on custom variables.

- [Generate and translate product descriptions with AI | Maxim](https://www.getmaxim.ai/docs/evaluate/how-to/evaluate-chains/create-product-description-generator#create-prompt-that-generates-product-description): *   **One-line description:** This page explains how to use Maxim Prompt Chains to generate and translate product descriptions using AI, including adding translation support, data processing, and connecting to external services.

- [Get trace by ID | Maxim](https://www.getmaxim.ai/docs/api/log-repositories/traces/get): *   **One-line description:** This page describes how to retrieve a specific trace by its ID using the Maxim API.

- [Search logs in a log repository | Maxim](https://www.getmaxim.ai/docs/api/log-repositories/search/post): *   **One-line description:** This page documents the API endpoint for searching logs within a log repository using the Maxim AI platform.

- [Test multi-turn AI conversations | Maxim](https://www.getmaxim.ai/docs/evaluate/quickstart/simulate-and-evaluate-multi-turn-conversations#configure-your-http-endpoint): **One-line description:** This page describes how to use Maxim's Workflows to automatically test AI chat interactions using conversation simulation.

- [Overview | Maxim](https://www.getmaxim.ai/docs/observability/evaluating-logs/auto-evaluation#4-data-engine): * **One-line description:** This page introduces Maxim, a platform designed to streamline AI application development and deployment by offering tools for experimentation, evaluation, observation, and data management.

- [Agentic Systems Evaluation: Agent-as-a-Judge Methodology](https://www.getmaxim.ai/blog/agent-evaluation): **One-line description:** This page describes the "Agent-as-a-Judge" methodology for evaluating agentic systems using other agentic systems, along with the DevAI dataset for evaluating evaluators, and compares its performance to human and LLM judges.

- [Track token usage and costs | Maxim](https://www.getmaxim.ai/docs/observe/how-to/log-your-application/track-token-usage-and-cost#code-examples-by-language): *   **One-line description:** This page explains how to track token usage and costs in LLM applications using Maxim, including code examples and custom pricing options.
