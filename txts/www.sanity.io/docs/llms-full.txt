# AI-accelerated content creation

#### Explore Canvas

[Introduction to Canvas](/docs/canvas/canvas-user-guide)

[Content mapping for Canvas](/docs/canvas/canvas-content-mapping)

[Configure content mapping](/docs/canvas/configure-content-mapping)





# Introduction

## Canvas at a glance

Sanity Canvas is a powerful writing tool that combines the best of human creativity and AI assistance to help you craft compelling, well-informed content. 

At its core, Sanity Canvas is designed around two key principles:

4. **Context is king**: Attach relevant notes, research, and guidelines to your document. Canvas ensures that your writing is always grounded in the specific context of your project. 
[Read more about notes](#k051cf71039bf)
4. **AI as collaborator**: Canvas' built-in AI is more than just a text generator. It's an intelligent co-author that leverages the power of large language models while staying anchored in the context you provide. 
[Read more about the embedded AI](#k8cde9e5037d7)

Whether you're drafting a blog post, crafting product copy, or putting together a comprehensive report, Canvas empowers you to produce your best work more efficiently. 

## Feature walk-through video 

![Video](https://stream.mux.com/vCtyUNV00Et6kHsOOhqLfn29NgBOTF5rK)

To get started you can watch a short video introducing the most important features, or get into the documentation by [taking a tour of the Canvas interface](#8552fa172bc7), or keep on reading to get your hands dirty straight away with a practical example.

[Canvas – This is how we write now](https://sanity.io/create)

[Content Mapping - Introduction](/docs/canvas/canvas-content-mapping)

[Content mapping - Setup and configuration](/docs/canvas/configure-content-mapping)



## Quickstart Example – Drafting a blog post

In this follow-along example you'll be writing a blog post with some select travel advice for spud aficionados. 

- Go to the Canvas app in your organization dashboard and create a new document ([Help](#k5d8dafd1d613)). Title it *"The best travel destinations for potato lovers"*.
- Start off by making a note. Notes are where you keep all your research, contextual information and other supporting material. In the notes panel on the left side, find the button labeled **Create your first note** and click it.

![The Canvas interface with the Notes panel open](https://cdn.sanity.io/images/3do82whm/next/41eb2a8e6226dac65d16692c6ec0ea0f2e541fd5-5348x1858.png)

- To start off with some solid information, paste this list of potato museums around the world into your note. The AI assistant will draw from your notes when making suggestions.

```markdown
# List of potato museums

## Austria 

Waldviertler Erdäpfelwelt (Waldviertler potato world) is a museum with interactive displays located in the town hall of Schweiggers displaying the history and uses of potatoes to the present day.

## Belgium 

Musée vivant de la pomme de terre ("Living Museum of the Potato") in Genappe is part of the Wallonia Botanical Gardens and also houses a collection of onions from northern Europe.

Frietmuseum in Bruges is dedicated to chips (or fries in American-English) and is located in one of Bruges' oldest buildings, dated 

## Canada

The Canadian Potato Museum in O'Leary, Prince Edward Island, claims to contain the world's largest collection of potato artifacts. It is also home to a Potato Hall of Fame. A 14-foot (4.3 m) high giant potato made of fiberglass stands at the entrance and visitors can learn about the origins of the wild potato up to modern-day agricultural practices.
Potato World is a museum dedicated to the potato. It is located in Florenceville-Bristol,New Brunswick, known as the french fry capital of the world.

## Denmark 

Danmarks Kartoffelmuseum ("Danish Potato Museum") in Otterup is part of the Hofmansgave estate. The Hofmansgaves were responsible for popularising the potato in Denmark where potatoes were known as "German lumps".

## France 

Moulin Gentrey in Harsault (fr) is a former starch mill dating from 1870 which contains a small potato museum as part of a historical tour of starch making for the textile industry.

## Germany 

Deutsches Kartoffelmuseum ("German Potato Museum") in Fußgönheim (de) is housed in a former synagogue next to the Fußgönheim Agricultural Museum. The museum dates from 1987.Das Kartoffelmuseum ("The Potato Museum") in Munich dates from 1996 and is run by the Otto Eckart Foundation on behalf of Pfanni GmbH, a division of Unilever.Vorpommersches Kartoffelmuseum ("Potato Museum of Vorpommern") in Tribsees (de)

## Italy 

Museo della patata ("Potato Museum") in Budrio

## Lithuania

 There is a Bulvės muziejus ("Potato Museum") in Kudirkos Naumiestis, near the border with Kaliningrad.

## United States 

Potato Museum in Albuquerque, New Mexico, was originally housed in the basement of E. Thomas and Meredith Hughes' basement in Washington, D.C., before moving to Albuquerque in 1993. It began in 1975 and is a nonprofit organization.Idaho Potato Museum is in Blackfoot, Idaho, and among the exhibits has the world's largest potato chip (crisp in British-English), a Pringle measuring 25 inches (64 cm)
```

- Doing so, you'll notice that the Canvas AI has helpfully named our note and classified it as a **Fact.** If your result was different, bring up the note category menu and change it to **Fact**.

![Shows the note category menu opened](https://cdn.sanity.io/images/3do82whm/next/d90d6f5af6d37239e981fed52ee19c1e1052774b-5348x1858.png)

- Before moving on to the document editor to do some writing, create one more note with some context for the task at hand. Once again, click **Create note** in the side panel on the left, and paste the following into it:

> `We are writing a blog post featuring exciting destinations for potato-aficionados.`

![Shows an instruction block in Sanity Canvas](https://cdn.sanity.io/images/3do82whm/next/7a3b4605f984b3c65654a59917f6099cd5b3e69f-5348x1858.png)

This will inform the AI assistant about your goal and intention, which in turn will enable it to make better suggestions. Make sure this new note is classified as **Context**.

- With your notes ready, move your focus to the document editor. This huge blank space is sort of intimidating. Time to ask your AI assistant to help out. - Enter the following: "Write an outline for a blog post featuring 3 of the best travelling destinations for potato lovers". 
- With your cursor positioned at the end of the sentence, hit CMD-ENTER (CTRL-ENTER on Windows). This will turn the sentence into an instruction for the writing



![Shows an instruction block in Sanity Create](https://cdn.sanity.io/images/3do82whm/next/9241f89163e6b8a940eb7123a16977c31fe8d7a8-1528x744.png)

> [!TIP]
> Protip
> You can also create a new instruction by hitting the slash key / on an empty line. Slash commands are discussed in greater details further on in this article!

- The assistant, now primed with both the context of the task at hand and a source of information to draw from should be able to do a pretty decent first draft. From here, you can accept the incoming changes or fiddle around with the instruction to further hone the output.

![Shows the AI assistant's suggestion](https://cdn.sanity.io/images/3do82whm/next/7d234f112c4e9179a4912db5782d0bf96069861b-5348x3516.png)

- After accepting or discarding what the assistant provided, place your cursor at the end of a paragraph. Notice the subtle circle icon that follows you around? This is the AI contextual menu, affectionately known as "the Blip". Give it a click.

![a list of resources includes links to museums and attractions](https://cdn.sanity.io/images/3do82whm/next/b5760132cf9354471c70320592177b3e0d13328e-701x437.png)

- You'll be presented with a few options for interacting with the AI capabilities of Canvas. - **Ghostwrite** will tell the assistant to just pick a direction and go, letting you press TAB to tell it to keep writing after each sentence.
- **Show me options** will instruct the assistant to suggest a few different ways to proceed.
- **Rewrite paragraph** will tell the assistant to do another take on the current paragraph.



![Shows the Blip in "Show me options"-mode](https://cdn.sanity.io/images/3do82whm/next/4eeb70b58c8a746c58d2d52f188e53c74324f518-2124x962.png)



#### Next steps: 

- Experiment with notes: - Try to add a **Style** note to influence the tone of the blog post.
- Find a great piece of prose and add it as **Inspiration.**
- [Read more about notes](#b3c8c19dad0e)


- Keep exploring the AI capabilities:- Ask the AI to give you some different options on how to proceed.
- Use `CMD + .` (command period) to start the AI ghostwriter. Hit tab to continue once it pauses.
- [Read more about AI ](#8cde9e5037d7)


- Read the article on [Content Mapping](/docs/canvas/canvas-content-mapping) to learn about linking your Canvas documents to your [Sanity Studio](https://www.sanity.io/studio).
- Read on to learn more about Sanity Canvas.

## Touring the Canvas interface

### Documents

Documents are the core conceptual unit of Canvas. The document editor is where your work is done, providing a familiar, minimalist writing environment for you to craft your content, while the document sidebar is where you'll find all your existing notes, as well as notes shared with you by others and templates for new documents. Here's what you need to know about working with documents in Canvas:

#### Finding your documents

You can browse through all your existing documents with some helpful filters by opening the left sidebar. This is also where you'll find any templates created by you or other people in your organization. Templates are helpful starting points for new documents.

![The Canvas document list showing a list of all documents](https://cdn.sanity.io/images/3do82whm/next/d244d37deac2ff38b8e913cbdb9cc706dc80348d-5348x3516.png)

#### Creating a new document

You can quickly create a new document from either the sidebar or from within the document browser.

![Shows the menu option for creating a new document](https://cdn.sanity.io/images/3do82whm/next/7a3e9d544ed494944b75d5145d1f1033d0915bde-5348x1858.png)

#### Deleting documents

If you need to remove a document, you can do so from within the document editor view. Click the ellipsis menu in the top right corner, and then click delete. Note that this action is permanent, so be sure you truly want to delete the document before confirming.

![Shows the menu option for deleting the selected document](https://cdn.sanity.io/images/3do82whm/next/16487db1c5d09844941503d412fcffe26e75e7b6-5348x1858.png)

### The document editor

Canvas offers a clean, distraction-free writing environment that will feel instantly familiar to anyone who has used a modern word processor or text editor. The interface is designed to put your content front and center, allowing you to focus on getting your thoughts down without any clutter or unnecessary features getting in the way.

![Shows the document editor in distraction free mode](https://cdn.sanity.io/images/3do82whm/next/4a1ecc05023f7cc794c4049650b8b46fdbbe998e-5348x3516.png)

#### Slash commands and Markdown formatting

The writing space in Canvas supports a range of formatting options to help you structure and style your content. You can use familiar slash commands to quickly apply headings, lists, quotes, and more without taking your hands off the keyboard.

![Shows a contextual menu invoked by typing "/". It has options for different headings, blockquotes, and lists. Additionally it has an option labeled "Instruction"](https://cdn.sanity.io/images/3do82whm/next/01bd6f189f361321c828cfb9f307531e4d90a3c7-1528x996.png)

In addition to slash commands, Canvas also supports a subset of standard [Markdown](https://en.wikipedia.org/wiki/Markdown) syntax for formatting text. You can use hyphens or numbers to make lists, and hash symbols to denote headings. For those who prefer a more visual approach, basic formatting options like bold, italic, and underline are also available via buttons in a popover whenever text is selected.

![a text box that says notes are a key feature in sanity create that provide context facts style](https://cdn.sanity.io/images/3do82whm/next/5d65ef45e697d5fc4c7b34f28b929c6c6896c6ed-3246x930.png)

Whether you prefer slash commands, Markdown, or the toolbar, Canvas aims to make formatting your document quick, intuitive, and distraction-free. The goal is to let you focus on your writing while still having easy access to the tools you need to make your content clear, scannable, and visually appealing.

## Notes

Every writer knows that behind every polished piece of prose lies a mountain of notes, research, and insights that inform and guide the work, but never see the light of day in the final draft. Canvas is a writing tool that not only acknowledges, but celebrates and elevates this often unseen but essential part of the process.

![](https://cdn.sanity.io/images/3do82whm/next/570499883691e46fd4332154e5159a10da67259f-2800x1032.png)

Notes are a key feature in Canvas that provide context, facts, style guidelines, and inspiration to inform your writing and enable the built-in AI co-writer to make relevant and informed suggestions. By attaching relevant notes to your document, you give the AI the background knowledge and topical awareness it needs to be of actual help.

#### Show or hide the notes side panel

Your notes live in the side panel to the left of your main content editing area. If you don't see the side panel it might have been closed when someone needed to unclutter a bit. If this is the case you can open the side panel by clicking the button labeled **Notes** in the left sidebar area.

![Shows the note panel in open and closed states](https://cdn.sanity.io/images/3do82whm/next/cc8284912090fc99a1f4ce3988be7f98d902f7fa-5348x3516.png)

To resize the side panel, or hide it altogether, grab the divider between the sidebar and main editor area and drag it all the way to the left to hide the side panel, or resize it to your liking. Clicking the divider will also toggle between hiding or showing the side bar.

![Shows how hovering the divider makes it draggable](https://cdn.sanity.io/images/3do82whm/next/9b02f3bfd631b699ee87645128e8a0f671e626aa-5348x3516.png)

### Creating notes

To create a note, click the button with a **+** icon at the very top of the notes panel. You might proceed give your note a title and select a category, or just start writing. You can add content to your note in the form of text, images, URLs, or even PDF files. Canvas will do its best to determine the type of note (as will be discussed in the next section) and generate a suitable title. You can, of course, override the AI's suggestions in either case.

![Shows the note sidebar and the button to add a new note](https://cdn.sanity.io/images/3do82whm/next/bd8c46f28e5dbe77ad729424370f1325adeeecbd-537x213.png)

#### Different notes for different goals

Canvas offers four distinct types of note, each serving a different purpose:

- **Context Notes**: Provide high-level background information and framing for the document, such as project briefs, target audience details, or internal enablement material.
- **Fact Notes**: Contain specific data points, quotes, or pieces of information that should be treated as factual and incorporated into the content where relevant.
- **Style Notes**: Outline the desired voice, tone, and stylistic guidelines for the document, ensuring the ghostwriter adopts the appropriate tone and style for the piece.
- **Inspiration Notes**: Collect examples, analogies, or creative prompts to inspire the writing and infuse it with engaging elements.The AI assistant uses these notes to gain a deep understanding of the project at hand and tailor its suggestions accordingly.

### How notes inform the AI assistant

When you provide notes, the AI uses this information to guide its content generation. Context notes help the AI understand the big picture and overall purpose of the document. Fact notes ensure accuracy by providing specific data points to incorporate. Style notes allow the AI to adopt the appropriate voice and tone for the piece. And inspiration notes give the AI creative fodder to draw from, helping to make the writing more engaging and colorful.

> [!TIP]
> Protip
> When the AI receives contradictory information in your notes, the note that is located the highest in the side panel is the one that takes presedence. Top notes beat bottom notes.

In essence, notes allow you to have a "conversation" with the ghostwriter, providing it with the knowledge and direction it needs to be a truly useful writing assistant. The more relevant and specific the notes you provide, the better the ghostwriter can tailor its output to your needs. So take the time to curate your notes carefully - it will pay off in the quality of the content Canvas helps you produce.

![Shows a sidebar with a bunch of notes of every type](https://cdn.sanity.io/images/3do82whm/next/c1eff35473b62e248b780744f31d1738e278b272-2346x1858.png)

### Note actions

#### Moving, renaming, duplicating, and deleting notes

**Move** and re-arrange notes by dragging them directly in the notes panel. Right clicking any note in the list will open a contextual menu that will reveal additional options, including those to either **duplicate** or **delete **a specific note.

Once a note has been selected, **rename** it by clicking the title or alternatively do this in each note's dedicated context menu, located in the top right of each selected note.

## AI ghostwriter

Canvas' built-in AI writing assistant is more than just a text generator. It's an informed collaborator that leverages the power of Large Language Models (LLMs) while staying grounded in the specific context of your project. By feeding the AI relevant notes, research, and guidelines, you transform it from a mere automaton into an insightful co-author capable of producing content that aligns with your unique needs and objectives.

What sets the Canvas AI apart is its ability to understand and utilize the notes you provide. Whether it's high-level context about the project's goals, specific facts to incorporate, stylistic guidelines to follow, or creative inspiration to draw from, the AI takes it all into account when generating suggestions. This means the content it produces is not only fluent and coherent but also relevant and tailored to your project.

Think of the AI as your brainstorming partner. It can help you flesh out ideas, expand on sections, and even polish your writing to ensure it hits the right tone and style. But it's not a replacement for your own insights and expertise. The key is to use the AI as a tool to enhance and accelerate your writing process, not to fully automate it.

By combining the efficiency of AI with the directed context of your notes, Canvas empowers you to craft content that is both deeply informed and compellingly written. It's a new way of working that puts you in control while harnessing the power of artificial intelligence to boost your productivity and creativity as a writer.

### Capabilities

- Generating new content based on your notes and existing text
- Expanding on ideas and fleshing out sections
- Refining and polishing your writing
- Adapting to the style and tone specified in your notes
- Incorporating facts and inspiration from your notes

### Meet the Blip – How to use the AI  assistant

The AI assistant in Canvas can be interacted with in a number of ways, but the most readily apparent is the subtle circle icon that follows you around the document, affectionally known as "the Blip".

![Shows the AI contextual menu in its inactive and active state](https://cdn.sanity.io/images/3do82whm/next/b5760132cf9354471c70320592177b3e0d13328e-701x437.png)

Clicking the Blip will open a menu of different instructions to run in the context of your current position in the document.

#### Ghostwrite

The **Ghostwrite** option is your go-to for generating new content or expanding on existing ideas. When you select this option, the AI assistant will analyze your current position in the document, along with any relevant notes and surrounding context, to suggest a continuation of your writing.Depending on where your cursor is placed, the AI may suggest completing the current sentence, starting a new paragraph, or even beginning a new section with a relevant heading. The goal is to provide a seamless and contextually appropriate continuation that flows naturally from your existing content.

#### Show options

**Show options** presents you with a range of alternative suggestions for how to continue your writing. When you click this option, the AI will generate multiple possible paths forward based on your current context and notes.These options might include different ways to complete the current thought, introduce a new idea, or transition to a related topic. By offering a variety of suggestions, the AI allows you to explore different creative directions and choose the one that best fits your vision.

![Shows the AI contextual menu with suggestions on how to proceed](https://cdn.sanity.io/images/3do82whm/next/4eeb70b58c8a746c58d2d52f188e53c74324f518-2124x962.png)

#### Rewrite paragraph

This option will instruct the assistant to do another take on the current paragraph. You will be given the opportunity to give the assistant a short brief on what you'd like.

![Shows an instruction element with the resulting output under](https://cdn.sanity.io/images/3do82whm/next/f93b238a0b0fdb9b85dd894a34d86f1cbd58ec2b-727x741.png)

### It's bots all the way down –  AI assistant in notes 

![Shows the AI contextual menu in a note](https://cdn.sanity.io/images/3do82whm/next/47770fc2cbad4f41869226cccb1ede7c42b06ea9-540x342.png)

In addition to using the AI co-writer to help write your main document, you can also invoke it within individual notes for more targeted assistance.

You can select any text and hit CMD-Return to run the selected text as an instruction, or you can locate the **Blip**-button to access the same menu of options discussed in the previous section.

Some examples of instructions you could run inside a note:

- Summarize the key points of the note
- Identify any potential contradictions or inconsistencies
- Suggest additional facts or examples to include
- Rephrase the note in a different tone or style
- Answer a specific question based on the note's content

Once you submit your prompt, the ghostwriter will process the note and provide a response directly within the note editor. You can then choose to incorporate the ghostwriter's suggestions, modify them, or discard them as you see fit.

This in-note AI assistance can be incredibly useful when you need help refining or expanding on a particular piece of context without disrupting your main writing flow.


![Shows an AI instruction inside a note](https://cdn.sanity.io/images/3do82whm/next/a770bf0764658791dccee7149e14eaef05aef1a0-546x809.png)

#### Best practices

- The more relevant and specific your notes, the better the AI can tailor its output to your needs
- Break down complex topics into smaller sections and use the AI to help flesh out each part
- Don't rely solely on the AI - review, edit, and add your own insights and perspective
- Experiment with different prompts and note combinations to find what works best for your writing style and goals







# Automatic Content Mapping

> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

Canvas is a great tool for freestyle writing, but when it's time to put your creative output to work, you'll want to move everything into a structured environment where it can enjoy all of the benefits of treating content as data—Sanity Studio! 

> [!WARNING]
> Gotcha
> Some initial setup by a studio maintainer is needed to make content mapping work. Visit the article on configuring content mapping to learn more.

For example, you might sketch out a blog post in Canvas, and then connect your work to a new document in Sanity Studio of a specific content type—like a `blogPost`, with fields like `title`, `excerpt`, `body`, and `tags`. 

A pretty clever mapping agent—from here on lovingly referred to as "the bot"—will go to work in the background identifying which parts of your rich content in Canvas corresponds to which document fields in your studio and automatically mapping content appropriately. Subject to your overrides, of course. 

![A side by side view of Canvas and the Studio form](https://cdn.sanity.io/images/3do82whm/next/12a7a0278863030e0037ae130977b58feaf2ae27-5348x3516.png)

You also have the option of marking certain parts of your document as **context**, to make the bot ignore your "notes to self" and other non-content. You can even include little helpful pointers to the bot, like:  `// slug: my-cool-post` or `!! title below`. The bot will try to infer meaning and decide what is content and what is context. Anything it gets wrong, you can fix!

## Get started

### Locate your project in the studio panel

- In Canvas, look for the button in the top right corner labeled **Studio **or, on smaller screens, with **an icon resembling three boxes arranged in a diagram** (a schema!).

![A side by side view of Studio buttons](https://cdn.sanity.io/images/3do82whm/next/ea28be77f21b02a45d72e5d7642c856326596f16-602x335.png)

- Click the **Studio** button to reveal the **Studio** panel:

![The Studio panel in Canvas](https://cdn.sanity.io/images/3do82whm/next/86ebdb07bb1ab73bd890e6654b05342c1745ba0c-1276x1023.png)

- Find your project in the **Studio** menu. If not automatically selected for you, find your studio deployment and workspace in the appropriate dropdowns. Then, find the document type you want to map your content to. 

> [!WARNING]
> Gotcha
> Can't see any projects in the dropdown? You may have to contact the person or people responsible for maintaining the studio and ask them to enable content mapping in Sanity Studio.

![Shows the studio link panel, now populated with the appropriate details](https://cdn.sanity.io/images/3do82whm/next/a3394db81990857e917df2e629caa7eb26f2284e-963x1023.png)

### Select and apply a document type 

- With your document type selected, click the button labeled **Connect and start mapping ->** to proceed.
- The link panel will change to show a "minimap" of the selected content type, with its fields laid out in a tree structure. Fields with a little arrow on their left can be clicked to expand and reveal their values, or drill down deeper into nested fields.

![A mapped document with the Studio panel open](https://cdn.sanity.io/images/3do82whm/next/db2f76b43ac5950a1f1613979bf103893ed50747-2674x1758.png)

Note also the bottom right status indicator, which shows the mapping agent already making progress. It will keep working in the background, intelligently mapping your content to corresponding fields. 

## Exploring the Link Panel further

As your content is mapped, you'll see the minimap tree of document fields starting to fill out with content. You might also notice the colors changing as the mapping agent finishes with a field.

![A mapped document with the Studio panel open, with all fields unfurled](https://cdn.sanity.io/images/3do82whm/next/5c2ad5fd6ae5911c1c59a92c4e9c7c3f5935853e-2674x1758.png)

### Using colors to discern mapping state

![Green – Automatically mapped. Gray – Treated as context. Yellow – Manually mapped.  Black / white – Not yet analyzed.](https://cdn.sanity.io/images/3do82whm/next/c165edd6bd1216f399d6a998f6377747e4b3e64b-2800x1078.png)

As the content is mapped, you'll notice your screen getting progressively more colorful. Content that was automatically mapped to a field will be tinted **green**, as will the corresponding field in the link panel minimap, while anything the bot has decided is **context** will get a light gray color.

The **yellow**-colored field in the screenshot above indicates a field where an editor has actively overruled the suggested mapping and manually linked a bit of content to a field, while text in **black** or **white** (depending on whether or not the dark mode is active) indicates content that the mapping agent hasn't yet analyzed.

### Adjusting the results

While the automatic mapping is quite good (really!), you may at times want to manually adjust how your content in Canvas matches up with your studio schema. The tools you need to make these changes are at your fingertips.

- To map a content block, like a paragraph or an image, to a specific field, click on the item to reveal its context menu, and find the option to **Map to field... **as shown below.

![](https://cdn.sanity.io/images/3do82whm/next/dd704093b9fbf68888b1055b7cb34bf84a6bdc47-2800x1800.png)

- Selecting **Map to field...** will cause the interface to direct focus to the link panel, where you can select an appropriate field to map your selected content to. In the example below, we mapped the first image in the document to the **Cover Image > Asset** field. User-defined mappings are shown in yellow, instead of green for auto-mapped or gray for context.

![The studio panel when custom field mapping mode is enabled](https://cdn.sanity.io/images/3do82whm/next/73d809751a129e01dd1cbc3fa354a648a5bd5352-2674x1758.png)

- Similarly, if any content is mapped incorrectly, you can unmap it by clicking the **Clear mapping** button. Note that unless you explicitly reassign it as  context, the mapping agent will try to re-map on its next pass until everything has been neatly categorized with a color.

![](https://cdn.sanity.io/images/3do82whm/next/ff639d0b26090e9b2e1e7f827ff50f424c535428-2800x1078.png)

- For more granular control, you can select specific sentences or phrases and map them individually to Studio fields.

![](https://cdn.sanity.io/images/3do82whm/next/3e64eae6f1a1cec4ebf9d4ce94c1579e232eb1d2-719x277.png)

- As also demonstrated in the previous screenshot, leaving some contextual hints for the mapping agent can be quite effective. You can read more about this in the section on [content mapping tips and tricks](#block_40).

With these tools, you can control exactly how your content in Canvas will be translated into structured data in Sanity Studio. If you haven't already, this would be a good time to link your work to a new document in your studio.

## Link your work to a new document in your studio 

> [!TIP]
> Protip
> In this article, we’ve chosen to complete the mapping work first, and then create the studio document for narrative clarity. However, you’re free to do it the other way around—choose the workflow that suits you best!

Once you're happy with your mappings, find the button labeled **+ Link to new studio document** near the top of the studio panel.

![The link to new studio document button in the studio panel](https://cdn.sanity.io/images/3do82whm/next/f4c96172a668aa54a0103e80eb041fbaa40e2f12-1328x489.png)

Clicking it should result in a visual confirmation of success, and the button label changing to **Linked document**. Click it to open the connected studio with your new document selected. 

![A view of the studio once it's been linked to Canvas](https://cdn.sanity.io/images/3do82whm/next/930f33274d3a4b467219a8f98ebdcaca8a8c7bba-2674x1758.png)

You'll notice that your new document in the studio is in a read-only state while linked to its counterpart in Canvas. 

Any further changes you make in Canvas will be synced with the studio document automatically, so you can continue refining and expanding your content without worrying about manually transferring anything.

### Unlink your document from Canvas to edit it in Sanity Studio

As mentioned, your document will appear as **read-only** in your studio while linked to Canvas. You can think of this as the Canvas document being the **source of truth** for both versions while the link persists. In order to edit your document in Sanity Studio, you need to unlink it from its source in Canvas.

When the time comes, locate the **Unlink** button in the contextual menu next to your **Publish** button to sever the connection and edit your document in the studio. 

![The document context menu showing how to unlink a document from Canvas](https://cdn.sanity.io/images/3do82whm/next/4cce0bd87287bbac6d05eecb23608c4a79b1b3e1-817x677.png)

Clicking **Unlink** will cause a dialog to appear, informing you of the consequences. Confirm to dismiss it and unlock the document for editing in the studio.

![](https://cdn.sanity.io/images/3do82whm/next/266989fae1de6484671bbb46c8104b9fa1710a98-2800x1800.png)

> [!TIP]
> Protip
> Unlinking does not delete anything! You can keep on editing your document in Sanity Create, though the changes will no longer sync to the studio version. They are no longer connected.

## Content mapping tips and tricks

- **Procedural discovery!** The bot works procedurally on one bit of content at a time, but it can and will make several passes, so it might re-visit and re-evaluate mappings as it moves through your content. You can use this to your advantage by adding content hints above content blocks to quickly make the bot reconsider its choices. 

![](https://cdn.sanity.io/images/3do82whm/next/cc35c33aab4fa16ddce5b5e0ca7eb6fb63c2c365-720x463.png)

In the example above, the image was originally judged to be part of the blog post `body` field, but remapped to the `coverImage` field after some gentle nudging.

- **Provide some context!** Mark words, lines, or whole blocks as context to make the mapping agent treat your notes as notes that should not be mapped to any field. The bot will also read you context for clues on how to treat content, so feel free to be conversational. `// slug: my-cool-slug` or `[description below]` might do wonders. 

![](https://cdn.sanity.io/images/3do82whm/next/a8c1ee0678d7b9ede0c2007dd05a3fff99ef693b-2800x1078.png)

## Troubleshooting

### Can't find your project?

Make sure your studio has been [configured properly](/docs/canvas/configure-content-mapping) to allow Canvas to connect. This involves configuring and deploying the relevant studio.

### Can't see your content type, or content type is missing some fields

Make sure the relevant types or fields aren't configured to be excluded from content mapping. This, too, involves configuring and deploying the studio in question.

### The bot is making too many mistakes when mapping content

Try leaving some contextual clues to help the bot figure out what's what. There are no hard rules when it comes to what the bot will and will not pick up on, but as a general guideline: If it would be hard for a human co-author to catch your context notes, the bot will probably not do great either. Some examples:

- Partial mapping simple values with a simple inline instruction like `slug: my-cool-slug`
- Using headings as mapping clues for blocks
- Leave a note in plain text. `Note: Use this part for description`
- ... and if all else fails, manually adjust the mapping to get it just right

### I can't seem to map the title of my Canvas document to any field

Mapping the title is currently not possible, due to vague unspecified technical limitations. We're working on it!



# Migrating from Create

## In Create, locate the **Export to Canvas** option

While in[ Sanity Create](https://sanity.io/app/create), open the right sidebar menu and locate the option to export your documents to Canvas. It should be in the bottom section of the sidebar. 

![A screenshot of the "Export to Canvas" menu option](https://cdn.sanity.io/images/3do82whm/next/12437a59830a0994b0c319e780b1e1f73f956c8d-362x208.png)

## Select your organization

If you belong to more than one organization, choose the appropriate option from the list of organizations. If you don't belong to any organization yet, you have the option to create one.

![A screenshot of the organization list](https://cdn.sanity.io/images/3do82whm/next/9e81af61805cf48628c6dd383a7449e2374221fd-560x695.png)

> [!WARNING]
> Not all content will be exported
> As mentioned in the export dialog, some things won't be included in your exported documents. File assets, content mapping and studio link data, as well as comments do not carry over to Canvas.

## Select your documents, and start the export

Once you have chosen the appropriate organization to target for your exported content, select some or all of the available documents to bring with you.

![Screenshot of the document selection screen](https://cdn.sanity.io/images/3do82whm/next/f30d71ae06c1e4d774c0531256937f470dfef33b-620x695.png)

Once you are satisfied with your selection, click **Start Export ->**

A status view should appear to keep you up to date on the progress.

![Shows the export progress view](https://cdn.sanity.io/images/3do82whm/next/feb7a841529277b43b6d7643bcdfaf557c961249-579x343.png)

## Open dashboard and find your documents

Once the export progress is complete, click the button labeled **Open Dashboard**. The [dashboard should open with the Canvas tab active](https://sanity.io/welcome), and your documents should be listed.

![Shows a list of all documents in the dashboard](https://cdn.sanity.io/images/3do82whm/next/63f5d0ec09916d74506a978149e498c6f6973631-1295x988.png)



# Configure content mapping

> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

Content mapping between [Sanity Canvas](https://sanity.io/create) and [Sanity Studio](https://www.sanity.io/studio) is an excellent addition to every content team's toolbox. It grants the best of both worlds: 

- A free-form creative writing space with a context-aware writing assistant for the ideation phase 
- A powerful studio for structured content operations when it's time to put that content to work

All without content teams wasting precious time and bandwidth copying and pasting their work from one place to the other. 

Visit the [introduction to content mapping](/docs/canvas/canvas-content-mapping) to learn more about how content mapping works once implemented. Read on to learn how to set it up.

## Enable content mapping in Sanity Studio

Getting your studio ready for content mapping requires that you make your schema available to Canvas by storing a copy in your dataset. For most users, this should be a simple task of updating your studio and deploying it, but if your case is more complex you might want to consult [this article](/docs/apis-and-sdks/schema-deployment) which goes into all the details.

For Sanity-hosted studios:

- Make sure your project is updated to `v3.88.1` or later of Sanity Studio. `@latest` is always recommended! 
- Deploy your studio by running the command  `npx sanity deploy`
- If your studio is embedded or self-hosted, follow [this guide](/docs/dashboard/dashboard-configure) on getting set up for content mapping by onboarding your studio to Dashboard correctly.

## Configuring Schemas for Content Mapping

To tailor how Canvas handles your studio schema, you can use the `options.canvasApp` configuration that has been added to all schema types.

This configuration allows you to:

- Exclude specific types or fields from appearing in Canvas using `options.canvasApp.exclude`
- Provide additional context to the mapping agent about the intended purpose of a type or field using `options.canvasApp.purpose`

> [!TIP]
> Protip
> Excluding fields that aren't useful to edit in Canvas is beneficial in more than one way! You'll deliver a cleaner and more intuitive experience to your content team, and you'll avoid problems that can occur in Canvas when faced with overly complex schemas. The number of fields handled by content mapping is hard capped at 1000. If your document schema runs up against this limit, consider excluding certain fields.
> 
> Be particularly diligent with your exclusions for schemas that:
> 
> Are really big
> 
> Have a high number of types
> 
> Have big arrays of several different types

### Excluding Types and Fields

To prevent a document type from being selectable in Canvas, set the `exclude` option to `true`:

```typescript
import {defineType} from 'sanity'

export default defineType({
  name: 'policy',
  type: 'document',
  description: 'Super-sensitive stuff',
  options: {
    canvasApp: {
      exclude: true
    },
  },
  fields: [
    // ...
  ]
});
```

Similarly, you can exclude specific fields within a document type by setting `options.canvasApp.exclude` to `true` on the field level:

```typescript
import {defineField, defineType} from 'sanity'

export default defineType({
  name: 'article',
  type: 'document',
  fields: [
    defineField({
    name: 'internalNotes',
    type: 'text',
    options: {
      canvasApp: { exclude: true }
      }
    }),
    // ...     
  ]
});
```

In this example, the `article` type is still available in Canvas, but the `internalNotes` field will not be shown or possible to target for mapping.

### Adding Context with Purpose

The `options.canvasApp.purpose` option allows you to provide additional context to the mapping agent about the intended purpose or usage of a specific type or field. This can help the agent make more accurate decisions when mapping content from Canvas to your studio schema.

For example, if you have a `tags` field in your schema that’s intended specifically for SEO keywords rather than general content categorization, you can clarify this using the purpose option:

```typescript
import {defineField, defineType} from 'sanity'

export default defineType({
  name: 'article',
  type: 'document',
  fields: [
    {
      name: 'tags',
      type: 'array',
      of: [{type: 'string'}],
      options: {
        canvasApp: {
          purpose: 'SEO keywords to improve search visibility, not general categorization tags.',
        },
      },
    },
    // ...
  ],
})

```

Consider using the `purpose` option when added clarity would be helpful. Often, the automatic mapping will get it right, so give it a try first and add `purpose` details only if needed to refine the mapping. 





# Processing and enhancing content workflows

#### Automate content processes

[Functions](/docs/compute-and-ai/functions-introduction)

[GROQ-powered webhooks](/docs/compute-and-ai/webhooks)



#### Automate with AI

[Agent Actions](/docs/agent-actions)

[Generate quick start](/docs/agent-actions/generate-quickstart)

[Translate quick start](/docs/agent-actions/translate-quickstart)



#### Add semantic search

[Embeddings Index API](/docs/compute-and-ai/embeddings-index-api-overview)



#### Integrate with AI applications

[Introducing Sanity MCP Server: Connect your content to AI tools](/blog/introducing-sanity-model-context-protocol-server)





# Introduction

Functions enable you to execute custom logic whenever changes occur in your content—all without requiring your own infrastructure. 



With Functions, you can:

- Enrich, validate, and constrain your content in new ways.
- Create complex workflows.
- Connect changes in your content to external applications and services. Refresh a CDN cache, trigger social posts, update inventory, and more.

[Functions quick start](/docs/compute-and-ai/function-quickstart)



## Requirements

- Functions run on **Node.js v22.x**. We encourage you to use the same version in local testing to avoid unsupported features or syntax changes.
- `sanity` CLI v3.92.0 or higher is required to interact with Blueprints and Functions. You can always run the latest CLI commands with `npx sanity@latest`.

## Core concepts

### Functions

Functions are small, single-purpose pieces of code that run on Sanity's cloud infrastructure. They act on changes in your content and allow you to extend the capabilities of your existing content management workflows. Functions are linked to projects, but not datasets. This means the same functions will run across all of your datasets.

When changes in your content trigger a function, they pass along details about the document. You can further refine this with a GROQ projection, ensuring your function has the data it needs.

Functions can access the full range of Sanity's APIs so you can interact with all of your content, not just the document that triggered the change.

All function code resides in a dedicated directory for each individual function.

```text
marketing_site/
├─ studio/
├─ next-app/
├─ functions/
│  ├─ my-function/ <-- directory matches the function name
│  │  ├─ index.ts
│  │  ├─ package.json
│  │  ├─ node_modules/

```

To keep functions slim and organized, treat them like small projects. Add packages directly to each function's directory.

### Blueprints

A function on its own doesn't know much about the larger Sanity ecosystem. That's where Blueprints come in. A blueprint is a template that describes Sanity resources. For Functions, blueprints describe when and where your function should trigger. 

#### Learn more about Blueprints

[Blueprints introduction](/docs/compute-and-ai/blueprints)



### Event-driven workflow

Functions work on an event system. They react to changes in your data. Did an editor publish a new document? Run a function. New comments on a draft? Run a function.

> [!NOTE]
> Limited events at launch
> At this time, Functions support activating when a document is Published. Support for more event types is coming in the future.

### Testing and logging

Your development process might need some adjustments to work with functions. As they run remotely on Sanity's infrastructure, you'll rely on local test commands and checking the logs from the CLI to debug your function logic.

### Usage and cost considerations

Functions use three variables when calculating cost.

- Invocations: The total number of times your function runs.
- Memory: The amount of memory a function uses to run. This defaults to 1GB, but you can adjust it up to 10GB in the blueprint configuration for each function.
- Duration: The execution time of the function. 

Memory and duration combine to to give a GB-second calculation. For example, a function with 1GB of memory that runs for 2 seconds is 2GB-seconds. Multiply that by the number of total invocations, and you have your total GB-seconds. 

As another example, if your functions average 1GB in memory-size and 40ms in duration, you could run 500k invocations to reach 20K GB-seconds.

Every function will be different, and your total usage accounts for all of your organization's functions. [Learn more on the pricing page](https://www.sanity.io/pricing).

## Limitations

### Max function size

**Limit**: 200MB

Although your individual function's TypeScript or JavaScript code may appear small, it can rapidly expand in size when packages are included.

We strongly suggest keeping your functions small. The larger the function, the slower it is. You can limit the size, and therefore increase the execution speed by:

- Limit dependency usage to only what's necessary.
- Choose performant, slim libraries.
- If you must use large libraries, consider bundling or tree-shaking in advance.

If your functions require too many dependencies, it may help to narrow their purpose and split the logic into multiple functions.



### Max function execution time

Functions default to a max execution time of 10 seconds. In the [Blueprint configuration](/docs/specifications/blueprint-config), this can be configured from 1 to 900 seconds.



### No robot token support for deployment

At this time, you should only deploy Blueprints with personal authentication tokens. You can still use robot tokens inside a function



# Quick start

Functions allow you to run small, single-purpose code whenever your content in Sanity changes. This guide explains how to set up your project, initialize your first blueprint, add a function, and deploy it to Sanity's infrastructure.

> [!CAUTION]
> Avoid recursive loops
> At this time, functions don't prevent recursive loops. Use caution when writing functions that may trigger themselves. For example, don't publish a document with the client that meets the same criteria as the document that triggered it.
> 
> If you initiate a recursive function, your organization may be rate-limited for the remainder of the month. If you think you've deployed a recursive function, immediately override the deployment with new code, or destroy the blueprint.

Prerequisites:

- `sanity` CLI v3.92.0 or higher is required to interact with Blueprints and Functions. You can always run the latest CLI commands with `npx sanity@latest`.
- Node.js v22.x. We highly suggest working on this version as it is the same version that your functions will run when deployed to Sanity.
- An existing project and [a role with Deploy Studio permissions](/docs/user-guides/roles) (the `deployStudio` grant). 

## Set up your project

To create your first function, you need to initialize a blueprint. Blueprints are templates that describe Sanity resources. In this case, a blueprint describes how your function and how it will respond to updates in your Sanity project. We recommend keeping functions and blueprints a level above your Studio directory. 

For example, if you have a Marketing Website that uses Sanity, you may have a structure like this:

```text
marketing-site/
├─ studio/
├─ next-app/
```

If you initialize the blueprint in the `marketing-site` directory, functions and future resources will live alongside the `studio` and `next-app` directory.

> [!TIP]
> Functions and Blueprints match your workflow
> While the example is our preferred way to organize Blueprints, Functions, and Sanity projects, you can initialize your first blueprint wherever you like. You can even store them inside your Studio directory.

## Create a blueprint

Initialize your first blueprint with the `init` command. It will prompt you for the following:

- Format: This guide uses **TypeScript**. For details on the other configuration types, check the [Blueprint reference](/docs/specifications/blueprint-config).
- Project: Select your organization and project. This is the project that will trigger functions.

```sh
npx sanity@latest blueprints init
```

This creates a few configuration files in your project directory. If you selected TypeScript or JavaScript, it prompts you to install the dependencies. Follow the prompt and run the install command for your package manager of choice.

## Create a function

Use the `sanity blueprints add function` command to add a new function. The CLI prompts you for the following:

- **Function name**: Set a name for your function. This determines the function name and the name of its directory. For this example, we'll call it "log-event".
- **Function type**: Select the document action that will trigger the function. For this example, select **Document Publish**.
- **Function language**: Select your preferred language. For this example, we'll select **TypeScript**.

```sh
npx sanity blueprints add function
```

After defining the name and selecting a type, follow the prompt and add the function declaration to your `sanity.blueprint.ts` configuration. Your file should look like this:

```
import {defineBlueprint, defineDocumentFunction} from '@sanity/blueprints'

export default defineBlueprint({
  resources: [
    defineDocumentFunction({name: 'log-event'}),
  ],
})

```

If you've followed the directory structure mentioned earlier, you'll see it grow to something like this:

```text
marketing-site/
├─ studio/
├─ next-app/
├─ sanity.blueprint.ts
├─ functions/
│  ├─ log-event/
│  │  ├─ index.ts
│  │  ├─ package.json
│  │  ├─ package-lock.yaml
│  │  ├─ node_modules/

```

After updating the `sanity.blueprint.ts` file, open `functions/log-event/index.ts` in your editor. 

> [!TIP]
> The `documentEventHandler` function
> TypeScript functions can take advantage of the documentEventHandler helper function to provide type support. Examples in this article include both TypeScript and JavaScript function syntax.

Every function exports a `handler` from the index file.

```
import { documentEventHandler } from '@sanity/functions'

export const handler = documentEventHandler(async ({ context, event }) => {
  const time = new Date().toLocaleTimeString()
  console.log(`👋 Your Sanity Function was called at ${time}`)
})
``````javascript
export async function handler({context, event}) {
  const time = new Date().toLocaleTimeString()
  console.log(`👋 Your Sanity Function was called at ${time}`)
}
```

The handler receives a `context` and an `event`. The context contains information to help you interact with your Sanity datastore, such as `clientOptions` to configure a `@sanity/client`. 

The `event` contains information about the action that triggered the function. Most functions will use `event.data`, which contains the contents of the Sanity document. You can learn more in the [Function handler reference](/docs/specifications/function-wrapper).

### Limit the scope with GROQ

As configured, this function will run every time any document publishes and return the entire document to `event.data`. This includes system documents. You'll almost always want to scope your functions to specific scenarios and document types with GROQ filters.   

Open the `sanity.blueprint.ts` file and update it to include an `event` object with the `on` and `filter` properties:

```typescript
import {defineBlueprint, defineDocumentFunction} from '@sanity/blueprints'

export default defineBlueprint({
  resources: [
    defineDocumentFunction({
      name: 'log-event',
      event: {
        on: ['publish'],
        filter: '_type == "post"'
      }
    }),
  ],
})
```

The `on` property takes an array of trigger events. Currently, "publish" is the only valid event type.

`filter` accepts a GROQ filter that limits which documents will trigger the function. Only include the filter contents, *the portion inside the square brackets*, of your GROQ query. For example, rather than `*[_type == 'post']`, only include _`type == 'post'`. 

You can also include a `projection` property on `event` to shape the contents passed to the event. Like the filter, only include the contents of the projection. Omit the curly brackets(`{}`).

```typescript
import {defineBlueprint, defineDocumentFunction} from '@sanity/blueprints'

export default defineBlueprint({
  resources: [
    defineDocumentFunction({
      name: 'log-event',
      event: {
        on: ['publish'],
        filter: '_type == "post"',
        projection: "_id, content"
      }
    }),
  ],
})
```

Projections don't limit what fields trigger the function, only which data is passed into the function.

> [!WARNING]
> Limited GROQ support
> Unlike webhooks, you cannot use Delta GROQ in Function filters at this time.

## Test the function locally

You can test functions locally with the `functions test` command. Local testing is a great way to experiment without affecting your usage quota.

To test a function without passing in any data, run the following:

```sh
npx sanity functions test log-event
```

If you run this on the starter function from earlier, you'll see a response similar to the following:

```text
Logs:
👋 Your Sanity Function was called at 11:48:21 AM
```

You can also pass in JSON as a file (`--file`) or data string (`--data`) to substitute the `event` payload. The file, or data string, should be in JSON format and match the expected input from the event. For example `{ "data": { ... } }`. Make sure your sample data matches the expectations of your filter and projection.

```sh
npx sanity functions test log-event --file sample-document.json
``````sh
npx sanity functions test log-event --data '{ "_type": "post", "_id": "123456", "content": "test content" }'
```

> [!TIP]
> Capture a document for easier testing
> To make testing easier, it's helpful to capture the contents of an example document. You can do this from the document view in Studio by selecting "..." from the top right, selecting inspect, and then selecting Raw JSON. Copy the contents and save them to a JSON file. Now you can pass this to the function with the test command.
> 
> If you're using the project structure from the beginning of this guide, you can navigate down into your studio directory and run:
> 
> npx sanity@latest documents get "<document-id>" > ../sample-document.json
> 
> Replace with a document ID, and this will create a sample-document.json file in the directory above your studio. Don't forget to cd .. back up to the parent after.

Update your function to log the `event` and you'll see the data/file contents in the *Logs* part of the output the next time you run the test command.

```typescript
import { documentEventHandler } from '@sanity/functions'

export const handler = documentEventHandler(async ({ context, event }) => {
  const time = new Date().toLocaleTimeString()
  console.log(`👋 Your Sanity Function was called at ${time}`)
  console.log('Event:', event)
})
``````javascript
export async function handler({context, event}) {
  const time = new Date().toLocaleTimeString()
  console.log(`👋 Your Sanity Function was called at ${time}`)
  console.log('Event:', event)
}
```

Here is an example command and result from the updated function:

```sh
npx sanity functions test log-event --data '{ "_type": "post", "_id": "123456", "content"
: "test content" }'
``````text
Logs:
👋 Your Sanity Function was called at 10:23:22 AM
event: { data: { _id: '123456', content: 'test content' } }
```

> [!TIP]
> Development playground
> In addition to the sanity functions test command, there's also a more interactive development playground available. 
> 
> Run the sanity functions dev command to start a local test server where you can select projects and datasets, change test data, and view the results.

## Deploy a function

Once you're satisfied that the function works as expected, you can deploy it by deploying the blueprint.

```sh
npx sanity blueprints deploy
```

You can begin using your function when the deployment is finished. If you set a filter earlier, edit a document that matches it and publish the changes to trigger the function. 

If you need to change the function, update your code and re-run the deploy command to push the new changes live.

## Check the logs

When you tested the function locally, you saw the logs directly in your console. Once deployed, the function and its logs are in the cloud.

View the logs with the `functions logs` command. Replace `log-event` with your function name.

```sh
npx sanity functions logs log-event
```

This command outputs the function's logs. Try updating your document, publishing the change, and running the command again to see new logs.

> [!NOTE]
> System documents
> If you didn't limit the scope of the function by setting a GROQ filter earlier, every change to a published document will run the function. This can greatly increase your usage, so it's best to create specific filters for your documents.

## Destroy a deployed blueprint

Sometimes you want to remove a deployed function so it won't run anymore or affect any future usage quotas. The `blueprints destroy` command removes, or undeploys*,* the blueprint and all of its functions from Sanity's infrastructure. It does not remove your local files. 

Remove the test function:

```sh
npx sanity blueprints destroy
```

To remove the function from the blueprint locally, you can remove it from the `resources` array in the `sanity.blueprint.ts` file, then delete the `log-event` folder from the `functions` directory.

### Redeploying a destroyed blueprint

When you run `blueprints destroy`, it's as if you never used `blueprints init` during setup. The only difference is you still have all the files in your directory. To use this blueprint again and redeploy it, you'll need to let Sanity know about it. You can do this by running the following:

```sh
npx sanity blueprints config --edit --test
```

This launches an editing interface that lets you reconfigure the blueprint, if needed, and it reconnects the blueprint to Sanity. Now you can add more functions or redeploy. Keep in mind that any environment variables added before destroying the blueprint will not carry over.



# Add environment variables

Environment variables allow you to keep secrets, like tokens or API keys, hidden and out of version control. Functions allow you to manage environment variables from the CLI so they are available to your deployed functions.

In this guide, you'll learn to add environment variables and access them from within your function code.

Prerequisites:

- Complete the [Functions quick start](/docs/compute-and-ai/functions-introduction), or be comfortable creating and deploying a function.
- `sanity` CLI v3.88.1 or higher is required to interact with Blueprints and Functions. You can always run the latest CLI commands with `npx sanity@latest`.

## Create a function

If you don't already have a Blueprint and function set up, create them now.

Initialize a Blueprint:

```sh
npx sanity@latest blueprints init
```

Add a function:

```sh
npx sanity blueprints add function
```

In this example, we'll set the function to trigger on **Document Publish**, use **TypeScript**, and set the name to **envExample**.

```text
✔ Enter function name: envExample
✔ Choose function type: Document Publish
✔ Choose function language: TypeScript
```

This creates a function in the `functions/envExample` directory.

## Develop locally

Environment variables aren't available locally, but you can simulate them by appending the variable and value to your CLI commands. 

Start by updating the function to display the variable. For this example we'll reference a variable called `SANITY_SECRET_SAUCE`.

```
import { documentEventHandler } from '@sanity/functions'

export const handler = documentEventHandler(async ({ context, event }) => {
  console.log(`The secret: ${process.env.SANITY_SECRET_SAUCE}`)
})
``````javascript
export async function handler({context, event}) {
  console.log(`The secret: ${process.env.SANITY_SECRET_SAUCE}`)
}
```

All environment variables are accessible on `process.env`.

To test the function's access to a variable, we'll append it to the CLI command.

```sh
SANITY_SECRET_SAUCE="content operating system" npx sanity functions test envExample
```

If everything worked, you'll see this output:

```text
Logs:
The secret: content operating system
```

Now that we know it works locally, let's make it work on Sanity's infrastructure.

## Add an environment variable

Before you can add environment variables, you need to deploy the blueprint.

```sh
npx sanity blueprints deploy
```

With the blueprint deployed, you can add environment variables to the function.

Add them with the `functions add env <function-name> <variable-name> <variable-value>` command.

```sh
npx sanity functions env add envExample SANITY_SECRET_SAUCE "content operating system"
```

Now make changes to your documents, and check the logs for the function. You should see a similar log to the one from the local test.

```sh
npx sanity functions logs envExample
```

You've now deployed and accessed an environment variable from a function.

As with other examples, if you're done with this function and blueprint, it's a good practice to `destroy` it to prevent unexpected billing.

```sh
npx sanity blueprints destroy
```

## Additional `env` commands

As we saw with the command earlier, environment variables are linked to individual functions. In addition to `add`, you can use the following commands to interact with them:

- `sanity functions env list <function-name>`: List all environment variables for the given function.
- `sanity functions env remove <function-name> <variable-name>`: removes the variable from the deployed function.

For additional usage information, add `--help` after each CLI command. You can read more about the CLI in the [Functions CLI reference](/docs/cli-reference/functions).



# Configure @sanity/client

Functions have the ability to connect back to Sanity and manipulate not only the incoming document, but any document in your dataset. By including details about your project in the context, Functions enable you to configure a `@sanity/client` instance and interact with any Sanity API.

In this guide, you'll learn to install and configure `@sanity/client` in a Sanity Function. Then you'll use it to make interact with your project dataset.

Prerequisites:

- Complete the [Functions quick start](/docs/compute-and-ai/function-quickstart), or be comfortable writing and deploying a Sanity Function.
- `sanity` CLI v3.88.1 or higher is required to interact with Blueprints and Functions. You can always run the latest CLI commands with `npx sanity@latest`.

## Create a function

If you don't already have a Blueprint and function set up, create them now.

Initialize a Blueprint:

```sh
npx sanity@latest blueprints init
```

Add a function:

```sh
npx sanity blueprints add function
```

In this example, we'll set the function to trigger on **Document Publish**, use **TypeScript**, and set the name to **clientExample**.

```text
✔ Enter function name: clientExample
✔ Choose function type: Document Publish
✔ Choose function language: TypeScript
```

This creates a function in the `functions/clientExample` directory.

## Add `@sanity/client` to the function

Functions are self-contained. To install the Sanity client, first navigate into the `clientExample` directory so that you're in the same directory as the `index.ts` file. 

Next, install `@sanity/client`:

```sh
npm install @sanity/client
```

Once the install completes, open the function's `index.ts` file and update the starter code. 

- Import `createClient` .
- Use `context.clientOptions` to configure the client.
- Make and log a request to the API to test the client.

```
import { documentEventHandler } from '@sanity/functions'
import { createClient } from '@sanity/client'

export const handler = documentEventHandler(async ({ context, event }) => {
  const client = createClient({
    ...context.clientOptions,
    apiVersion: '2025-05-08',
  })
  const posts = await client.fetch('*[_type == "post"]')
  console.log(posts)
})
``````javascript
import { createClient } from '@sanity/client'
export async function handler({context, event}) {
  const client = createClient({
    ...context.clientOptions,
    apiVersion: '2025-05-08',
  })
  const posts = await client.fetch('*[_type == "post"]')
  console.log(posts)
}
```

The context includes a `clientOptions` object with details about your project, dataset, and a robot token. 

> [!TIP]
> Obfuscated tokens
> Logs will obfuscate clientOptions.token by replacing parts of the token with asterisks. This is limited to logging, and you can safely use the token to make API calls.

Read more about clientOptions in the [handler reference](/docs/specifications/function-wrapper).

Use `clientOptions` along with any preferred settings to create a new client. Aside from the values included with `clientOptions`, you must also set your preferred `apiVersion`. 

Additional client-specific configuration options, and usage guides for the JavaScript client are available in the [@sanity/client readme](https://github.com/sanity-io/client/).

## Tips for using the client

- Be cautious mutating the incoming document—the one that triggered the function—in a way that will trigger it again.
- Don't re-fetch the `event` document unless you need. Use the incoming payload to save a request.
- The Sanity client isn't required to make requests to Sanity. If you're focused on the fastest, lightest function possible, you can build API calls manually with Node's `fetch` and the token, projectId, and dataset from `context.clientOptions`.



# Common patterns

Functions create the ability for countless content-driven opportunities. This guide collects common patterns and approaches to working with Functions.

Prerequisites:

- Complete the [Functions quick start](/docs/compute-and-ai/function-quickstart), or be comfortable writing and deploying a Sanity Function.
- `sanity` CLI v3.92.0 or higher is required to interact with Blueprints and Functions. You can always run the latest CLI commands with `npx sanity@latest`.

The examples below assume you've created a new function, and configured it to trigger based on your own schema requirements.

## Ping an endpoint on publish

A common approach to invalidating CDNs and triggering new builds is to ping, or make a GET request, to an endpoint. Some require you to provide specifics, such as the endpoint or slug for targeted refreshes. Others only require a single URL.



Create a function and configure it to trigger when your target document publishes. For the example, make sure to [define an environment variable](/docs/compute-and-ai/function-env-vars) named `DEPLOY_HOOK_URL`.

```
import { documentEventHandler } from '@sanity/functions'

export const handler = documentEventHandler(async ({ context, event }) => {
  const URL = process.env.DEPLOY_HOOK_URL
  if (!URL) {
    throw new Error("DEPLOY_HOOK_URL is not set")
  }
  try {
    await fetch(URL)
  } catch (error) {
    console.error(error)
  }
})
``````javascript
export async function handler({context, event}) {
  const URL = process.env.DEPLOY_HOOK_URL
  if (!URL) {
    throw new Error("DEPLOY_HOOK_URL is not set")
  }
  try {
    await fetch(URL)
  } catch (error) {
    console.error(error)
  }
}
```

To find the deploy or trigger URL for your provider, check their documentation. We've included a few common links below:

- [Vercel: Create and trigger deploy hooks](https://vercel.com/docs/deploy-hooks)
- [Azure: purge content](https://learn.microsoft.com/en-us/rest/api/cdn/endpoints/purge-content?view=rest-cdn-2025-04-15&tabs=HTTP)

## Automatically translate documents

You can combine Translate with Functions to translate documents automatically.

We recommend completing [the quick start](/docs/agent-actions/translate-quickstart) if you haven't used Translate before.

First, create a function and configure it to only trigger when a document's language is in your "from" language. Here's an example of the function resource in `sanity.blueprint.ts`.

```typescript
import {defineBlueprint, defineDocumentFunction} from '@sanity/blueprints'

export default defineBlueprint({
  resources: [
    defineDocumentFunction({
      name: "translate",
      event: {
        on: ["publish"],
        filter: "_type == 'post' && language == 'en-US'",
        projection: "_id"
      }
    }),
  ],
})
```

> [!TIP]
> Use caution when creating documents
> The GROQ filter in this example is important. It makes sure that the function only runs when the language is set to English. When we generate a new translation in the next code block, Translate sets that field to Greek. This stops the new document from triggering the same function and creating a recursive loop.
> 
> You could also create draft or version documents to prevent the "on publish" function from triggering.

For this approach, we have documents with a `language` set. We only want the English language files.

19. [Import and configure the @sanity/client](/docs/compute-and-ai/functions-js-client).
19. Capture the document `data` from the `event`.
19. Construct a `translate` request.

```
import { documentEventHandler } from '@sanity/functions'
import { createClient } from '@sanity/client'

export const handler = documentEventHandler(async ({ context, event }) => {
  const { data } = event
  const client = createClient({
    ...context.clientOptions,
    apiVersion: 'vX',
  })
  const targetLanguage = {
    id: "el-GR",
    title: "Greek"
  }
  // Create a consistent ID based on the source and target language.
  // This allows the function to override the document in the future
  const targetId = `${data._id}-${targetLanguage.id}`

  try {

    await client.agent.action.translate({
      // Replace with your schema ID
      schemaId: "sanity.workspace.schema.default",
      
      // Tell the client to run the action asynchronously.
      // We don't need to wait for it to complete.
      async: true,
      
      // Tell the client the ID of the document to use as the source.
      documentId: data._id,

      // Set the language field to the target language.
      languageFieldPath: "language",
      
      // Set the operation mode
      // createOrReplace will override the ID in future invocations.
      targetDocument: { 
        operation: "createOrReplace",
        _id: targetId
      },
      
      // Set the 'from' and 'to' language
      fromLanguage: {id: "en-US", title: "English"},
      toLanguage: {id: targetLanguage.id, title: targetLanguage.title},
    });
  } catch (error) {
    console.error(error)
  }
})
``````javascript
import { createClient } from '@sanity/client'

export async function handler({context, event}) {
  const { data } = event
  const client = createClient({
    ...context.clientOptions,
    apiVersion: 'vX',
  })
  const targetLanguage = {
    id: "el-GR",
    title: "Greek"
  }
  // Create a consistent ID based on the source and target language.
  // This allows the function to override the document in the future
  const targetId = `${data._id}-${targetLanguage.id}`

  try {

    await client.agent.action.translate({
      // Replace with your schema ID
      schemaId: "sanity.workspace.schema.default",
      
      // Tell the client to run the action asynchronously.
      // We don't need to wait for it to complete.
      async: true,
      
      // Tell the client the ID of the document to use as the source.
      documentId: data._id,

      // Set the language field to the target language.
      languageFieldPath: "language",
      
      // Set the operation mode
      // createOrReplace will override the ID in future invocations.
      targetDocument: { 
        operation: "createOrReplace",
        _id: targetId
      },
      
      // Set the 'from' and 'to' language
      fromLanguage: {id: "en-US", title: "English"},
      toLanguage: {id: targetLanguage.id, title: targetLanguage.title},
    });
  } catch (error) {
    console.error(error)
  }
}
```



Now, when you publish an English-language document, it will create a Greek version. [Learn more about Agent Actions here](/docs/agent-actions/introduction).

## Set an undefined value with `setIfMissing`

You may have values in documents that are sometimes set by people, but otherwise could be derived programatically. This example uses GROQ's `!defined` function and a `setIfMissing` patch to add a the current date and time as the published date to a document, but only when it hasn't been set. 

For this example, you'll need to:

25. Set up a new function or edit an existing one.
25. Import and configure the `@sanity/client` if you haven't already.

First, modify the following filter and add it to your function's `event` in the `sanity.blueprint.ts` configuration.

```text
"filter": "_type == 'post' && !defined(firstPublished)"
```

Adjust the `_type` and `firstPublished` values to match properties from your schema.  `!defined` checks that the property is not set, which prevents the function from running if the document receives future updates.

Next, create a `setIfMissing` patch to set the same field from the filter. `setIfMissing` is  redundant here, as it *should* be empty if `!defined` worked as intended. It's still a useful to approach when you only want to update empty fields.

```
import { documentEventHandler } from '@sanity/functions'
import { createClient } from '@sanity/client'

export const handler = documentEventHandler(async ({ context, event }) => {
  const { data } = event
  const client = createClient({
    ...context.clientOptions,
    apiVersion: "2025-02-19"
  })
  
  try {
    await client.patch(data._id, {
      setIfMissing: {
        firstPublished: new Date().toISOString()
      }
    })
  } catch (error) {
    console.error(error)
  }
})
``````javascript
import { createClient } from '@sanity/client'

export async function handler({context, event}) {
  const { data } = event
  const client = createClient({
    ...context.clientOptions,
    apiVersion: "2025-02-19"
  })
  
  try {
    await client.patch(data._id, {
      setIfMissing: {
        firstPublished: new Date().toISOString()
      }
    })
  } catch (error) {
    console.error(error)
  }
} 
```



# Handler reference

[Overview](/docs/compute-and-ai/functions-introduction)

[Quick start](/docs/compute-and-ai/function-quickstart)



Every Function must export a `handler`. Handlers contain the logic that the Function infrastructure runs when your document changes trigger the function.

Create a function handler with the `sanity blueprints add function` command. Every handler receives an object containing `context` and `event` parameters.



## `context` properties

#### Properties

| Property | Description |
|----------|-------------|
| clientOptions | Coming soon |


### `clientOptions` properties

#### Properties

| Property | Description |
|----------|-------------|
| projectId | The ID of the project that triggered this function. |
| dataset | The dataset name of the project that triggered this function. |
| apiHost | Defaults to https://api.sanity.io. |
| token | A token provided by the function with access to your Sanity project. Note: the token is obfuscated in logs for security. You can directly use it to configure the Sanity client or to make API calls. |


### Example context

```javascript
{
  clientOptions: {
    apiHost: 'https://api.sanity.io',
    projectId: 'abc123',
    dataset: 'production',
    token: '***************'
  }
}
```

## `event` properties

#### Properties

| Property | Description |
|----------|-------------|
| data | A Sanity Document containing the default document shape and available values, or the shape defined in the Blueprints configuration function's event.projection. |


### Example event

```javascript
{
  data: {
    _id: '1234',
    _type: 'article',
    title: 'Functions quick start',
    _createdAt: "2025-04-24T16:26:58.901Z",
    _publishedAt: "2025-04-24T16:26:58.901Z",
    
  }
}
```

## Example handler

```
import { documentEventHandler } from '@sanity/functions'

export const handler = documentEventHandler(async ({ context, event }) => {
  console.log("Context: ", context)
  console.log("Event: ", event)
})
``````javascript
export async function handler({context, event}) {
  console.log("Context: ", context)
  console.log("Event: ", event)
}
```

## TypeScript types

When you create a new TypeScript function with `sanity blueprint add`, you'll be prompted to add types. 

If you did not add types as part of the init process, they are available in the [@sanity/functions](https://www.npmjs.com/package/@sanity/functions) package:

```sh
npm install -D @sanity/functions
```

You can then import and use the `documentEventHandler` helper to provide type support. See the example TS handler above for implementation details.



# Introduction

Blueprints enable infrastructure-as-code level management of Sanity resources. At this time, Blueprints are limited to managing Functions.

## Requirements

- `sanity` CLI v3.92.0 or higher is required to interact with Blueprints and Functions. You can always run the latest CLI commands with `npx sanity@latest`.
- Write access to your organization and project settings.

## Core concepts

### Blueprint

Like a configuration file, a blueprint lets you define and customize Sanity resources.

[Blueprint configuration reference](/docs/specifications/blueprint-config)



### Resource

Core Sanity components are resources. For the time being, [Functions](/docs/compute-and-ai/functions-introduction) are the only resource supported by Blueprints.

#### Learn more about Functions

[Functions](/docs/compute-and-ai/functions-introduction)

[Functions quick start](/docs/compute-and-ai/function-quickstart)





# Configuration file reference

[Overview](/docs/compute-and-ai/blueprints)



The Blueprints configuration file defines resources, like Functions, for deployment to Sanity's infrastructure.

Interact with Blueprints by using the `npx sanity blueprints` [CLI command](/docs/cli-reference/cli-blueprints).

The top-level of the blueprint configuration file contains the following properties:

#### Properties

| Property | Description |
|----------|-------------|
| blueprintVersion * | Defines the version of the Blueprints specification to use when parsing the configuration. Uses the YYYY-MM-DD format. |
| resources * | An array of Sanity resources. Right now this is limited to Function resources, but will expand in the future. |


## Resources

The following properties are shared across all resources. Additional resource-specific properties follow in the sections below.

#### Properties

| Property | Description |
|----------|-------------|
| name * | A unique function name. Must be an alphanumeric string that can contain dashes or underscores. |
| type * | A resource type. For Sanity resources, this is made up of the sanity namespace, category, subcategory, and resource types separated by single periods. For example: sanity.function.document. |


### Functions

In addition to the common resource properties, functions also contain the following resources.

#### Properties

| Property | Description |
|----------|-------------|
| src * | The path, relative to the blueprint configuration file, of the individual function directory. For example, functions/myFunction. |
| event | Configuration options for the triggering event. See the event properties section below for details. |
| timeout | The max invocation time, in seconds, of the function.

Default: 10

Minimum: 1

Maximum: 900 |
| memory | Sets the max memory allocation, in GBs.

Default: 1

Min: 1

Max: 10 |
| env | Set environment variables for the function. The env object accepts custom keys with string values. This is an alternative approach to using the sanity functions env CLI command. |


#### `event` properties

#### Properties

| Property | Description |
|----------|-------------|
| on | Defines the type of event trigger. The options are:

publish: Activates when a document is published.

These actions trigger on individual documents with unique _id values. For example, updating or deleting a draft of a document will not trigger an update or delete on the published document. |
| filter | A valid GROQ filter. This accepts most core GROQ filter functionality found in groq-js. Delta GROQ is not supported at this time.



Only include the contents of the filter, not any other surrounding syntax.

✅ Do this: _type == "article"

❌ Not this: [_type == "article"] |
| projection | A valid GROQ projection. Omit the outer wrapping parentheses.



✅ Do this: title, _id, slug

❌ Not this: { title, _id, slug } |


#### Example

```
import {defineBlueprint, defineDocumentFunction} from '@sanity/blueprints'

export default defineBlueprint({
  resources: [
    defineDocumentFunction({
      name: "log-event",
      event: {
        on: ["publish"],
        filter: "_type == 'post'",
        projection: "title, _id, _type"
      },
      env: {
        example: 'value'
      }
    })
  ]
})

``````json
{
  "blueprintVersion": "2024-10-01",
  "resources": [
    {
      "name": "log-event",
      "src": "functions/log-event",
      "type": "sanity.function.document",
      "event": {
        "on": [
          "publish"
        ],
        "filter": "_type == 'post'",
        "projection": "title, _id, _type"
      },
      "env": {
        "example": "value"
      }
    }
  ]
}
```

## TypeScript / JavaScript helpers

You can configure Blueprints with TypeScript and JavaScript. If you select either during `sanity blueprints init`, the CLI prompts you to install the [@sanity/blueprints](https://github.com/sanity-io/blueprints-node) package. You can also add it to an existing project by adding it to your Blueprints-level project directory.

```sh
npm i @sanity/blueprints
``````sh
pnpm add @sanity/blueprints
```

The helpers provide defaults and allow you to omit some configuration options. You can always override these defaults by explicitly setting the values as you would with the JSON format.



# HTTP API reference



# Introduction

> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

With Sanity, you can retrieve documents from a dataset and present them how you want. Matching documents in the search results can match a literal string or a regular expression.

But what if you need to search for documents based on what they are **about**?

*Embeddings Index API* lets you do just that.

After creating an index based on a dataset, we will feed your documents to a small embeddings model with OpenAI every time you publish content. Then, we store the resulting embeddings in a vector database.

This way, you can search the embeddings to retrieve corresponding documents by sending requests to the Embeddings Index API. Behind the scenes, this embeds your search string and returns the most similar documents in your dataset.

> [!NOTE]
> Embeddings Index API functionality is available through the Embeddings Index CLI, the Embeddings Index UI for Sanity Studio, and the Embeddings Index HTTP API.

> [!WARNING]
> Gotcha
> Embeddings Index API is currently in beta. Features and behavior may change without notice.
> 
> Embeddings Index API is available to projects on the Growth plan and above.

> Using this feature requires Sanity to send data to OpenAI and Pinecone to store vector interpretations of documents.

[AI Assist oveview](/docs/ai-assist)

[Embeddings Index CLI reference](/docs/libraries/embeddings-index-cli-reference)

[Embeddings Index HTTP API reference](/docs/http-reference/embeddings-index)



## What can you use embeddings for?

Embeddings are simplified representations of more complex data. While they simplify the original content, they keep contextual information. Therefore, embeddings can serve use cases that leverage machine learning, prediction, and search.

For example, you can use embeddings to:

- **Implement semantic search**: you can make semantic search available to your editors or customers so that they can use it to find similar documents. The embeddings index offers a fast lookup that you can use for document similarity searches. In fact, the embeddings index ships with a [Studio UI component](https://www.npmjs.com/package/@sanity/embeddings-index-ui) that demonstrates finding similar documents or documents similar to a phrase.
- **Enable related content instructions with AI Assist**: you can [enable AI Assist to work with reference fields](https://www.sanity.io/docs/install-and-configure-sanity-ai-assist#5e563473de84) for documents as long as they are included in an embeddings index.
- **Build Large Language Model (LLM) agents**: you could fine-tune a model to generate different inflections of output; however, this isn't a good way to teach it about new concepts or facts. A more effective approach is to use embeddings to represent domain knowledge and then feed whole documents or summaries into new prompts. This is a great way to give LLMs long-term memory.

## Getting better comparisons

If you don't include a [projection](/docs/content-lake/how-queries-work) in your index configuration, we process the entire document JSON into a less verbose format and embed all of it, breaking it into chunks as we go to accommodate the limit of the model doing the embedding.

If you compare your documents with excerpts from other documents, this may work fine for you out of the box. Occasionally, you might need to reshape your documents to something that looks more like your query string.

*For example:* you want to create a great document search based on short strings from users. If you wish to optimize this, you may generate summaries of every document in your collection using a LLM. You could then embed only the summaries. Inversely, when searching, you could have the LLM model doing the querying imagine what a summary of the document represented by the search string would look like.

In this example, you would be comparing apples to apples—summaries of actual documents and the summary of a document that could represent the search string. Just using entire documents and search strings will still produce results, but the quality will probably be lower.

## Setting up an embeddings index

You can create an embeddings index in one of the following ways:

- With the [Embeddings Index CLI](https://www.npmjs.com/package/@sanity/embeddings-index-cli).
- With the [Embeddings Index UI](https://www.npmjs.com/package/@sanity/embeddings-index-ui) for Sanity Studio
- With the [Embeddings Index HTTP API](/docs/http-reference/embeddings-index).

The following practical example guides you through configuring an embeddings index for a Sanity project using the [Embeddings Index CLI](/docs/libraries/embeddings-index-cli-reference).

### Prerequisites

- The Sanity CLI. The CLI ships with the [main Sanity package](https://www.npmjs.com/package/sanity).
You need it to log in to Sanity, which enables consuming the Embeddings Index CLI.
- The [Embeddings Index CLI](https://www.npmjs.com/package/@sanity/embeddings-index-cli). To install it, follow the instructions in the [Embeddings Index CLI reference](/docs/libraries/embeddings-index-cli-reference).
The example assumes that the CLI is installed for the local Sanity project.

> [!WARNING]
> Gotcha
> In its current state, the embeddings-index API does not support dataset aliases. This means that you have to use the real dataset name in all requests.

### Creating an embeddings index

To create an embeddings index, open a terminal session, and then run:

```sh
# Create an embeddings index by passing arguments
embeddings-index create --indexName "<name-of-the-index>" --dataset "<name-of-the-dataset>" --filter "<GROQ-filter>" --projection "<GROQ-projection>"

# Alternatively, create an embeddings index by passing a JSON manifest
embeddings-index create --manifest <manifest-file-name>.json
```

Creating an index can take time, depending on the number of existing documents and the indexer load.

### Defining an embeddings index

You can define the configuration of an embeddings index in one of the following ways:

- By passing configuration arguments when you create the index in the CLI.
- By storing the configuration details in a JSON manifest file.

#### Defining the index in the CLI

To define a new embeddings index in the root directory of a Sanity project, pass the following required arguments with the `embeddings-index create` command:

- `--indexName`: assign a descriptive name to the index.
- `--dataset`: specify the name of an existing dataset. This is the target dataset to index.
- `--filter`: specify the filtering criteria to include in the index only the selected subset of documents from the database.
The filter must be a valid [GROQ filter](/docs/content-lake/how-queries-work) *without the square brackets* that wrap the value assigned to `_type`.
Example: `_type=='tutorial'`
- `--projection`: specify the projection criteria to include in the index only the selected subset of properties from the filtered documents.
The projection must be a valid [GROQ projection](/docs/content-lake/query-cheat-sheet), including curly brackets.
Example: `{title, author}`

Alternatively, you can create an embeddings index by passing a JSON manifest file with the `--manifest` argument:

- `--manifest <manifest-file-name>.json`

**Example**

```sh
# Create embeddings index with arguments
# 'filter' has no '[]' square brackets
# 'projection' keeps '{}' curly brackets
embeddings-index create --indexName "my-embeddings-index" --dataset "production" --filter "_type=='myDocumentType'" --projection "{...}"

# Create embeddings index with JSON manifest
# The JSON manifest is in the project root directory
embeddings-index create --manifest embeddings-index-manifest.json
```

#### Defining the index in a JSON manifest

To store, reuse, and manage embeddings indexes with source code control and versioning, define their configuration in a JSON manifest file. Save the embeddings indexes `manifest.json` file to the root directory of a Sanity project. 

A JSON manifest file defining an embeddings index must contain the following required fields:

```json
{
  indexName: string,
  dataset: string,
  filter: string,
  projection: string
}
```

**Example**

```json
{
  "indexName": "my-embeddings-index",
  "dataset": "production",
  "filter": "_type=='myType'", // No '[]' square brackets
  "projection": "{...}" // Keeps '{}' square brackets
}
```

To create a JSON manifest file, invoke the [manifest command](/docs/libraries/embeddings-index-cli-reference):

```sh
embeddings-index manifest --out manifest.json --indexName "<name-of-the-index>" --dataset "<name-of-the-dataset>" --filter "<GROQ-filter>" --projection "<GROQ-projection>"
```

### Checking an embeddings index status

You can check the status of your embeddings indexes to monitor the creation progress or the completeness of the indexes.

To check the status of all embeddings indexes in a Sanity project, run:

```sh
embeddings-index-cli list
```

To check the status of a specific embeddings index in a Sanity project, run:

```sh
embeddings-index-cli get --indexName "<name-of-the-index>"
```

## Query an index

To query an index, make a request with the [Embeddings Index HTTP API](/docs/http-reference/embeddings-index).

```sh
curl --request POST 'https://<project-id>.api.sanity.io/<api-version>/embeddings-index/query/<dataset>/<index-name>' \
     --header 'Authorization: Bearer <bearer-token>' \
     --header 'Content-Type: application/json' \
     --header 'Accept: application/json' \
     --data '{  
                "query": "sci-fi adventure with cowboys and aliens",
                "maxResults": 10,
                "filter": {
                  "type": ["summary", "synopsis", "userReview"]
                }
             }'
```

This example uses the query endpoint to search against an index and filter by document type. [Learn more about querying the API](https://www.sanity.io/docs/embeddings-index-http-api-reference#ce88034da6ac).



# HTTP API reference



# Introduction

## Webhooks at a glance

Webhooks are a way to integrate applications with automated HTTP requests. Typically you use it to connect services by creating a special URL that accepts incoming requests. What happens when the request resolves depends on the application or service.

> [!NOTE]
> Webhooks are typically used for, but not limited to:
> 
> 
> Setting up notifications to systems like Slack, Discord, or email services
> 
> Keep external logs and update auditing systems
> 
> Update content in other services
> 
> Trigger automation and workflows

Some services only support receiving webhooks; others can both receive and send them. The Sanity Content Lake supports both sophisticated outgoing webhooks, and receiving incoming webhooks to any appropriate API endpoint, provided they have the proper payload and authentication.

## Webhooks in your Sanity Content Lake

You can create and manage outgoing webhooks in the API section of your project settings, which you'll find at [sanity.io/manage](https://www.sanity.io/manage). Webhooks can also be managed through the CLI or directly through the project APIs.

![API settings with the webhook overview showing a “Trigger site rebuild” webhook.](https://cdn.sanity.io/images/3do82whm/next/13bdd207a065dbdc51d06151b1a170d40cc26378-1053x624.png)

## Configuration

The following fields are available for webhooks. You can find them all under the webhooks section under the API in your project's settings on [sanity.io/manage](https://www.sanity.io/manage).

### Name and description

You can name your webhooks and give them a description. The description field, while optional, is a useful way to add helpful context about your webhook.

### URL

The URL field is where you specify the endpoint to which the webhook request is sent. If you want to test the webhook before entering the production endpoint, you can use services like [webhook.site](https://webhook.site), or [Beeceptor](https://beeceptor.com/). You can also use [ngrok](https://ngrok.com/) or [Localtunnel](https://localtunnel.github.io/www/) to test a hook against your local environment.

### Trigger on

Webhooks can be triggered when a document is **created**, **updated**, **deleted**, or any combination of these.

- **Create** - triggers on the creation of a new document.
- **Update** - triggers on every change to a document once created.
- **Delete** - triggers on the deletion of a document

Between these, you'll be able to react to all major interactions with the documents in question.



> [!NOTE]
> Pro tip!
> By default, your webhooks will not trigger on draft or version events. They will only trigger when changes to the document are published and not for every single occurrence while you edit. Triggering on draft and version events can be enabled, but be careful or you may end up causing huge amounts of traffic to your endpoint!

### Filter

A GROQ filter specifying which documents will, when changed, trigger your webhook. A filter is what you commonly see between the `*[` and `]` in a GROQ query. This field supports all the GROQ functions you'd expect and has additional support for functions in the [delta::](https://www.sanity.io/docs/groq-functions#a64594a50318) namespace, as well as [before() and after()](https://www.sanity.io/docs/groq-functions#bbcf50816968).

If left empty, it will apply to all documents (`*[]`).

Webhook filter does not support the following kind of queries and will just yield to `false`:

- Sub-queries, e.g. `_type == "book" && author._ref in *[_type=="author" && name=="John Doe"]._id`
- Cross dataset references: `_type == "book" && author->featured` where author is a [cross-dataset reference](https://www.sanity.io/docs/cross-dataset-references).

See our [Intro to Filters](https://www.sanity.io/guides/filters-in-groq-powered-webhooks) guide for tips on using filters in webhooks.

### Projection

A GROQ projection defining the payload (or body) of the outgoing webhook request. This field supports GROQ functions in the [delta::](https://www.sanity.io/docs/groq-functions#a64594a50318) namespace, as well as [before() and after()](https://www.sanity.io/docs/groq-functions#bbcf50816968).

If left empty, it will include the whole document *after* the change that triggered it.

> [!WARNING]
> Gotcha
> “Sub-queries” are not supported for webhook projections. For example, the following query will not work: { "relatedPost": *[^._id in related[]._ref]{_id, title, slug}}

See our [Intro to Projections](https://www.sanity.io/guides/projections-in-groq-powered-webhooks) guide for tips on using projections in webhooks.

### Status

Enable or disable your webhook.

> [!NOTE]
> Disabling webhooks
> When a webhook is disabled all pending requests will be canceled.

### HTTP method

This field configures the webhook's [HTTP request method](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods). It can be set to POST, PUT, PATCH, DELETE, or GET. Some endpoints require incoming requests to use a specific method to work.

### HTTP headers

Additional HTTP headers. You can add multiple headers. A common example is adding an `Authorization: Bearer <token>` header to authenticate the webhook request.

> [!WARNING]
> Gotcha
> Be mindful if you share webhooks that header configuration will be included with sensitive information if you don't remove it before sharing the link.

A webhook will always include the following headers and values:

- [connection](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Connection): close
- [accept-encoding](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Encoding): gzip
- idempotency-key: <a unique key> See documentation below.
- [content-type](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Type): application/json
- [content-length](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Length): <the length of the payload in bytes>
- [user-agent](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent): [Sanity.io](http://Sanity.io) webhook delivery
- [host](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Host): <the endpoint URL host>

As well as the following Sanity-specific headers that can be useful for logging and debugging your webhooks:

- `sanity-transaction-id`: ID of transaction.
- `sanity-transaction-time`: Timestamp of transaction.
- `sanity-dataset`: Name of dataset (also available in projection today as `sanity::dataset()`).
- `sanity-document-id`: Document ID being notified about.
- `sanity-project-id`: ID of project (also available in projection today as `sanity::projectId()`).
- `sanity-webhook-id`: ID of webhook.
- `sanity-operation`: Either create, update or delete

> [!NOTE]
> Info
> The projection will always be returned as JSON. If you for some reason need it to be another content type, you’ll have to pass it through a serverless function or a custom endpoint and do the transformation there.

### API version

Defaults to the `v2021-03-25` of the query API. Can be overridden using the [Webhooks API](/docs/http-reference/webhooks) in cases where you want to create webhooks with old behavior that might have been deprecated.

### Drafts and versions

By default, documents in the `drafts.` and `versions.` ID-namespace will be automatically ignored. Enable the drafts or version setting if you want the triggers and filter to apply to draft or version documents. Note: version support was added in version 2025-02-19.

> [!WARNING]
> Gotcha
> This might cause a lot of webhooks to trigger whenever someone is working inside Sanity Studio, since almost every keystroke represents an update. Webhooks are limited to 1 concurrent request, but you should also make sure that your endpoint is able to handle the incoming events.

### Secret

To let receiving services verify the origin of any outgoing webhook, you may add a secret that will be hashed and included as part of the webhook request's headers. You may find our [webhook toolkit library](https://github.com/sanity-io/webhook-toolkit) helpful for working with secrets. If you want to roll your own; we model the encryption and decryption of secrets on the same standard as [Stripe](https://stripe.com/docs/webhooks/signatures#verify-manually).

## Idempotency-key

Requests include a new header that can be used to de-duplicate deliveries: `idempotency-key`.

This is necessary because webhooks will sometimes be retried, and our system has *at least one* delivery. Using the unique idempotency key lets the receiver ignore messages it has already received.

We follow [this new draft standard](https://datatracker.ietf.org/doc/draft-ietf-httpapi-idempotency-key-header/) for idempotency.

## Sharing webhooks

Webhook configurations can be shared with a URL. This is practical if you want to quickly repurpose webhooks across projects or share with the community. You can generate a share URL by going to [sanity.io/manage/webhooks/share](https://www.sanity.io/manage/webhooks/share) or by finding the share button in the three-dot-menu in the webhooks overview. 

> [!WARNING]
> Gotcha
> Note that all the configuration is stored as part of the URL. Be mindful of any sensitive information that might be part of the configuration and that it will be shared in plain text. It can be wise to replace secret tokens and so on with capitalized placeholder text.

## Debugging webhooks

### Attempts log

> [!TIP]
> Protip
> Use the attempts log to determine whether your webhooks are being successfully delivered.

You can find the attempts log if you click the three-dotted menu for a given webhook. The log will include information about the response a webhook request got. The attempts log is available as an API endpoint at:

```
https://api.sanity.io/v2021-10-04/hooks/projects/${projectId}/${id}/attempts
```

### Message log

> [!TIP]
> Protip
> Use the message log if you want to know whether all outstanding messages for a webhook have been delivered. 

The message log is available as an API endpoint at:

```
https://${projectId}.api.sanity.io/v2021-10-04/hooks/${id}/messages
```

The log contains a list of messages in the queue and any delivery attempts for each:

- If all the messages returned have the status `queued` then your processing has fallen behind. This may indicate that your webhook processing is too slow and/or that your webhook filter is too broad and is generating a vast number of messages. 
- If your webhook request handler takes longer to process a message than the rate at which you are generating changes that trigger the webhook then the queue will never be cleared.

### HTTP Status codes

The HTTP status codes are used to determine if delivery is successful:

- 200-range will be treated as a success
- 400-range will be treated as undeliverable, as the server said it was a client error (with one exception—see next item)
- 429 will be retried using an exponential back-off pattern
- 500-range will be retried using an exponential back-off pattern

## Technical limits, retries, and timeouts

Webhooks are limited to 1 concurrent request.

We will retry sending a Webhook request for up to 30 minutes with an exponential back-off. This limit is subject to changes in the future.

A webhook request will time out after 30 seconds.

## **Webhooks origin IP addresses**

The full list of IP addresses that Sanity webhooks calls originate from, can be found on this file:

[https://www.sanity.io/files/webhooks-egress-ips.txt](https://www.sanity.io/files/webhooks-egress-ips.txt)

The IP addresses generally don’t change but they may be updated from time to time, on planned or unplanned/emergency maintenance. For planned changes, we aim to announce upcoming changes 7 days in advance on Sanity’s status page feed here: [https://www.sanity-status.com/](https://www.sanity-status.com/#). Unplanned maintenance changes will happen without notice, but the URL file will be immediately up-to-date.

> [!WARNING]
> Gotcha
> If you’re aiming to use these addresses for IP filtering/security purposes, make sure you keep your tooling up-to-date with the URL above in an automated/unattended way.





# Best practices

The below is our best practice guidelines on how webhooks should be used and how your system should handle them.

## Configuration

GROQ webhooks should be configured to trigger on the narrowest possible set of changes. Make sure the filter is as specific as possible and avoid triggering webhooks on draft changes unless absolutely necessary. Drafts can change frequently as content is being edited, which could result in a high volume of webhooks that may be costly or overwhelming for your systems.

## Delays

In rare circumstances there can be delays in the delivery of webhooks. If receiving timely updates is critical to your app, this should be considered in webhook handling. For example, you could check the `sanity-transaction-time` header and compare this to the current date and time - if you see times over a certain age you might trigger a catch up via API calls.

Delays could also mean webhooks can potentially be received out of order. Therefore is can be useful to check the `_updatedAt` value on a document to ensure you're using the latest data. It can sometimes be worth considering whether you're best to use the data in a webhook or use the webhook to trigger a query.

## Recovery from downtime

It's important to consider that downtime can occur with any webhook setup, whether it's on the side of your application or the provider itself. 

To mitigate the impact of potential downtime, it's recommended to implement a mechanism for recovering missed data via API calls. This ensures your application can stay up-to-date even if webhooks are temporarily unavailable.

## Idempotence

Idempotence in the context of webhooks refers to the ability to process the same webhook payload multiple times without adverse effects.

For example, if a webhook is delivered and processed successfully, but the acknowledgement response fails to reach the sender due to a network issue, the sender might retry sending the same payload. In an idempotent system, receiving and processing the same payload again would not result in duplicate data or unintended side effects.

Sanity provides an `idempotency-key` header which you can use to ignore messages that might be in a state of being processed or that have been processed already. By checking the `idempotency-key`, you can ensure that your application processes each unique webhook payload only once, even if it is delivered multiple times.

## Reconciliation

Relying solely on webhooks isn't recommended in any application - delivery can't always always guaranteed due to network issues, application downtime, or other factors.

You might want to run regular sync jobs – at hourly, daily or other intervals – to make sure everything updated between syncs is reconciled. This sync could filter using the `_updatedAt` field on Sanity documents to find everything which has changed since the last sync.

## Scalability

As the volume of webhooks received by your application increases, it can become challenging to process all of them in real-time. To handle this scalability issue, it's recommended to implement a queueing system for incoming webhooks.

When a webhook is received, instead of processing it immediately, your application should add it to a queue for asynchronous processing. This allows your webhook endpoint to quickly acknowledge receipt of the webhook and return a response within the 30-second timeout window Sanity implements.

It's important to note that the response returned by your webhook endpoint should indicate that the webhook was received successfully, not that it was fully processed. This distinction is crucial because the actual processing of the webhook happens asynchronously through the queue.

By decoupling the receipt and processing of webhooks using a queue, you can ensure that your application remains responsive and can handle a high volume of incoming webhooks without overwhelming your system. The queue acts as a buffer, allowing you to process webhooks at a pace that your application can handle, while still acknowledging their receipt in a timely manner.

Implementing a robust queueing system for webhook processing is a best practice for building scalable and reliable applications that can handle increasing webhook traffic as your system grows.

## Security

When setting up webhooks, it's important to consider security measures to protect your application and data. Webhooks are sent across HTTP and you will want to ensure the data you receive is sent from Sanity.

Here are a few key points to keep in mind:

- **Secrets**: Sanity allows you to configure a secret token for your webhooks. This secret should be a unique, random string that is only known to your application and Sanity. When Sanity sends a webhook payload to your endpoint, it includes this secret in the request headers. Your application should verify the secret to ensure that the webhook is coming from a trusted source (Sanity) and not from a malicious actor.
- **Sanity Webhook Toolkit**: Sanity offers a [webhook toolkit](https://github.com/sanity-io/webhook-toolkit), which is a set of utilities for handling webhooks in a secure and reliable manner. The toolkit includes features like signature verification, which helps ensure the integrity and authenticity of the webhook payloads you receive. Although the toolkit is written in TypeScript, the concepts and principles it promotes are language-agnostic.
- **IP Whitelisting**: Sanity provides a [specific set of IP addresses](https://www.sanity.io/files/webhooks-egress-ips.txt) from which webhooks are sent. You can configure your application to only accept webhook requests originating from these trusted IP addresses. This adds an extra layer of security by preventing unauthorized sources from sending fake webhook payloads to your endpoint.

By implementing these security measures, you can protect your application from potential threats and ensure that the webhooks you receive are genuine and trustworthy.





# HTTP API reference



# HTTP API Reference

#### Get started

[Authentication](/docs/content-lake/http-auth)

[URL Format](/docs/content-lake/http-urls)

[Patches](/docs/content-lake/http-patches)



#### Popular endpoints

[Query](/docs/http-reference/query)

[Mutation](/docs/http-reference/mutation)

[Actions](/docs/http-reference/actions)



#### New endpoints

[Media Library API](/docs/http-reference/media-library)

[Agent Actions](/docs/http-reference/agent-actions)

[Live](/docs/http-reference/live)





# Actions



# Copy



# Backups



# Doc



# Export



# History



# Jobs



# Mutation



# Query



# Scheduling



# Webhooks



# Agent Actions



# Embeddings Index



# Assets



# Listen



# Live



# Media Library



# Access



# Projects



# Roles



# Libraries and tooling

#### Clients

[JavaScript Client](https://github.com/sanity-io/client)

[PHP Client](https://github.com/sanity-io/sanity-php)

[Rust Client](https://github.com/Riley1101/sanity-rs)

[LINQ (C#) Client](https://github.com/oslofjord/sanity-linq)

[Flutter Client](https://pub.dev/packages/sanity_client)



#### Portable Text

[Editor Playground](https://playground.portabletext.org/)

[Standalone Portable Text Editor](https://www.portabletext.org)

[React serializer](https://github.com/portabletext/react-portabletext/)

[Vue serializer](https://github.com/portabletext/vue-portabletext/)

[Svelte serializer](https://github.com/portabletext/svelte-portabletext/)

[HTML serializer](https://github.com/portabletext/to-html/)



#### Frontend tooling

[Next.js toolkit](https://github.com/sanity-io/next-sanity)

[Nuxt module](https://sanity.nuxtjs.org/)

[Astro integration](https://github.com/sanity-io/sanity-astro)





# Embeddings Index CLI reference

> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

> Using this feature requires Sanity to send data to OpenAI and Pinecone to store vector interpretations of documents.

> [!WARNING]
> Gotcha
> Embeddings Index API is currently in beta. Features and behavior may change without notice.
> 
> Embeddings Index API is available to users on the Team plan and above.

> [!NOTE]
> Embeddings Index API functionality is available through the Embeddings Index CLI, the Embeddings Index UI for Sanity Studio, and the Embeddings Index HTTP API.

The Sanity Embeddings Index CLI offers commands to create, delete, fetch, and query embeddings indexes in a Sanity project.

You can install the Embeddings Index CLI:

- Globally, to make its commands available in the terminal regardless of the current directory path.
- Locally, on a per-project basis.

To execute the commands without installing the Embeddings Index CLI, invoke them through the [npx](https://www.npmjs.com/package/npx) package runner.

The Embeddings Index CLI commands work only in the context of a local Sanity project:

```sh
# Go to the root directory of a Sanity project
cd path-to/my-sanity-project/root-dir

# Invoke the embeddings-index CLI commands
embeddings-index-cli <command> [<arguments>]

# Alternatively: invoke the commands without installing
npx @sanity/embeddings-index-cli <command> [<arguments>]
```

## Prerequisites

- The Sanity CLI. The CLI ships with the [main Sanity package](https://www.npmjs.com/package/sanity).
You need it to log in to Sanity, which enables consuming the Embeddings Index CLI.
- The [Embeddings Index CLI](https://www.npmjs.com/package/@sanity/embeddings-index-cli). 

## Installing the Embeddings Index CLI

```sh
# Installing the Embeddings Index CLI globally
npm install --global --save-dev @sanity/embeddings-index-cli

# Installing the Embeddings Index CLI for a specific Sanity project
cd path-to/my-sanity-project/root-dir
npm install --save-dev @sanity/embeddings-index-cli

# Running the Embeddings Index CLI commands without installation
npx @sanity/embeddings-index-cli <command> [<arguments>]
```

## Embeddings Index CLI commands

To view the built-in help, run:

```
# Prints the help for the available commands and arguments
embeddings-index-cli --help

# Alternatively, without installing the CLI
npx @sanity/embeddings-index-cli --help
```

### Commands

#### Properties

| Property | Description |
|----------|-------------|
| create | Creates a new embeddings index in the current Sanity project.
It requires the following arguments:

--indexName: assign a descriptive name to the index.

--dataset: specify the name of an existing dataset. This is the target dataset to index. Note that the embeddings index API does not support dataset aliases.

--filter: specify the filtering criteria to include in the index only the selected subset of documents from the database.
The filter must be a valid GROQ filter without the square brackets that wrap the value assigned to _type.
Example: _type=='tutorial'

--projection: specify the projection criteria to include in the index only the selected subset of properties from the filtered documents.
The projection must be a valid GROQ projection, including curly brackets.
Example: {title, author}

Alternatively, you can create an embeddings index by passing a JSON manifest file with the --manifest argument:

--manifest <manifest-file-name>.json

For more information on creating a JSON manifest file, see the CLI manifest command in this reference. |
| delete | Deletes an existing embeddings index in the current Sanity project.
It requires the following argument:

--indexName: the name of the index to delete.

Alternatively, you can specify an existing JSON manifest file instead of indexName:

--manifest <manifest-file-name>.json |
| get | Retrieves status information about a specific embeddings index in the current Sanity project.
It requires the following argument:

--indexName: the name of the index whose status you want to retrieve.

Alternatively, you can specify an existing JSON manifest file instead of indexName:

--manifest <manifest-file-name>.json |
| list | Gets the status of all existing embeddings indexes in a Sanity project. |
| manifest | Creates a JSON manifest file with the configuration of an embeddings index, and saves the file to the specified location.

It requires the following arguments:

--out: specify the name of the JSON manifest file and, if necessary, the path to the directory to save it to.
If you don't specify a path, the JSON manifest file is saved to the current location in the Sanity project.
Example: <manifest-file-name>.json

--indexName: see the same argument under create.

--dataset: see the same argument under create.

--filter: see the same argument under create.

--projection: see the same argument under create. |
| query | Queries an embeddings index.
Returns an array of document IDs with their relevance score, based on the queried input string.

It requires the following arguments:

--indexName: the name of the index you want to query

--text: enter the content that you want to retrieve from the database using the embeddings index.
The content can be a string of text or a valid JSON-formatted document.

Examples

Query the embeddings index to retrieve relevant documents whose content matches the following text string:

"This is a song about vegetables."

Query the embeddings index to retrieve relevant documents whose content matches the following JSON document:

'{"_type": "lyrics", "title": "Call Any Vegetable"}' |


### Options

#### Properties

| Property | Description |
|----------|-------------|
| --debug | Prints the stack trace. Useful to inspect errors. |
| --help | Prints the CLI built-in help. |
| --silent | Doesn't print any information or warning messages.
Use either --silent or --verbose. Don't specify both options. |
| --verbose | Logs extensive information and warning messages.
Use either --silent or --verbose. Don't specify both options. |
| --version | Prints the version number of the currently installed embeddings index CLI. |


## Further reading

[embeddings-index-cli package on the npm registry](https://www.npmjs.com/package/@sanity/embeddings-index-cli)







# Specifications

#### Query language

[Syntax](/docs/specifications/groq-syntax)

[Data Types](/docs/specifications/groq-data-types)

[Functions](/docs/specifications/groq-functions)

[Full GROQ spec](https://spec.groq.dev/)



#### Compute and AI

[Blueprint configuration reference](/docs/specifications/blueprint-config)

[Function handler reference](/docs/specifications/function-wrapper)



#### Beyond Sanity

[Portable Text](https://www.portabletext.org/)

[Content Source Maps](https://github.com/sanity-io/content-source-maps)

[Mendoza](https://github.com/sanity-io/mendoza/tree/main)





# Syntax

> [!TIP]
> Protip
> If you are just getting started with the Sanity query language GROQ, you should probably read the how-to first.

A typical GROQ query has this form:

```groq
*[ <filter> ]{ <projection> }
```

4. `*`  returns all documents in the dataset that the current user has permissions to read. 
4. The documents are passed to a filter (`[]`), which retains documents for which the expression evaluates to `true`. 
4. The retained documents are passed to an optional projection. The projection determines how the result should be formatted. If no projection is specified, all data is returned.

A GROQ query of this form operates as a query pipeline, where the results from each component are passed as inputs to the next. The filter and projection are optional, and a query can have any number of them in any order.

In pipeline components, document attributes can be accessed by name. For example, this query would fetch directors born since 1970 and return their name, year of birth, and a list of their movies:

```groq
*[ _type == "director" && birthYear >= 1970 ]{
  name,
  birthYear,
  "movies": *[ _type == "movie" && director._ref == ^._id ]
}
```

For a complete introduction to GROQ, please see the [how-to](/docs/content-lake/how-queries-work).

## JSON Superset

GROQ's syntax is a superset of JSON, so any valid JSON value is a valid GROQ query (that returns the given value). Below are a few examples of JSON values:

```json
"Hi! 👋"
```

```json
["An", "array", "of", "strings"]
```

```json
{
  "array": ["string", 3.14, true, null],
  "boolean": true,
  "number": 3.14,
  "null": null,
  "object": {"key": "value"},
  "string": "Hi! 👋"
}
```

For more information on JSON syntax, see the [JSON specification](https://tools.ietf.org/html/rfc8259).

## Whitespace

Whitespace is not significant in GROQ, except for acting as a token separator and comment terminator. Any sequence of the following characters is considered whitespace, with Unicode code points in parenthesis:

- Tab (`U+0009`)
- Newline (`U+000A`)
- Vertical tab (`U+000B`)
- Form feed (`U+000C`)
- Carriage return (`U+000D`)
- Space (`U+0020`)
- Next line (`U+0085`)
- Non-breaking space (`U+00A0`)

Whitespace inside a string literal is interpreted as-is.

## Comments

Comments serve as query documentation and are ignored by the parser. They start with `//` and run to the end of the line:

```groq
{
  // Comments can be on a separate line
  "key": "value" // Or at the end of a line
}
```

Comments cannot start inside a string literal.

## Expressions

An expression is one of the following:

- A literal, attribute lookup, parameter, or constant.
- An operator invocation (and, by extension, a pipeline).
- A function call.

Expressions can be used anywhere that a value is expected, such as object values, array elements, operator operands, or function arguments. The expression is in effect replaced by the value which it evaluates to.

> [!WARNING]
> Gotcha
> Due to parser ambiguity with filters, the following access operators can only take literals, not arbitrary expressions: array element access (e.g. array[0]), array slices (e.g. array[1..3]), and object attribute access (e.g. object["attribute"]).

### Selectors

A selector is a subset of an expression used to search for fields inside a document. You can only use them in certain functions—at this time, Delta GROQ functions, to select part of a document. See the [Delta GROQ functions](/docs/specifications/groq-functions) and the Selectors section for a list of available functions and selectors.

## Literals

Literals are inline representations of constant values, e.g., `"string"` or `3.14`. GROQ supports all JSON literals, with a few enhancements and additional data types.

For more information on the data types themselves, see the [data types](/docs/specifications/groq-data-types) reference.

### Boolean and Null Literals

The constants `true`, `false`, and `null`.

### Integer Literals

A sequence of digits, e.g., `42`. Leading zeroes are ignored.

### Float Literals

Floats have an integer part, a fractional part, and an exponent part. The integer part is required, and at least one of the fractional or exponent parts must be given.

The integer part is equivalent to an integer literal. The fractional part is a decimal point `.` followed by a sequence of digits. The exponent part is `e` or `E`, followed by an optional `+` or `-` sign followed by an integer specifying base-10 exponentiation.

The following are examples of float literals:

```json
3.0
3.14
3e6      // Equivalent to 3000000.0
3.14eE0  // Equivalent to 3.14
3.14e-2  // Equivalent to 0.0314
```

### String Literals

A sequence of zero or more UTF-8 encoded characters surrounded by single or double quotes, e.g., `"Hello world! 👋"`. The following escape sequences are supported (mirroring JSON), all of which are valid in both single- and double-quoted string literals:

- `\\`: backslash
- `\/`: slash
- `\'`: single quote
- `\"`: double quote
- `\b`: backspace
- `\f`: form feed
- `\n`: newline
- `\r`: carriage return
- `\t`: tab
- `\uXXXX`: UTF-16 code point, where `XXXX` is the hexadecimal character code
- `\uXXXX\uXXXX`: UTF-16 surrogate pair

### Array Literals

A comma-separated list of values enclosed by `[]`, e.g. `[1, 2, 3]`. An optional trailing comma may follow the final element.

### Object Literals

A comma-separated list of key-value pairs enclosed by `{}`, where the key and value of each pair is separated by `:`, e.g. `{"a": 1, "b": 2}`. Keys must be strings. An optional trailing comma may follow the final pair.

### Pair Literals

Two values separated by `=>`, e.g. `"a" => 1`.

### Range Literals

Two values separated by `..` (right-inclusive) or `...` (right-exclusive), e.g. `1..3` or `1...3`.

## Identifiers

Identifiers name query entities such as attributes, parameters, functions, and some operators. Identifiers must begin with `a-zA-Z_`, followed by any number of characters matching `a-zA-Z0-9_`. Parameters are prefixed with `$`.

### Reserved Keywords

The following keywords are reserved and cannot be used as identifiers:

- `false`
- `null`
- `true`

## Attribute Lookup

A bare identifier looks up the value of the corresponding attribute in the document or object at the root of the current scope. For example, the following query `category` returns the value of the `category` attribute of the document currently being considered by the filter:

```groq
*[ category == "news" ]
```

If the attribute does not exist, or if the root value of the scope is not a document or object, then the identifier will return `null`.

> [!TIP]
> Protip
> JSON allows attribute keys to be any arbitrary UTF-8 string. In cases where the key is not a valid GROQ identifier, it can instead be accessed by using the @ operator (typically returning the current document) and the [] attribute access operator, e.g. @["1 illegal name 🚫"].

### Attribute Scope

Attribute lookups are scoped such that the same identifier may refer to different attributes in different contexts. New scopes are created by pipeline components, typically by iterating over the piped array elements and evaluating an expression in the scope of each element.

#### @ Operator – Access current scope

The `@` operator can be used to access the root value of the current scope.

```groq
// @ refers to the current number being evaluated
// Returns numbers in the array if they're greater than or equal to 10
numbers[ @ >= 10 ]

// @ refers to the myArray value
// This query returns the number of items in the myArray array
*{"arraySizes": myArray[]{"size": count(@)}} 
```

#### ^ Operator – Access the parent scope

Scopes can also be nested, in which case the `^` operator can be used to access the root value of the parent scope. Consider the following query:

```groq
*[ _type == "movie" && releaseYear >= 2000 ]{
  title,
  releaseYear,
  crew{name, title},
  "related": *[ _type == "movie" && genre == ^.genre ]
}
```

In the filter, `_type` and `releaseYear` access the corresponding attributes of each document passed from `*`. Similarly, in the projection, `title`, `releaseYear`, and `crew` access the corresponding attributes from each document passed from the filter. However, in the nested `crew` projection, `name` and `title` access the attributes of each object passed from the `crew` object - notice how the outer and inner `title` identifiers refer to different attributes (one is from the movie, the other is from the crew member).

The `related` pipeline components also create new scopes where `_type` and `genre` refer to the attributes of each document fetched from the preceding `*` operator, not those of the surrounding projected document. Notice how the `^` operator is used to access the document at the root of the parent (outer) scope and fetch its `genre` attribute.

## Operators

GROQ supports nullary, unary, and binary operators, which return a single value when invoked. Unary operators can be either prefix or postfix (e.g. `!true` or `ref->`), while binary operators are always infix (e.g. `1 + 2`). Operators are made up of the characters `=<>!|&+-*/%@^`, but identifiers can also be used to name certain binary operators (e.g., `match` which case they are considered reserved keywords.

## Functions

GROQ function calls are expressed as a function identifier immediately followed by a comma-separated argument list in parentheses, e.g., `function(arg1, arg2)`. An optional trailing comma may follow the final argument. Functions can take any number of arguments (including zero), and return a single value.

## Pipe Functions

Pipe functions ([order()](https://www.sanity.io/docs/groq-pipeline-components#9a5a019b1465) and [score()](https://www.sanity.io/docs/groq-functions#be9f618a7086)) must be preceded by the [pipe operator](https://www.sanity.io/docs/groq-operators#80749b4f431a) (`|`). The left-hand expression will be an array that the pipe operator will pass to the right-hand pipe function, returning a new array.

`*[_type == "post"] | order(_createdAt desc)` will pass an array of all documents with a `_type` of `post` into the `order()` function, returning a new array of those documents sorted by the `_createdAt` property in descending order.



# Data Types

GROQ is strongly typed, meaning there is no implicit type conversion. Type conflicts (e.g. `1 + "a"`) will yield `null`.

For more information on how to express literal values for various data types, see the [Syntax section](/docs/specifications/groq-syntax).

## Basic Data Types

### Boolean

Logical truth values, i.e.`, true` and `false`.

### Float

Signed 64-bit double-precision floating-point numbers, e.g., `3.14`, using the [IEEE 754 binary64 format](https://en.wikipedia.org/wiki/Double-precision_floating-point_format#IEEE_754_double-precision_binary_floating-point_format:_binary64). These have a magnitude of roughly 10⁻³⁰⁷ to 10³⁰⁸ and can represent 15 significant figures with exact precision - beyond this; significant figures are rounded to 53-bit precision. The special IEEE 754 values of infinity and NaN (not a number) are not supported and are coerced to `null`.

### Integer

Signed 64-bit integers, e.g., `42`, with a range of -2⁶³ to 2⁶³-1.

### Null

An unknown value expressed as `null`. This is the SQL definition of null, which differs from the typical definition of "no value" in programming languages, and implies among other things, that `1 + null` yields `null` (1 plus an unknown number yields an unknown number). See the [Operators section](/docs/specifications/groq-operators) for further implications of this.

### String

A UTF-8 encoded string of characters, e.g., `"Hi! 👋"`. The maximum string length is undefined but is fundamentally limited by the maximum document size and maximum HTTP request size as listed in [Technical Limits](/docs/content-lake/technical-limits).

## Composite Data Types

### Array

An ordered collection of values, e.g. `[1, 2, 3]`. Can contain any combination of other types, including other arrays.

### Object

An unordered collection of key/value pairs (referred to as attributes) with unique keys, e.g. `{"a": 1, "b": 2}`. Keys must be strings, while values can be any combination of other types, including other objects. If duplicate keys are specified, the last key is used.

### Pair

A pair of values, e.g. `"a" => 1`. Pairs can contain any combination of other types, including other pairs, and are mainly used internally with projection conditionals and `select()`. In returned JSON, pairs are represented as arrays with two values.

### Range

An interval containing all values ordered between the start and end values (for details on ordering, see "Comparison operators" in the [Operators section](/docs/specifications/groq-operators)). The starting value is always included, while the end may be either included or excluded. A right-inclusive range is expressed as two values separated by `..`, e.g., `1..3` returns `1,2,3`, while a right-exclusive range is separated by `...`, e.g., `1...3` returns `1,2`.

Ranges can have endpoints of any basic data type, but both endpoints must be of the same type (except integers and floats, which can be used interchangeably). Ranges with incompatible or invalid endpoints types will yield `null`.

> [!CAUTION]
> Known issue
> Ranges currently may not work with all ordered types, and endpoint type conflicts may be handled incorrectly. Ranges also cannot be expressed in returned JSON.

Ranges are mainly used internally, e.g., with the `in` operator and array slice access operator. The endpoints may have context-dependent semantics, such as array slices with the range `[2..-1]` will cover the range from the third array element to the last element, while the same range is considered empty when used with `in`. For more details, see the documentation for [the relevant operators](/docs/specifications/groq-operators).

## Subtypes

Subtypes are subsets of basic or composite types. Operators and functions that can act on the supertype can always act on the subtype as well, but the behavior may be modified, and some operators and functions can only act on the subtype.

### Datetime

Datetimes are strings with ISO 8601-formatted date/time combinations, e.g., `2018-11-04T13:45:21Z`.

> [!CAUTION]
> Known issue
> Datetimes are currently treated as plain strings, so some operations may not work as expected, e.g. comparisons will not take the time zone into account.

### Document

Documents are objects which contain the following special attributes (in addition to other arbitrary attributes):

- `_id` (path, required): The unique ID of the document. IDs must begin with the characters `a-zA-Z0-9_`, followed by any of the characters `a-zA-Z0-9_.-`, and end with `a-zA-Z0-9_-`. IDs can have a maximum length of 128 characters and may not contain more than a single consecutive `.` character.
- `_type` (string, required): An arbitrary document type. Types may not be longer than 255 characters, may not contain `,` or `#`, and may not begin with `.` or `_`.
- `_rev` (string): A randomly generated revision ID corresponding to the transaction ID which generated this revision.
- `_createdAt` (datetime): The time when the document was created.
- `_updatedAt` (datetime): The time when the document was last modified.

### Path

Paths are strings that represent a node or branch in a tree (i.e., hierarchy). They are typically used for document IDs, where each path segment is separated by `.`, e.g., `articles.business.finance.4861`.

Paths can also be glob patterns, using `*` wildcards which do not cross `.` and `**` wildcards which do cross `.`. For example, the path `articles.business.finance.4861` is matched by the path `articles.business.**`, but not `articles.business.*`.

Paths are most commonly used in conjunction with the `in` operator when filtering documents, e.g., `_id in path("articles.business.**")`.

### Reference

References are objects that represent a reference to a different document. They have the following special attributes (in addition to other arbitrary attributes):

- `_ref` (path, required): The ID of the referenced document.
- `_weak` (boolean): If `true`, referential integrity is not enforced, i.e., the reference is allowed to point to a non-existent document. Defaults to `false`.



# Parameters

Parameters are client-provided values that are substituted into queries before execution. Their names must begin with `$` followed by a valid identifier, and their values must be JSON literals of any type (take care to quote strings). Since they are JSON literals they can only contain values, not arbitrary GROQ expressions, and are safe to pass from user input.

For example, the following query may be given parameters such as `$type="myType"` and `$object={"title": "myTitle", "value": 3}`:

```
*[ _type == $type && title == $object.title && value > $object.value ]
```

In the HTTP API, parameters are passed via URL query parameters, see the [HTTP API documentation](/docs/http-reference/query) for details.

## Predefined Parameters

> [!WARNING]
> Gotcha
> Predefined parameters are deprecated and will be removed in a future version of the API. Please use the functions identity() and now() instead.

The following parameters are predefined and available for use in all GROQ queries:

- `$identity` (string): The ID of the current user, or `<anonymous>` for unauthenticated users.
- `$now` ([datetime](https://www.sanity.io/docs/groq/groq-data-types#datetime)): The current server time (UTC).



# Operators

## Logical Operators

GROQ supports the use of the following logical operators:

- AND (`&&`)
- OR (`||`) 
- NOT (`!`)

The operand – the value that the operator evaluates – must be a boolean or `null`. If the evaluation yields an invalid type, it will be coerced to `null`.

### `&&` – Logical AND

`&&` returns `true` if both operands are `true`.

Specifically, `&&` follows the evaluation order:

8. returns `false` if either operand is `false`
8. otherwise, returns `null` if either operand is `null` (or not a boolean)
8. otherwise, returns `true`

#### Truth table

```groq
true && true   // returns true

true && false  // returns false

false && true  // returns false

false && false // returns false

true && null   // returns null

false && null  // returns false

null && true   // returns null

null && false  // returns false

null && null   // returns null
```

#### GROQ examples

```groq
// Checks if a document: 
// is of _type "author"
// AND has a name value of "John Doe"
// If both are true, returns all documents matching
*[_type == "author" && name == "John Doe"]


// Checks if a document:
// is of _type "movie"
// AND has a title of "Arrival"
// If both are true, returns all documents matching
*[_type == "movie" && title == "Arrival"]

// Checks if a document:
// is of _type "movie"
// AND includes the string 'sci-fi' in its genres field
// If both are true, returns all documents matching
*[_type == "movie" && "sci-fi" in genres]
```

### `||` – Logical OR

`||` returns `true` if either operand is `true`.

Specifically, `||` follows the evaluation order:

16. returns `true` if either operand is `true`
16. otherwise, returns `null` if either operand is `null` (or not a boolean)
16. otherwise, returns `false`

#### Truth Table

```groq
true || true   // Returns true

true || false  // Returns true

false || true  // Returns true

false || false // Returns false

true || null   // Returns true

false || null  // Returns null

null || true   // Returns true

null || false  // Returns null

null || null   // Returns null
```

#### GROQ examples

```groq
// Checks if a document:
// has a number property `popularity` greater than 15
// OR has a releaseDate after 2016-04-25
// Returns all documents that match EITHER condition
*[popularity > 15 || releaseDate > "2016-04-25"]

// Checks if a document:
// has a postCount greater than 20
// OR has a boolean property `featured` equal to true
// Returns all documents that match EITHER condition
*[postCount > 20 || featured]

// Checks if a document:
// has a name value of "John Doe"
// OR has a slug.current property containing the word "forever"
// Returns all documents that match EITHER condition
*[name == "John Doe" || slug.current match "forever"]
```

### `!` – Logical Not

`!` returns the logical negation of the value of the operand.

> [!WARNING]
> Gotcha
> !value will always return null unless value is either true or false.

#### Truth table

```groq
!true  // Returns false

!false // Returns true

!null  // Returns null
```

#### GROQ examples

```groq
// Returns all docs that don't start with a.b.
*[!(_id in path("a.b.**"))]

// Returns all documents where the boolean `awardWinner` is false
*[!awardWinner]
```

## Comparison Operators

Comparison operators compare two values, returning `true` if the comparison holds or `false` otherwise. The operands must be of the same type (except for integers and floats, which are interchangeable). If the operands are of different types, the comparison returns `null`. If any operand is `null`, the comparison returns `null`.

Comparisons are only supported for booleans, integers, floats, and strings. Comparisons using any other data types will return `null`.

Equality comparisons (`==` and `!=`) have the following semantics:

- **Booleans**: identical logical truth values.
- **Integers and floats**: identical real number values.
- **Strings**: identical lengths and Unicode code points (case sensitive).
- **Nulls**: always yield `null`.

Ordered comparisons (`>`, `<`, `>=`, and `<=`) use the following order:

- **Booleans**: `true` is greater than `false`.
- **Integers and floats**: numerical order.
- **Strings**: numerical Unicode code point order (i.e., case-sensitive), compared character-by-character. For overlapping strings, shorter strings are ordered before longer strings.
- **Nulls**: always yield `null`.

> [!CAUTION]
> Known issue
> String fields that are longer than 1024 characters are not available for search using equality operators (==, !=, <, <=, >, and >=) and are not sortable.
> 
> The match operator works on a string field of any length.

### `==` Equality

Returns `true` if the operands are considered equal.

### `!=` Inequality

Returns `true` if the operands are considered not equal.

### `>` Greater Than

Returns `true` if the left-hand operand is greater than (ordered after) the right-hand operand.

### `<` Lesser Than

Returns `true` if the left-hand operand is lesser than (ordered before) the right-hand operand.

### `>=` Greater Than or Equal

Returns `true` if the left-hand operand is considered greater than or equal to the right-hand operand.

### `<=` Lesser Than or Equal

Returns `true` if the left-hand operand is considered lesser than or equal to the right-hand operand.

### `in` Compound Type Membership

```groq
// Returns true if document's _type
// is included in the array (either "movie" or "person")
*[_type in ["movie", "person"]]

// Returns true if "myTag" is in tags array
*["myTag" in tags]
```

Returns `true` if the left-hand operand is contained within the right-hand operand. The right-hand operand may be an [array](https://www.sanity.io/docs/groq-data-types#9b39ffa2eba7), [range](https://www.sanity.io/docs/groq-data-types#6ca4a5ee2658), or [path](https://www.sanity.io/docs/groq-data-types#2061826a7987).

If the right-hand operand is an array, the left-hand operand may be of any type. The left-hand operand is compared for equality (`==`) with each element of the array, returning `true` as soon as a match is found. If no match is found, it returns `null` if the array contains a `null` value, otherwise `false`.

If the right-hand operand is a range, the left-hand operand must be of the same type as the range endpoints. Returns `true` if the left-hand operand is ordered between the range endpoints, otherwise `false`.

If the right-hand operand is a path, the left-hand operand must be a string or path. Returns `true` if the left-hand operand is matched by the right-hand path pattern, otherwise `false`.

#### not `in`

To find results where the left-hand operand is **not** contained within the right-hand operand, the entire expression can be negated with the [logical not operator](https://www.sanity.io/docs/groq-operators#6c2acd53a6f3), `!`. Parentheses are required to apply the logical not operator to the entire expression.

```groq
// Returns true if document's _type
// is *not* included in the array (neither "movie" nor "person")
*[!(_type in ["movie", "person"])]

// Returns true if "myTag" is in tags array
*[!("myTag" in tags)]
```

> [!WARNING]
> Gotcha
> In GROQ v1, it was possible to use == to compare a string against an array of strings (e.g., someArray[].tags == "something"). This behaviour was inconsistent with the GROQ specification and was fixed in v2021-03-25 (it will no longer return true).
> 
> The in operator correctly compares the left hand operand to each element in an array for equality, returning true when a match is found (e.g., "something" in someArray[].tags). This approach is consistent with the GROQ specification and is the ideal way to compare a string against an array of strings.

## Access Operators

Because of the nested nature of data, it's often important to access members of compound data types like objects and arrays. Access operators allow the use of nested content in operations.

Access operators will return `null` if the member does not exist or the operator is incompatible with the left-hand operand's type. Access operators can be chained. Each operator accesses the result of the preceding chain, e.g., `object.ref->array[1]`.

### `*` Everything

Takes no operands and returns an array of all stored documents that the current user has access to. It is typically used at the beginning of a GROQ query pipeline, such as `*[ _type == "movie" ]{ title, releaseYear }`.

```groq
// Returns all items in the root array
*[]

// Returns all items from the root array
// matching the filter provided (documents with _type of "movie")
*[_type == "movie"]
```

> [!WARNING]
> Gotcha
> Not to be confused with the * multiplication operator, which takes two operands (the factors to be multiplied).

### `@` This

Takes no operands and returns the root value of the current scope, or `null` if no root value exists. 

For example, in the document filter `*[ @.attribute == "value" ]` it refers to the currently filtered document, and in the expression `numbers[@ >= 10]` it refers to the currently filtered number of the `numbers` array.

```groq
// @ refers to the root value (document) of the scope
*[ @["1"] ] 

// @ refers to the myArray array
// Returns the total number of items in my Array
*{"arraySizes": myArray[]{"size": count(@)}} 
```

### `^` Parent

Takes no operands and returns the root value of the parent scope, or `null` if no parent scope exists.

```groq
// Value of ^ is the current doc in the "someParent" array
*[_type == "someParent"]{ 
  "referencedBy": *[ references(^._id) ]
}

// Using the ^ operator to refer to the enclosing document. Here ^._id refers to the id
// of the enclosing person record.
*[_type=="person"]{
  name,
  "relatedMovies": *[_type=='movie' && references(^._id)]{ title }
}

// person.someObj.parentName returns root name value
*[_type=="person"]{
  name,
  someObj{
    name,
    "parentName": ^.name
  }
}
```

#### Accessing higher scopes

Multiple parent operators can be chained together to access two or more scopes up.

```groq
*[_type == "content"]{
  "children": *[references(^._id)]{
    "grandchildren": *[references(^._id) && references(^.^._id)]
  }
}
```

In the example above, the `"children"` filter will return documents that reference the parent document (each one returned from `*[_type == "content"]`).

In the `"grandchildren"` filter, we want to return documents that reference the parent document (which will be returned from the `"children"` filter) as well as the grandparent document (which will be returned from `*[_type == "content"]`). The chained parent operator (`^.^`) is required to go two levels up the scope.

To move higher in scope, additional parent operators can be added.

### `.<identifier>` Object Attribute Access

Returns the value of the object attribute given by the right-hand identifier, e.g. `object.attribute`.

```groq
// Returns the name string from someObject
*[_type == "document"] {
  "nestedName": someObject.name
}
```

### `[<string>]` Object Attribute Access

Returns the object attribute with the given key, e.g., `object["attribute"]`. This is equivalent to the `.` access operator, but useful when the attribute name is not a legal GROQ identifier.

```groq
// Returns the illegalIdentifier value from someObject
*[_type == "document"] {
  "nestedName": someObject["illegalIdentifier"]
}
```

> [!WARNING]
> Gotcha
> The attribute name must be a string literal due to parser ambiguity with filters.

### `->` Reference Access (dereference)

Returns the document referenced by the left-hand reference instead of the reference values. It may optionally be followed by an attribute identifier or data projection, in which case it returns the value of the given attribute(s) of the referenced document. If the reference points to a non-existent document (for a weak reference), it returns `null`.

```groq
*[_id == "someDocument"]{
  referencedDoc->, // Returns all data
  "referenceName": referencedDoc->name // Returns the name value
  "referenceProjection" referenceDoc->{
    title,
    description
  } // Returns the title and description
}
```

### `[<integer>]` Array Element Access

Returns the array element at the given zero-based index, e.g., `array[2]` yields the third array element. Negative indices are based at the end of the array, e.g., `array[-2]` yields the second-to-last element.

> [!WARNING]
> Gotcha
> The element index must be an integer literal due to parser ambiguity with filters.

### `[<range>]` Array Slice

Returns a new array containing the elements whose indices fall within the range, e.g., `array[2..4]` yields the new array `[array[2], array[3], array[4]]`. Ranges may extend beyond array bounds.

- `..` includes the right-index, e.g. `1..3` returns 4 items
- `...` excludes the right-index, e.g. `1...3` returns 3 items.

Negative range endpoints are based at the end of the array, e.g., `array[2..-1]` yields all elements from the third to the last. If the right endpoint falls before the left endpoint the result is an empty array.

> [!WARNING]
> Gotcha
> The range must be a range literal due to parser ambiguity with filters.

### `[<boolean>]` Array Filter

Returns a new array with the elements for which the filter expression evaluates to `true`, e.g., `people[birthYear >= 1980]`. The filter is evaluated in the scope of each array element.

> [!TIP]
> Protip
> This operator is actually the filter pipeline component in disguise, since it has an implicit | operator before it.

### `[]` Array Traversal

Traverses the left-hand array, applying the optional right-hand access operator to each element and collecting the resulting values in a flat array - e.g., `array[].attribute` yields a flat array of the `attribute` attribute value of each array element, and `array[]->name` yields a flat array of the `name` attribute values of each document referenced by `array`. If no right-hand access operator is given, it defaults to returning a flat array containing each traversed element.



```groq
// "cast" loops through the castMembers array
// each person dereferences to return each person's name
*[_type=='movie']{title,'cast': castMembers[].person->name}

// Returns
{
  title: "Interstellar",
  cast: [
    "Matthew McConaugheh",
    "Anne Hathaway",
    "Matt Damon",
    ...
  ]
}
```

### `...` Array/Object Expansion

Expands the right-hand array or object into the surrounding literal array or object.

```groq
// In an array
[ ...[1,2], 3, ...[4,5] ]
// Returns
[1,2,3,4,5]
 
// In an object
{ ...{"a": 1}, "b":2, {"c":3} }
//returns
{ "a": 1, "b": 2, "c":3 }
```

## Arithmetic Operators

Arithmetic operators accept any combination of float and integer operands. If any operand is a float, or if the result has a non-zero fractional part, the result is a float; otherwise, it is an integer.

The `+` operator is also used to concatenate two strings, arrays, or objects.

> [!WARNING]
> Gotcha
> Floating-point arithmetic is fundamentally imprecise, so operations on floats may produce results with very small rounding errors, and the results may vary on different CPU architectures. For example, 3.14+1 yields 4.140000000000001. The round() function can be used to round results.

### `+` Addition and Concatenation

Adds two numbers, e.g., `3+2` yields `5`. Also acts as a prefix operator for positive numbers, e.g., `+3` yields `3`.

Also concatenates two strings, arrays, or objects, e.g., `"ab"+"cd"` yields `"abcd"`. If two objects have duplicate keys, the key from the right-hand object replaces the key from the left-hand one.

### `-` Subtraction

Subtracts two numbers, e.g., `3-2` yields `1`. Also acts as a prefix operator for negative numbers, e.g., `-3`.

### `*` Multiplication

Multiplies two numbers, e.g., `3*2` yields `6`.

### `/` Division

Divides two numbers, e.g., `3/2` yields `1.5`. Division by `0` yields `null`.

### `**` Exponentiation

Raises the left-hand operand to the power of the right-hand operand, e.g., `2**3` yields `8`.

Fractional and negative exponents follow the normal rules of roots and inverse exponentiation, so e.g., the square root of `4` is taken with `4**(1/2)` (yielding `2`), and the inverse square root of `4` is taken with `4**-(1/2)` (yielding `0.5`).

### `%` Modulo

Returns the remainder of the division of its operands, e.g.,, `5%2` yields `1`. The remainder has the sign of the dividend and a magnitude less than the divisor.

## Full-Text Search Operators

Full-text search operators perform searches of text content using inverted search indexes. Content is tokenized as words (i.e. split on whitespace and punctuation), with no stemming or other processing.

### `match` Full-text Search

Searches the left-hand operand for individual words that match the text pattern(s) given in the right-hand operand, returning `true` if a match is found; otherwise it returns `false`. If the right-hand operand contains `null` then `match` will return `null`.

Patterns are strings that use `*` as wildcards, and any number of wildcards can be used at any position. For example, `foo*` matches any word starting with `foo`, and `foo*bar` matches any word starting with `foo` and ending with `bar`. If the pattern does not contain any wildcards, it must exactly match a whole word in the left-hand operand.

Both the left-hand and right-hand operands can be either strings or arrays of strings. All patterns in the right-hand operand must match anywhere in the left-hand operand, e.g. `["foobar", "baz"] match ["foo*", "*bar"]` returns `true`. The right-hand operand is also tokenized in the same way as the underlying content (by splitting on whitespace and punctuation) so that, e.g. `"foo bar"` is equivalent to `["foo", "bar"]`. 

```groq
// title contains a word starting with "wo"
*[title match "wo*"] 

// title and body combined contains a word starting with "wo" and the full word "zero"
*[[title, body] match ["wo*", "zero"]]

// title must contain both the full words "hello" and "goodbye"
*[title match ["hello", "goodbye"]]
```

> [!CAUTION]
> Known issue
> match with left-hand arrays currently only work as documented with array traversal expressions, e.g. array[].value match "pattern", and then only in certain cases. If the left-hand operand is an array attribute, e.g. array match "pattern", then it never matches. If the left-hand operand is a literal array, e.g. [a, b] match ["x","y"], then all right-hand patterns must match any single string (instead of anywhere in all strings).

> [!TIP]
> Protip
> To match against a body of portable text, one approach is to use the pt::text() function. This will strip out all the marks and return just the joined body of the portable text (almost as if it were plain text). Function details and an example using match can be found here.

## Pipe Function Call Expression

GROQ comes with built-in pipe functions ([order()](https://www.sanity.io/docs/groq-pipeline-components#9a5a019b1465) and [score()](https://www.sanity.io/docs/groq-functions#be9f618a7086)) that provide additional features. Pipe functions always accept an array on the left-hand side and return another array. The syntax is optimized for being able to chain pipe functions together with other compound expressions.

### `|` Pipe Operator

Pipe functions must be preceded with the pipe operator (`|`). The left-hand expression will be an array that's passed to the right-hand function:

```groq
// Left-hand expression | Right-hand function

* | order(_id asc)
*[_type == "post"] | order(date desc)
*[]{ _id, title } | order(_createdAt asc)

// Note in the last example that the expression used
// in the order function does not need to be passed
// in the projection.
```

The pipe operator may also precede a projection, though in that case it is optional.

> [!TIP]
> Protip
> When using the score() function, pipe it in after the filter, but before a projection. Score works off the raw, unmodified data returned from the filter so it's best to use it before other functions or projections.

## Operator Precedence

Operator precedence is listed below, in descending order and with associativity in parenthesis:

- `.` (left), `|` (left)
- `->` (left)
- `**` (right)
- `*` (left), `/` (left), `%` (left)
- `+` (left), `-` (left), `!` (right)
- `...` (right)
- `==` , `!=`, `>`, `>=`, `<`, `<=` (all left), `in` (left), `match` (left)
- `&&` (left)
- `||` (left)
- `*` (none), `@` (none), `^` (none)

Precedence can be overridden by grouping expressions with `()`, e.g. `(1+2)*3`.



# Functions

Functions in GROQ take a set of arguments of specific types and return a single value of a specific type. They may be polymorphic, i.e., accept several argument type variations possibly returning different types, and may take a variable number of arguments. Function calls return `null` if arguments have invalid types, and an error if the function does not exist.

## Function namespaces

Namespaces allow for a stronger grouping of functionality within [the GROQ specification](https://sanity-io.github.io/GROQ/). They create dedicated scopes for global functions, as well as safer distinctions for specific implementations of GROQ.

When using GROQ to query, Sanity Content Lake has a few key namespaces: 

- The `global` namespace – all the base GROQ functions
- The `pt` namespace – specific functions pertaining to Portable Text
- The `geo` namespace – functions for managing and querying against geolocation
- The `sanity` namespace – functions for querying against the current project environment
- The `delta` namespace – functions for reasoning about changes to a document
- The `math` namespace - functions for performing mathematical operations on numerical inputs
- The `string` namespace - functions for searching and processing strings
- The `array` namespace - functions for processing arrays

### Accessing functions in a namespace

All functions exist within a namespace and can be accessed via a call to the function with a prefix a string of the namespace name, followed by two colons and then the function name.

```groq
// The pt namespace contains functions related to Portable Text
// This function returns a plain text version of a Portable Text object
pt::text(ptNode)

// The geo namespace contains functions related to geolocation
// This function returns true if the second argument is fully contained in the first
geo::contains(polygon, point)
```

## Global functions

Functions that exist for all implementations of GROQ exist in the global namespace. They can be accessed without using the namespace string.

```groq
// Non-namespaced
references('someId')
// equates to
global::references('someId')

```

### `coalesce`

`coalesce(<any>...) <any>`

Takes a variable number of arguments of any type and returns the first non-`null` argument if any, otherwise `null` - e.g., `coalesce(null, 1, "a")` returns `1`.

```groq
// If title.es exists return title.es
// Else return title.en
// If neither exist, return null
*[_type == "documentWithTranslations"]{
  "title": coalesce(title.es, title.en)
} 


// If rating exists, return rating,
// Else return string of 'unknown'
*[_type == 'movie']{
  'rating': coalesce(rating, 'unknown')
}

```

### `count`

`count(<array>) <integer>`

Returns the number of elements in the passed array, e.g. `count([1,2,3])` returns `3`.

```groq
// Returns number of elements in array 'actors' on each movie
*[_type == 'movie']{"actorCount": count(actors)} 

// Returns number of R-rated movies
count(*[_type == 'movie' && rating == 'R']) 
```

### `dateTime`

`dateTime(<string>) <datetime>`

Accepts a string in [RFC3339](https://tools.ietf.org/html/rfc3339) format (e.g. `1985-04-12T23:20:50.52Z`) and returns a DateTime. This is also the format used in the `_createdAt` and `_updatedAt` fields. Typically used to let GROQ know to treat a string as a date, especially useful when you need to compare them or perform time arithmetic operations.

Subtracting two DateTimes returns the number of seconds between those time stamps. Adding a number to a `DateTime` returns the `DateTime` that amount of seconds later (or earlier if the number is negative).

```groq
*[_type == "post"]{
  title,
  publishedAt,
  "timeSincePublished": dateTime(now()) - dateTime(publishedAt)
}
```

> [!TIP]
> Protip
> You can create RFC3339-dateTime strings in JavaScript with the Date.prototype.toISOString() method.

### `defined`

`defined(<any>) <boolean>`

Returns `true` if the argument is non-`null`, otherwise `false`.

```groq
// Returns all documents if awardWinner has any value (of any type)
*[defined(awardWinner)] 
```

> [!CAUTION]
> Known issue
> String fields that are longer than 1024 characters will not provide the expected result from defined().
> 
> The match operator works on a string field of any length.

### `identity`

`identity() <string>`

Returns the identity (user ID) of the user performing the current action, or the special values `<anonymous>` for unauthenticated users and `<system>` for system-initiated actions.

### `length`

`length(<array|string>) <integer>`

Returns the length of the argument, either the number of elements in an array or the number of Unicode characters in a string, e.g., `length([1,2,3])` returns `3`, and `length("Hi! 👋")` returns `5`.

```groq
// Return posts with more than 2 authors
*[_type == "post" && length(authors) > 2]{
  title,
  authors[]->{
    name
  }
}
```

> [!WARNING]
> Gotcha
> While length() works on arrays, you should consider using count() as it's optimized for arrays.

### `lower` / `upper`

`lower(<string>) <string>`

`upper(<string>) <string>`

The `lower()` and `upper()` functions take a string and return back the string in all lowercase characters or all uppercase characters.

```groq
*{
  "upperString": upper("Some String"), // Returns "SOME STRING"
  "lowerString": lower("Some String")  // Returns "some string" 
}
```

### `now`

`now() <string>`

Returns the current time in [RFC3339](https://tools.ietf.org/html/rfc3339) format with microsecond resolution in the UTC time zone, e.g., `2021-08-19T15:51:24.846513Z`. The current time is stable within an operation such that multiple calls return identical values. This generally refers to the start time of the operation except for listener queries. This refers to the event's transaction time.

> [!NOTE]
> Caching now() in APICDN
> Using now() and our APICDN creates a conundrum: how long should now() be cached?
> 
> We have created a special caching rule that says now() is only valid for 120 seconds, plus the normal 60 seconds stale-while-revalidate after that. This gives us the opportunity to cache queries not containing now() much longer.

> [!WARNING]
> Gotcha
> Mutations using query parameters trigger two separate operations internally: first execution of queries to determine which documents to update, then a transaction to actually update the documents. now() will return different times for these two operations, referring to the start time of each operation.

```groq
// Give me all posts with a publish date in the future
*[_type == "post" && dateTime(now()) < dateTime(publishedAt)]
```

### `path`

`path(<string>) <path>`

Coerces the passed string to a path, e.g. `"a.b" in path("a.*")`.

```groq
// _id matches a.b.c.d but not a.b.c.d.e
*[_id in path("a.b.c.*")] 

// _id matches a.b.c.d and a.b.c.d.e
*[_id in path("a.b.c.**")] 

// All draft documents
*[_id in path("drafts.**")]

// Only published documents
*[!(_id in path("drafts.**"))]
```

### `references`

`references(<path|string|array>) <boolean>`

Implicitly takes the document at the root of the current scope and recursively checks whether it contains any references to the given document ID(s). It is typically used in query filters, e.g., `*[ references("abc")]` will return any documents that contain a reference to the document `abc`. If providing the function with an array of document ids, it will return `true` if any of the ids are referenced. [Learn more about references](https://www.sanity.io/docs/how-queries-work#db43dfd18d7d).

```groq
// Using the ^ operator to refer to the enclosing document. Here ^._id refers to the id
// of the enclosing person record.
*[_type=="person"]{
  name,
  "relatedMovies": *[_type=='movie' && references(^._id)]{ title }
}

```

### `round`

`round(<integer|float>[, <integer>]) <integer|float>`

Rounds the given number to the nearest integer, or to the number of decimal places given by the second, optional argument - e.g. `round(3.14)` yields `3` and `round(3.14, 1)` yields `3.1`.

### `select`

`select(<pair|any>...) <any>`

Used for conditionals, i.e. "if-else" expressions. Takes a variable number of arguments that are either pairs or any other type and iterates over them. When encountering a pair whose left-hand value evaluates to `true`, the right-hand value is returned immediately. When encountering a non-pair argument, that argument is returned immediately. Falls back to returning `null`.

```groq
// If age is 18+, return "adult" string
// Else if age is 13+, return "teen" string
// Else return "child" string
select(
  age >= 18 => "adult",
  age >= 13 => "teen",
  "child"
)

// If popularity integer is more than 20, return "high" string
// Else if popularity is more than 10, return "medium" string
// Else if popularity is less than or equal to 10, return "low"
*[_type=='movie']{
  ..., 
  "popularity": select(
    popularity > 20 => "high",
    popularity > 10 => "medium",
    popularity <= 10 => "low"
)}

// You can also use select in a shorter from
// Let's say we want to conditionally join references 
// inside a Portable Text field
*[_type == "article"]{
  ...,
  body[]{
    ...,
    _type == "product" => {
      ...,
      @->{
        name,
        price
      }
    }
  }
}
```

### `score`

`score()` can be used as a pipe operator function to assign a score to each document. See [the section on scoring](#k4798b2cba8df) at the end of this document.

### `string`

`string(<integer|float|boolean|datetime|string>) <string>`

Returns the string representation of a given scalar value. Returns `null` when passed an invalid value, including `null`.

```groq
{
  "stringInteger": string(21),         // Returns "21"
  "stringFloat": string(3.14159),      // Returns "3.14159"
  "stringSciNotation": string(3.6e+5), // Returns "360000"
  "stringTrue": string(true),          // Returns "true"
  "stringFalse": string(false),        // Returns "false"
  "stringString": string("A string"),  // Returns "A string"
}
```

One use case for `string()` is to combine a string with a scalar type, which would otherwise return `null`.

```groq
*[0] {
  'secondsAgo': dateTime(now()) - dateTime(_createdAt),
} {
  'minutesSinceCreated': 'Created ' + string(secondsAgo / 60) + ' minutes ago.'
}
```

Another use case is to coerce a date field into a string in [RFC3339](https://tools.ietf.org/html/rfc3339) format, which is useful when you need to compare datetime values or perform time arithmetic operations.

```groq
// Let's imagine a document with a year field,
// which contains a four-digit number – in this example, 2009
*[0] {
  year    // Returns 2009
}

// To compare year to a datetime field, such as now(), _createdAt,
// or _updatedAt, the year field must be converted to a string in RFC3339 format,
// but trying to append a string to the year field returns null
*[0] {
  'constructedYear': year + "-01-01T00:00:00Z"    // Returns null
}

// Using the string() function, year can be coerced to a string
// and structured in RFC3339 format
*[0] {
  'constructedYear': string(year) + "-01-01T00:00:00Z"    // Returns "2009-01-01T00:00:00Z"
}

// In this way, the year field can be used to perform time arithmetic operations
*[0] {
  'secondsSinceYear': dateTime(now()) - dateTime(string(year) + "-01-01T00:00:00Z")
}
```

## Geolocation functions

The `geo` namespace contains a number of useful functions for creating and querying against locations in your data. Each function must be prefixed with the `geo::` namespace syntax.

> [!WARNING]
> Gotcha
> The geo() function and functions in the geo:: namespace require v2021-03-25 or later of the GROQ API.

### Geo documents

The functions in this section require a data type of `geo`. These `geo` documents are represented JSON as [GeoJSON](https://geojson.org/). Functions that accept a `geo` type will attempt to coerce non-`geo`-type data into the proper format following the `geo()` constructor rules.

### `geo(object)`

The `geo()` function accepts an object as a parameter and, if possible, coerces the value to a geo-type document by a set of rules. These objects are represented in JSON as GeoJSON.

- If the object is already a geo document, return the object.
- If the object has a set of `lat` and `lng` (or `lon`) keys, return a `geo` object for the given point. If additional data exists on the object it will be removed from the final geo document.
- If the object contains the key `type` (*note: not _type*), and the value of `type` matches one of the following strings then return a `geo` object with those values:- Point
- LineString
- Polygon
- MultiPoint
- MultiLineString
- MultiPolygon
- GeometryCollection


- If none of the conditions are met, return `null`.

### `geo::latLng(latFloat, lngFloat)`

The `latLng` function is a short-hand for creating a new geo object for a singular point. Returns a geo object from the latitude and longitude floats provided.

```groq
// Returns a geo object corresponding to the center of Oslo
geo::latLng(59.911491, 10.757933)
```

### `geo::distance(geo-point, geo-point)`

The `distance()` function takes points and returns a numeric value for the distance between in meters.

> [!WARNING]
> Gotcha
> The function only works between points. If lines or polygons are provided, the function will return null.

```groq
// Returns the distance in meters between Oslo and San Francisco
// 7506713.963060733
geo::distance(
  geo::latLng(59.911491, 10.757933),
  geo::latLng(37.7749, 122.4194)
)

// Returns all documents that are storefronts
// within 10 miles of the storefront geopoint
*[
  _type == 'storefront' &&
  geo::distance(geoPoint, $currentLocation) < 16093.4
]
```

### `geo::contains(geoPolygon, geo)`

The `contains()` function returns true when the first geographic geography value fully contains the geographic geometry value. If either parameter is not a geo object – or not able to be coerced to a geo object following the rules of the `geo()` constructor function – the function returns `null`.

```groq
// Returns true if the neighborhood region is fully contained by the city region
geo::contains(cityRegion, neighborhoodRegion)

// For a given $currentLocation geopoint and deliveryZone area
// Return stores that deliver to a user's location
*[
  _type == "storefront" &&
  geo::contains(deliveryZone, $currentLocation)
]
```

### `geo::intersects(geo, geo)`

The `intersects()` function returns true when the two areas overlap or intersect. If either parameter is not a geo object – or not able to be coerced to a geo object following the rules of the `geo()` constructor function the function returns `null`.

```groq
// Creates a "marathonRoutes" array that contains
// all marathons whose routes intersect with the current neighborhood
*[_type == "neighborhood"] {
  "marathonRoutes": *[_type == "marathon" && 
                        geo::intersects(^.neighborhoodRegion, routeLine)  
                      ]
}
```

## Portable Text functions

The `pt` namespace contains functions for parsing Portable Text. Each function must be prefixed with the `pt::` namespace syntax.

> [!WARNING]
> Gotcha
> Functions in the pt:: namespace require v2021-03-25 or later of the GROQ API.

### `pt::text(<Portable Text array|object>) <string>`

The `text()` function is a Sanity Content Lake GROQ filter that takes in document fields which are either a Portable Text block or an array of blocks, and returns a string in which blocks are appended with a double newline character (`\n\n`). Text spans within a block are appended without space or newline. 

The function exists within the `pt` namespace and must be prefixed with `pt::`. 

> [!WARNING]
> Gotcha
> The text() function only works on text spans in the root children, i.e., alt text in an Image block will not be in the final plain text.

```groq
// Returns the body Portable Text data as plain text
*[_type == "post"] 
  { "plaintextBody": pt::text(body) }
  
// Scores posts by the amount of times the string "GROQ"
// appears in a Portable Text field
*[_type == "post"]
  | score(pt::text(body) match "GROQ")
```

## Sanity functions

The `sanity` namespace contains functions for querying against the current environment. Each function must be prefixed with the `sanity::` namespace syntax.

> [!WARNING]
> Gotcha
> Functions in the sanity:: namespace require v2021-03-25 or later of the GROQ API unless otherwise noted.

### `sanity::projectId() <string>`

The `projectId()` function returns the project ID of the current studio environment.

### `sanity::dataset() <string>`

The `dataset()` function returns the dataset of the current studio environment.

```groq
{
  'projectId': sanity::projectId(),  // Returns 'hm31oq0j', for example
  'dataset': sanity::dataset()       // Returns 'production', for example
}
```

> [!WARNING]
> Gotcha
> sanity::versionOf and sanity::partOfRelease require API version 2025-02-19 or later.

### `sanity::versionOf(<string>) <array>`

When used in the context of a filter, the `sanity::versionOf(<doc-id>)` function accepts a document ID and returns an an array of matching documents, including drafts, published, and release versions with the supplied document ID. This ID should be the root ID, without any path prefixes.

```groq
*[sanity::versionOf(<doc-id>)]
```

For example, a document ID of `foo` may return documents such as `foo`, `drafts.foo`, `versions.a.foo`, `versions.b.foo`.

### sanity::partOfRelease(<string>) <array>

The partOfRelease() function accepts a release name and returns an array of all documents that are part of that release. 

> [!TIP]
> Protip
> Release names are the final piece of a release ID. For example, a release with an id of _.releases.rEGM2JqQ3 has a name of rEGM2JqQ3. Use just the name final portion of the ID when referencing releases by name.

For example, release `a` would return `versions.a.foo`, `versions.a.bar`. `versions.a.baz`, etc.

## Delta functions

Delta-GROQ is an extension of GROQ which makes it possible to reason about *changes* done to a document. I.e. in the context of webhooks. The following functions are available:

- A `before()` function which returns the attributes done *before* the change.
- An `after()` function which returns the attributes *after* the change.
- `delta::changedAny()` which returns true if certain attributes have changed.
- `delta::changedOnly()` which returns true if *only* some attributes have changed.
- `delta::operation()` which returns a string value of `create`, `update` or `delete` according to which operation was executed.

> [!WARNING]
> Gotcha
> These functions are only available in delta mode. For the time being this means the same as saying they are available only when working with webhooks.

### `before()` and `after()`

The functions `before()` and `after()` return the attributes before and after the change. When the change is a create operation then `before()` is null, and when the change is a delete operation then `after()` is null.

These allow you to create expressive filters (`after().score > before().score` will only match when the score *increases*) and let you refer to both old and new values in projections (`'Title changed from ' + before().title + ' to ' + after().title`).

### `changedAny()` and `changedOnly()`

These diff functions are used with the namespace prefix.

```javascript
delta::changedAny(selector) -> bool
delta::changedOnly(selector) -> bool

// Example: Return true when title has changed
delta::changedAny(title)
```

Notice that these functions accept a *selector* and not a full GROQ expression. See the next section for how they work. `delta::changedAny()` uses the selector to search for values and returns `true` if any of them have changed. `delta::changedOnly()` uses the selector to search for values and returns `true` if there are no changes anywhere else.

These are very useful for filtering: You can use `delta::changedAny(title)` to only match changes done to a specific field.

We've also added variants of these functions which are available inside regular GROQ and works on provided objects:

```javascript
diff::changedAny(before, after, selector) -> bool
diff::changedOnly(before, after, selector) -> bool

// Example: This returns true because last name has changed.
diff::changedAny(
  {"firstName":"Bob","lastName":"Odenkirk"},
  {"firstName":"Bob","lastName":"Holm"},
  (firstName, lastName)
) 
```

> [!TIP]
> Editor experience
> Since the _rev and _updatedAt fields will always change when there is a change to a document, they are automatically ignored with the delta::changedOnly() function. This allows you to use delta::changedOnly(title) rather than needing to specify delta::changedOnly((title, _rev, _updatedAt)).

### Selectors

Selector is a new concept in GROQ which represents parts of a document. These are the currently supported selectors (shown used with `changedAny`):

```javascript
// One field:
delta::changedAny(title)

// Multipe fields:
delta::changedAny((title, description))

// Nested fields:
delta::changedAny(slug.current)

// Fields on arrays:
delta::changedAny(authors[].year)

// Nested fields on arrays:
delta::changedAny(authors[].(year, name))

// Filters:
delta::changedAny(authors[year > 1950].name)
```

## Array functions

The `array` namespace contains functions for processing arrays. Each function must be prefixed with the `array::` namespace syntax.

### `array::join(source <array[string|number|boolean]>, separator<string>) <string>`

Concatenates the elements in `source` into a single string, separating each element with the given `separator`. Each element will be converted to its string representation during the concatenation process.

Returns `null` if `source` is not an array, or if `separator` is not a string.

If any element in `source` does not have a string representation the string `<INVALID>` will be used in its place.

```groq
// Returns "a.b.c"
array::join(["a", "b", "c"], ".")

// Returns `null`
array::join(1234, ".")
array::join([1, 2, 3], 1)

// Returns "a.b.<INVALID>.d"
array::join(["a", "b", c, "d"], ".")
```

### `array::compact(<array>) <array>`

Returns a copy of the original array with all `null` values removed.

```groq
// Returns [1, 2, 3]
array::compact([1, null, 2, null, 3])
```

### `array::unique(<array>) <array>`

Returns a copy of the original array with all duplicate values removed. There is no guarantee that the returned array preserves the ordering of the original array.

Only values that can be compared for equality are considered for uniqueness, specifically `string`, `number`, `boolean`, and `null` values.

For example, `array::unique([[1], [1]])` will return `[[1], [1]]` since arrays cannot be compared for equality.

```groq
// Returns [1, 2, 3, 4, 5]
array::unique([1, 2, 2, 2, 3, 4, 5, 5])
```

### `array::intersects(<array>, <array>)  <boolean>`

Compares two arrays, returning true if they have any elements in common.

Only values that can be compared for [equality](https://www.sanity.io/docs/groq-operators#3b7211e976f6) are considered when determining whether there are common values.

```groq
// Returns true
array::intersects([1, 2, 3], [3, 4, 5])

// Returns false
array::intersects([1, 2, 3], ['foo', 'bar', 'baz'])
```

## Math functions

The `math` namespace contains functions that perform mathematical operations on numerical inputs. Each function must be prefixed with the `math::` namespace syntax.

### `math::avg(<array[number]>) <number|null>`

Returns the average value (arithmetic mean) of an array of numbers.

Returns `null` if the array does not contain at least one numeric value, or if any element is a non-numeric value. `null` values are ignored.

```groq
// Returns 2.5
math::avg([1, 2, 3, 4, null])

// Returns `null`
math::avg([1, 2, 3, 4, "5"])
```

### `math::max(<array[number]>) <number|null>`

Returns the largest numeric value of an array of numbers.

Returns `null` if the array does not contain at least one numeric value, or if any element is a non-numeric value. `null` values are ignored.

```groq
// Returns 1000
math::max([1, 10, 100, 1000])

// Returns `null`
math::max([1, "10", 100, 1000])

// Returns `null`
math::max([])
```

### `math::min(<array[number]>) <number|null>`

Returns the smallest numeric value of an array of numbers.

```groq
// Returns 1
math::min([1, 10, null, 100, 1000])

// Returns `null`
math::min([1, "10", 100, 1000])

// Returns `null`
math::min([])
```

### `math::sum(<array[number]>) <number|null>`

Returns the sum of an array of numbers.

Returns `0` for an empty array.

Returns `null` if the array does not contain at least one numeric value, or if any element is a non-numeric value. `null` values are ignored.

```groq
// Returns 10
math::sum([1, 2, 3, 4, null])

// Returns `null`
math::sum([1, 2, 3, 4, "5"])

// Returns 0
math::sum([])
```

## String functions

The `string` namespace contains functions that search and process strings. Each function must be prefixed with the `string::` namespace syntax.

### `string::startsWith(searchString<string>, prefix<string> ) <true|false>`

Returns `true` if the first N characters of `searchString` exactly match the N characters of `prefix`, otherwise `false`.

Returns `true` if `prefix` is an empty string.

```groq
// Returns `true`
string::startsWith("alphabet", "alpha")
string::startsWith("alphabet", "")

// Returns `false`
string::startsWith("alphabet", "bet")
```

### `string::split(original<string>, separator<string> ) <array[string]>`

Returns an array of substrings of `original` that are separated by `separator`.

If `separator` is an empty string, it returns an array containing each individual character of `original`, according to Unicode character splitting rules.

If `original` or `separator` are not strings, return `null`.

```groq
// Returns ["Split", "this", "sentence", "up"]
string::split("Split this sentence up", " ")

// Returns ["a", "b", "c"]
string::split("abc", "")

// Returns `null`
string::split(12, "1")
string::split("This is 1 way to do it", 1)
```

## Query Scoring

### `score(scoreExp)`

The `score()` function takes an arbitrary number of valid GROQ expressions and assigns a score to each result as a new field called `_score`. The `_score` field can be used to sort and filter items in an array.

> [!WARNING]
> Gotcha
> score() is a pipe function and must be separated from expressions, filters, projections, and other pipe functions that precede it with the pipe operator (|). 
> 
> score() operates on unmodified data. Place it after a filter, *[_type == "post"], but before any projections.

The score is calculated depending on the expressions used. For a `match` expression, `_score` is impacted by:

- the **frequency of matches per expression** ("One Fish, Two Fish, Red Fish, Blue Fish" matches "fish" more frequently than "The Rainbow Fish" matches "fish");
- the **frequency of matches overall** (matching three terms in a `score()` function expression versus matching one term);
- the **relevance of matches**, including:- **word count** ("Big Fish" – two words – matches "fish" with greater relevance than "A Fish Called Wanda" – four words – matches "fish") and
- **word length** (matching a shorter term will also return a higher `_score` than matching a longer term – "Blue" matches "blue" with greater relevance than "Clouds" matches "clouds").



A logical OR is identical to comma-separated terms and a logical AND is identical to a match with combined terms:

```groq
// These score() functions behave identically
* | score(title match "Red" || title match "Fish")
* | score(title match "Red", title match "Fish")

// These score() functions behave identically
* | score(title match "Red" && title match "Fish")
* | score(title match "Red Fish")
```

For each matched boolean expression, `_score` is incremented.

```groq
// Each term that is true will increment _score by 1
* | score(featured, internal, _type == 'post')

// A 'post' document with featured and internal fields
// that are both true would receive a _score of 3
```

> [!WARNING]
> Gotcha
> Documents that don't match the score expression(s) return a _score value of 0, but are not automatically removed from the array. The third example in the following code block provides a solution to remove results with a _score value of 0.

```groq
// Adds points to the score value depending 
// on the use of the string "GROQ" in each post's description 
// The value is then used to order the posts 
*[_type == "post"] 
  | score(description match "GROQ") 
  | order(_score desc) 
  { _score, title }
  
// Adds a point for matches in the title OR description
*[_type == "post"] 
  | score(title match "GROQ" || description match "GROQ") 
  | order(_score desc) 
  { _score, title }
  

// Orders blog posts by GROQ matches
// Then filters the results for only items that matched
// by checking for _score values greater than 0
*[_type == "post"] 
  | score(description match "GROQ") 
  | order(_score desc) 
  { _score, title }
  [ _score > 0 ]
```

> [!WARNING]
> Gotcha
> score() cannot take a complex expression, including the use of functions (besides boost()), dereferencing, or subqueries.

### `boost(scoreExp, boostValue)`

The `boost()` function can be used to create a sense of weight in a scoring algorithm. It accepts two required arguments: an expression and the amount to boost the score if the expression returns true.

Like in the `score()` function, a matched expression will increase `_score` for each instance of a match, but by a multiple of the boost value. For example, a `boostValue` of `3` would increase `_score` by three times the amount it would increment by default (that is, without `boost()`).

The `boost()` function is used inside a `score()` function.

Boost values greater than `1` will give that expression a greater-than-normal impact on `_score`. Boost values less than `1` (but greater than `0`) will give that expression a lesser-than-normal impact on `_score`. A boost value of `0`, while permitted, has the same effect as removing that expression from the `score()` function.

> [!WARNING]
> Gotcha
> The boost() function accepts only constant positive integers and floats.

```groq
// Adds 1 to the score for each time $term is matched in the title field
// Adds 3 to the score if (movie > 3) is true
*[_type == "movie" && movieRating > 3] | 
  score(
    title match $term,
    boost(movieRating > 8, 3)
  )
```

Providing multiple boosts of different values in one `score()` can create robust sorting.

```groq
// Creates a scoring system where $term matching in the title
// is worth more than matching in the body
*[_type == "movie" && movieRating > 3] | score(
  boost(title match $term, 4),
  boost(body match $term, 1),
  boost(movieRating > 8, 3)
)

// Scores games by the "impressive" difference in goals
*[_type == "game"] | score(
		boost(pointDifference > 5, 5),
		boost(pointDifference > 10, 10)
	)
```

Boost values between `0` and `1` can be used to affect `_score` to a lesser extent than a default match would. This might be useful when there is a need to finely differentiate `_score` values that might otherwise be equal.

```groq
// Boosts _score for matches in the title OR description,
// but a match on the description now has less of an impact on _score
*[_type == "post"] 
  | score(title match "GROQ" || boost(description match "GROQ", 0.3)) 
  | order(_score desc) 
  { _score, title }
```

## Additional functions

### `releases::all()`

An alias to return all releases. This is the same as performing a check on the document type. For example:

```groq
releases::all()

// This is equivalent
*[_type == "system.release"]
```

Releases are part of the [Content Releases feature](/docs/apis-and-sdks/content-releases-api).

## Custom functions

Custom functions are reusable, modular, user-defined functions. 

```groq
fn myFunctions::unfurl($ref) = $ref->{...};

*[] {
  title,
  author: myFunctions::unfurl(author)
}
```

Custom GROQ functions must be defined at the beginning of a query, and must end with a semicolon (`;`). They are made up of the following parts:

- `fn`: All functions must start with the `fn` declaration`.`
- Namespace: Custom functions must live on their own namespace. In the example above, the namespace is `myFunctions`.
- Name: The function's name, `unfurl` in the example above, identifies the function within the namespace.
- Parameters: Like functions in other languages, parameters pass data into the function. Parameters are denoted with `$`-prefixed keys, like `$ref` in the example. Custom functions are limited to one parameter at this time.
- Function body: The body of the function is the content after the equal (`=`) sign. Function bodies are limited to the formats below.

At this time, functions do not support:

- Recursion.
- Accessing the parent scope.
- Passing multiple parameters.
- Using a parameter more than once in the body.

### Function body formats

#### Standard projection

Create a reusable projection.

Format: `$param{...}`

```groq
// Function definition
fn user::bio($param) = $param{name, age}; 

// Usage
*[_type == "person"] { "info": user::bio(@) }
```

#### Follow a reference

Create a reusable projection that follows a reference.

Format: `$param->{...}`

```groq
// Function definition
fn user::bio($param) = $param->{name, age}; 

// Usage
*[_type == "post"] { "author": user::bio(author) }
```

#### Access an array

Create a reusable projection for items in an array.

Format: `$param[]{...}`

```groq
// Function definition
fn user::children($param) = $param[]{name}; 

// Usage
*[_type == "person"] { "children": user::children(children) }
```

#### Array of references

Create a reusable projection that follows references in an array.

Format: `$param[]->{...}`

```groq
// Function definition
fn user::children($param) = $param[]->{name}; 

// Usage
*[_type == "person"] { "children": user::children(children) }
```



# Pipeline Components

A pipeline connects a set of components by passing the left-hand array's value to the pipeline component given by the right-hand operand and yielding its return value. Some components must be adjacent (e.g., `*` and `[filter]` must be `*[filter]`), some must be separated with the pipe operator (e.g., `* | order()`), and others may optionally use the pipe operator (e.g., projections can be `* {projection}` or `* | {projection}`).

> [!TIP]
> Protip
> White space is not significant in GROQ except in a string literal, when terminating a comment, or when acting as a token separator (e.g., the spaces are required around match in title match "movie"), which means *[filter] can be * [filter], * | order() can be *|order(), etc.

The following query pipeline fetches all documents from the data store, passes them to a filter, then a projection, and finally orders the results:

```groq
*[ _type == "movie" && releaseYear >= 1980]{
  title,
  releaseYear,
  genre
} | order(releaseYear desc, title asc) |
```

A pipeline component typically iterates over the array elements and evaluates an expression in the scope of each iterated value (as described in "Attribute Scope" in the [Syntax section](/docs/specifications/groq-syntax)). The evaluated expression typically determines the component's return value.

> [!CAUTION]
> Known issue
> The behavior of piping non-array values is currently undefined.

## `[<boolean>]` Filter Component

Iterates over the elements of the piped array, evaluates the given boolean expression in the scope of each element and returns an array of elements for which the expression evaluated to `true`.

For example, the following filter will retain documents of type `movie` where the `releaseYear` attribute is greater than or equal to 1980:

```groq
*[ _type == "movie" && releaseYear >= 1980]
```

> [!CAUTION]
> Known issue
> Filters may retain elements for which the expression evaluates to other values as well, e.g. integers and strings.

## `[<range>]` Slice Component

Returns an array containing the elements of the piped array whose zero-based indices fall within the range, e.g. `*[2..4]` yields elements 3, 4, and 5 from the piped array. The range may extend beyond the array bounds.

Negative range endpoints are based at the end of the piped array. The syntax `array[2..-1]` yields all elements from the third to the last. If the right endpoint falls before the left endpoint, the result is an empty array.

> [!WARNING]
> Gotcha
> The range must be a literal range due to parser ambiguity with filters.

## `[<integer>]` Subscript Component

Returns the element at the given zero-based index of the piped array, e.g., `*[2]` returns the third element of the piped array, or `null` if not found. Negative indices are based at the end of the array, e.g., `array[-2]` yields the second-to-last element.

> [!WARNING]
> Gotcha
> The index must be a literal integer due to parser ambiguity with filters.

## `{}` Projection Component

Projections iterate over the elements of the piped array, generate an object of the given form evaluated in the scope of each element, and appends it to the output array. The projection may optionally be preceded by a pipe operator (`|`).

For example, the projection `*{"key": value}` iterates over all documents, and for each document generates an object with a single key `key` whose value is set to the value of the `value` attribute of the document, if any.

Attribute values in projections can be arbitrary GROQ expressions, e.g. `*{"name": firstName + " " + lastName}`.

> [!WARNING]
> Gotcha
> In v1 of the GROQ API, any attribute whose value evaluates to null is not included in the projection. As of v2021-03-25, an explicitly-named attribute whose value evaluates to null is included in the projection.

If bare attributes are given in a projection, this inserts the corresponding attribute from the input element in the output object - e.g., the projection `{_id, name}` is exactly equivalent to `{"_id": _id, "name": name}`.

> [!WARNING]
> Gotcha
> Generated projection keys do not modify the scope, so e.g. {"key": "value", "other": key} will evaluate to "other": null, since inserting "key" does not change the object at the root of the scope. If this is desired, it can be accomplished by chaining projections, e.g {"key": "value} {key, "other": key}.

Other objects can be expanded into the projection with the `...` operator. For example, `{name, ...properties}` will take all attributes from the `properties` object and place them in the root of the projected object along with the root `name` attribute.

A bare `...` is syntactic sugar for `...@`, i.e., it inserts all attributes from the currently iterated element into the projection. For example, `{..., "key": "value"}` generates an object with all of the object's original attributes in addition to the generated `key` attribute.

If multiple keys with the same name are given, then the latest key wins. 

> [!WARNING]
> Gotcha
> In v1 of the GROQ API, the expansion operator is always evaluated first regardless of its position in the projection (i.e., the projection {"name": "someName", ...}, will replace the original name attribute of the object, if any, even though the named key is used first).
> 
> As of v2021-03-25 of the API, when the expansion operator (...) is used after an explicitly-named key, a duplicate key will overwrite the value of the named key.

Since projection values are arbitrary GROQ expressions, nested projections are supported (and encouraged).

```groq
*[ _type == "book" ]{
  title,
  "authors": authors[]{
    "name": firstName + " " + lastName,
    birthYear,
  }
}
```

In this case, the nested `authors` projection takes its input from the `authors` array and generates an output array of projected objects as usual. This projection is evaluated in a new scope (as described in "Attribute Scope" in the [Syntax section](/docs/specifications/groq-syntax)), and the parent scope can be accessed via the `^` operator.

Projections also have syntactic sugar for conditionals, expressed as `condition => {}`. If `condition` evaluates to `true`, the object on the right-hand side of `=>` is expanded into the projection. For example, the following projection will include the `movies` attribute containing a list of related movies if the person is a director. Otherwise, the attribute will be omitted:

```groq
{
  name,
  role == "director" => {
    "movies": *[ _type == "movie" && director._ref == ^._id ]
  }
}
```

This syntax is exactly equivalent to `...select(condition => {})`. Each conditional in a projection is evaluated separately - for cases where multiple conditions overlap and only a single result (the first) should be included. The full `select()` syntax must be used instead.

## `order(<expr>...) <array>` Order Component

`order()` sorts the piped array according to the given expression and returns an array of sorted elements, e.g., `* | order(name asc, age desc)`. The direction can be either `asc` or `desc`, defaulting to `asc` if not given. Any number of sort expressions can be given, which specify sorting first-to-last (e.g., the expression above would be sorted first by ascending `name` and then by descending `age`).

For details on the ordering of various data types, see "Comparison Operators" in the [Operators section](/docs/specifications/groq-operators).



# Joins

Joins are supported via the reference access operator `->`, via subqueries that use the parent scope operator `^`, and via the `references()` function. The reference data type can be used in documents to explicitly reference other documents and enforce referential integrity, but joins can be made using arbitrary join conditions not involving reference fields at all.

Consider the following documents representing an employee and a department:

```json
{
  "_id": "alice",
  "_type": "employee",
  "name": "Alice Anderson",
  "department": {"_ref": "engineering"}
}
{
  "_id": "engineering",
  "_type": "department",
  "name": "Engineering"
}
```

The employee can be joined with the department by applying the `->` reference access operator to the `department` field, which fetches the referenced `engineering` document and inserts it into the projected document under the `department` attribute:

```
*[ _type == "employee" ]{ ..., department-> }

{
  "_id": "alice",
  "_type": "employee",
  "name": "Alice Anderson",
  "department": {
    "_id": "engineering",
    "_type": "department",
    "name": "Engineering"
  }
}

```

The department can also be joined with its employees by using a subquery that refers to the department's ID via the  `^` parent scope operator:

```
*[ _type == "department" ]{
  ...,
  "employees": *[ _type == "employee" && department._ref == ^._id ]
}

{
  "_id": "engineering",
  "_type": "department",
  "name": "Engineering",
  "employees": [
    {
      "_id": "alice",
      "_type": "employee",
      "name": "Alice Anderson",
      "department": {"_ref": "engineering"}
    }
  ]
}

```

Or the `references()` function can be used to fetch any employees that contain a reference to the department anywhere in their content:

```
*[ _type == "department" ]{
  ...,
  "employees": *[ _type == "employee" && references(^._id) ]
}

{
  "_id": "engineering",
  "_type": "department",
  "name": "Engineering",
  "employees": [
    {
      "_id": "alice",
      "_type": "employee",
      "name": "Alice Anderson",
      "department": {"_ref": "engineering"}
    }
  ]
}

```

These mechanisms can be used to perform arbitrarily complex joins, see their respective reference documentation for more details.

The rest of this section will demonstrate how GROQ can be used to implement joins equivalent to those in relational algebra and SQL databases. In the following examples, the "left relation" will refer to the current document, while the "right relation" will refer to the joined documents.

## Outer Joins

### Left Outer Join

A left outer join combines each document in the left relation with any documents in the right relation that match the join condition. Documents in the left relation that do not have any corresponding matches in the right relation are still included in the result, typically with a `null` value or similar.

The simplest left outer join is made with the reference access operator `->`, which joins the referenced document from the right relation into the left relation, or `null` if the referenced document does not exist. For example, the following query fetches all employees and joins them with their referenced department, if any:

```
*[ _type == "employee" ]{ ..., department-> }
```

Left outer joins can also use arbitrary join conditions through subqueries. For example, the following query joins all departments which reference an employee in its `employees` array:

```
*[ _type == "employee" ]{
  ...,
  "departments": *[ _type == "department" && ^._id in employees[]._ref ],
}
```

### Right Outer Joins

Right outer joins are identical to left outer joins, except that the left and right relations are swapped. Right outer joins are not directly supported in GROQ, but the same effect is easily accomplished by using a left outer join with the left and right relations swapped.

### Full Outer Joins

Full outer joins fetch all documents in both the left and right relations, and combine them on matching join conditions. Full outer joins do not directly translate to a document database, since the result is not a two-dimensional set of tuples. However, a similar effect can be obtained by fetching all documents in both relations and joining any matches using projection conditionals.

For example, the following query fetches all `employee` documents and all `department` documents, then joins referenced departments into employees via the `->` operator, and employees into departments via a subquery using `^`:

```
*[ _type in ["employee", "department"]]{
  ...,
  _type == "employee" => {
    department->,
  },
  _type == "department" => {
    "employees": *[ _type == "employee" && department._ref == ^._id ],
  },
}

```

## Inner Joins

### Equijoins

Equijoins combine documents in the left relation with documents in the right relation that match on an equality condition. Documents in the left relation that do not match any documents in the right relation are not included. This is accomplished in GROQ by using a left outer join and then filtering out any documents which did not have any matches - for example:

```
*[ _type == "employee" ]{ ..., department-> }[ !defined(department) ]
```

The equality can be made explicit by using a subquery instead of the `->` access operator:

```
*[ _type == "employee" ]{
  ...,
  "department": *[ _type == "department" && _id == ^.department._ref ][0],
}[ !defined(department) ]
```

### Non-Equijoins

Non-equijoins are similar to equijoins, except they join on inequalities rather than equalities (e.g. `>`, `<`, or `!=`). For example, the following query joins employees with all departments whose `baseSalary` field is greater than the employee's `salary` field, and removes employees which do not match the join condition:

```
*[ _type == "employee" ]{
  ...,
  "betterDepartments": *[ _type == "department" && baseSalary > ^.salary ],
}[ count(betterDepartments) > 0 ]

```

### θ-joins

θ-joins are similar to equijoins and non-equijoins, except they join documents on any arbitrary join condition. For example, the following query joins employees with any departments whose `baseSalary` is higher than the employee's `salary`, whose `manager` is not equal to the employee's `boss`, and whose `city` field is equal to the employee's `city` field:

```
*[ _type == "employee" ]{
  ...,
  "betterDepartments": *[
    _type == "department"
    && baseSalary > ^.salary
    && manager != ^.boss
    && city == ^.city
  ],
}[ count(betterDepartments) > 0 ]

```

### Semijoins

A semijoin checks whether a document in the left relation matches one in the right relation on the join condition, but does not actually include any contents from the right relation in the result. Semijoins are performed by including joins in a GROQ filter, for example in the following query that fetches all employees that belong to the `finance` department:

```
*[ _type == "employee" && department->_id == "finance" ]
```

Semijoins can also be done with subqueries, like the following example which fetches employees that are contained within the `employees` array of at least one department.

```
*[ _type == "employee" && count(*[ _type == "department" && ^._id in employees[]._ref ]) > 0 ]
```

### Antijoins

Antijoins are similar to semijoins, except they check whether the document in the left relation does *not* match any documents in the right relation on the join condition. For example, the following query fetches all employees that do not belong to any department:

```
*[ _type == "employee" && !defined(department->) ]
```

Antijoins can also be done with subqueries, like the following example which fetches employees that are not contained within the `employees` array of any department.

```
*[ _type == "employee" && count(*[ _type == "department" && ^._id in employees[]._ref ]) == 0 ]
```

### Natural Joins

Natural joins combine documents that have common fields with equal values. Natural joins as traditionally defined are not supported by GROQ, but a similar effect can be accomplished using the `references()` function which checks whether a document contains a reference to a given document anywhere within it.

For example, the following query fetches any employees that contain any references to the department anywhere in their structure, and removes departments with no employees:

```
*[ _type == "department" ]{
  ...,
  "employees": *[ _type == "employee" && references(^._id) ],
}[ count(employees) > 0 ]
```

## Other Joins

### Self-joins

Self-joins join documents against other documents in the same relation. For example, the following query fetches employees whose `salary` values are greater than the currently considered employee:

```
*[ _type == "employee" ]{
  ...,
  "betterPaid": *[ _type == "employee" && salary > ^.salary ],
}
```

Self-joins can even join against the same document, like in the following contrived example:

```
*[ _type == "employee" ]{ ..., "self": *[ _id == ^._id ][0] }
```

### Cross Joins

Cross joins (or Cartesian products) join all documents in the left relation with all documents in the right relation. For example, the following query fetches all employees and joins them with all departments:

```
*[ _type == "employee" ]{..., "allDepartments": *[ _type == "department" ]}
```






# Handler reference

[Overview](/docs/compute-and-ai/functions-introduction)

[Quick start](/docs/compute-and-ai/function-quickstart)



Every Function must export a `handler`. Handlers contain the logic that the Function infrastructure runs when your document changes trigger the function.

Create a function handler with the `sanity blueprints add function` command. Every handler receives an object containing `context` and `event` parameters.



## `context` properties

#### Properties

| Property | Description |
|----------|-------------|
| clientOptions | Coming soon |


### `clientOptions` properties

#### Properties

| Property | Description |
|----------|-------------|
| projectId | The ID of the project that triggered this function. |
| dataset | The dataset name of the project that triggered this function. |
| apiHost | Defaults to https://api.sanity.io. |
| token | A token provided by the function with access to your Sanity project. Note: the token is obfuscated in logs for security. You can directly use it to configure the Sanity client or to make API calls. |


### Example context

```javascript
{
  clientOptions: {
    apiHost: 'https://api.sanity.io',
    projectId: 'abc123',
    dataset: 'production',
    token: '***************'
  }
}
```

## `event` properties

#### Properties

| Property | Description |
|----------|-------------|
| data | A Sanity Document containing the default document shape and available values, or the shape defined in the Blueprints configuration function's event.projection. |


### Example event

```javascript
{
  data: {
    _id: '1234',
    _type: 'article',
    title: 'Functions quick start',
    _createdAt: "2025-04-24T16:26:58.901Z",
    _publishedAt: "2025-04-24T16:26:58.901Z",
    
  }
}
```

## Example handler

```
import { documentEventHandler } from '@sanity/functions'

export const handler = documentEventHandler(async ({ context, event }) => {
  console.log("Context: ", context)
  console.log("Event: ", event)
})
``````javascript
export async function handler({context, event}) {
  console.log("Context: ", context)
  console.log("Event: ", event)
}
```

## TypeScript types

When you create a new TypeScript function with `sanity blueprint add`, you'll be prompted to add types. 

If you did not add types as part of the init process, they are available in the [@sanity/functions](https://www.npmjs.com/package/@sanity/functions) package:

```sh
npm install -D @sanity/functions
```

You can then import and use the `documentEventHandler` helper to provide type support. See the example TS handler above for implementation details.



# Configuration file reference

[Overview](/docs/compute-and-ai/blueprints)



The Blueprints configuration file defines resources, like Functions, for deployment to Sanity's infrastructure.

Interact with Blueprints by using the `npx sanity blueprints` [CLI command](/docs/cli-reference/cli-blueprints).

The top-level of the blueprint configuration file contains the following properties:

#### Properties

| Property | Description |
|----------|-------------|
| blueprintVersion * | Defines the version of the Blueprints specification to use when parsing the configuration. Uses the YYYY-MM-DD format. |
| resources * | An array of Sanity resources. Right now this is limited to Function resources, but will expand in the future. |


## Resources

The following properties are shared across all resources. Additional resource-specific properties follow in the sections below.

#### Properties

| Property | Description |
|----------|-------------|
| name * | A unique function name. Must be an alphanumeric string that can contain dashes or underscores. |
| type * | A resource type. For Sanity resources, this is made up of the sanity namespace, category, subcategory, and resource types separated by single periods. For example: sanity.function.document. |


### Functions

In addition to the common resource properties, functions also contain the following resources.

#### Properties

| Property | Description |
|----------|-------------|
| src * | The path, relative to the blueprint configuration file, of the individual function directory. For example, functions/myFunction. |
| event | Configuration options for the triggering event. See the event properties section below for details. |
| timeout | The max invocation time, in seconds, of the function.

Default: 10

Minimum: 1

Maximum: 900 |
| memory | Sets the max memory allocation, in GBs.

Default: 1

Min: 1

Max: 10 |
| env | Set environment variables for the function. The env object accepts custom keys with string values. This is an alternative approach to using the sanity functions env CLI command. |


#### `event` properties

#### Properties

| Property | Description |
|----------|-------------|
| on | Defines the type of event trigger. The options are:

publish: Activates when a document is published.

These actions trigger on individual documents with unique _id values. For example, updating or deleting a draft of a document will not trigger an update or delete on the published document. |
| filter | A valid GROQ filter. This accepts most core GROQ filter functionality found in groq-js. Delta GROQ is not supported at this time.



Only include the contents of the filter, not any other surrounding syntax.

✅ Do this: _type == "article"

❌ Not this: [_type == "article"] |
| projection | A valid GROQ projection. Omit the outer wrapping parentheses.



✅ Do this: title, _id, slug

❌ Not this: { title, _id, slug } |


#### Example

```
import {defineBlueprint, defineDocumentFunction} from '@sanity/blueprints'

export default defineBlueprint({
  resources: [
    defineDocumentFunction({
      name: "log-event",
      event: {
        on: ["publish"],
        filter: "_type == 'post'",
        projection: "title, _id, _type"
      },
      env: {
        example: 'value'
      }
    })
  ]
})

``````json
{
  "blueprintVersion": "2024-10-01",
  "resources": [
    {
      "name": "log-event",
      "src": "functions/log-event",
      "type": "sanity.function.document",
      "event": {
        "on": [
          "publish"
        ],
        "filter": "_type == 'post'",
        "projection": "title, _id, _type"
      },
      "env": {
        "example": "value"
      }
    }
  ]
}
```

## TypeScript / JavaScript helpers

You can configure Blueprints with TypeScript and JavaScript. If you select either during `sanity blueprints init`, the CLI prompts you to install the [@sanity/blueprints](https://github.com/sanity-io/blueprints-node) package. You can also add it to an existing project by adding it to your Blueprints-level project directory.

```sh
npm i @sanity/blueprints
``````sh
pnpm add @sanity/blueprints
```

The helpers provide defaults and allow you to omit some configuration options. You can always override these defaults by explicitly setting the values as you would with the JSON format.



# Edit content directly from your frontend



# Introduction

Visual Editing lets content creators see their changes live and edit content directly from the webpage. Think of it as a bridge between your Sanity Studio and your website - your content team can view drafts, click and drag elements to edit them, and see changes in real-time.

![The Presentation tool showing live preview with click to edit](https://cdn.sanity.io/images/3do82whm/next/353dc19f1f898610f33ce2d8624975863f9f7854-2000x1295.png)

## What you can do

Visual Editing streamlines content workflow by enabling teams to:

- **Live Preview**: See draft content immediately as you edit 
- **Click-to-edit and drag and drop**: Jump directly from your website to the right field in Studio
- **Global Preview**: Preview content across your whole site, not just single pages
- **Framework Agnostic**: Works with your existing components and most modern JavaScript frameworks and hosting platforms
- **No Extra Build**: Preview without deploying branches or running preview builds

> [!TIP]
> Protip
> Sanity's Visual Editing tooling is available on all plans, including free, and supported by most modern frameworks and hosting platforms.
> 
> Note that some hosting platforms, like Vercel, have Visual Editing offerings with platform-specific features. Sanity's tooling should also be compatible with these solutions as long as they are based on Content Source Maps.

## Getting started

The Integrated [Visual Editing with Next.js](https://www.sanity.io/learn/course/visual-editing-with-next-js/understanding-visual-editing) over at [Sanity Learn](https://www.sanity.io/learn) is a great resource for learning the ins and outs of Visual Editing, even if you plan to be using a different front end.

If you prefer to get right into tinkering with some code, there are several [template projects](https://www.sanity.io/templates) to be found at the [Sanity exchange](https://www.sanity.io/exchange) that will get you started.

Remember: You can start simple with basic preview on a single route and add more routes as you need them. Visual Editing is designed to grow with your needs. 

### Guides

Check out our framework-specific guides below for a head start. Following along one of these will help you set up Visual Editing from the beginning.

- [Next.js App Router](/docs/visual-editing/visual-editing-with-next-js-app-router)
- [Next.js Pages Router](/docs/visual-editing/visual-editing-with-next-js-pages-router)
- [Remix](/docs/visual-editing/visual-editing-with-react-router)
- [Astro](https://www.sanity.io/guides/sanity-astro-blog)

In addition to these Sanity-curated guides, you can find more guides, tools, templates and inspiration at the [exchange](https://www.sanity.io/exchange).

## Not Sure Where to Start?

- Understand how it works? Read more about [the Presentation tool](/docs/visual-editing/configuring-the-presentation-tool) and how to think about [fetching content for Visual Editing](/docs/visual-editing/fetching-content-for-visual-editing).
- Explore using overlays for [click-to-edit](/docs/visual-editing/visual-editing-overlays) and [drag-and-drop](/docs/visual-editing/enabling-drag-and-drop) interactions.
- Dive into [custom components and controls](/docs/visual-editing/custom-overlay-components) for the Presentation tool.
- See it in action? Check out a template project!- [Next.js template project](https://www.sanity.io/templates/nextjs-sanity-clean)
- [Remix template project](https://www.sanity.io/templates/sanity-remix-typescript-tailwindcss-zod)
- [Angular template project (community submitted)](https://www.sanity.io/templates/angular-blog-with-built-in-content-editing)


- Need troubleshooting? [Visit our troubleshooting guide](/docs/visual-editing/troubleshooting-visual-editing).



# Next.js (App Router)

Following this guide will enable you to:

- Render overlays in your application, allowing content editors to jump directly from Sanity content to its source in Sanity Studio.
- Edit your content and see changes reflected in an embedded preview of your application in Sanity’s Presentation tool.
- **Optional:** Provide live content updates and seamless switching between draft and published content.

## Prerequisites

- A Sanity project with [a hosted or embedded Studio](/docs/studio/deployment). If using an embedded studio, make sure you use [route groups](https://nextjs.org/docs/app/building-your-application/routing/route-groups) to separate your app and studio layouts.
- A Next.js application using App Router. Follow [this documentation](https://nextjs.org/docs/app/getting-started) to set one up.

## Next.js application setup

The following steps should be performed in your Next.js application.

### Install dependencies

Install the dependencies that will provide your application with data fetching and Visual Editing capabilities.

```bash
npm install @sanity/client next-sanity

```

### Set environment variables

Create a `.env` file in your application’s root directory to provide Sanity specific configuration.

You can use [Manage](https://www.sanity.io/manage) to find your project ID and dataset, and to create a token with [Viewer permissions](https://www.sanity.io/docs/roles#e2daad192df9) which will be used to fetch preview content.

The URL of your Sanity Studio will depend on where it is [hosted](https://www.sanity.io/docs/deployment) or [embedded](https://www.sanity.io/docs/embedding-sanity-studio).

```bash
# .env

# Public
NEXT_PUBLIC_SANITY_PROJECT_ID="YOUR_PROJECT_ID"
NEXT_PUBLIC_SANITY_DATASET="YOUR_DATASET"
NEXT_PUBLIC_SANITY_STUDIO_URL="https://YOUR_PROJECT.sanity.studio"
# Private
SANITY_VIEWER_TOKEN="YOUR_VIEWER_TOKEN"

```

## Application setup

### Configure the Sanity client

Create a Sanity client instance to handle fetching data from Content Lake.

Configuring the `stega` option enables automatic overlays for basic data types when preview mode is enabled. You can read more about how stega works [here](https://www.sanity.io/docs/stega).

```tsx
// src/sanity/client.ts

import { createClient } from "next-sanity";

export const client = createClient({
  projectId: process.env.NEXT_PUBLIC_SANITY_PROJECT_ID,
  dataset: process.env.NEXT_PUBLIC_SANITY_DATASET,
  apiVersion: "2024-12-01",
  useCdn: true,
  token: process.env.SANITY_VIEWER_TOKEN,
  stega: {
    studioUrl: process.env.NEXT_PUBLIC_SANITY_STUDIO_URL,
  },
});

```

### Draft mode

Draft mode allows authorized content editors to view and interact with draft content.

Create an API endpoint to enable draft mode when viewing your application in Presentation tool.

```tsx
// src/app/api/draft-mode/enable/route.ts

import { client } from "@/sanity/client";
import { defineEnableDraftMode } from "next-sanity/draft-mode";

export const { GET } = defineEnableDraftMode({
  client: client.withConfig({
    token: process.env.SANITY_VIEWER_TOKEN,
  }),
});

```

Create a server action which can be used to disable draft mode. Add a delay to ensure a loading state is shown.

```tsx
// src/app/actions.ts

'use server'

import {draftMode} from 'next/headers'

export async function disableDraftMode() {
  const disable = (await draftMode()).disable()
  const delay = new Promise((resolve) => setTimeout(resolve, 1000))

  await Promise.allSettled([disable, delay]);
}

```

Create a new component for disabling draft mode. We will render this for content authors when viewing draft content in a non-Presentation context.

```tsx
// src/components/DisableDraftMode.tsx

"use client";

import { useTransition } from "react";
import { useRouter } from "next/navigation";
import { disableDraftMode } from "@/app/actions";

export function DisableDraftMode() {
  const router = useRouter();
  const [pending, startTransition] = useTransition();
  
  if (window !== window.parent || !!window.opener) {
    return null;
  }

  const disable = () =>
    startTransition(async () => {
      await disableDraftMode();
      router.refresh();
    });

  return (
    <div>
      {pending ? (
        "Disabling draft mode..."
      ) : (
        <button type="button" onClick={disable}>
          Disable draft mode
        </button>
      )}
    </div>
  );
}

```

### Enable Visual Editing

The `<VisualEditing>` component handles rendering overlays, enabling click to edit, and refreshing pages in your application when content changes.

Import it into your root layout, and render it conditionally when draft mode is enabled alongside the `<DisableDraftMode>` component you created above.

```tsx
// src/app/layout.tsx

import { VisualEditing } from "next-sanity";
import { draftMode } from "next/headers";
import { DisableDraftMode } from "@/components/DisableDraftMode";

export default async function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body>
        {children}
        {(await draftMode()).isEnabled && (
          <>
            <VisualEditing />
            <DisableDraftMode />
          </>
        )}
      </body>
    </html>
  );
}


```

### Render a page in preview mode

Add configuration to your `client.fetch` calls when draft mode is enabled in order to fetch up-to-date preview content with stega encoding.

```tsx
// ./src/app/[slug]/page.tsx

import { defineQuery } from "next-sanity";
import { draftMode } from "next/headers";
import { client } from "@/sanity/client";

const query = defineQuery(
  `*[_type == "page" && slug.current == $slug][0]{title}`
);

export default async function Page({
  params,
}: {
  params: Promise<{ slug: string }>;
}) {
  const { slug } = await params;
  const { isEnabled } = await draftMode();

  const data = await client.fetch(
    query,
    { slug },
    isEnabled
      ? {
          perspective: "previewDrafts",
          useCdn: false,
          stega: true,
        }
      : undefined
  );

  return <h1>{data.title}</h1>;
}

```

## Studio setup

To setup Presentation tool in your Sanity Studio, import the tool from `sanity/presentation`, add it to your `plugins` array, and configure `previewUrl`, optionally passing an origin, path, and endpoints to enable and disable preview mode.

We similarly recommend using environment variables loaded via a `.env` file to support development and production environments.

```tsx
// sanity.config.ts

import { defineConfig } from "sanity";
import { presentationTool } from "sanity/presentation";

export default defineConfig({
  // ... project configuration
  plugins: [
    presentationTool({
      previewUrl: {
        origin: process.env.SANITY_STUDIO_PREVIEW_ORIGIN,
        preview: "/",
        previewMode: {
          enable: "/api/draft-mode/enable",
        },
      },
    }),
    // ... other plugins
  ],
});

```

## Optional Extras

### Live Content API

> [!WARNING]
> Gotcha
> The Live Content API is currently considered experimental and may change in the future.

[The Live Content API](https://www.sanity.io/docs/live-content-api?utm_source=github&utm_medium=readme&utm_campaign=next-sanity) can be used to receive real time updates in your application when viewing both draft content in contexts like Presentation tool, and published content in your user-facing production application.

Implementing Visual Editing using the Live Content API is recommended for the best experience for both users and content editors.

### Update Sanity client

First, update your client configuration. The token can be removed from the base client instance as we pass it as configuration in the next step.

```typescript
// src/sanity/client.ts

import { createClient } from "next-sanity";

export const client = createClient({
  projectId: process.env.NEXT_PUBLIC_SANITY_PROJECT_ID,
  dataset: process.env.NEXT_PUBLIC_SANITY_DATASET,
  apiVersion: "2024-12-01",
  useCdn: true,
--  token: process.env.SANITY_VIEWER_TOKEN,
  stega: {
    studioUrl: process.env.NEXT_PUBLIC_SANITY_STUDIO_URL,
  },
});

```

### Configure `defineLive`

Configure `defineLive` to enable automatic revalidation and refreshing of your fetched content.

The Viewer token can be used as both `browserToken` and `serverToken`, as the `browserToken` is only shared with the browser when draft mode is enabled.

```tsx
// src/sanity/live.ts

import { defineLive } from "next-sanity";
import { client } from "./client";

const token = process.env.SANITY_VIEWER_TOKEN;

export const { sanityFetch, SanityLive } = defineLive({
  client,
  serverToken: token,
  browserToken: token,
});

```

### Layout and pages

The `<SanityLive>` component is responsible for making all `sanityFetch` calls in your application *live*, so should always be rendered. It will also enable seamless switching between draft and published content when viewing your application in Presentation tool.

```typescript
// src/app/layout.tsx

import { VisualEditing } from "next-sanity";
import { draftMode } from "next/headers";
import { SanityLive } from "@/sanity/live";

export default async function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body>
        {children}
         <SanityLive />
        {(await draftMode()).isEnabled && <VisualEditing />}
      </body>
    </html>
  );
}

```

Replace your `client.fetch` calls with the newly exported `sanityFetch` function.

Explicitly passing the options parameter based on the draft mode status is no longer necessary as `sanityFetch` handles setting the correct options internally.

```typescript
// src/app/[slug]/page.tsx

import { defineQuery } from 'next-sanity'
import { sanityFetch } from '@/sanity/live'

const query = defineQuery(
  `*[_type == "page" && slug.current == $slug][0]{title}`,
)

export default async function Page({
  params
}: {
  params: Promise<{slug: string}>;
}) {

const { data } = await sanityFetch({
      query,
      params,
});

  return <h1>{data.title}</h1>;
}

```





# Next.js (Pages Router)

Following this guide will enable you to:

- Render overlays in your application, allowing content editors to jump directly from Sanity content to its source in Sanity Studio.
- Edit your content and see changes reflected in an embedded preview of your application in Sanity’s Presentation tool.
- Provide instant updates and seamless switching between draft and published content.

> [!WARNING]
> Gotcha
> This guide is for the Next.js Pages Router. Go here for the guide on Next.js App Router.

## Prerequisites

- A Sanity project with [a hosted or embedded Studio](/docs/studio/deployment). 
- A Next.js application using Pages Router. Follow [this guide](https://nextjs.org/docs/pages/building-your-application) to set one up.

## Next.js application setup

The following steps should be performed in your Next.js application.

### Install dependencies

Install the dependencies that will provide your application with data fetching and Visual Editing capabilities.

```bash
npm install next-sanity @sanity/visual-editing @sanity/react-loader

```

## Add environment variables

Create a `.env` file in your application’s root directory to provide Sanity specific configuration.

You can use [Manage](https://www.sanity.io/manage) to find your project ID and dataset, and to create a token with Viewer permissions which will be used to fetch preview content.

The URL of your Sanity Studio will depend on where it is [hosted](https://www.sanity.io/docs/deployment) or [embedded](https://www.sanity.io/docs/embedding-sanity-studio).

```bash
# .env

# Public
PUBLIC_SANITY_PROJECT_ID="YOUR_PROJECT_ID"
PUBLIC_SANITY_DATASET="YOUR_DATASET"
PUBLIC_SANITY_STUDIO_URL="https://YOUR_PROJECT.sanity.studio"
# Private
SANITY_VIEWER_TOKEN="YOUR_VIEWER_TOKEN"

```

## Application setup

### Configure the Sanity client

Create a Sanity client instance to handle fetching data from Content Lake.

Configuring the `stega` option enables automatic overlays for basic data types when preview mode is enabled. You can read more about how stega works [here](https://www.sanity.io/docs/stega).

```tsx
// src/sanity/client.ts

import { createClient } from "next-sanity";

export const client = createClient({
  projectId: process.env.NEXT_PUBLIC_SANITY_PROJECT_ID,
  dataset: process.env.NEXT_PUBLIC_SANITY_DATASET,
  apiVersion: "2024-12-01",
  useCdn: true,
  token: process.env.SANITY_VIEWER_TOKEN,
  stega: {
    studioUrl: process.env.NEXT_PUBLIC_SANITY_STUDIO_URL,
  },
});

```

### Draft mode

Draft mode allows authorized content editors to view and interact with draft content.

Create an API endpoint (note we use the `src/app` directory) to enable draft mode when viewing your application in Presentation tool.

```tsx
// src/app/api/draft-mode/enable/route.ts

import { client } from "@/sanity/client";
import { defineEnableDraftMode } from "next-sanity/draft-mode";

export const { GET } = defineEnableDraftMode({
  client: client.withConfig({
    token: process.env.SANITY_VIEWER_TOKEN,
  }),
});

```

Similarly, create an API endpoint to disable draft mode.

```tsx
// src/app/api/draft-mode/disable/route.ts

import { draftMode } from "next/headers";
import { NextRequest, NextResponse } from "next/server";

export async function GET(request: NextRequest) {
  (await draftMode()).disable();
  const url = new URL(request.nextUrl);
  return NextResponse.redirect(new URL("/", url.origin));
}

```

Create a new component with a link to the disable endpoint. We add conditional logic to only render this for content authors when viewing draft content in a non-Presentation context.

```tsx
// src/components/DisableDraftMode.tsx

import { useEffect, useState } from "react";

export function DisableDraftMode() {
  const [show, setShow] = useState(false);

  useEffect(() => {
    setShow(window === window.parent && !window.opener);
  }, []);

  return show && <a href={"/api/draft-mode/disable"}>Disable Draft Mode</a>;
}

```

### Enable Visual Editing

Create a Visual Editing wrapper component.

The `<VisualEditing>` component handles rendering overlays, enabling click to edit, and refreshing pages in your application when content changes. Render it alongside the `<DisableDraftMode>` component you created above.

We provide a basic refresh mechanism that will reload the page when changes are made in Presentation tool. You can optionally use loaders to provide seamless updates.

```tsx
// src/components/SanityVisualEditing.tsx

import { VisualEditing } from "@sanity/visual-editing/next-pages-router";
import { useLiveMode } from '@sanity/react-loader'
import { DisableDraftMode } from "@/pages/components/DisableDraftMode";
import { client } from "@/sanity/client";

const stegaClient = client.withConfig({stega: true})

export default function SanityVisualEditing() {
	useLiveMode({client: stegaClient})

  return (
    <>
      <VisualEditing />
      <DisableDraftMode />
    </>
  );
}

```

In the root layout file, dynamically import and render the `<SanityVisualEditing>` wrapper component when draft mode is enabled.

```tsx
// src/pages/_app.tsx

import type { AppProps } from "next/app";
import dynamic from "next/dynamic";

const SanityVisualEditing = dynamic(
  () => import("../components/SanityVisualEditing")
);

export interface SharedProps {
  draftMode: boolean;
}

export default function App({ Component, pageProps }: AppProps<SharedProps>) {
  const { draftMode } = pageProps;
  
  return (
    <>
      <Component {...pageProps} />
      {draftMode && <SanityVisualEditing />}
    </>
  );
}

```

### Set up loaders

Create a new file to configure loaders. Call `setServerClient`, with the client instance which should be used to fetch data on the server.

We also create a helper function to return fetch options based on the draft mode state, and export this alongside `loadQuery` for convenience.

```tsx
// src/sanity/ssr.ts

import * as serverOnly from "@sanity/react-loader";
import { client } from "./client";
import { ClientPerspective } from "next-sanity";

const { loadQuery, setServerClient } = serverOnly;

setServerClient(
  client.withConfig({
    token: process.env.SANITY_VIEWER_TOKEN,
  })
);

const loadQueryOptions = (context: { draftMode?: boolean }) => {
  const { draftMode } = context;
  return draftMode
    ? {
        perspective: "previewDrafts" as ClientPerspective,
        stega: true,
        useCdn: false,
      }
    : {};
};

export { loadQuery, loadQueryOptions };


```

### Render a page in preview mode

In `getStaticProps` use the `loadQuery` function created above. The `initial` data returned here is passed to `useQuery` in the page component.

When in Presentation, `useQuery` will handle live updates as content is edited.

```tsx
// src/pages/index.tsx

import { loadQuery } from "@/sanity/ssr";
import { useQuery } from "@sanity/react-loader";
import type { GetStaticProps, InferGetStaticPropsType } from "next";

const query = `*[_type == "page"][0]{title}`;

export const getStaticProps = (async (context) => {
  const options = loadQueryOptions(context);
  const initial = await loadQuery<{ title?: string }>(query, {}, options);
  return { props: { initial } };
}) satisfies GetStaticProps;

export type PageProps = InferGetStaticPropsType<typeof getStaticProps>;

export default function Page(props: PageProps) {
  const { initial } = props;
  const { data } = useQuery(query, {}, { initial });
  return <h1>{data.title}</h1>;
}

```

## Studio setup

To setup Presentation tool in your Sanity Studio, import the tool from `sanity/presentation`, add it to your `plugins` array, and set `previewUrl` to the base URL of your application.

We similarly recommend using environment variables loaded via a `.env` file to support development and production environments.

```tsx
// sanity.config.ts

import { defineConfig } from "sanity";
import { presentationTool } from "sanity/presentation";

export default defineConfig({
  // ... project configuration
  plugins: [
    presentationTool({
      previewUrl: {
        origin: process.env.SANITY_STUDIO_PREVIEW_ORIGIN,
        preview: "/",
        previewMode: {
          enable: "/api/draft-mode/enable",
          disable: "/api/draft-mode/disable",
        },
      },
    }),
    // ... other plugins
  ],
});

```

## Optional Extras

### Add data attributes for overlays

`useQuery` also returns an `encodeDataAttribute` helper method for generating `data-sanity` attributes. These attributes give you direct control over rendering [overlays](https://www.sanity.io/docs/loaders-and-overlays#1dbcc04a7093) in your application, and are especially useful if not using stega encoding.

```tsx
// src/pages/index.tsx

import { loadQuery } from "@/sanity/ssr";
import { useQuery } from "@sanity/react-loader";
import type { GetStaticProps, InferGetStaticPropsType } from "next";

const query = `*[_type == "page"][0]{title}`;

export const getStaticProps = (async (context) => {
  const options = loadQueryOptions(context);
  const initial = await loadQuery<{ title?: string }>(query, {}, options);
  return { props: { initial } };
}) satisfies GetStaticProps;

export type PageProps = InferGetStaticPropsType<typeof getStaticProps>;

export default function Page(props: PageProps) {
  const { initial } = props;
  const { data, encodeDataAttribute } = useQuery(query, {}, { initial });
  return <h1 data-sanity={encodeDataAttribute(["title"])}>{data.title}</h1>;
}

```





# Nuxt.js

Following this guide will enable you to:

- Render overlays in your application, allowing content editors to jump directly from Sanity content to its source in Sanity Studio.
- Edit your content and see changes reflected in an embedded preview of your application in Sanity’s Presentation tool.
- Provide instant updates and seamless switching between draft and published content.

## Prerequisites

- A Sanity project with a hosted or embedded Studio. Read more about hosting [here](https://www.sanity.io/docs/deployment).
- A Nuxt application with SSR. Follow [this guide](https://nuxt.com/docs/getting-started/installation) to set one up.

### Nuxt application setup

The following steps should be performed in your Nuxt application.

### Install dependencies

Install the dependencies that will provide your application with data fetching and Visual Editing capabilities.

```bash
npm install @sanity/client

npx nuxi@latest module add sanity
```

### Environment variables

Create a `.env` file in your application’s root directory to provide Sanity specific configuration.

You can use [Manage](https://www.sanity.io/manage) to find your project ID and dataset, and to create a token with [Viewer permissions](https://www.sanity.io/docs/roles#e2daad192df9) which will be used to fetch preview content.

The URL of your Sanity Studio will depend on where it is [hosted](https://www.sanity.io/docs/deployment) or [embedded](https://www.sanity.io/docs/embedding-sanity-studio).

```bash
# .env
# Public
SANITY_PROJECT_ID="YOUR_PROJECT_ID"
SANITY_DATASET="YOUR_DATASET"
SANITY_STUDIO_URL="https://YOUR_PROJECT.sanity.studio"
# Private
SANITY_VIEWER_TOKEN="YOUR_VIEWER_TOKEN"
```

## Application setup

### Sanity client

Create a Sanity client instance to handle fetching data from Content Lake.

Configuring the `stega` option enables automatic overlays for basic data types when preview mode is enabled. You can read more about how stega works [here](https://www.sanity.io/docs/stega).

```tsx
// nuxt.config.ts
export default defineNuxtConfig({
  modules: ['@nuxtjs/sanity'],
  sanity: {
    projectId: process.env.SANITY_PROJECT_ID
    dataset: process.env.SANITY_DATASET,
    visualEditing: {
	    token: process.env.SANITY_VIEWER_TOKEN,
	    studioUrl: process.env.SANITY_STUDIO_URL,
	    stega: true
    }
  }
})
```

### Rendering pages

Firstly, set up the queries you will use to fetch data from Content Lake.

```tsx
// queries.ts
export type PageResult = { title: string }

export const pageQuery = /* groq */`*[_type == "page"][0]{title}`
```

```tsx
// pages/index.vue
<script lang="ts">
import {pageQuery, type PageResult} from '../queries'

const {data, pending} = await useSanityQuery<PageResult>(pageQuery)
</script>

<template>
	<div v-if="pending">Loading...</div>
  <h1 v-else>{data.title}</h1>
</template>
```

## Studio setup

To setup Presentation tool in your Sanity Studio, import the tool from `sanity/presentation`, add it to your `plugins` array, and configure `previewUrl`, optionally passing an origin, path, and endpoints to enable and disable preview mode.

We similarly recommend using environment variables loaded via a `.env` file to support development and production environments.

```tsx
// sanity.config.ts
import {defineConfig} from 'sanity'
import {presentationTool} from 'sanity/presentation'

export default defineConfig({
  // ... project configuration
  plugins: [
    presentationTool({
      previewUrl: {
        origin: process.env.SANITY_STUDIO_PREVIEW_ORIGIN,
        preview: '/',
        previewMode: {
          enable: '/preview/enable',
          disable: '/preview/disable',
        },
      }
    }),
    // ... other plugins
  ],
})
```

## Optional Extras

### Adding data attributes

`useSanityQuery` also returns an `encodeDataAttribute` helper method for generating `data-sanity` attributes. These attributes give you direct control over rendering [overlays](https://www.sanity.io/docs/loaders-and-overlays#1dbcc04a7093) in your application, and are especially useful if not using stega encoding.

```html
// pages/index.vue
<script lang="ts">
import {pageQuery, type PageResult} from '../queries'

const {data, pending, encodeDataAttribute} = await useSanityQuery<PageResult>(pageQuery)
</script>

<template>
  <div v-if="pending">Loading...</div>
  <h1 v-else :data-sanity="encodeDataAttribute(['title'])">{data.title}</h1>
</template>
```





# SvelteKit

Following this guide will enable you to:

- Render overlays in your application, allowing content editors to jump directly from Sanity content to its source in Sanity Studio.
- Edit your content and see changes reflected in an embedded preview of your application in Sanity’s Presentation tool.
- **Optional:** Provide instant updates and seamless switching between draft and published content.

## Prerequisites

- A Sanity project with a hosted or embedded Studio. Read more about hosting [here](https://www.sanity.io/docs/deployment).
- A SvelteKit application using Svelte 5 with SSR. Follow [this guide](https://kit.svelte.dev/docs/creating-a-project) to set one up.

## SvelteKit application setup

The following steps should be performed in your SvelteKit application.

### Install dependencies

Install the dependencies that will provide your application with data fetching and Visual Editing capabilities.

```sh
npm install @sanity/client @sanity/visual-editing
```

### Set environment variables

Create a `.env` file in your application’s root directory to provide Sanity specific configuration.

You can use [Manage](https://www.sanity.io/manage) to find your project ID and dataset, and to create a token with [Viewer permissions](https://www.sanity.io/docs/roles#e2daad192df9) which will be used to fetch preview content.

The URL of your Sanity Studio will depend on where it is [hosted](https://www.sanity.io/docs/deployment) or [embedded](https://www.sanity.io/docs/embedding-sanity-studio).

```sh
# .env

# Public
PUBLIC_SANITY_PROJECT_ID="YOUR_PROJECT_ID"
PUBLIC_SANITY_DATASET="YOUR_DATASET"
PUBLIC_SANITY_STUDIO_URL="https://YOUR_PROJECT.sanity.studio"
# Private
SANITY_VIEWER_TOKEN="YOUR_VIEWER_TOKEN"
```

## Application setup

### Sanity client

Create a Sanity client instance to handle fetching data from Content Lake.

Configuring the `stega` option enables automatic overlays for basic data types when preview mode is enabled. You can read more about how stega works [here](https://www.sanity.io/docs/stega).

```typescript
// src/lib/sanity.ts

import {createClient} from '@sanity/client'
import {
  PUBLIC_SANITY_DATASET,
  PUBLIC_SANITY_PROJECT_ID,
  PUBLIC_SANITY_STUDIO_URL,
} from "$env/static/public";

export const client = createClient({
  projectId: PUBLIC_SANITY_PROJECT_ID,
  dataset: PUBLIC_SANITY_DATASET,
  useCdn: true,
  stega: {
	  enabled: true,
    studioUrl: PUBLIC_SANITY_STUDIO_URL,
  },
})
```

Create a server-only Sanity client instance using the Viewer token and client created above. This will be used to fetch draft content when in preview mode.

```typescript
// src/lib/server/sanity.ts

import {SANITY_VIEWER_TOKEN} from '$env/static/private'
import {client} from '$lib/sanity'

export const serverClient = client.withConfig({
  token: SANITY_VIEWER_TOKEN,
})
```

### Preview mode

Preview mode allows authorized content editors to view and interact with draft content.

In the [server hooks](https://kit.svelte.dev/docs/hooks#server-hooks) file, call and export the `handlePreview` [handle function](https://kit.svelte.dev/docs/hooks#server-hooks-handle), which adds preview mode to your application.

```typescript
// src/hooks.server.ts

import {handlePreview} from '@sanity/visual-editing/svelte';
import {serverClient} from '$lib/server/sanity'

export const handle = handlePreview({client: serverClient})
```

> [!TIP]
> Protip
> If your server hooks file already exports a handle function, use SvelteKit's sequence function.

### Rendering pages

First, define the queries you will use to fetch data from Content Lake.

```typescript
// src/lib/queries.ts

export type PageResult = { title: string }

export const pageQuery = /* groq */`*[_type == "page"][0]{title}`
```

Next, define a [load function](https://kit.svelte.dev/docs/load) that uses your query to fetch and return data.

When fetching content using the Sanity client in an application that implements visual editing using stega, make sure to set `stega` to `false` when preview mode is disabled.

```typescript
// src/routes/+page.server.ts

import {pageQuery as query, type PageResult} from '$lib/queries'
import type {PageServerLoad} from './$types'

export const load: PageServerLoad = async ({locals: {client, preview}}) => {
  const options = {stega: preview ? true : false}
  const page = await client.fetch<PageResult>(query, {}, options)

  return {page}
}
```

The load function’s return value will be available in the corresponding `.svelte` file via the `data` prop.

```typescript
// src/routes/+page.svelte

<script lang="ts">
  let {data}: Props = $props();
  let {page} = $derived(data)
</script>

<h1>{page.title}</h1>
```

### Enable Visual Editing

The handle function implemented above adds a `preview` property to the `locals` object, exposing the status of preview mode on the server. The server layout file lets you expose this value to client for use in a Svelte layout file.

```typescript
// src/routes/+layout.server.ts

import type {LayoutServerLoad} from './$types'

export const load: LayoutServerLoad = ({locals: {preview}}) => {
  return {preview}
}
```

The `<VisualEditing>` component handles rendering overlays, enabling click to edit, and refreshing pages in your application when content changes.

By exposing the `preview` value and conditionally rendering this component, you can ensure only content editors will have access to these features.

```typescript
// src/routes/+layout.svelte

<script lang="ts">
  import {VisualEditing} from '@sanity/visual-editing/svelte'

  let {children, data} = $props();
</script>

{@render children()}
{#if data.preview}
  <VisualEditing />
{/if}
```

## Studio setup

To set up Presentation tool in your Sanity Studio, import the tool from `sanity/presentation`, add it to your `plugins` array, and configure `previewUrl`, optionally passing an origin, path, and endpoints to enable and disable preview mode.

We similarly recommend using environment variables loaded via a `.env` file to support development and production environments.

```typescript
// sanity.config.ts

import {defineConfig} from 'sanity'
import {presentationTool} from 'sanity/presentation'

export default defineConfig({
  // ... project configuration
  plugins: [
    presentationTool({
      previewUrl: {
        origin: process.env.SANITY_STUDIO_PREVIEW_ORIGIN,
        preview: '/',
        previewMode: {
          enable: '/preview/enable',
          disable: '/preview/disable',
        },
      }
    }),
    // ... other plugins
  ],
})
```

## Optional Extras

### Instant updates and perspective switching

Instant updates and perspective switching require opting into using [loaders](https://www.sanity.io/docs/loaders) to fetch data.

Add the Svelte specific loader to your application dependencies:

### Install dependencies

```sh
npm install @sanity/svelte-loader
```

### Update server hooks

Update your server hooks file to call `setServerClient` and replace the `handlePreview` [handle function](https://kit.svelte.dev/docs/hooks#server-hooks-handle) added during the initial setup with `createRequestHandler`.

This function also handles adding Preview mode, but crucially also sets up the `loadQuery` helper function which will be used for fetching content on the server.

```typescript
// src/hooks.server.ts

import {createRequestHandler, setServerClient} from '@sanity/svelte-loader'
import {serverClient} from '$lib/server/sanity'

setServerClient(serverClient)

export const handle = createRequestHandler()
```

### Use loaders

Create a server `load` function for your page. Using `locals.loadQuery` ensures fetching from Content Lake is performed using the correct perspective: draft content can be fetched if preview mode is enabled, otherwise published content is always returned.

```typescript
// src/routes/+page.server.ts

import {pageQuery as query, type PageResult} from '$lib/queries'
import type {PageServerLoad} from './$types'

export const load: PageServerLoad = async ({locals: {client, preview}}) => {
  const options = {stega: preview ? true : false}
  const page = await client.fetch<PageResult>(query, {}, options)

  return {page}
}
```

```typescript
// src/routes/+page.server.ts

import {pageQuery as query, type PageResult} from '$lib/queries'
import type {PageServerLoad} from './$types'

export const load: PageServerLoad = async ({locals: {loadQuery}}) => {
  const initial = await loadQuery<PageResult>(query)
  return {query, options: {initial}}
}
```

Structuring the load function return value in this way ensures we can pass the `data` value directly to the `useQuery` function.

When your application is viewed in Presentation tool, `useQuery` provides instant content updates and seamless switching between draft and published content.

```typescript
// src/routes/+page.svelte

  <script lang="ts">
    import {useQuery} from '@sanity/svelte-loader'
    import type {PageData} from './$types'

    export let data: PageData
    const query = useQuery(data)
    $: ({ data: page } = $query)
 </script>

 <h1>{page.title}</h1>
```

### Enable live mode

Finally, in the root layout component, render the `LiveMode` component with your client instance to enable instant updates when in preview mode.

```typescript
// src/routes/+layout.svelte

  <script lang="ts">
    import {VisualEditing} from '@sanity/visual-editing/svelte'
    import {LiveMode} from '@sanity/svelte-loader'
    import {client} from '$lib/sanity'

    let {children, data} = $props();
  </script>

  {@render children()}
  {#if data.preview}
    <VisualEditing />
    <LiveMode {client} />
  {/if}
```

### TypeScript: `event.locals`

The handler functions referenced in this guide add properties to SvelteKit’s `event.locals` object. If your application is written in TypeScript, extend the `App.Locals` interface to ensure type safety.

If using `handlePreview`:

```typescript
// app.d.ts

import type {VisualEditingLocals} from '@sanity/visual-editing/svelte'

declare global {
  namespace App {
    interface Locals extends VisualEditingLocals {}
  }
}

export {}
```

If using `createRequestHandler`:

```typescript
// app.d.ts

import type {LoaderLocals} from '@sanity/svelte-loader'

declare global {
  namespace App {
    interface Locals extends LoaderLocals {}
  }
}

export {}
```

### Adding data attributes

`useQuery` also returns an `encodeDataAttribute` helper method for generating `data-sanity` attributes. These attributes give you direct control over rendering [overlays](https://www.sanity.io/docs/loaders-and-overlays#1dbcc04a7093) in your application, and are especially useful if not using stega encoding.

```typescript
// src/routes/+page.svelte

<script lang="ts">
  import {useQuery} from '@sanity/svelte-loader'
  import type {PageData} from './$types'

  export let data: PageData
  const query = useQuery(data)
  $: ({data: page, encodeDataAttribute} = $query)
</script>

<h1 data-sanity={encodeDataAttribute(['title'])}>
  {page.title}
</h1>
```

### Conditional rendering in preview mode with `isPreviewing`

Your application might need to conditionally render elements if preview mode is enabled, for example to notify Content Editors that they are viewing draft content, or to provide a mechanism for disabling preview mode.

Using the `preview` value that the server layout exposed in the initial setup, call `setPreviewing` to hydrate the value of the `isPreviewing` helper.

```typescript
// src/routes/+layout.ts

import {setPreviewing} from '@sanity/visual-editing/svelte'
import type {LayoutLoad} from './$types'

export const load: LayoutLoad = ({data}) => {
  setPreviewing(data.preview)
  return data
}
```

Use the `isPreviewing` helper in any `.svelte` file in your application.

```typescript
// src/components/DisplayPreview.svelte

<script lang="ts">
import {isPreviewing} from '@sanity/visual-editing/svelte'
</script>

{#if $isPreviewing}
  <div>Preview Mode is Enabled!</div>
{/if}
```





# React Router/Remix

Following this guide will enable you to:

- Render overlays in your application, allowing content editors to jump directly from Sanity content to its source in Sanity Studio.
- Edit your content and see changes reflected in an embedded preview of your application in Sanity’s Presentation tool.
- **Optional**: Provide instant updates and seamless switching between draft and published content.

## Prerequisites

- A Sanity project with a hosted or embedded Studio. Read more about hosting [here](https://www.sanity.io/docs/deployment).
- A React Router application. Follow [this guide](https://reactrouter.com/start/framework/installation) to set one up.

## React Router application setup

The following steps should be performed in your React Router application.

### Install dependencies

Install the dependencies that will provide your application with data fetching and Visual Editing capabilities.

```bash
npm install @sanity/client @sanity/visual-editing @sanity/preview-url-secret

```

### Add environment variables

Create a `.env` file in your application’s root directory to provide Sanity specific configuration.

You can use [Manage](https://www.sanity.io/manage) to find your project ID and dataset, and to create a token with Viewer permissions which will be used to fetch preview content.

The URL of your Sanity Studio will depend on where it is [hosted](https://www.sanity.io/docs/deployment) or [embedded](https://www.sanity.io/docs/embedding-sanity-studio).

```bash
# .env

# Public
PUBLIC_SANITY_PROJECT_ID="YOUR_PROJECT_ID"
PUBLIC_SANITY_DATASET="YOUR_DATASET"
PUBLIC_SANITY_STUDIO_URL="https://YOUR_PROJECT.sanity.studio"
# Private
SANITY_VIEWER_TOKEN="YOUR_VIEWER_TOKEN"

```

## Application setup

### Configure the Sanity client

Create a Sanity client instance to handle fetching data from Content Lake.

Configuring the `stega` option enables automatic overlays for basic data types when preview mode is enabled. You can read more about how stega works [here](https://www.sanity.io/docs/stega).

```tsx
// src/sanity/client.ts

import { createClient } from "@sanity/client";

declare global {
  interface Window {
    ENV: {
      PUBLIC_SANITY_PROJECT_ID: string;
      PUBLIC_SANITY_DATASET: string;
      PUBLIC_SANITY_STUDIO_URL: string;
    };
  }
}

const env = typeof document === "undefined" ? process.env : window.ENV;

export const client = createClient({
  projectId: env.PUBLIC_SANITY_PROJECT_ID,
  dataset: env.PUBLIC_SANITY_DATASET,
  apiVersion: "2024-12-01",
  useCdn: true,
  stega: {
    studioUrl: env.PUBLIC_SANITY_STUDIO_URL,
  },
});

```

### Add preview mode

Preview mode allows authorized content editors to view and interact with draft content.

Create a preview helper file to manage preview sessions and return context about the current preview state.

```typescript
// app/sanity/preview.ts

import { createCookieSessionStorage } from "react-router";
import type { FilteredResponseQueryOptions } from "@sanity/client";
import crypto from "node:crypto";

const { getSession, commitSession, destroySession } =
  createCookieSessionStorage({
    cookie: {
      httpOnly: true,
      name: "__sanity_preview",
      path: "/",
      sameSite: !import.meta.env.DEV ? "none" : "lax",
      secrets: [crypto.randomBytes(16).toString("hex")],
      secure: !import.meta.env.DEV,
    },
  });

async function previewContext(
  headers: Headers
): Promise<{ preview: boolean; options: FilteredResponseQueryOptions }> {
  const previewSession = await getSession(headers.get("Cookie"));

  const preview =
    previewSession.get("projectId") === process.env.PUBLIC_SANITY_PROJECT_ID;

  return {
    preview,
    options: preview
      ? {
          perspective: "previewDrafts",
          stega: true,
          token: process.env.SANITY_VIEWER_TOKEN,
        }
      : {
          perspective: "published",
          stega: false,
        },
  };
}

export { commitSession, destroySession, getSession, previewContext };

```

Create an API endpoint to enable preview mode when viewing your application in Presentation tool.

```typescript
// app/routes/api/preview-mode/enable.ts

import { redirect } from "react-router";
import { validatePreviewUrl } from "@sanity/preview-url-secret";
import { client } from "~/sanity/client";
import { commitSession, getSession } from "~/sessions";
import { projectId } from "~/sanity/projectDetails";
import { Route } from "./+types/enable";

export const loader = async ({ request }: Route.LoaderArgs) => {
  if (!process.env.SANITY_VIEWER_TOKEN) {
    throw new Response("Preview mode missing token", { status: 401 });
  }

  const clientWithToken = client.withConfig({
    token: process.env.SANITY_VIEWER_TOKEN,
  });

  const { isValid, redirectTo = "/" } = await validatePreviewUrl(
    clientWithToken,
    request.url
  );

  if (!isValid) {
    throw new Response("Invalid secret", { status: 401 });
  }

  const session = await getSession(request.headers.get("Cookie"));
  await session.set("projectId", projectId);

  return redirect(redirectTo, {
    headers: {
      "Set-Cookie": await commitSession(session),
    },
  });
};

```

Similarly, create an API endpoint to disable draft mode.

```typescript
// app/routes/api/preview-mode/disable.ts

import { redirect } from "react-router";
import { destroySession, getSession } from "~/sessions";
import { Route } from "./+types/disable";

export const loader = async ({ request }: Route.LoaderArgs) => {
  const session = await getSession(request.headers.get("Cookie"));

  return redirect("/", {
    headers: {
      "Set-Cookie": await destroySession(session),
    },
  });
};

```

Add these routes as new entries in your application’s `routes` file.

```typescript
// app/routes.ts

import { type RouteConfig, index, route } from "@react-router/dev/routes";

export default [
  // Other routes
  route("api/preview-mode/enable", "routes/api/preview-mode/enable.ts"),
  route("api/preview-mode/disable", "routes/api/preview-mode/disable.ts"),
] satisfies RouteConfig;


```

Create a new component with a link to the disable endpoint. We add conditional logic to only render this for content authors when viewing draft content in a non-Presentation context.

```tsx
// src/components/DisablePreviewMode.tsx

import { useEffect, useState } from "react";

export function DisablePreviewMode() {
  const [show, setShow] = useState(false);

  useEffect(() => {
    setShow(window === window.parent && !window.opener);
  }, []);

  return show && <a href="/api/preview-mode/disable">Disable Preview Mode</a>;
}


```

### Enable Visual Editing

Create a Visual Editing wrapper component.

The imported `<VisualEditing>` component handles rendering overlays, enabling click to edit, and refreshing pages in your application when content changes. Render it alongside the `<DisablePreviewMode>` component you created above.

```tsx
// app/components/SanityVisualEditing.tsx

import { VisualEditing } from "@sanity/visual-editing/react-router";
import { DisablePreviewMode } from "./DisablePreviewMode";

export function SanityVisualEditing() {
  return (
    <>
      <SanityVisualEditing />
      <DisablePreviewMode />
    </>
  );
}


```

In the root layout, use the loader to pass preview mode context and your public environment variables so that they can be accessed on the client.

Render the `<SanityVisualEditing>` wrapper component when preview mode is enabled.

```tsx
// app/root.tsx

import {
  Outlet,
  Scripts,
  ScrollRestoration,
  useRouteLoaderData,
} from "react-router";
import type { Route } from "./+types/root";
import { SanityVisualEditing } from "~/components/SanityVisualEditing";
import { previewContext } from "~/sanity/previewContext";

export async function loader({ request }: Route.LoaderArgs) {
  const { preview } = await previewContext(request.headers);

  const ENV = {
    PUBLIC_SANITY_PROJECT_ID: process.env.PUBLIC_SANITY_PROJECT_ID,
    PUBLIC_SANITY_DATASET: process.env.PUBLIC_SANITY_DATASET,
    PUBLIC_SANITY_STUDIO_URL: process.env.PUBLIC_SANITY_STUDIO_URL,
  };

  return { preview, ENV };
}

export function Layout({ children }: { children: React.ReactNode }) {
  const { preview, ENV } = useRouteLoaderData("root");

  return (
    <html lang="en">
      <head>
        <meta charSet="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
      </head>
      <body>
        {children}
        <ScrollRestoration />
        <Scripts />
        {preview && <SanityVisualEditing />}
         <script
          dangerouslySetInnerHTML={{
            __html: `window.ENV = ${JSON.stringify(ENV)}`,
          }}
        />
      </body>
    </html>
  );
}

export default function App() {
  return <Outlet />;
}

```

### Render a page in preview mode

Update your existing `client.fetch` calls with the options returned by the `previewContext` function. This ensures published content is served to end-users, and draft content with overlays is served when in preview mode.

```tsx
// app/routes/home.tsx

import type { Route } from "./+types/home";
import type { SanityDocument } from "@sanity/client";
import { client } from "~/sanity/client";
import { previewContext } from "~/sanity/previewContext";

const query = `*[_type == "page"][0]{title}`;
type Response = SanityDocument<{title?: string }>

export async function loader({ request }: Route.LoaderArgs) {
  const { options } = await previewContext(request.headers);
  const data = await client.fetch<Response>(query, {}, options);
  return data;
}

export default function Home({ loaderData }: Route.ComponentProps) {
  return <h1>{loaderData.title}</h1>;
}

```

## Studio setup

To setup Presentation tool in your Sanity Studio, import the tool from `sanity/presentation`, add it to your `plugins` array, and set `previewUrl` to the base URL of your application.

We similarly recommend using environment variables loaded via a `.env` file to support development and production environments.

```tsx
// sanity.config.ts

import { defineConfig } from "sanity";
import { presentationTool } from "sanity/presentation";

export default defineConfig({
  // ... project configuration
  plugins: [
    presentationTool({
      previewUrl: {
        origin: process.env.SANITY_STUDIO_PREVIEW_ORIGIN,
        preview: "/",
        previewMode: {
          enable: "/api/preview-mode/enable",
          disable: "/api/preview-mode/disable",
        },
      },
    }),
    // ... other plugins
  ],
});

```

## Optional extras

### Enable instant updates and perspective switching

Instant updates and perspective switching require opting into using [loaders](https://www.sanity.io/docs/loaders) to fetch data.

Add the React specific loader to your application dependencies:

### Install dependencies

```bash
npm install @sanity/react-loader

```

### Set up loaders

Create a new loader file which calls `setServerClient` and sets up the `loadQuery` helper function which will be used for fetching content on the server.

```tsx
// src/sanity/loader.ts

import * as serverOnly from "@sanity/react-loader";
import { client } from "~/sanity/client";

const { loadQuery, setServerClient } = serverOnly;

setServerClient(client.withConfig({ token: process.env.SANITY_VIEWER_TOKEN }));

export { loadQuery };

```

Update your preview helper file to support `loadQuery` and remove the token option, as this is now configured at the server client level.

```typescript
// app/sanity/preview.ts

import { createCookieSessionStorage } from "react-router";
import type { loadQuery } from "~/sanity/loader.server";
import crypto from "node:crypto";

const { getSession, commitSession, destroySession } =
  createCookieSessionStorage({
    cookie: {
      httpOnly: true,
      name: "__sanity_preview",
      path: "/",
      sameSite: !import.meta.env.DEV ? "none" : "lax",
      secrets: [crypto.randomBytes(16).toString("hex")],
      secure: !import.meta.env.DEV,
    },
  });

async function previewContext(
  headers: Headers


): Promise<{ preview: boolean; options: Parameters<typeof loadQuery>[2] }> {
  const previewSession = await getSession(headers.get("Cookie"));

  const preview =
    previewSession.get("projectId") === process.env.PUBLIC_SANITY_PROJECT_ID;

  return {
    preview,
    options: preview
      ? {
          perspective: "previewDrafts",
          stega: true,
        }
      : {
          perspective: "published",
          stega: false,
        },
  };
}

export { commitSession, destroySession, getSession, previewContext };

```

### Render a page in preview mode

In `loader` use the `loadQuery` function created above. The `initial` data returned here is passed to `useQuery` in the page component.

When in Presentation, `useQuery` will handle live updates as content is edited.

```typescript
// app/routes/home.tsx

import type { Route } from "./+types/home";
import type { SanityDocument } from "@sanity/client";
import { loadQuery } from "~/sanity/loader.server";
import { previewContext } from "~/sanity/previewContext";

const query = `*[_type == "page"][0]{title}`;
type Response = SanityDocument<{title?: string }>

export async function loader({ request }: Route.LoaderArgs) {
  const { options } = await previewContext(request.headers);
  const data = await loadQuery<Response>(query, {}, options);
  return data;
}

export default function Home({ loaderData }: Route.ComponentProps) {
  const { data } = useQuery(query, {}, { initial: loaderData });
  return <h1>{data.title}</h1>;
}

```

### Add data attributes for extended overlay support

`useQuery` also returns an `encodeDataAttribute` helper method for generating `data-sanity` attributes. These attributes give you direct control over rendering [overlays](https://www.sanity.io/docs/loaders-and-overlays#1dbcc04a7093) in your application, and are especially useful if not using stega encoding.

```tsx
// app/routes/home.tsx

import type { Route } from "./+types/home";
import type { SanityDocument } from "@sanity/client";
import { loadQuery } from "~/sanity/loader.server";
import { previewContext } from "~/sanity/previewContext";

const query = `*[_type == "page"][0]{title}`;
type Response = SanityDocument<{title?: string }>

export async function loader({ request }: Route.LoaderArgs) {
  const { options } = await previewContext(request.headers);
  const data = await loadQuery<Response>(query, {}, options);
  return data;
}

export default function Home({ loaderData }: Route.ComponentProps) {
  const { data, encodeDataAttribute } = useQuery(
    query,
    {},
    { initial: loaderData }
  );

return <h1 data-sanity={encodeDataAttribute(["title"])}>{data.title}</h1>;
}

```





# Presentation tool

The Presentation Tool enables [Visual Editing](/docs/visual-editing/introduction-to-visual-editing) by integrating a live preview of your web application within Sanity Studio. 

It provides: 

- **Live preview**: See draft content updates in real-time within the Studio
- **Click-to-edit**: Interactive overlays that help content creators find and edit the right fields
- **Page building**: Advanced capabilities for adding, moving, and removing content sections
- **Preview sharing**: A way for content creators to share a preview with others
- **Locations**: Add shortcuts to open preview where a document is used from the Structure tool

For Studio-user friendly documentation, [go to the user guide of the Presentation tool](/docs/visual-editing/preview-and-page-building).

![](https://cdn.sanity.io/images/3do82whm/next/fe1039dd43af99e1145bce644d4480309f676cb1-1224x931.png)

[Visual Editing – Introduction](/docs/visual-editing/introduction-to-visual-editing)

[Overlays](/docs/visual-editing/visual-editing-overlays)

[Fetching content for Visual Editing](/docs/visual-editing/fetching-content-for-visual-editing)

[Custom overlay components](/docs/visual-editing/custom-overlay-components)



## Prerequisites

To use the Presentation tool, your studio must be updated to `v3.40.0` or preferably `latest`.

```sh
npm install sanity@latest
```

> [!TIP]
> Protip
> You must also add Visual Editing support into your front-end application. 
> 
> We recommend exploring the implementation guides (located in the left-side navigation), these will also take you through setting up the Presentation tool.

## Configuration steps

Similarly to the other tools and plugins, you add the Presentation tool to your main [studio configuration file](/docs/studio/configuration). The example below shows a typical setup for a Next.js project where specific endpoints are supplied to let Sanity Studio put the front end into preview mode.

### Basic configuration

14. Import `presentationTool` from `sanity/presentation`
14. Add it to the `plugins` array
14. Add your frontend-specific details, such as the `origin` URL and endpoints for enabling or disabling preview mode

```typescript
// sanity.config.ts
// ...all other imports
import { presentationTool } from 'sanity/presentation'

export default defineConfig({
  // ... all other config settings
  plugins: [
    // ...all other plugins
    presentationTool({
      previewUrl: {
        origin: 'https://my-cool-site.com',
        previewMode: {
          enable: '/api/draft-mode/enable',
          disable: '/api/draft-mode/disable',
        },
      },
    }),
  ],
})
```

> [!TIP]
> Protip
> If you are working with an embedded studio you can skip the origin property on line 10 of the example.

### Map content to front-end routes with `locations` resolver function

To map Sanity documents to their corresponding front-end routes, create a `locations` resolver function. This function tells the Presentation Tool how each document type corresponds to a route in the front end.

![](https://cdn.sanity.io/images/3do82whm/next/022193147e2e0db819f4c6e86aa738f5735191d6-688x418.png)

#### Add route definitions for your document types

In the example below, we map documents of type `product` to `/products/${slug}` and add them to the top-level index. You will need to substitute your own types and routes as appropriate.

```typescript
// ./lib/presentation/resolve-production-url.ts
import {defineLocations} from 'sanity/presentation'

export default defineLocations({
  // Map document types to frontend routes
  product: {
    select: {title: 'title', slug: 'slug.current'},
    resolve: (doc) => ({
      locations: [
        {title: doc.title, href: `/products/${doc.slug}`},
        {title: 'Products Index', href: `/products`},
      ],
    }),
  },
  // ...
})
```

#### Import the resolver function into `sanity.config.ts` and pass it to the `presentationTool` config

```typescript
import {defineConfig} from 'sanity'
import {presentationTool} from 'sanity/presentation'
import resolveProductionUrl from './lib/presentation/resolve-production-url'

export default defineConfig({
  // ...
  plugins: [
    presentationTool({
      resolve: resolveProductionUrl,
      previewUrl: {
        origin: 'https://my-cool-site.com',
        previewMode: {
          enable: '/api/draft-mode/enable',
          disable: '/api/draft-mode/disable',
        },
      },
    }),
  ],
})
```

### Configuration reference

The Presentation tool takes the following configuration attributes:

#### Properties

| Property | Description |
|----------|-------------|
| previewUrl | Accepts an object with keys for setting the origin URL of your front end, as well as defining endpoints for enabling or disabling preview mode in your front end. Also accepts a plain string in the shape of a URL for projects that don't need the detailed setup. |
| icon | Sets an icon to represent the Presentation tool in the Studio's navigation bar. To use Sanity icons, install the @sanity/icons package. |
| name | Sets a name for the Presentation tool. It's not visually represented in the Studio UI, but it’s included in the Studio's URL for the Presentation tool.

This is useful if you want to implement multiple instances of Presentation, for example, for different channels, domains, or preview environments.

Default value: presentation |
| title | Sets the title that is displayed in the Studio's navigation bar. Keep it short and descriptive to help editorial users understand what they can do with the tool. 

Default value: Presentation |
| resolve | Return the document-to-URL mapping to add affordances to quickly open documents in the Presentation tool. Go to the Presentation Resolver API documentation to learn more. |




[Add a custom preview header or navigator component](/docs/visual-editing/customizing-preview-header-and-navigation)







# Fetch preview content

To get started with Visual Editing in a specific framework, we recommend following the step-by-step guides in the left navigation. Return to this article if you want to understand the underlying concepts more deeply.

## Understanding Visual Editing Features

Visual Editing enables content editors to use [the Presentation tool](/docs/visual-editing/configuring-the-presentation-tool) to see and interact with their draft content directly in the context of your front-end application. With Sanity, Visual Editing follows a progressive enhancement model, offering three main layers of functionality:

- **Live Preview**: See draft content updates in real-time within the Studio 
- **Click-to-Edit**: Interactive overlays that help editors find and edit the right fields 
- **Page Building**: Advanced capabilities for adding, moving, and removing content sections

The features available to you depend on your framework choice and how you implement content fetching.

> [!TIP]
> Protip
> You can start with a basic implementation that provides core preview functionality, then enhance it with more interactive features as needed.

[Visual Editing – Introduction](/docs/visual-editing/introduction-to-visual-editing)

[The Presentation tool](/docs/visual-editing/configuring-the-presentation-tool)

[Overlays](/docs/visual-editing/visual-editing-overlays)

[Custom overlay components](/docs/visual-editing/custom-overlay-components)



## Server-side fetching (basic implementation)

Server-side fetching provides the foundation for Visual Editing with minimal setup:

- Supports live preview and click-to-edit features 
- Works with any framework that supports server-side rendering
- Refreshes content within seconds of changes
- Requires minimal configuration

### How it works

1. When an content creator opens the Presentation tool, it creates a preview URL that included a secret that determine whether or not to activate draft mode.

2. When your application receives a request with draft mode enabled:

- It validates the preview session
- Fetches the latest draft content from Content Lake with [Stega-encoded Content Source Maps](/docs/visual-editing/visual-editing-overlays) 
- Renders the page with this draft content on the server

3. When content changes occur in the Studio:

- It signals to the `<VisualEditing />` component (or a custom client-side script) in your frontend that new content is available 
- It refreshes the page, thus triggering a rerender on the server
- The page refreshes to show the updated content

4. Click-to-edit overlays work by:

- Draft content is appended with [Stega-encoded Content Source Maps](/docs/visual-editing/visual-editing-overlays)
- Content not supported by automatic Stega-encoding can be manually mapped with helper functions that generates the correct data attributes
- The `<VisualEditing />` component has code that maps through the DOM and injects an overlay interface into your preview
- When clicked, the Studio opens with the corresponding field focused for editing

### Requirements

- Server-side rendering support 
- Server-side authentication token with read permissions 
- Method to toggle preview mode on the server

### Best for

- Getting started with Visual Editing 
- Projects that need a simple implementation 
- Frameworks with built-in revalidation APIs

## Loaders (enhanced implementation)

Building on the server-side foundation, Loaders are framework-specific and leverage both server-side and client-side rendering to:

- Enable near-instant refresh rates for previews
- Support all Visual Editing features including drag and drop page building

> [!TIP]
> Protip
> In frameworks like Next.js, you can implement this without too much overhead with the next-sanity toolkit. 

### How it works

1. Initial page load:

- Server performs the initial content fetch (like in server-side fetching described above)
- Application hydrates with this initial content and initializes a client-side loader
- Leverages the users authenticated browser session
- A real-time connection is established with Presentation tool using the `postMessage` protocol 

2. Content updates in the Studio:

- The Presentation tool immediately signals the new changes to the front end
- The UI updates instantly without a full page refresh

3. Page building interactions:

- The Studio provides an enhanced overlay interface 
- Content creators can add, remove, and reorder content sections 
- Changes are immediately reflected through optimistic updates 
- The application syncs with the actual content state in the background

4. Framework-specific implementations:

- Next.js App Router: Uses `defineLive` for seamless integration 
- Other React frameworks: Use framework-specific loaders or data fetching patterns

5. Optimistic updates for drag and drop with `useOptimistic`:

- Provides immediate feedback for editing interactions 
- Works independently of the data fetching method 
- Essential for interactions like drag-and-drop reordering 
- Bridges the gap when server responses would feel too slow 
- Only requires `<VisualEditing />` component for supported frameworks
- Can be used alongside any data fetching approach

### Requirements

- Framework with client-side hydration (React, Vue, Svelte) 
- Framework-specific preview mode support (can also be self-implemented)

### Best for

- Projects needing the full Visual Editing experience 
- Applications requiring real-time content updates 
- Frameworks like Next.js (App Router) that support advanced features

## Framework Support 

### Full Support (Server + Client)

- Next.js (App Router): Full page building experience via `defineLive` 
- Next.js (Pages Router): Support via loaders patterns
- React-based frameworks: Full page building capabilities 
- Remix: Support via loaders pattern 
- Nuxt: Support via loaders pattern 
- SvelteKit: Support via loaders pattern

### Basic Support (Server-Side Only)

- Any framework with server-side rendering 
- Static site generators with SSR mode (e.g., Astro in SRR/hybrid mode)

### Coming Soon

- Enhanced support for Vue.js and Svelte frameworks 
- Expanded page building capabilities beyond React

## Implementation Considerations

- Start with server-side fetching for basic preview functionality 
- Add client-side fetching when you need real-time updates or page building
- Consider your hosting platform's capabilities for server-side revalidation
- Static site generators require a separate preview deployment with SSR enabled

## Note on static site generators (SSGs)

While static site generators can make it simple to build websites, they are not well-suited for providing a Visual Editing experience for content teams. Static site generators pre-render pages at build time, meaning any content changes require a full or partial site rebuild. This usually doesn't scale well as the site grows and by the nature of being static can't provide the real-time editing experience that Visual Editing offer.

However, some frameworks, like [Astro](https://www.sanity.io/plugins/sanity-astro), also support server-side rendering. In scenarios where you want to use static site generation for your production builds, you can set up a secondary deployment for previews that uses the server-side rendering approach.



# Content Source Maps

The Content Lake can enrich all queries with metadata describing the **source** (the document and attribute) of every content fragment retrieved in the query. This metadata is useful for tooling that runs on the front end and interprets this information to provide additional ways of interacting with the content for users: for example, displaying content provenance details for compliance reviewers, providing direct links to where to edit the content for editors, and even allowing in-line editing directly in the front-end experience.

This metadata is called Content Source Maps, and it follows the Content Source Maps specification. You can find details about the Content Source Maps specification in the [official specification GitHub repository](https://github.com/sanity-io/content-source-maps).

> [!NOTE]
> Further reading
> If you want to see how to take advantage of content source maps in your frontends, please see the following articles:
> 
> Sanity Client: Get started with Content Source Maps
> 
> Preview Kit: Content Source Maps

To request a Content Source Map on a GROQ query, you need to pass the parameter `resultSourceMap=true` when you send your query to the Content Lake HTTP API. When you provide this parameter, the result of your query will include an additional element with the corresponding content source map.

> [!WARNING]
> Gotcha
> Content Source Maps are supported only in versions 2021-03-25 or later of the Content Lake API.

For example, imagine these three documents exist in your dataset:

- A document representing the author George Orwell

```json
{
  "_id": "author-george-orwell-4c9f",
  "_type": "author",
  "died": "1950-01-21",
  "dob": "1903-05-25",
  "firstName": "George",
  "lastName": "Orwell"
}
```

- Another document representing the book “Animal Farm” by author George Orwell (a reference to the document for this author above)

```json
{
  "_id": "book-animal-farm-3856",
  "_type": "book",
  "description": "It tells the story of a group of farm animals who rebel against their human farmer",
  "title": "Animal Farm",
  "author": {
    "_ref": "author-george-orwell-4c9f"
  }
}
```

- And another document representing the book “Nineteen Eighty-Four” by author George Orwell as well (as before, a reference to the first document above)

```json
{
  "_id": "book-1984-12eb",
  "_type": "book",
  "description": "Nineteen Eighty-Four (also published as 1984) is a dystopian social science fiction novel and cautionary tale by English writer George Orwell.",
  "title": "Nineteen Eighty-Four",
  "author": {
    "_ref": "author-george-orwell-4c9f"
  }
}
```

You can run the following GROQ query to obtain a consolidated view that provides the author's last name and a set with the names of the books they have written:

```groq
*[_type ==  'author'] {
  "authorName": lastName,
  "booksWritten": *[_type == 'book' && references(^._id)].title
}
```

If you run this query passing the `resultSourceMap` parameter (i.e. `https://<project-id>.api.sanity.io/v2023-05-03/data/query/<dataset>?query=<GROQ-query>&resultSourceMap=true`), the response will look like this:

```json
{
  "result": [
    {
      "authorName": "Orwell",
      "booksWritten": ["Nineteen Eighty-Four", "Animal Farm"]
    }
  ],
  "resultSourceMap": {
    "documents": [
      {
        "_id": "author-george-orwell-4c9f"
      },
      {
        "_id": "book-1984-12eb"
      },
      {
        "_id": "book-animal-farm-3856"
      }
    ],
    "paths": ["$['lastName']", "$['title']"],
    "mappings": {
      "$[0]['authorName']": {
        "source": {
          "document": 0,
          "path": 0,
          "type": "documentValue"
        },
        "type": "value"
      },
      "$[0]['booksWritten'][0]": {
        "source": {
          "document": 1,
          "path": 1,
          "type": "documentValue"
        },
        "type": "value"
      },
      "$[0]['booksWritten'][1]": {
        "source": {
          "document": 2,
          "path": 1,
          "type": "documentValue"
        },
        "type": "value"
      }
    }
  }
}

```

Observe that the `resultsSourceMap` entry includes:

- Under `documents`, a list of documents from where the content in the query results comes from, i.e.: - author document with id `author-george-orwell-4c9f`,
- book document with id `book-1984-12eb`
- and book document with id `book-animal-farm-3856`


- Under `paths`, a list of the attribute names from where the content in the query results comes from, i.e.:- attribute name `lastName`(specified as `"$['lastName']"`)
- and attribute name `title` (specified as `"$['title']"`)


- Under mappings, a map where each entry indicates, for each element of the response, what document and path (attribute) from the lists above it comes from. 

In this example:

```json
"$[0]['authorName']": {
    "source": {
      "document": 0,
      "path": 0,
      "type": "documentValue"
    },
    "type": "value"
}
```

The first map entry describes that in the first element in the response (`$[0]`), the attribute "authorName" (`['authorName']`) comes from (`source:`) the document in the first position of the “documents” set (`"document": 0`, which is `author-george-orwell-4c9f` ), and the attribute in the first position in the “paths” set (`"path": 0`, which is `lastName`); i.e. the value `"authorName": "Orwell"` in the response, comes from the `document author-george-orwell-4c9f`, and attribute `lastName`.



```json
"$[0]['booksWritten'][0]": {
  "source": {
    "document": 1,
    "path": 1,
    "type": "documentValue"
  },
  "type": "value"
}
```

The second map entry describes that in the first element in the response (`$[0]`), the attribute “booksWritten” (`['booksWritten']`), its first element (`[0]`) comes from (`source:`) the document in the second position of the “documents” set (`"document": 1`, which is `book-1984-12eb` ), and the attribute in the second position in the “paths” set (`"path": 1`, which is `title`); i.e. the value in the first element of `booksWritten` in the response (the string `"Nineteen Eighty-Four"`), comes from the document `book-1984-12eb`, and attribute `title`.



```json
"$[0]['booksWritten'][1]": {
  "source": {
    "document": 2,
    "path": 1,
    "type": "documentValue"
  },
  "type": "value"
}
```

The third map entry describes that in the first element in the response (`$[0]`), the attribute “booksWritten” (`['booksWritten']`), its second element (`[1]`) comes from (`source:`) the document in the third position of the “documents” set (`"document": 2`, which is `book-animal-farm-3856`), and the attribute in the second position in the “paths” set (`"path": 1`, which is `title`); i.e. the value in the second element of `booksWritten` in the response (the string `"Animal Farm"`), comes from the document `book-animal-farm-3856`, and attribute `title`.



## GROQ compatibility

> [!WARNING]
> Gotcha
> In the current version of the Content Source Maps capability, only the GROQ functions listed below will provide content source metadata in a query. Also note that the feature is currently only supported in the Content Lake, (i.e. implementations such as groq-js are not yet supported).

Types:

- [Object](/docs/object-type) (includes Portable Text)
- [Array](/docs/array-type)
- [Number](/docs/number-type)
- [String](/docs/string-type)

Traversal:

- Attribute access traversal
- Element access traversal
- Slice traversal
- Filter traversal
- Array postfix traversal
- Projection traversal
- Dereference traversal

Conditionals:

- Select (partially supported: result only, for string and number type)
- Conditional projection traversal (partially supported: result only, for string and number type)

Operators:

- Dereference

Functions:

- string (coercing) (partially supported: string(number) only)
- lower
- upper
- pt::text()

## GraphQL

Content Source Maps are supported in the Sanity [GraphQL API v2023-08-01](https://www.sanity.io/changelog?platforms=GraphQL#change-9ec89318-a340-4e23-91d9-3154da5b6244) and later. Read more about this in the [GraphQL docs](/docs/content-lake/graphql).



# Stega enconding

Stega (from “steganography) is an encoding method developed by Sanity in collaboration with Vercel that allows metadata mappings to be embedded into an application. It encodes [Content Source Maps](/docs/visual-editing/content-source-maps), a standard for annotating fragments in a JSON document with metadata about their origin.

With Stega enabled, data rendered in your application looks the same but contains invisible metadata that Sanity's Visual Editing tooling can detect. This enables click-to-edit overlays directly in your application without requiring you to manually set up mapping for components.

> [!WARNING]
> Gotcha
> Stega encoding adds additional data to your strings in preview mode. The trade-off is that if you use these strings in business logic, your application might act differently in preview mode. You can use helper functions to clean out Stega-encoding before passing them to non-component contexts.

[Introduction to Visual Editing](/docs/visual-editing/introduction-to-visual-editing)

[Visual Editing Resources](/docs/archive/visual-editing-resources)

[Overlays ("click-to-edit")](/docs/visual-editing/visual-editing-overlays)

[Using Stega with Sanity Client](https://github.com/sanity-io/client?tab=readme-ov-file#using-visual-editing-with-steganography)



## How Stega works

Stega-encoding takes the [Content Source Maps](/docs/visual-editing/content-source-maps) JSON and encodes it into invisible UTF-8 encoded characters that are appended to a string value. The Visual Editing tooling can pick this up, decode it, and use for the click-to-edit functionality:

```json
Oxford Shoes&ZeroWidthSpace;&ZeroWidthSpace;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwj;&#xFEFF;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&zwnj;&#xFEFF;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&#xFEFF;&zwj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwj;&zwnj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&#xFEFF;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwj;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&#xFEFF;&zwj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwj;&zwj;&ZeroWidthSpace;&zwj;&#xFEFF;&#xFEFF;&ZeroWidthSpace;&zwj;&#xFEFF;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&zwj;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwnj;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&#xFEFF;&ZeroWidthSpace;&zwj;&#xFEFF;&zwj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwj;&zwnj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&#xFEFF;&zwnj;&zwnj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&#xFEFF;&#xFEFF;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwj;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&#xFEFF;&zwj;&ZeroWidthSpace;&#xFEFF;&zwj;&#xFEFF;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&#xFEFF;&zwj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&zwj;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&zwj;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwj;&ZeroWidthSpace;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwj;&#xFEFF;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwj;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&#xFEFF;&zwj;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwnj;&zwnj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&zwj;&zwj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&ZeroWidthSpace;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&#xFEFF;&zwnj;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&zwj;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwnj;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&#xFEFF;&ZeroWidthSpace;&zwj;&#xFEFF;&zwj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwj;&zwnj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&#xFEFF;&zwnj;&zwnj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&#xFEFF;&zwj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&zwj;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwj;&ZeroWidthSpace;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwj;&#xFEFF;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwj;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&#xFEFF;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&zwnj;&zwj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&#xFEFF;&#xFEFF;&zwnj;
```

## What is Content Source Maps?

[Content Source Maps](/docs/visual-editing/content-source-maps) is a standard representation for annotating fragments in a JSON document with metadata about their origin: the field, document, and dataset they originated from. It provides a separate document alongside the content that adds the metadata without changing the original document's layout.

Content Source Maps enable annotating JSON documents with "source" metadata, allowing end users to navigate directly to the source to edit it. In the future, they will also enable the annotation of JSON documents with arbitrary metadata for other use cases.

The Content Source Map offers a standard method for representing the mapping between content values and their sources. Here is an example Content Source Map:

```json
{
  "documents": [
    {
      "_id": "author-1"
    },
    {
      "_id": "author-2"
    }
  ],
  "paths": ["$['name']"],
  "mappings": {
    "$[0]": {
      "type": "value",
      "source": {
        "type": "documentValue",
        "document": 0,
        "path": 0
      }
    },
    "$[1]": {
      "type": "value",
      "source": {
        "type": "documentValue",
        "document": 1,
        "path": 0
      }
    }
  }
}
```

[Troubleshooting Visual Editing](/docs/visual-editing/troubleshooting-visual-editing)







# Overlays for click-to-edit

Overlays are a core part of Sanity's [Visual Editing](/docs/visual-editing/introduction-to-visual-editing) that enables interactive editing experiences directly in your front end. They range from simple click-to-edit functionality to [advanced drag-and-drop page building capabilities](/docs/visual-editing/enabling-drag-and-drop).

![Screenshot of a Sanity Studio interface displaying a blog post titled ‘Visual Editing.’ The post description reads, ‘Your one stop shop for everything about Visual Editing and the Presentation tool in Sanity Studio.’ The editor on the right includes fields for the title and description, along with a warning indicating that the document is used on all pages. The post preview includes an image of a desk with a plant and lamp, and a subheading for ‘The difficult second post’ dated November 18, 2024.](https://cdn.sanity.io/images/3do82whm/next/5b8a4d329a6f44cbff5f884f7c7113f121958019-1459x1110.png)

## Understanding Overlays

Overlays serve two main purposes in Visual Editing:

- **Click-to-edit**: Highlights content areas and takes you directly to the corresponding field in the Studio 
- **Page building**: Enables drag-and-drop interactions for adding, moving, and removing sections when supported by your framework

[Custom overlay components](/docs/visual-editing/custom-overlay-components)

[Drag-and-drop page building](/docs/visual-editing/enabling-drag-and-drop)



### How Overlays Work 

For overlays to function, they need to:

- Identify Content: Locate elements in your DOM that contain Sanity content 
- Map to Studio: Create correct references to documents and fields in the Studio 
- Enable Interactions: Support the appropriate level of interactivity based on your framework

## How to enable Overlays

### Automatically with Stega Encoding

The simplest approach uses [Stega encoding](/docs/visual-editing/stega), which automatically adds invisible [Content Source Maps](/docs/visual-editing/content-source-maps) to text content:

```typescript
// Stega is usually enabled at the client level
const client = createClient({
  // ...other config
  stega: {
    enabled: true
  }
})

```

### Manually with Data Attributes

For non-text content or custom interactions, use data attributes:

```typescript
import { createDataAttribute } from "@sanity/visual-editing"

function Section({ documentId, documentType, sections }) {
  const attr = createDataAttribute({ 
    id: documentId, 
    type: documentType,
    path: 'sections'
  })

  return (
    <div data-sanity={attr().toString()}>
      {sections.map(section => (
        <div
          key={section._key}
          data-sanity={attr(`sections[_key=="${section._key}"]`).toString()}
        >
          {section.content}
        </div>
      ))}
    </div>
  )
}

```

### Framework-Specific Loading

When using framework-specific loaders, you get a pre-configured encoding helper. Here is how it can look with the React loader:

```typescript
export default function Page() {
  const { data, encodeDataAttribute } = useQuery(query)
  
  return (
    <div data-sanity={encodeDataAttribute(['sections'])}>
      {/* Your content */}
    </div>
  )
}

```

## Progressive Enhancement

Overlays follow a progressive enhancement model based on your framework's capabilities:

22. **Basic (All frameworks)** - Click-to-edit functionality
- Content highlighting
- Direct Studio navigation


22. **Advanced (React/React-based frameworks)** - Full page building experience
- Drag-and-drop section management
- Real-time content updates



## Framework Support

- **Advanced support**: Next.js App Router and other React-based frameworks
- **Basic Support**: Any framework with server-side rendering
- **Coming soon**: Advanced support for Vue.js and Svelte frameworks

## Implementation Tips

- Start with Stega encoding for text-based content
- Use `createDataAttribute` for non-text content and custom interactions
- Consider framework-specific loaders for enhanced capabilities
- Test overlay behavior both in the Presentation tool

## Note on Vercel Integration

When using Vercel's Visual Editing:

- Overlays appear automatically in preview builds if Stega is enabled
- No additional configuration needed if Stega is enabled because the overlays are powered by the Vercel toolbar
- Still supports manual overlay configuration for iframe contexts





# Drag and drop

Visual Editing offers page building capabilities that allow content editors to add, move, remove, and reorder content sections directly within their website's preview. Drag and drop enables content creators to visually rearrange content within the context of their application/website — allowing them to re-order array items with immediate visual feedback and dynamic zoomed-out overviews.

![Graphic illustrating a draggable user interface. It features transparent rectangular overlays labeled ‘Draggable Overlay’ with a thin blue outline, positioned over numbered placeholders (‘1,’ ‘2,’ ‘3,’ ‘4’) in a grid layout. The design is on a light blue background, emphasizing interaction and movement.](https://cdn.sanity.io/images/3do82whm/next/0980577d20dbf094133f894c8662844250d273a1-1274x824.png)

## Prerequisites

To implement page building features, you need:

- Visual Editing configured and enabled, with up-to-date dependencies
- Content structured to using arrays for reorderable sections
- Some understanding of [Stega/Content Source Maps](/docs/visual-editing/stega) and how to enable [overlays](/docs/visual-editing/visual-editing-overlays) manually.
- Studio on version `3.65.0` or above (`npm install sanity@latest`)

### Browser/device support

Drag and drop is supported in the following browsers/versions:

- Chrome ≥ 108
- Safari ≥ 15.6
- Firefox ≥ 115
- Edge ≥ 126

> [!WARNING]
> Gotcha
> Drag and drop is currently not compatible with touch-based devices.

[Visual Editing – Introduction](/docs/visual-editing/introduction-to-visual-editing)

[Overlays](/docs/visual-editing/visual-editing-overlays)

[Fetching content for Visual Editing](/docs/visual-editing/fetching-content-for-visual-editing)

[Custom overlay components](/docs/visual-editing/custom-overlay-components)



## Understanding the building blocks

Presentation's drag and drop functionality is framework-agnostic and can be implemented without significant changes to your codebase. It uses Overlays for visual representation, and updates your structured content directly. It does not mutate or reorder the DOM.

In a Presentation drag and drop sequence:

14. An [Overlay](https://www.sanity.io/docs/visual-editing-overlays) element is dragged to a new position on the page.
14. The array order in the Presentation tool is updated, reflecting the item’s new position.
14. Your front-end receives the updated Sanity data and re-renders as normal.

## Content modeling for page building

Drag and drop for page building, and similar layout systems, works with array-based content. Your schema (content model) should:

- Use arrays to represent reorderable sections
- Define content blocks as object types

```javascript
// Example schema
defineField({
  name: 'sections',
  type: 'array',
  of: [
    defineArrayMember({ type: 'hero' }),
    defineArrayMember({ type: 'features' }),
    defineArrayMember({ type: 'callToAction' })
  ]
})

```

> [!TIP]
> Protip
> You can nest array type fields, but it is required that you wrap the nested array in an object type.

## In your front end application

To enable the drag and drop functionality in your front end, you must:

- Implement [Visual Editing](/docs/visual-editing/introduction-to-visual-editing)
- Apply data attributes to the array items, and optionally the array parent if you want to enable click-to-edit for it
- Make sure the array is rendering as a client-side component (`'use client'` with React Server Components-based frameworks)

### Add data attributes to elements

> [!TIP]
> Protip
> There are a few different concepts of "paths" in Sanity. In the context below, we are discussing form paths in particular, which you can learn more about in this article: How form paths work. 

To enable drag and drop functionality:

26. Add `data-sanity` attributes to the array elements
26. Include required information: - Document ID (`_id`)
- Document type (`_type`)
- Array item key (`_key`)
- Path to array schema type (`arrayName[_key=="<the-section-key>"]`)



These attributes connect your UI elements to the underlying content structure.

You can use the `createDataAttribute` helper function to achieve this:

```typescript
// /components/SectionParent.tsx
import {createDataAttribute} from '@sanity/visual-editing'
import {Sections} from '@/compoents/Sections'

// Your Sanity configuration
const config = {
  projectId: 'your-project-id',
  dataset: 'production',
  baseUrl: 'https://your-studio-url.sanity.studio',
}

export function SectionParent({documentId, documentType, sections: initialSections}) {
  return (
    <div
      data-sanity={createDataAttribute({
        ...config,
        id: documentId,
        type: documentType,
        path: 'sections',
      }).toString()}
    >
      <Sections data={sections} />
    </div>
  )
}
```

### Implement optimistic updates

Load the array item data through the [useOptimistic](/docs/visual-editing/useoptimistic-reference) hook from the Visual Editing package (or framework-specific toolkit) to ensure that the user experience is fast and not slowed down by network latency.

The `useOptimistic` hook exposes ways of controlling the state and when to update the UI, which you typically want only when the array data has changed:

```typescript
const sections = useOptimistic<PageSection[] | undefined, SanityDocument<PageData>>(
  initialSections,
  (currentSections, action) => {
    // The action contains updated document data from Sanity
    // when someone makes an edit in the Studio

    // If the edit was to a different document, ignore it
    if (action.id !== documentId) {
      return currentSections
    }

    // If there are sections in the updated document, use them
    if (action.document.sections) {
      return action.document.sections
    }

    // Otherwise keep the current sections
    return currentSections
  }
)
```

> [!TIP]
> Protip
> The useOptimistic hook is supplementary to data fetching and works independently.

### How useOptimistic works

Typically, mutations created in your application need to be committed to Content Lake via the Presentation tool, and content refetched before the UI can be updated.

![Diagram illustrating a sequence of interactions between three components: ‘Application,’ ‘Presentation Tool,’ and ‘Content Lake.’ 	•	The ‘Application’ sends a mutation to the ‘Presentation Tool.’ 	•	The ‘Presentation Tool’ forwards the mutation to the ‘Content Lake.’ 	•	The ‘Content Lake’ commits the mutation and returns the content to the ‘Presentation Tool.’ 	•	The ‘Presentation Tool’ sends the content back to the ‘Application.’  Each interaction is depicted with labeled arrows connecting the components, providing a clear flow of data.](https://cdn.sanity.io/images/3do82whm/next/51476ebe49fecd191631aafbb3ab437ebbc0aaad-3600x2000.png)

The `useOptimistic` hook uses a local document store to enable developers to opt-in to instant updates for specific content. UI can be updated with the anticipated result of a mutation, avoiding the delay required when submitting and refetching data from Content Lake.

`useOptimistic` detects when up-to-date content does eventually arrive and resets its internal state, ready to handle the next mutation.

![Diagram illustrating a more detailed sequence of interactions between ‘Application,’ ‘Document Store,’ ‘Presentation Tool,’ and ‘Content Lake.’ 	•	The ‘Application’ sends a mutation to the ‘Document Store.’ 	•	The ‘Document Store’ commits the mutation and returns content, with an additional step labeled ‘useOptimistic,’ which allows the ‘Application’ to return content while the server commits mutations. 	•	After the mutation is committed, the ‘Document Store’ forwards the mutation to the ‘Presentation Tool.’ 	•	The ‘Presentation Tool’ sends the mutation to the ‘Content Lake.’ 	•	The ‘Content Lake’ commits the mutation and returns the updated content. 	•	The final content is returned back to the ‘Application.’  Each step is represented by arrows connecting the components, highlighting the flow of data and the use of optimistic updates.](https://cdn.sanity.io/images/3do82whm/next/83a7a8b7908551fdc6a344f5fda5bcf19796d48e-3600x2000.png)

### Reconciling References

Array re-ordering is an ideal use case for `useOptimistic`. However, when composing pages with re-usable blocks, array items may contain references to other documents.

`useOptimistic` actions only provide an up-to-date snapshot of the mutated document, so you need to ensure that any references within the array item itself point to the correct documents in your original query result.

Typically, the optimistic ordering of an updated array can be used, with each item's content set using the data from the passthrough `state` value, if it exists.

```typescript
const sections = useOptimistic(page.sections, (state, action) => {
  if (action.id === page._id) {
    return action.document.sections.map(
      (section) => state?.find((s) => s._key === section?._key) || section
    );
  }
  return state;
});
```

You can find [the useOptimistic reference documentation here](/docs/visual-editing/useoptimistic-reference).

### Minimal example

Below is a minimal example of how to implement drag and drop in React.

```tsx
// /components/Sections.tsx
'use client'
import {createDataAttribute, useOptimistic} from '@sanity/visual-editing'
import type {SanityDocument} from '@sanity/client'

// Minimal type definitions
type PageSection = {
  _key: string
  _type: string
}

type PageData = {
  _id: string
  _type: string
  sections?: PageSection[]
}

type SectionsProps = {
  documentId: string
  documentType: string
  sections?: PageSection[]
}

// Your Sanity configuration
const config = {
  projectId: 'your-project-id',
  dataset: 'production',
  baseUrl: 'https://your-studio-url.sanity.studio',
}

export function Sections({documentId, documentType, sections: initialSections}: SectionsProps) {
  const sections = useOptimistic<PageSection[] | undefined, SanityDocument<PageData>>(
    initialSections,
    (currentSections, action) => {
      if (action.id === documentId && action.document.sections) {
        return action.document.sections
      }
      return currentSections
    },
  )

  if (!sections?.length) {
    return null
  }

  return (
    <div
      data-sanity={createDataAttribute({
        ...config,
        id: documentId,
        type: documentType,
        path: 'sections',
      }).toString()}
    >
      {sections.map((section) => (
        <div
          key={section._key}
          data-sanity={createDataAttribute({
            ...config,
            id: documentId,
            type: documentType,
            path: `sections[_key=="${section._key}"]`,
          }).toString()}
        >
          {/* Render your section content here */}
          {section._type}
        </div>
      ))}
    </div>
  )
}
```

> [!TIP]
> Protip
> On the 'use client' requirement
> 
> The component that holds the array needs to be rendered on the client for useOptimistic to work. While it's generally a good rule of thumb to avoid client-side JavaScript, the footprint of this hook is minimal, and it's only conditionally rendered when Visual Editing is enabled in preview.
> 
> It's important to remember that sometimes you hurt performance if you render too much on the server. If the JSON data you need, and the amount of JS required to render it, is less than the HTML you produce and send down the wire with RSC, then you should make it a client component. 
> 
> With page building scenarios that can very often be the case.
> 

The drag and drop enabled sections can now be imported into a page route component:

```tsx
// /[slug]/page.tsx

import {notFound} from 'next/navigation'
import {sanityFetch} from '@/sanity/fetch'
import {PAGE_QUERY} from '@/sanity/queries'
import {Sections} from '@/components/Sections'

export default async function Page({params}) {
  const {data} = await sanityFetch({query: PAGE_QUERY, params})
  if (!data) {
    notFound()
  }
  
  return (
    <main>
      <Sections
        documentId={data._id}
        documentType={data._type}
        sections={data.sections}
      />
    </main>
  )
}
```

## The user experience of drag and drop

Once an array child has a `data-sanity` attribute, drag and drop will be enabled by default. This will be reflected in the element’s Overlay label:

![UI component featuring a blue rectangular button with rounded corners. Inside, there are a dotted grid icon, a document icon, and the text ‘Element Label’ in white. The background is light blue, giving it a clean and minimalistic design.](https://cdn.sanity.io/images/3do82whm/next/b377dea74ada88cffe6b30cf7993e5dffb76ea87-1200x600.png)

Drag and drop is designed for simple UX and low-touch integration. To achieve this, it makes some assumptions:

57. The web page is using a left-to-right, top-to-bottom format with a logical content flow.
57. Drag groups can be broken into two categories — horizontal and vertical.

Presentation will calculate the direction of a drag group based on the alignment of its children.

A drag group with children that share a y-axis is `horizontal`:

![Graphic showing a simple grid layout with four rectangular blocks labeled ‘1,’ ‘2,’ ‘3,’ and ‘4.’ The blocks are outlined and filled with a light blue background. An arrow below the layout indicates a directional flow or sequence, moving left to right.](https://cdn.sanity.io/images/3do82whm/next/592890036d79adce90ecf1ef3fc8af4c7485e3c0-1200x600.png)

A drag group with children that do not share a y-axis is `vertical`:

![Graphic displaying two horizontal rectangular blocks labeled ‘1’ and ‘2’ in a stacked arrangement. A vertical arrow on the left points downward, indicating a flow from the top block to the bottom block. The background is light blue with a minimalist design.](https://cdn.sanity.io/images/3do82whm/next/7342a0231405c4451a1cc98cf6d342e55c1cc9ae-1200x600.png)

### Minimap

When dragging an item that belongs to a group that is larger than the screen height, press the `shift` key while scrolling or dragging to enter minimap mode. This applies a three-dimensional transform to the page, focusing the group within the viewport. This makes it easier to move sections to slots outside of the immediate viewport:

![Video](https://stream.mux.com/z5d02wk23LIhYv300IhA0201f8vuWwjaz00bf)

## Customizing drag and drop

You can customize the drag and drop behavior in the following ways:

### Data attributes

Drag and drop’s default behavior can be customized using HTML data-attributes:

- `data-sanity-drag-disable`: Disable drag and drop.
- `data-sanity-drag-flow=(horizontal|vertical)`: Override the default drag direction. 
- `data-sanity-drag-group`: Manually assign an element to a drag group. Useful when there are multiple elements representing the same data on a page.
- `data-sanity-drag-prevent-default`: Prevent data from updating after drag sequences. Useful for defining custom insert behavior (see **Custom events** below).
- `data-sanity-drag-minimap-disable`: Disable Minimap for specific element

### Custom events

Drag and drop emits a custom `sanity/dragEnd` event when an element is dropped.

`sanity/dragEnd` events can be used alongside Presentation’s `useDocuments` functionality to override the default drag and drop mutation logic. This is useful for defining custom behavior for non left-to-right/top-to-bottom languages, or other bespoke use cases.

The code below provides a boilerplate for adding custom patching logic to drag and drop events:

```tsx
'use client'

import {at, createIfNotExists, insert, patch, remove} from '@sanity/mutate'
import {get as getFromPath} from '@sanity/util/paths'
import {getArrayItemKeyAndParentPath, useDocuments} from '@sanity/visual-editing'
import {useEffect} from 'react'

function getReferenceNodeAndInsertPosition(position: any) {
  if (position) {
    const {top, right, bottom, left} = position
    if (left || top) {
      return {node: (left ?? top)!.sanity, position: 'after' as const}
    } else if (right || bottom) {
      return {node: (right ?? bottom)!.sanity, position: 'before' as const}
    }
  }
  return undefined
}

export function DnDCustomBehaviour() {
  const {getDocument} = useDocuments()

  useEffect(() => {
    const handler = (e: CustomEvent) => {
      const {insertPosition, target, dragGroup} = e.detail

      if (dragGroup !== 'prevent-default') return

      const reference = getReferenceNodeAndInsertPosition(insertPosition)
      if (reference) {
        const doc = getDocument(target.id)
        // We must have access to the document actor in order to perform the
        // necessary mutations. If this is undefined, something went wrong when
        // resolving the currently in use documents
        const {node, position} = reference
        // Get the key of the element that was dragged
        const {key: targetKey} = getArrayItemKeyAndParentPath(target)
        // Get the key of the reference element, and path to the parent array
        const {path: arrayPath, key: referenceItemKey} = getArrayItemKeyAndParentPath(node)
        // Don't patch if the keys match, as this means the item was only
        // dragged to its existing position, i.e. not moved
        if (arrayPath && referenceItemKey && referenceItemKey !== targetKey) {
          doc.patch(async ({getSnapshot}) => {
            const snapshot = await getSnapshot()
            // Get the current value of the element we dragged, as we will need
            // to clone this into the new position
            const elementValue = getFromPath(snapshot, target.path)
            return [
              // Remove the original dragged item
              at(arrayPath, remove({_key: targetKey})),
              // Insert the cloned dragged item into its new position
              at(arrayPath, insert(elementValue, position, {_key: referenceItemKey})),
            ]
          })
        }
      }
    }

    window.addEventListener('sanity/dragEnd', handler as EventListener)

    return () => {
      window.removeEventListener('sanity/dragEnd', handler as EventListener)
    }
  }, [getDocument])

  return <></>
}
```

> [!WARNING]
> Gotcha
> useDocuments is currently only available as a React hook.



## Troubleshooting

### Preventing Stega children from overriding array paths

Occasionally, a Stega-encoded string can override drag and drop on a parent array item. Here, the `title` string occupies the entire `<button>` element. The `title` automatically has an Overlay created for it, which prevents interaction with the parent Overlay:

```jsx
<button
  data-sanity={dataAttribute({
    id: parentDocument._id,
    type: parentDocument._type,
    path: `arrayItems[_key=="${arrayItem._key}"]`,
  })}
>
  {arrayItem.title}
</button>

```

To prevent this, use `stegaClean` :

```jsx
import {stegaClean} from '@sanity/client/stega'

<button
  ...
>
  {stegaClean(arrayItem.title)}
</button>

```

Or add some visual padding to the array child to create space for the “draggable” area:

```jsx
<button
  ...
  style={{padding: '1rem'}}
>
  {arrayItem.title}
</button>

```



# Overlay and control components

> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

Custom overlay components allow you to extend the functionality of [visual editing overlays](/docs/visual-editing/visual-editing-overlays) with React components. These components enhance the editing experience by enabling direct in-app content editing and displaying metadata or controls to content editors.

With custom overlays, you can:

- Add interactive controls such as color pickers or sliders to configure complex objects (e.g., 3D models).
- Display additional context, such as related product data from external systems.

You can also [customize the Presentation tool's preview header](/docs/visual-editing/customizing-preview-header-and-navigation), giving you the flexibility to toggle custom overlays, or add controls, status indicators, or other UI elements that enhance the editor experience.

[Visual Editing – Introduction](/docs/visual-editing/introduction-to-visual-editing)

[The Presentation tool](/docs/visual-editing/configuring-the-presentation-tool)

[Fetching content for Visual Editing](/docs/visual-editing/fetching-content-for-visual-editing)

[Custom overlay components](/docs/visual-editing/custom-overlay-components)



## Prerequisites

Before getting started, ensure the following:

- [Visual Editing](/docs/visual-editing/introduction-to-visual-editing) enabled with up-to-date dependencies in your front end
- Sanity Studio v3.65.0 or later (`npm install sanity@latest`)

> [!WARNING]
> Gotcha
> Custom overlay component arcurrently only supported for React.

## Custom overlay components

You can mount any React component in an overlay, but the `OverlayComponent` type provides type safety. Below is a simple example that renders the name of the field associated with the overlay:

```tsx
// ./overlay-components.tsx
"use client"
import {type OverlayComponent} from '@sanity/visual-editing'

export const FieldNameOverlay: OverlayComponent = ({field}) => (
  <div className="absolute bottom-0 left-0 m-1 rounded bg-black bg-opacity-50 px-2 py-1 text-xs text-white">
    {field?.name}
  </div>
)
```

> [!WARNING]
> Gotcha
> Custom overlay components and resolvers should be rendered client-side, commonly done with the "use client" directive for React.

### Using custom overlay component resolvers

Resolvers determine which custom components to mount for specific overlays. Use the `defineOverlayComponents` helper to conditionally resolve components based on overlay context.

This function runs each time an overlay renders, and the context object it receives can be used to determine which components to return.

Resolver functions can return:

- JSX elements.
- React component(s), single or array.
- Object(s) with `component` and `props` values. Use the `defineOverlayComponent` for convenience and type safety, single or array.
- `undefined` or `void` when no custom components should be mounted.

Below is an example for how to resolve different custom overlay components conditionally:

```tsx
// ./component-resolver.tsx
"use client"
import {
  defineOverlayComponent,
  defineOverlayComponents,
} from '@sanity/visual-editing/unstable_overlay-components'
import {TitleControl, HighlightOverlay, UnionControl, UnionTypeMarker} from './overlay-components.tsx'

export const components = defineOverlayComponents((context) => {
  const {document, element, field, type, parent} = context

  // Mount a component in overlays attached to string
  // fields named 'title'
  if (type === 'string' && field.name === 'title') {
    return TitleControl
  }

  // Return JSX directly
  if (type === 'string' && field.name === 'subtitle') {
    return <div>Subtitle</div>
  }

  // Mount a component in overlays attached to any element
  // corresponding to a 'product' document
  if (document.name === 'product') {
    const color = element.dataset.highlightColor || 'red'
    return defineOverlayComponent(HighlightOverlay, {color})
  }

  // Mount multiple components in overlays attached to any
  // member element of a union type
  if (parent?.type === 'union') {
    return [
      UnionTypeMarker,
      defineOverlayComponent(UnionControl, { direction: 'vertical' })
    ]
  }

  return undefined
})


```

Depending on your framework and implementation, the resolver function should be passed via the `components` property of the object passed to the `enableVisualEditing` function, or the `components` prop of the `<VisualEditing>` component. For example:

```tsx
// app/(website)/layout.tsx
import { VisualEditing } from "next-sanity";
import { draftMode } from "next/headers";
import { components } from "./component-resolver.tsx";

// minimal Next.js-like example
export default async function RootLayout({ children }) {
  return (
    <html>
      <body>
        <main>{children}</main>
        {isDraftMode && <VisualEditing components={components} />}
      </body>
    </html>
  );
}

```

### Using custom overlay controls

Custom overlay controls enable powerful editing capabilities directly in your application, from basic string manipulation to advanced controls for controlling 3D scenes.

> [!TIP]
> Protip
> Custom overlay controls will automatically use the logged-in user's authentication to update content. This means that any permissions that the user has will still be respected. 

Install the `@sanity/mutate` package in your front-end project to create the necessary patches for updating data. Refer to that package’s [documentation](https://github.com/sanity-io/mutate) for available methods.

```bash
npm install @sanity/mutate

```

The example below illustrates to mount a button in an overlay which appends an exclamation mark on the end of a string value when clicked.

```tsx
// ./overlay-components.tsx
"use client"
import {at, set} from '@sanity/mutate'
import {get} from '@sanity/util/paths'
import {useDocuments, type OverlayComponent} from '@sanity/visual-editing'

export const ExcitingStringControl: OverlayComponent = (props) => {
  const {node, PointerEvents} = props
  // Get the document ID and field path from the Sanity node.
  const {id, path} = node;

  const {getDocument} = useDocuments()
  // Get the optimistic document using the document ID.
  const doc = getDocument(id)

  const onClick = () => {
    doc.patch(async ({getSnapshot}) => {
      const snapshot = await getSnapshot()
	    // Get the current value using the document snapshot and the field path.
      const currentValue = get<string>(snapshot, path)
      // Return early if the string is already exciting.
      if (currentValue?.endsWith('!')) return []
      // Append "!" to the string.
      const newValue = `${currentValue}!`
      // Use `@sanity/mutate` functions to create the document patches.
      return [at(node.path, set(newValue))]
    })
  }

  return (
    // By default, overlays don't receive pointer events.
    // Use the `PointerEvent` wrapper component to allow interaction.
    <PointerEvents>
      <button
        // Tailwind CSS classes
        className="absolute right-0 rounded bg-blue-500 px-2 py-1 text-sm text-white"
        onClick={onClick}
      >
        🎉
      </button>
    </PointerEvents>
  )
}

```

## Access custom preview header state in custom overlay components

The visual editing package exports an named `useSharedState` hook which given the unique key defined in [a custom preview header](/docs/visual-editing/customizing-preview-header-and-navigation) (for example, `highlighting`), will return the value shared by the corresponding `useSharedState` Presentation tool hook.

Below, we have added the custom highlighting overlay component to the custom overlays file that is rendering a semi-transparent overlay when highlighting is enabled, and nothing when disabled:

```tsx
// ./overlay-components.tsx
"use client"
import {useSharedState, type OverlayComponent} from '@sanity/visual-editing'

export const FieldNameOverlay: OverlayComponent = (props) => {
  const {field} = props

  return (
	  // Tailwind CSS classes
    <div className="absolute bottom-0 left-0 m-1 rounded bg-black bg-opacity-50 px-2 py-1 text-xs text-white">
      {field?.name}
    </div>
  )
}

export const HighlightOverlay: OverlayComponent = () => {
  const highlight = useSharedState<boolean>('highlighting')

  if (!highlight) {
    return null
  }

  return (
    <div
      style={{
        position: 'absolute',
        inset: 0,
        backgroundColor: 'rgba(0, 0, 255, 0.25)',
      }}
    />
  )
}


```

## Hooks reference

### useDocument

| Method | Description |
|--------|-------------|
| useDocuments(): { getDocument, mutateDocument } | The useDocuments hook can be used in overlay components to access and update the documents currently in use on a page. |
| getDocument(documentId): { id, get, patch, commit } | Returns an optimistic document interface with the following methods:

id: string - The document ID.

get: (path?: string): SanityDocument | PathValue - Returns the document snapshot or the specific value at the given path.

patch: (patches: OptimisticDocumentPatches, options?: {commit?: boolean | {debounce: number}}) => void - Applies patches to the document, will commit patches by default.

commit: () => void - Commits pending changes. |
| mutateDocument(documentID, mutations, options): void |  |


### useSharedState

| Method | Description |
|--------|-------------|
| useSharedState(key, value): Your serializeable state | The useSharedState enables you to share state between the Presentation tool and your custom overlay components in your front end’s preview. |


## Resources

- [Visual Editing](/docs/visual-editing/introduction-to-visual-editing)
- [Overlays](/docs/visual-editing/visual-editing-overlays)
- [@sanity/mutate](https://github.com/sanity-io/mutate)
- [Sanity UI](https://www.sanity.io/ui)



# Preview header and navigation

> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

The Presentation tool also allows you to customize the preview header, giving you the flexibility to add controls, status indicators, or other UI elements that enhance the editor experience. 

This is particularly useful for enabling users to:

- [Interact with specific overlays](/docs/visual-editing/custom-overlay-components)
- Toggle features
- Access contextual tools directly from the preview interface.

## Prerequisites

Before getting started, ensure the following:

- [Visual Editing](/docs/visual-editing/introduction-to-visual-editing) enabled with up-to-date dependencies in your front end
- Sanity Studio v3.65.0 or later (`npm install sanity@latest`)

## Define a custom preview header component

First, create a custom header component to mount in the Presentation tool. In this case, you are creating a dropdown with a single toggle for enabling and disabling highlighting.

![Interface view of a web application featuring a toggle switch labeled ‘Edit,’ a URL input field displaying ‘http://localhost:3005/,’ and a dropdown menu option to ‘Enable Highlighting.’](https://cdn.sanity.io/images/3do82whm/next/db8d45afe1574cbfe53567cf40d85132334861bc-1706x350.png)

You can use the `renderDefault` prop to keep existing functionality whilst appending a custom control. You use `@sanity/ui` to render the new elements.

> [!TIP]
> Protip
> Remember to install Sanity UI as a dependency, if you haven't already:
> 
> npm install @sanity/ui

## Share state between overlays and the preview header

If you have multiple [custom overlays](/docs/visual-editing/custom-overlay-components), you may need to allow content editors the ability to toggle specific overlays on or off, or provide more fine grained control over which overlay UI elements to render.

To do this, you need to share state between your Presentation tool in the Studio and your front end in preview mode that is rendered in the tool's iframe.

Both the Presentation tool and `@sanity/visual-editing` package provide `useSharedState` hooks. These hooks allow you to share state defined in the Presentation tool with custom overlay components in your front end. Both accepts two parameters: a unique string identifier as the `key`, and the state `value` itself.

> [!WARNING]
> Gotcha
> Only JSON serializable state can be passed to useSharedState, this is data types like string, number, boolean, null, arrays, or plain objects.

Below is an example of a custom preview header that renders the out-of-box header UI (`props.renderDefault(props)`), with an additional new menu with a toggle for highlight components using Sanity UI:

```tsx
// ./CustomPreviewHeader.tsx

import {CheckmarkIcon, CloseIcon, EllipsisVerticalIcon} from '@sanity/icons'
import {useSharedState, type PreviewHeaderProps} from '@sanity/presentation'
import {Button, Menu, MenuButton, MenuItem} from '@sanity/ui'
import {useState, type FunctionComponent} from 'react'

export const CustomPreviewHeader: FunctionComponent<PreviewHeaderProps> = (props) => {
  const [enabled, setEnabled] = useState(false)
  useSharedState('highlighting', enabled)

  // Render the default header component, and append a new control
  return (
    <>
      {props.renderDefault(props)}
      <MenuButton
        button={
          <Button fontSize={1} icon={EllipsisVerticalIcon} mode="bleed" padding={2} space={2} />
        }
        id="custom-menu"
        menu={
          <Menu style={{maxWidth: 240}}>
            <MenuItem
              fontSize={1}
              icon={enabled ? CloseIcon : CheckmarkIcon}
              onClick={() => setEnabled((enabled) => !enabled)}
              padding={3}
              tone={enabled ? 'caution' : 'positive'}
              text={enabled ? 'Disable Highlighting' : 'Enable Highlighting'}
            />
          </Menu>
        }
        popover={{
          animate: true,
          constrainSize: true,
          placement: 'bottom',
          portal: true,
        }}
      />
    </>
  )
}

```

Now, you can access this state in a custom overlay component. Find instructions for how to do this in [the custom overlay component documentation](/docs/visual-editing/custom-overlay-components).

## Mount a custom preview header component

> [!NOTE]
> Unstable feature
> unstable_navigator has the unstable prefix because the API is likely to change. Don’t use it in a production environment unless you are ready to change it when the API stabilizes.

Pass custom preview header component via `components.unstable_header` in the Presentation tool configuration object, as below:

```tsx
// sanity.config.ts
import {defineConfig} from "sanity"
import {presentationTool} from "sanity/presentation"
import {CustomHeader} from "./CustomHeader"

export default defineConfig({
  // ...
  plugins: [
    presentationTool({
      // ...
      components: {
        unstable_header: {
          component: CustomHeader,
        },
      },
    }),
  ],
});


```

## Mount a custom navigator component

Optionally, you can enhance the Presentation tool with a custom document navigator component to help users select different documents or views in the front-end UI.

Example:

```typescript
// Import your custom navigator component
import {NavigatorComponent} from './presentation/NavigatorComponent'

export default defineConfig({
  // Your configuration for the project
  // ...

  plugins: [
    presentationTool({
      // ...
      component: {
  			// Pass the custom component to the plugin
        unstable_navigator: NavigatorComponent
      },
    })
  ],
})
```

### Navigator properties

#### Properties

| Property | Description |
|----------|-------------|
| component * | Specifies the navigator component to use with the Presentation tool. The component specified will be rendered as the content of the navigator panel. |
| minWidth | Sets the minimum width of the navigator component when it’s rendered in the UI. For the component to render, its value must be > 0 and other than null. |
| maxWidth | Sets the maximum width of the navigator component when it’s rendered in the UI. For the component to render, its value must be > 0 and other than null. |


## Hooks reference

### useSharedState

| Method | Description |
|--------|-------------|
| useSharedState(key, value): Your serializeable state | The useSharedState enables you to share state between the Presentation tool and your custom overlay components in your front end’s preview. |


## Resources

- [Visual Editing](/docs/visual-editing/introduction-to-visual-editing)
- [Overlays](/docs/visual-editing/visual-editing-overlays)
- [@sanity/mutate](https://github.com/sanity-io/mutate)
- [Sanity UI](https://www.sanity.io/ui)



# Resolver API

The Presentation tool lets you add affordances to open a document and its preview quickly. This is done by adding configuration to `resolve.mainDocuments` and `resolve.locations` where you define what content goes into a specific route.

Main Document Resolvers lets you express the most important document for any given route. Moreover, the Locations Resolvers lets you define patterns for other routes that content from any given document might be used.

[Configuring the Presentation tool](/docs/visual-editing/configuring-the-presentation-tool)



## Main Document Resolvers

The Main Document Resolver API provides a method of resolving a main document from a given route or route pattern.

Often, a route in an application will be closely tied to a document in Content Lake. For instance, a blog post will likely draw most of its content from a single post document. We could describe this as the "main" document for that post route.

```tsx
// sanity.config.ts
import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'
import {defineDocuments, presentationTool} from 'sanity/presentation'

export default defineConfig({
  /* ... */
  plugins: [
    presentationTool({
      /* ... */
      resolve: {
        mainDocuments: defineDocuments([
          {
            route: '/posts/:slug',
            filter: `_type == "post" && slug.current == $slug`,
          },
        ]),
      },
    }),
    structureTool(),
  ],
})

```

When a main document is defined, Presentation will automatically display it when navigating to the matching route in our application.

The `resolve.mainDocuments` property of Presentation tool’s configuration accepts an array of objects, each with a route pattern and method for resolving a document. This can be a document `type`, an object with a GROQ `filter` and optional `params`, or a `resolve` function that returns one.

Internally, when Presentation detects a navigation event, it will compare the current URL against one of these resolver methods. The first matching document will be considered the main document, and be automatically displayed as the navigation occurs.

```tsx
// sanity.config.ts
import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'
import {defineDocuments, presentationTool} from 'sanity/presentation'

export default defineConfig({
  /* ... */
  plugins: [
    presentationTool({
      /* ... */
      resolve: {
        mainDocuments: defineDocuments([
          // Document type, useful shorthand for singleton documents.
          {
            route: '/products',
            type: 'productsListing',
          },
          // GROQ filter with static parameters.
          {
            route: '/products/a-unique-product',
            filter: `_type == "product" && slug.current == $slug`,
            params: {slug: 'has-a-different-slug'},
          },
          // GROQ filter, infer the parameters from the route definition.
          {
            route: '/products/:slug',
            filter: `_type == "product" && slug.current == $slug`,
          },
          // Resolve function for more complex cases, for example modifying the slug.
          {
            route: '/pages/:type/:slug',
            resolve(ctx) {
              const {params} = ctx
              return {
                filter: `_type == $type && slug.current == $slug}`,
                params: {
                  type: params.type,
                  slug: params.slug.replaceAll('_', '-'),
                },
              }
            },
          },
          // Supports both array and absolute URL route definitions
          {
            route: ['https://sanity.io/:slug', 'https://sanity.studio/:slug'],
            filter: `_type == "page" && slug.current == $slug`,
          },
        ]),
      },
    }),
    structureTool(),
  ],
})

```

> [!TIP]
> Protip
> defineDocuments() is an optional helper function for TypeScript users. You can provide an array directly if you don't need type safety. 

The paths of each `route` value are parsed using [path-to-regexp](https://github.com/pillarjs/path-to-regexp) to extract the parameters.

If an origin is provided, it will **not** be parsed for parameters, i.e. this will be matched literally: `https://:subdomain.sanity.io`

## Document Locations Resolvers

The Document Locations Resolver API allows you to define *where* data is being used in your application(s).

It gives content editors visibility of the pages that display the content they are editing, and the potential impact of the changes they make.

The `resolve.locations` property of Presentation tool’s configuration accepts an object whose keys each correspond to a document type in your schema. The corresponding value provides a method for resolving document location state.

If you're used to [configuring preview options](https://www.sanity.io/docs/previews-list-views#770fd57a8f95) in your schema, the API will feel familiar.

The `select` object defines the fields that should be returned in the document object passed to the `resolve` function. The `resolve` function itself accepts a document and should return some `DocumentLocationsState`.

Alternatively, you can directly pass `DocumentLocationsState`.

> [!TIP]
> Protip
> defineLocations() is an optional helper function for TypeScript users. You can provide an array directly if you don't need type safety. 

```tsx
import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'
import {defineLocations, presentationTool} from 'sanity/presentation'

export default defineConfig({
  /* ... */
  plugins: [
    presentationTool({
      /* ... */
      resolve: {
        locations: {
          // Resolve locations using values from the matched document
          product: defineLocations({
            select: {
              title: 'title',
              slug: 'slug.current',
            },
            resolve: (doc) => ({
              locations: [
                {
                  title: doc?.title || 'Untitled',
                  href: `/products/${doc?.slug}`,
                },
                {
                  title: 'Products',
                  href: '/products',
                },
              ],
            }),
          }),
          // Define static locations
          productsListing: defineLocations([
            {
              title: 'Products',
              href: '/products',
            },
          ]),
          // Provide a notice when a document is used across all pages
          siteSettings: defineLocations({
            message: 'This document is used on all pages',
            tone: 'caution',
          }),
        },
      },
    }),
    structureTool(),
  ],
})

```

### Advanced Location Resolvers

The above pattern should cover the vast majority of uses cases. However if you need more fine grained control, `resolve.locations` also accepts a `DocumentLocationResolver` function.

In most cases, you will call `context.documentStore.listenQuery` with a GROQ query that returns the queried documents as an Observable.

We recommend installing `rxjs` as a dependency in your Studio project to make it easier to interact with the Observable object returned from `listenQuery`:

```bash
npm install rxjs

```

```tsx
// locations.ts
import {DocumentLocationResolver} from 'sanity/presentation'
import {getDraftId} from 'sanity'
import {map} from 'rxjs'

// Pass 'context' as the second argument
export const locations: DocumentLocationResolver = (params, context) => {
  // Set up locations for post documents
  if (params.type === 'post') {
    const query = {
      fetch: `*[_id==$id][0]{slug,title}`,
      listen: `*[_id in [$id,$draftId]]`,
    }
    const params = {id, draftId: getDraftId(id)}
    // Subscribe to the latest slug and title
    const doc$ = context.documentStore.listenQuery(
      query,
      params,
      {perspective: 'previewDrafts'}, // returns a draft article if it exists
    )
    // Return a streaming list of locations
    return doc$.pipe(
      map((doc) => {
        // If the document doesn't exist or have a slug, return null
        if (!doc || !doc.slug?.current) {
          return null
        }
        return {
          locations: [
            {
              title: doc.title || 'Untitled',
              href: `/post/${doc.slug.current}`,
            },
            {
              title: 'Posts',
              href: '/',
            },
          ],
        }
      }),
    )
  }
  return null
}


```

![A screenshot from the Studio's content form showing a list of locations where a product document is being used.](https://cdn.sanity.io/images/3do82whm/next/d0b2ee5c27abfcd1f3837386110c2439df2c15e7-560x406.png)

The locate callback property is a resolver that takes a document `id` and `type` parameters, and returns a state object that contains the locations that can be previewed for a given document.

We recommend containing the Document Location Resolver in a dedicated file to reduce clutter in your Studio configuration file. In the example above, it's exported as a named JavaScript variable so it can be imported to the studio configuration file like this:

```typescript
// sanity.config.ts
import {defineConfig} from 'sanity'
import {presentationTool} from 'sanity/presentation'
import {locations} from './locations'
// ...other imports


export default defineConfig({
  // ...configuration for the project

  plugins: [
    presentationTool({
      previewUrl: SANITY_STUDIO_PREVIEW_URL,
      resolvers: {
        locations
      }
    }),
    // ...other plugins
  ],
})
```

### Show all locations where a document is being used

Typically, with structured content, content from a document might be used in multiple locations by [means of references](/docs/studio/connected-content). With GROQ, you can query for all documents that refer to a specific document ([or use other join logic](/docs/specifications/groq-joins)) and use that to build a list of locations for where a document is being used. 

Below is an example showing how to build a list of locations for a "person" document that is being referred to by a "post" document as its author:

```typescript
// locations.ts
import {
  DocumentLocationResolver,
  DocumentLocationsState,
} from 'sanity/presentation'
import { map, Observable } from 'rxjs'

export const locations: DocumentLocationResolver = (params, context) => {
  if (params.type === 'post' || params.type === 'person') {
    /* 
      Listen to all changes in the selected document 
      and all documents that reference it
    */
    const doc$ = context.documentStore.listenQuery(
      `*[_id==$id || references($id)]{_type,slug,title, name}`,
      params,
      { perspective: 'previewDrafts' },
    ) as Observable<
      | {
          _type: string
          slug?: { current: string }
          title?: string | null
          name?: string | null
        }[]
      | null
    >
    // pipe the real-time results to RXJS's map function
    return doc$.pipe(
      map((docs) => {
        if (!docs) {
          return {
            message: 'Unable to map document type to locations',
            tone: 'critical',
          } satisfies DocumentLocationsState
        }
        // Generate all the locations for person documents
        const personLocations = docs
          .filter(({ _type, slug }) => _type === 'person' && slug?.current)
          .map(({ name, slug }) => ({
            title: name || 'Name missing',
            href: `/authors/${slug.current}`,
          }))

        // Generate all the locations for post documents
        const postLocations: Array<any> = docs
          .filter(({ _type, slug }) => _type === 'post' && slug?.current)
          .map(({ title, slug }) => ({
            title: title || 'Name missing',
            href: `/posts/${slug.current}`,
          }))

        return {
          locations: [
            ...personLocations,
            ...postLocations,
            // Add a link to the "All posts" page when there are post documents
            postLocations.length > 0 && {
              title: 'All posts',
              href: '/posts',
            },
            // Add a link to the "All authors" page when there are person documents
            personLocations.length > 0 && {
              title: 'All authors',
              href: '/authors',
            },
          ].filter(Boolean),
        } satisfies DocumentLocationsState
      }),
    )
  }

  return null
}
```

## Document Location Resolver Reference

In addition to the example above, the Document Location Resolver can return customized top-level messages and visual cues:

#### Properties

| Property | Description |
|----------|-------------|
| message | Override the top-level text in the document locations UI. Useful if you want to customize the string (replace "pages") or display warnings.

Default value: Used on ${number} pages

Examples:

Unable to resolve locations for this document

Used in ${docs.length} email campaign${docs.length === 1 ?? 's'} |
| tone | Gives the document locations UI a background color. It can be used to signal criticality.

Default: positive |
| locations | An array of document locations objects with title and href properties. The href can be absolute or relative and will open in the Presentation tool. |




# useOptimistic hook

The `useOptimistic` hook uses a local document store to enable developers to opt-in to instant updates for specific content. UI can be updated with the anticipated result of a mutation, avoiding the delay required when submitting and refetching data from Content Lake.

It's primarily used for enabling drag and drop functionality for Visual Editing. You can learn more about how to use it in [the Drag and drop documentation](/docs/visual-editing/enabling-drag-and-drop).

## `useOptimistic`

| Method | Description |
|--------|-------------|
| useOptimistic(passthrough, reducer): void | Returns

When no mutations are pending, the hook returns the original passthrough value.

When mutations are pending, it returns the optimistically updated state as determined by the reducers.

In production, the useOptimistic is a no-op and will always return the passthrough value. |
| Reducers(state, action): void | A reducer function or array of reducers that determine how the state should be updated in response to optimistic mutations. |


Reducer actions

Here is the signature for the `action` parameter for reducers:

#### Properties

| Property | Description |
|----------|-------------|
| document | The document that was updated |
| id | The published document ID (the same as the _id) |
| originalId | The original document ID |
| type | The type of action occurring (only mutate is currently supported) |






# Troubleshooting



## Troubleshooting Overlays

### **Styling of editable fields is incorrect**

If the text on the page is breaking out of its container – or its container is much wider than normal – it can be resolved by splitting the encoded text out from the original text.

> Note: This is not due to the encoded characters themselves. This problem should only present itself if the element also uses negative letter-spacing in its CSS or is inside of a `<ReactWrapBalancer>`.

Then identify where the problematic element is rendered in code, for example:

```tsx
export function MyComponent({ text }: { text: string }) {
  return <h1>{text}</h1>;
}

```

Rewrite using `@vercel/stega` to avoid any styling issues:

```tsx
import { vercelStegaSplit } from "@vercel/stega";

export function MyComponent({ text }: { text: string }) {
  const { cleaned, encoded } = vercelStegaSplit(text);

  return (
    <h1>
      {cleaned}
      <span style={{ display: "none" }}>{encoded}</span>
    </h1>);
}

```

If you find yourself doing this more than once, you might like to extract this logic to a reusable component:

```tsx
import { vercelStegaSplit } from "@vercel/stega";

export default function Clean({ value }: { value: string }) {
  const { cleaned, encoded } = vercelStegaSplit(value);

  return encoded ? (
    <>
      {cleaned}
      <span style={{ display: "none" }}>{encoded}</span>
    </>) : (
    cleaned
  );
}

export function MyComponent({ text }: { text: string }) {
  return (
    <h1>
      <Clean value={text} />
    </h1>);
}

```

### Overlay displays over the wrong element

If the wrong element is highlighted when hovering, an additional attribute can be added to a containing element.

For example, if the following component highlights the `<h1>` and you want it to highlight the `<section>` element:

```html
<section>
	<h1>{dynamicTitle}</h1>
	<div>Hardcoded Tagline</div>
</section>

```

Add a data attribute to highlight the correct item:

- For Visual Editing with `@sanity/visual-editing`, add `data-sanity-edit-target`
- For Vercel Visual Editing, add `data-vercel-edit-target`

```html
<section data-sanity-edit-target>
	<h1>{dynamicTitle}</h1>
	<div>Hardcoded Tagline</div>
</section>

```

****

## **Troubleshooting stega-encoding**

### There are weird characters in your DOM

These are most probably the Stega encoded strings! They are a subset of [HTML entities](https://developer.mozilla.org/en-US/docs/Glossary/Entity) that, when rendered, produce invisible output. Below is the string value of “Oxford Shoes” when it contains a Stega-encoded Content Source Map:

```json
Oxford Shoes&ZeroWidthSpace;&ZeroWidthSpace;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwj;&#xFEFF;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&zwnj;&#xFEFF;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&#xFEFF;&zwj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwj;&zwnj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&#xFEFF;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwj;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&#xFEFF;&zwj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwj;&zwj;&ZeroWidthSpace;&zwj;&#xFEFF;&#xFEFF;&ZeroWidthSpace;&zwj;&#xFEFF;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&zwj;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwnj;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&#xFEFF;&ZeroWidthSpace;&zwj;&#xFEFF;&zwj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwj;&zwnj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&#xFEFF;&zwnj;&zwnj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&#xFEFF;&#xFEFF;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwj;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&#xFEFF;&zwj;&ZeroWidthSpace;&#xFEFF;&zwj;&#xFEFF;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&#xFEFF;&zwj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&zwj;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&zwj;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwj;&ZeroWidthSpace;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwj;&#xFEFF;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwj;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&#xFEFF;&zwj;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwnj;&zwnj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&zwj;&zwj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&ZeroWidthSpace;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&#xFEFF;&zwnj;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&zwj;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwnj;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&#xFEFF;&ZeroWidthSpace;&zwj;&#xFEFF;&zwj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwj;&zwnj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&#xFEFF;&zwnj;&zwnj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&#xFEFF;&zwj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&zwnj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&zwj;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&zwj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwj;&ZeroWidthSpace;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&#xFEFF;&zwnj;&#xFEFF;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwj;&#xFEFF;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwnj;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwnj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&zwj;&ZeroWidthSpace;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&zwj;&zwnj;&zwnj;&zwnj;&zwj;&#xFEFF;&zwj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&zwj;&#xFEFF;&#xFEFF;&zwnj;&zwj;&#xFEFF;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&zwnj;&zwj;&zwj;&zwnj;&zwnj;&#xFEFF;&ZeroWidthSpace;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwnj;&ZeroWidthSpace;&zwnj;&#xFEFF;&ZeroWidthSpace;&zwj;&zwnj;&zwj;&ZeroWidthSpace;&zwnj;&zwnj;&zwj;&zwnj;&zwj;&zwnj;&#xFEFF;&zwnj;&ZeroWidthSpace;&ZeroWidthSpace;&#xFEFF;&#xFEFF;&zwnj;&ZeroWidthSpace;&zwj;&ZeroWidthSpace;&zwj;&zwnj;&#xFEFF;&#xFEFF;&zwnj;
```

When rendered in an HTML document, this string will still display as “Oxford Shoes.” You use [this tool](https://mothereff.in/html-entities) to test the string by pasting the value above into the “Encoded” input field.

### Comparing field values doesn’t work in preview mode

Your application likely evaluates values from the Content Lake to perform specific logic. If these values contain invisible encoded metadata, they may no longer work.

For example, imagine a function that determines that a Sanity document's market value is the same as the current market:

```tsx
function showDocument(document: SanityDocument, currentMarket: string) {
  return document.market === currentMarket
}

```

Without stega enabled, this function works as expected. However, if `document.market` contains encoded metadata, this comparison will fail.

If `document.market` is never shown on the page and will not benefit from Visual Editing, removing it from the encoded paths in `encodeSourceMapAtPath` may be best.

Alternatively, clean the value before comparing it:

```tsx
import {stegaClean} from "@sanity/client/stega"

function showDocument(document: SanityDocument, currentMarket: string) {
  return stegaClean(document.market) === currentMarket
}

```

Since you'll likely do this more than once, consider extracting to a helper function.



# Visual Editing on sites hosted on Vercel

![Heading element with the text: "Having fun while travelling". The heading has an overlay marking it as editable with a label reading "Open in Sanity Studio"](https://cdn.sanity.io/images/3do82whm/next/5c546d0a0c282340b54a8ca9bab3a8aac7747251-1656x524.png)

The feature works using a new technology invented by Sanity called [Content Source Maps](/docs/visual-editing/content-source-maps), which is also available as an API for you to use to build your own implementation. 

Getting started with Visual Editing requires minimal changes to your website’s front-end code, only some configuration in an enhanced Sanity Client.

Visual Editing can be used on any hosting with the installation and configuration of the [@sanity/overlays](https://github.com/sanity-io/overlays) package. 

[Vercel Visual Editing](https://vercel.com/docs/workflow-collaboration/visual-editing/cms-guide) offers a similar experience without the additional package – as Vercel automatically adds the clickable buttons in preview builds – and benefits from the Vercel toolbar's other features like comments and preview.

## How does Visual Editing work?

This video walkthrough demonstrates Vercel Visual Editing. Read more [on their documentation](https://vercel.com/docs/workflow-collaboration/visual-editing).

![Visual Editing with Sanity & Vercel](https://www.youtube.com/watch?v=ZslEtOdsL1Q)

Visual Editing overlays contain a clickable **Edit in Sanity Studio **button onto each editable element on your website. Clicking the button opens your Studio in a new tab with the editor pane focused on the field corresponding to the front-end element.

![Shows a web page titled "Visual Editing Demo", with a graphic with the legend "Content editing at your fingertips"](https://cdn.sanity.io/images/3do82whm/next/afa03d41ea32daa7cfb19cbbccebb031bf3b250a-2822x2334.png)

Content Source Maps embeds metadata into your query results, containing information about the document and field in your Content Lake that the data originated from. 

Visual Editing uses this metadata to link each element in your front end to its corresponding document and field in your Sanity Studio, allowing for easy access and editing, particularly for teammates who are less familiar with your content model.

![Shows the graphic from the web page in the former image being edited in Sanity Studio ](https://cdn.sanity.io/images/3do82whm/next/7765583186e19afdf5ae59e26918fc29d6e32e08-2822x2334.png)

## How to enable Visual Editing

To enable Visual Editing on Vercel or any other hosting, follow these steps:

### **1. Install the enhanced Sanity client** 

To include metadata for Visual Editing in your query results, you need the enhanced Sanity client included with either [next-sanity](https://github.com/sanity-io/next-sanity) (for Next.js projects) or [@sanity/preview-kit](https://github.com/sanity-io/preview-kit) (framework-agnostic). **Using the vanilla JavaScript client @sanity/client will not work! **

If you are already importing `createClient` from either `next-sanity` or `@sanity/preview-kit/client`, skip to step 4. Otherwise, start by installing the appropriate npm package. 

```sh
# for Next.js applications
npm i next-sanity

# framework-agnostic version
npm i @sanity/preview-kit
```

### 2. Change your import statements

The client included in `next-sanity` and `@sanity/preview-kit` is a drop-in replacement for the vanilla JavaScript client with some extra features. Therefore, after installing either of these clients, everything should still work the same when you change your import statements.

```typescript
// Depending on which package you installed, replace this:
import {createClient} from '@sanity/client'

// ... with one of these:
import {createClient} from 'next-sanity'
import {createClient} from '@sanity/preview-kit/client'
```

### 3. Configure the client

Finally, add the following lines to your client configuration:

```typescript
const client = createClient({
  projectId: '<projectId>',
  dataset: 'production',
  apiVersion: '2022-05-03',
  useCdn: true,
  studioUrl: '/studio', // Or: 'https://my-cool-project.sanity.studio'
	encodeSourceMap: true, // Optional. Default to: process.env.VERCEL_ENV === 'preview', 
})
```

The `studioUrl` is necessary to allow Visual Editing to construct complete URLs to content in your Sanity Studio. It can be a relative path for embedded studios – e.g., `“/studio”` – or a fully qualified URL for studios hosted elsewhere. 

The `encodeSourceMap` property lets you conditionally enable or disable the generation of content source maps that are needed for Visual Editing. It is recommended that you only enable it for non-production builds.

With these steps complete, your front end should now include the metadata required for Visual Editing.

#### Using Vercel Visual Editing? You're done!

If you have access to Vercel Visual Editing, deploy a preview build of your front-end and look for the Edit icon on the Vercel toolbar. You should see clickable elements highlighted to open and edit content in your Sanity Studio.

For any hosting other than Vercel, continue to step 4

> [!WARNING]
> Gotcha
> The first time you view a preview deployment that has Visual Editing enabled, you will be prompted to add the preview URL to the list of allowed CORS origins for your project. This is necessary to enable the feature. Read more about CORS here.

### 4. Install @sanity/overlays

If you do not have access to Vercel Visual Editing, a similar experience is available with the [@sanity/overlays](https://github.com/sanity-io/overlays) package. 

```sh
npm i @sanity/overlays
```

This package is vanilla JavaScript and can be configured in any framework. Most commonly, you might import and run the enable function in a React application like this:

```tsx
import { enableVisualEditing } from '@sanity/overlays'

export default function App() {
  useEffect(enableVisualEditing, [])

  // ... return <html>
}
```

Ensure the function only runs once and at the root level. You might consider applying additional logic so that it is also only bundled and executed in non-production environments.

You should now – even in local development – be able to see highlighted elements that can be clicked to open their document and field in Sanity Studio.

![An apartment listing component with the title outlined and an "Edit in Sanity Studio" button](https://cdn.sanity.io/images/3do82whm/next/d905e8ad9b57d8b3ba56f181617e8d73aad6fdaf-712x602.png)

## Additional client configuration options for handling Content Source Maps

### Log encoded paths to the console

The enhanced Sanity client accepts an optional `logger` parameter. Pass it the global `console`, to show debug information about which fields are being encoded and which (if any) are skipped.

```typescript
const client = createClient({
  projectId: '<projectId>',
  dataset: 'production',
  apiVersion: '2022-05-03',
  useCdn: true,
  studioUrl: '/studio',
  logger: console,
})
```

This should provide a nicely formatted report in your console, whether you are looking at your browser’s dev tools or your hosting's logs.

```
[@sanity/preview-kit]: Creating source map enabled client
[@sanity/preview-kit]: Stega encoding source map into result
  [@sanity/preview-kit]: Paths encoded: 3, skipped: 17
  [@sanity/preview-kit]: Table of encoded paths
  ┌─────────┬──────────────────────────────────┬───────────────────────────┬────────┐
  │ (index) │                     path         │           value           │ length │
  ├─────────┼──────────────────────────────────┼───────────────────────────┼────────┤
  │    0    │ ["footer",0,"children",0,"text"] │ '"The future is alrea...' │   67   │
  │    1    │ ["footer",1,"children",0,"text"] │     'Robin Williams'      │   14   │
  │    2    │             ["title"]            │     'Visual Editing'      │   14   │
  └─────────┴──────────────────────────────────┴───────────────────────────┴────────┘
  [@sanity/preview-kit]: List of skipped paths [
    [ 'footer', number, '_key' ],
    [ 'footer', number, 'children', number, '_key' ],
    [ 'footer', number, 'children', number, '_type' ],
    [ 'footer', number, '_type' ],
    [ 'footer', number, 'style' ],
    [ '_type' ],
    [ 'slug', 'current' ],
  ]
```

### Customizing which paths to encode

The `encodeSourceMapAtPath` callback can be used to make a custom selection of which paths to encode and which to skip.

You may wish to skip encoding on a path if its value is used in your front end, but it is not useful for an author to be able to edit it.

```typescript
const client = createClient({
  // ...rest of config omitted for brevity
  encodeSourceMapAtPath: props => {
	  if(props.path[0] === 'externalUrl') {
      // absolute urls in <a href> can't contain source maps, or they'll render as broken relative urls
      return false
    }
    return props.filterDefault(props)
  }
})
```

By default, all returned values will include metadata encoding unless they: 

- have keys starting with an underscore (e.g., `_id` and `_type`) 
- can be evaluated as a URL or ISO Date
- are not returned as a string (such as numbers, see [Encoding metadata on number fields](#a1d74fab43ae) below)
- contain the path `["slug", "current"]` (as the metadata encoding will break URLs and static build processes).

If you want keys like these to be included, you can skip invoking `props.filterDefault(props)` in your return statement.

```typescript
const client = createClient({
  // ...rest of config omitted for brevity
  
  // returning `true` in every case to include all keys
  encodeSourceMapAtPath: () => true
})
```

### Include the Content Source Map in your query result

To have the underlying Content Source Map returned as part of your query result, set the `filterResponse` option of the fetch call to `false`.

You may choose to include the Source Map for your own custom logic.

```typescript
// The snippet below works with the standard client
import {createClient} from '@sanity/client'
// As well as the preview kit one, regardless of whether encoding to stega is on or off
import {createClient} from '@sanity/preview-kit/client'
// And next-sanity
import {createClient} from 'next-sanity'

const client = createClient({
  // ...rest of config omitted for brevity
  apiVersion: '2022-05-03',
  resultSourceMap: true // Tells Content Lake to include content source maps in the response
})

// const result = await client.fetch(query, params)
const {result, resultSourceMap} = await client.fetch(
  query,
  params,
  {filterResponse: false} // This option returns the entire API response instead of selecting just `result`
)

doSomethingAwesome(resultSourceMap)
```

[Learn more about working with Content Source Maps](https://github.com/sanity-io/preview-kit#using-the-content-source-map-for-custom-logic), including how to develop your own implementation of Visual Editing.

## GraphQL

With the [Sanity GraphQL API v2023-08-01](https://www.sanity.io/changelog?platforms=GraphQL#change-9ec89318-a340-4e23-91d9-3154da5b6244) update, Visual Editing is now also available for GraphQL. Read about it in the [GraphQL docs](/docs/content-lake/graphql).

## Solutions

### Creating editable images

Images can become clickable links if the `alt` attribute contains a value with encoded metadata.

The [image schema type](/docs/image-type) can contain additional fields, for example, an `altText` string field:

```typescript
defineField({
  name: 'picture',
  type: 'image',
  fields: [defineField({name: 'altText', type: 'string'})],
})
```

Then in your front end, ensure this field value is used in the `alt` attribute. If it contains metadata encoding, Visual Editing will make the image a clickable element.

```jsx
<img src={urlFor(image)} alt={image?.altText} />
```

![A card listing for a property with a border around the image with a button "Edit in Sanity Studio"](https://cdn.sanity.io/images/3do82whm/next/2ec77938c01085d7e00a24c4baa24b18afc4e964-712x648.png)

See the documentation for more options when [presenting images](/docs/apis-and-sdks/presenting-images).

### Encoding metadata on number fields

Only values returned as strings can be encoded with metadata. Number values will not contain encoding by default.

To make number values editable, cast them to strings using the [string() GROQ function](/docs/specifications/groq-functions). This should add encoding to these values.

```groq
*[_type == "property"]{
  name, 
  description, 
  "beds": string(beds),
  "bathrooms": string(bathrooms)
}
```

You may, however, need to update your front end's logic to evaluate these values as strings instead of numbers.

## Troubleshooting

A number of the solutions below rely on the `vercelStegaSplit` function from the [@vercel/stega](https://www.npmjs.com/package/@vercel/stega) npm package. This works for any hosting provider, not just Vercel, as they both consume the same metadata.

Install it with:

```sh
npm i @vercel/stega
```

### Comparing field values fails

Your production front end likely evaluates values returned from the Content Lake to perform specific logic. If these values contain encoded metadata from Content Source Maps, likely, they will no longer work.

#### How to fix

For example, imagine a function that determines that a Sanity document's market value is the same as the current market:

```typescript
function showDocument(document: SanityDocument, currentMarket: string) {
  return document.market === currentMarket
}
```

Without Content Source Maps, this function works as expected. However, if `document.market` contains encoded metadata, this comparison will fail.

If `document.market` is never shown on the page and will not benefit from Visual Editing, it may be best to remove it from the encoded paths in `encodeSourceMapAtPath`. 

Alternatively, "clean" the value before comparing it. Since you'll likely do this more than once, consider extracting to a helper function.

```typescript
import {vercelStegaSplit} from '@vercel/stega'

function clean(value: string) {
  return vercelStegaSplit(value).cleaned
}

function showDocument(document: SanityDocument, currentMarket: string) {
  return clean(document.market) === currentMarket
}
```

### The styling of the editable fields is incorrect

If the text on the page is breaking out of its container – or its container is much wider than normal – it can be resolved by splitting the encoded text out from the original text.

> **Note:** This is not due to the encoded characters themselves. This problem should only present itself if the element also uses negative `letter-spacing` in its CSS or is inside of a `<ReactWrapBalancer>`.

Then identify where the problematic element is rendered in code, for example:

```tsx
function MyComponent({ text }) {
	return (
		<h1>{text}</h1>
	);
}
```

Rewrite using `@vercel/stega` to avoid any styling issues:

```tsx
import { vercelStegaSplit } from '@vercel/stega';

function MyComponent({ text }) {
	const { cleaned, encoded } = vercelStegaSplit(text);

	return (
		<h1>
			{cleaned}
			<span style={{ display: 'none' }}>{encoded}</span>
		</h1>
	);
}
```

If you find yourself doing this more than once, you might like to extract this logic to a reusable component:

```tsx
import {vercelStegaSplit} from '@vercel/stega'

export default function Clean({value}: {value: string}) {
  const {cleaned, encoded} = vercelStegaSplit(value)

  return encoded ? (
    <>
      {cleaned}
      <span style={{display: 'none'}}>{encoded}</span>
    </>
  ) : (
    cleaned
  )
}

function MyComponent({ text }) {
	return (
		<h1><Clean value={text} /></h1>
	);
}
```

### Formatting dates throws an error

Sometimes, you can experience type errors when trying to format dates.

#### How to fix

Identify where the date is formatted in code, for example:

```typescript
function formatDate(datestring) {
	const date = new Date(datestring);
	return date.nicelyFormatted();
}
```

Rewrite using `@vercel/stega` to avoid any styling issues:

```typescript
import { vercelStegaSplit } from '@vercel/stega';

function formatDate(datestring) {
	const { cleaned, encoded } = vercelStegaSplit(datestring);
	const date = new Date(cleaned);
	return `${date}${encoded}`;
}
```

### The wrong element is being highlighted

If the wrong element is highlighted when hovering them, it can be resolved by adding an attribute to the correct element.

#### How to fix

For example, if this component highlights the `<h1>` and you want it to highlight the entire `<section>` element:

```html
<section>
	<h1>{dynamicTitle}</h1>
	<div>Hardcoded Tagline</div>
</section>
```

Add a data attribute to highlight the correct item:

- For Visual Editing with `@sanity/overlays`, add `data-sanity-edit-target`
- For Vercel Visual Editing, add `data-vercel-edit-target`

```html
<section data-sanity-edit-target>
	<h1>{dynamicTitle}</h1>
	<div>Hardcoded Tagline</div>
</section>
```



# Sanity CLI

#### Common commands

[Init](/docs/cli-reference/init)

[Dev](/docs/cli-reference/dev)

[Docs](/docs/cli-reference/docs-in-cli)

[Deploy](/docs/cli-reference/deploy)

[TypeGen](/docs/cli-reference/cli-typegen)

[Schema](/docs/cli-reference/cli-schema)





# Configuration

The `sanity` Command Line Interface (CLI) is a handy tool for managing your Sanity projects in your terminal. Note that there are some commands that can only be run in a project folder and global ones.

[Learn more about the Sanity CLI](/docs/apis-and-sdks/cli)



## Configuration file

Sanity CLI can read configuration from a `sanity.cli.js` (`.ts`) file in the same folder that the command is run in. It will fall back on the configuration in the `sanity.config.ts` file.

#### Properties

| Property | Description |
|----------|-------------|
| api | Defines the projectId, dataset that the CLI should connect and run its commands on |
| server | Defines the hostname and port that the development server should run on. hostname defaults to localhost, and port to 3333. |
| graphql | Defines the GraphQL APIs that the CLI can deploy and interact with. |
| reactStrictMode | Wraps the Studio in <React.StrictMode> root to aid in flagging potential problems related to concurrent features (startTransition, useTransition, useDeferredValue, Suspense). Can also be enabled by setting SANITY_STUDIO_REACT_STRICT_MODE="true"\|"false".  It only applies to sanity dev in development mode and is ignored in sanity build and in production. Defaults to false. |
| vite | Exposes the default Vite configuration for the Studio so it can be changed and extended. |
| project | Contains the property basePath which lets you change the top-level slug for the Studio. You typically need to set this if you embed the Studio in another application where it is one of many routes. Defaults to an empty string. |


> [!WARNING]
> Gotcha
> If you run sanity --help outside a folder with a project configuration file and without a specified projectId flag, you will only see the subset of commands that's non project specific.

## GraphQLAPIConfig

#### Properties

| Property | Description |
|----------|-------------|
| id | ID of GraphQL API. Only (currently) required when using the --api flag for sanity graphql deploy, in order to only deploy a specific API. |
| workspace | Name of workspace containing the schema to deploy

Optional, defaults to default (eg the one used if no name is defined) |
| source | Name of source containing the schema to deploy, within the configured workspace

Optional, defaults to default (eg the one used if no name is defined) |
| tag | API tag for this API - allows deploying multiple different APIs to a single dataset 

Optional, defaults to default |
| playground | Whether or not to deploy a "GraphQL Playground" to the API url - an HTML interface that allows running queries and introspecting the schema from the browser. Note that this interface is notsecured in any way, but as the schema definition and API route is generally open, this does notexpose any more information than is otherwise available - it only makes it more discoverable.
Optional, defaults to true |
| generation | Generation of API to auto-generate from schema. New APIs should use the latest (gen3).

Optional, defaults to gen3 |
| nonNullDocumentFields | Define document interface fields (_id, _type etc) as non-nullable. If you never use a document type as an object (within other documents) in your schemas,  you can (and probably should) set this to true. Because a document type could be used inside other documents, it is by default set to false, as in these cases these fields can be null.

Optional, defaults to false |
| filterSuffix | Suffix to use for generated filter types.

Optional, Defaults to Filter. |


## Commands

```text
usage: sanity [--default] [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   build      Builds the Sanity Studio configuration into a static bundle
   codemod    Runs a code modification script
   cors       Configures CORS settings for Sanity projects
   dataset    Manages datasets, like create or delete, within projects
   debug      Gathers information on Sanity environment
   deploy     Builds and deploys Sanity Studio to Sanity hosting
   dev        Starts a local dev server for Sanity Studio with live reloading
   docs       Opens the Sanity documentation
   documents  Manages documents in your Sanity Content Lake datasets
   exec       Executes a script within the Sanity Studio context
   graphql    Deploys changes to your project's GraphQL API(s)
   help       Displays help information about Sanity
   hook       Sets up and manages webhooks within your Sanity project
   init       Initialize a new Sanity Studio project
   install    Installs dependencies of the current project
   login      Authenticates against the Sanity.io API
   logout     Logs out of the Sanity.io session
   manage     Opens the Sanity project management UI
   migration  Manages content migrations for Content Lake datasets
   preview    Starts a server to preview a production build of Sanity Studio
   projects   Interact with projects connected to your logged in user
   schema     Interacts with Sanity Studio schema configurations
   start      Alias for `sanity preview`
   telemetry  Interact with telemetry settings for your logged in user
   undeploy   Removes the deployed Sanity Studio from Sanity hosting
   users      Manages users of your Sanity project
   versions   Shows the installed versions of Sanity CLI and core components

See 'sanity help <command>' for specific information on a subcommand.
```

> [!NOTE]
> CLI option flag order
> For commands with option flags, add the option flag to the end after any arguments. When adding option flags to both commands and subcommands, make sure the command flags are before the subcommand. For example: 
> 
> sanity COMMAND [args] [--command-flags] SUBCOMMAND [args] --[subcommand-flags]
> 
> You can always run sanity COMMAND --help for usage tips and examples.

## Changing <hostname>.sanity.studio

To change the host name of your Sanity-hosted Studio (e.g., `https://<1oldHostName>.sanity.studio` to `https://<2newHostName>.sanity.studio`), please see [Undeploying the Studio](https://www.sanity.io/docs/deployment#702097e3bdad).

## Debugging `sanity` commands

Not to be confused with [sanity debug](https://www.sanity.io/docs/debug), which returns information about your Sanity environment, you can use the `DEBUG` environment variable with your `sanity` commands to get more verbose results and troubleshoot potential issues.

For full debugger results, use a wildcard on its own (`DEBUG=* sanity <command>`). For more targeted results, you can specify a namespace followed by a wildcard (`DEBUG=sanity* sanity <command>` or `DEBUG=sanity:cli* sanity <command>`).

> [!NOTE]
> Example
> Least verbose
> sanity dataset import production.tar.gz dev
> 
> More verbose, returning all debuggers in the sanity namespace
> DEBUG=sanity* sanity dataset import production.tar.gz dev
> 
> Most verbose, returning all debuggers
> DEBUG=* sanity dataset import production.tar.gz dev

Results can also be excluded by using a `-` prefix. `DEBUG=sanity*,-sanity:export* sanity dataset export production production.tar.gz` would return all debuggers in the `sanity` namespace except for `sanity:export` debuggers (e.g., `sanity:cli` and `sanity:client`) during export of the `production` dataset.

## Authorizing the CLI

In most cases, you'll use `sanity login` to authenticate with the Sanity API. When you need to run the CLI unattended, like in a CI/CD environment, set the `SANITY_AUTH_TOKEN` environment variable to a token. You can generate tokens in the [project management dashboard](https://sanity.io/manage).



# Blueprints

The `blueprints` CLI command enables initializing, managing, and deploying Blueprints and resources like Functions.

[Blueprints introduction](/docs/compute-and-ai/blueprints)

[Functions introduction](/docs/compute-and-ai/functions-introduction)



```text
usage: npx sanity blueprints [--default] [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   add     Add a Resource to a Blueprint
   config  View or edit local Blueprints configuration
   deploy  Deploy a Blueprint to create or update a Stack
   destroy Destroy a Blueprint to undeploy the Stack
   info    Retrieve information about a Blueprint Stack
   init    Initialize a new Blueprint manifest file
   logs    Display logs for the current Blueprint Stack
   plan    Enumerate Resources to be deployed
   stacks  List all Blueprint Stacks for the current Project

See 'npx sanity help blueprints <command>' for specific information on a subcommand.
```

## Commands

### `add`

```text
usage: npx sanity blueprints add <type> [--name <name>] [--fn-type <document-publish>] [--fn-lang <ts|js>] [--javascript]

   Add a Resource to a Blueprint

Arguments
  <type>  Type of Resource to add (currently only 'function' is supported)

Options
  --name, -n <name>              Name of the Resource
  --fn-type <type>               Type of Function to add (e.g. document-publish)
  --fn-language, --lang <ts|js>  Language of the Function. Default: "ts"
  --js, --javascript             Shortcut for --fn-language=js
  --fn-helpers, --helpers        Add helpers to the Function
  --no-fn-helpers                Do not add helpers to the Function
  --fn-installer,                Package manager to use for Function helpers
    --installer <npm|pnpm|yarn>    sets --fn-helpers to true
  --install, -i                  Shortcut for --fn-installer=npm

Examples:
  # Add a Function (TypeScript by default)
  sanity blueprints add function

  # Add a Function with a specific name and install helpers with npm
  sanity blueprints add function --name my-function -i

  # Add a Function with a specific type
  sanity blueprints add function --fn-type document-publish

  # Add a JavaScript Function
  sanity blueprints add function --js

  # Add a Function without helpers
  sanity blueprints add function --no-fn-helpers

  # Add a document-publish .js Function with helpers and install with npm
  sanity blueprints add function -n roboto --fn-type document-publish --js -i
```

### `config`

```text
usage: npx sanity blueprints config [--edit] [-e] [--test] [-t] [--project-id <id>]

   View or edit local Blueprints configuration

Options
  --edit, -e           Edit the configuration
  --test, -t           Test the configuration
  --project-id <id>    Project ID to use

Examples:
  # View current configuration
  sanity blueprints config

  # Edit configuration
  sanity blueprints config --edit

  # Test configuration
  sanity blueprints config --test

  # Edit and test configuration
  sanity blueprints config -et
```



### `deploy`

```text
usage: npx sanity blueprints deploy [--no-wait]

   Deploy a Blueprint to create or update a Stack

Options
  --no-wait    Do not wait for deployment to complete

Examples:
  # Deploy the current blueprint
  sanity blueprints deploy

  # Deploy the current blueprint without waiting for completion
  sanity blueprints deploy --no-wait
```



### `destroy`

```text
usage: npx sanity blueprints destroy [--force] [-f] [--no-wait]

   Destroy a Blueprint deployment

Options
  --force, -f    Force destroy without confirmation
  --no-wait      Do not wait for destroy to complete

Examples:
  # Destroy the current deployment
  sanity blueprints destroy

  # Force destroy without confirmation
  sanity blueprints destroy --force

  # Destroy without waiting for completion
  sanity blueprints destroy --no-wait
```

### `info`

```text
usage: npx sanity blueprints info 

   Retrieve information about a Blueprint Stack

Examples:
  # Retrieve information about the current Stack
  sanity blueprints info
```

### `init`

```text
usage: npx sanity blueprints init [dir] [--blueprint-type <type>] [--project-id <id>]

   Initialize a new Blueprint manifest file

Arguments
  [dir]  Path to initialize the Blueprint in

Options
  --blueprint-type, --type <json>    Type of Blueprint to create
  --project-id <id>                  Project ID to use

Examples:
  # Create a new Blueprint manifest file in the current directory
  sanity blueprints init

  # Create a new Blueprint manifest file in a specific directory
  sanity blueprints init my-sanity-project --type json
```

### `logs`

```text
usage: npx sanity blueprints logs [--watch] [-w]

   Display logs for the current Blueprint Stack

Options
  --watch, -w    Watch for new logs (streaming mode)

Examples:
  # Show logs for the current Stack
  sanity blueprints logs

  # Watch for new logs (streaming mode)
  sanity blueprints logs --watch
```

### `plan`

```text
usage: npx sanity blueprints plan 

   Enumerate Resources to be deployed

Safe to run at any time. Will not modify any Resources.

Examples:
  # Show deployment plan for the current Blueprint
  sanity blueprints plan
```



# Build



```markdown
usage: sanity build [OUTPUT_DIR]

   Builds the current Sanity configuration to a static bundle

Options
  --auto-updates / --no-auto-updates Enable/disable auto-updates of studio versions
  --source-maps Enable source maps for built bundles (increases size of bundle)
  --no-minify Skip minifying built Javascript (speeds up build, increases size of bundle)
  -y, --yes Use unattended mode, accepting defaults and using only flags for choices

Examples
  sanity build
  sanity build --no-minify --source-maps
```



# Codemod



```text
usage: sanity codemod [CODEMOD_NAME]

   Runs a code modification script

Runs a given code modification script on the current studio folder.
Running the command without a specified codemod name will list available transformations.

Options
  --dry Dry run (no changes are made to files)
  --extensions=EXT Transform files with these file extensions (comma separated list)
                   (default: js,ts,tsx)
  --no-verify Skips verification steps before running codemod

Examples
  # Show available code mods
  sanity codemod

  # Run codemod to transform react-icons imports from v2 style to v3 style,
  # but only as a dry-run (do not write the files)
  sanity codemod reactIconsV3 --dry
```



# CORS

```markdown
usage: sanity cors [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   add     Allow a new origin to use your project API through CORS
   delete  Delete an existing CORS-origin from your project
   list    List all origins allowed to access the API for this project

See 'sanity help cors <command>' for specific information on a subcommand.
```

## Commands

### Add

```
usage: sanity cors add [ORIGIN]

   Allow a new origin to use your project API through CORS

Options
  --credentials Allow credentials (token/cookie) to be sent from this origin
  --no-credentials Disallow credentials (token/cookie) to be sent from this origin

Examples
  sanity cors add
  sanity cors add http://localhost:3000 --no-credentials
```

### Delete

```
usage: sanity cors delete [ORIGIN]

   Delete an existing CORS-origin from your project

Examples
  sanity cors delete
  sanity cors delete http://localhost:3000
```

### List

```
usage: sanity cors list

   List all origins allowed to access the API for this project

Examples
  sanity cors list
```



# Dataset

```markdown
usage: sanity dataset [--default] [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   alias       You can manage your dataset alias using this command.
   copy        Manages dataset copying, including starting a new copy job, listing copy jobs and following the progress of a running copy job
   create      Create a new dataset within your project
   delete      Delete a dataset within your project
   export      Export dataset to local filesystem as a gzipped tarball
   import      Import documents to given dataset from ndjson file
   list        List datasets of your project
   visibility  Set visibility of a dataset

See 'sanity help dataset <command>' for specific information on a subcommand.
```

## Commands

### Alias

```text
usage: sanity dataset alias SUBCOMMAND [ALIAS_NAME, TARGET_DATASET]

   You can manage your dataset alias using this command.

Below are examples of the alias subcommand

Create Alias
  sanity dataset alias create
  sanity dataset alias create <alias-name>
  sanity dataset alias create <alias-name> <target-dataset>

Delete Alias
  Options
    --force Skips security prompt and forces link command

  Usage
    sanity dataset alias delete <alias-name>
    sanity dataset alias delete <alias-name> --force

Link Alias
  Options
    --force Skips security prompt and forces link command

  Usage
    sanity dataset alias link
    sanity dataset alias link <alias-name>
    sanity dataset alias link <alias-name> <target-dataset>
    sanity dataset alias link <alias-name> <target-dataset> --force

Un-link Alias
  Options
    --force Skips security prompt and forces link command

  Usage
    sanity dataset alias unlink
    sanity dataset alias unlink <alias-name>
    sanity dataset alias unlink <alias-name> --force
```

### Copy

```text
usage: sanity dataset copy [SOURCE_DATASET] [TARGET_DATASET]

   Manages dataset copying, including starting a new copy job, listing copy jobs and following the progress of a running copy job

Options
  --detach Start the copy without waiting for it to finish
  --attach <job-id> Attach to the running copy process to show progress
  --skip-history Don't preserve document history on copy
  --list Lists all dataset copy jobs corresponding to a certain criteria.
  --offset Start position in the list of jobs. Default 0. With --list.
  --limit Maximum number of jobs returned. Default 10. Maximum 1000. With --list.

Examples
  sanity dataset copy
  sanity dataset copy <source-dataset>
  sanity dataset copy <source-dataset> <target-dataset>
  sanity dataset copy <source-dataset> <target-dataset> --skip-history
  sanity dataset copy <source-dataset> <target-dataset> --detach
  sanity dataset copy --attach <job-id>
  sanity dataset copy --list
  sanity dataset copy --list --offset=2
  sanity dataset copy --list --offset=2 --limit=10
```

### Create

```text
usage: sanity dataset create [NAME]

   Create a new dataset within your project

Options
  --visibility <mode> Set visibility for this dataset (public/private)

Examples
  sanity dataset create
  sanity dataset create <name>
  sanity dataset create <name> --visibility private
```

### Delete

```text
usage: sanity dataset delete [datasetName]

   Delete a dataset within your project

Options
  --force Do not prompt for delete confirmation - forcefully delete

Examples
  sanity dataset delete
  sanity dataset delete my-dataset
  sanity dataset delete my-dataset --force
```

### Export

```text
usage: sanity dataset export [NAME] [DESTINATION]

   Export dataset to local filesystem as a gzipped tarball. Assets failing with HTTP status codes 401, 403 and 404 upon download are ignored and excluded from export.

Options
  --raw                     Extract only documents, without rewriting asset references
  --no-assets               Export only non-asset documents and remove references to image assets
  --no-drafts               Export only published versions of documents
  --no-compress             Skips compressing tarball entries (still generates a gzip file)
  --types                   Defines which document types to export
  --overwrite               Overwrite any file with the same name
  --asset-concurrency <num> Concurrent number of asset downloads
  --mode <stream|cursor> Uses a cursor when exporting, this might be more performant for larger datasets, but might not be as accurate if the dataset is being modified during export. Defaults to stream.

Examples
  sanity dataset export moviedb localPath.tar.gz
  sanity dataset export moviedb assetless.tar.gz --no-assets
  sanity dataset export staging staging.tar.gz --raw
  sanity dataset export staging staging.tar.gz --types products,shops
```

### Import

```text
usage: sanity dataset import [FILE | FOLDER | URL] [TARGET_DATASET]

   Import documents to given dataset from ndjson file

Options
  --missing On duplicate document IDs, skip importing document in question
  --replace On duplicate document IDs, replace existing document with imported document
  --allow-failing-assets Skip assets that cannot be fetched/uploaded
  --replace-assets Skip reuse of existing assets

Rarely used options (should generally not be used)
  --allow-assets-in-different-dataset Allow asset documents to reference different project/dataset
  --allow-system-documents Allow system documents like dataset permissions and custom retention to be imported

Examples
  # Import "moviedb.ndjson" from the current directory to the dataset called "moviedb"
  sanity dataset import moviedb.ndjson moviedb

  # Import "moviedb.tar.gz" from the current directory to the dataset called "moviedb",
  # replacing any documents encountered that have the same document IDs
  sanity dataset import moviedb.tar.gz moviedb --replace

  # Import from a folder containing an ndjson file, such as an extracted tarball
  # retrieved through "sanity dataset export".
  sanity dataset import ~/some/folder moviedb

  # Import from a remote URL. Will download and extract the tarball to a temporary
  # location before importing it.
  sanity dataset import https://some.url/moviedb.tar.gz moviedb --replace
```

### List

```text
usage: sanity dataset list

   List datasets of your project
```

### Visibility

```text
usage: sanity dataset visibility get/set [dataset] [mode]

   Set visibility of a dataset
```



# Debug

```markdown
usage: sanity debug [--secrets]

   Gathers information on Sanity environment

Used to find information about the Sanity environment, and to debug Sanity-related issues.

Options
  --secrets Include API keys in output

Examples
  # Show information about the user, project, and local/global Sanity environment
  sanity debug

  # Include API keys in the output
  sanity debug --secrets
```



# Deploy



```markdown
usage: sanity deploy [SOURCE_DIR] [--no-build]  [--source-maps] [--no-minify]

   Deploys a statically built Sanity studio

Options
  --auto-updates / --no-auto-updates Enable/disable auto-updates of studio versions
  --source-maps Enable source maps for built bundles (increases size of bundle)
  --no-minify Skip minifying built Javascript (speeds up build, increases size of bundle)
  --no-build Don't build the studio prior to deploy, instead deploying the version currently in `dist/`
  -y, --yes Unattended mode, answers "yes" to any "yes/no" prompt and otherwise uses defaults

Examples
  sanity deploy
  sanity deploy --no-minify --source-maps
```



# Dev



```text
usage: sanity dev [--port <port>] [--host <host>]

   Starts a development server for the Sanity Studio

Notes
  Changing the hostname or port number might require a new entry to the CORS-origins allow list.

Options
  --port <port> TCP port to start server on. [default: 3333]
  --host <host> The local network interface at which to listen. [default: "127.0.0.1"]

Examples
  sanity dev --host=0.0.0.0
  sanity dev --port=1942
```



# Docs

```markdown
usage: sanity docs

   Opens the Sanity documentation

```



# Documents



```markdown
usage: sanity documents [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   create   Create one or more documents
   delete   Delete a document by ID
   get      Get and print a document
   query    Query for documents
   validate Download and validate documents

See 'sanity help documents <command>' for specific information on a subcommand.
```

## Commands

### Create

```markdown
usage: sanity documents create [FILE]

   Create one or more documents

Options
  --replace On duplicate document IDs, replace existing document with specified document(s)
  --missing On duplicate document IDs, don't modify the target document(s)
  --watch   Write the documents whenever the target file or buffer changes
  --json5   Use JSON5 file type to allow a "simplified" version of JSON
  --id <id> Specify a document ID to use. Will fetch remote document ID and populate editor.
  --dataset NAME to override dataset

Examples
  # Create the document specified in "myDocument.json".
  sanity documents create myDocument.json

  # Open configured $EDITOR and create the specified document(s)
  sanity documents create

  # Fetch document with the ID "myDocId" and open configured $EDITOR with the
  # current document content (if any). Replace document with the edited version
  # when the editor closes
  sanity documents create --id myDocId --replace

  # Open configured $EDITOR and replace the document with the given content
  # on each save. Use JSON5 file extension and parser for simplified syntax.
  sanity documents create --id myDocId --watch --replace --json5
```

### Delete

```markdown
usage: sanity documents delete [ID] [...IDS]

   Delete a document by ID

Delete a document from the projects configured dataset

Options
  --dataset NAME to override dataset

Example
  # Delete the document with the ID "myDocId"
  sanity documents delete myDocId

  # ID wrapped in double or single quote works equally well
  sanity documents delete 'myDocId'

  # Delete document with ID "someDocId" from dataset "blog"
  sanity documents delete --dataset=blog someDocId

  # Delete the document with ID "doc1" and "doc2"
  sanity documents delete doc1 doc2
```

### Get

```markdown
usage: sanity documents get [DOCUMENT_ID]

   Get and print a document by ID

Get and print a document from the projects configured dataset

Options
  --pretty colorized JSON output
  --dataset NAME to override dataset

Examples
  # Get the document with the ID "myDocId"
  sanity documents get myDocId

  # ID wrapped in double or single quote works equally well
  sanity documents get 'myDocId'
```

### Query

```markdown
usage: sanity documents query [QUERY]

   Query for documents

Run a query against the projects configured dataset

Options
  --pretty colorized JSON output
  --dataset NAME to override dataset
  --project PROJECT to override project ID
  --anonymous Send the query without any authorization token
  --api-version API version to use (defaults to `v2022-06-01`)

Environment variables
  `SANITY_CLI_QUERY_API_VERSION` - will use the defined API version,
  unless `--api-version` is specified.

Examples
  # Fetch 5 documents of type "movie"
  sanity documents query '*[_type == "movie"][0..4]'

  # Fetch title of the oldest movie in the dataset named "staging"
  sanity documents query '*[_type == "movie"]|order(releaseDate asc)[0]{title}' --dataset staging

  # Use API version v2021-06-07 and do a query
  sanity documents query --api-version v2021-06-07 '*[_id == "header"] { "headerText": pt::text(body) }'
```

### Validate

```markdown
usage: sanity documents validate

   Downloads and validates all documents specified in a workspace

Options
  -y, --yes Skips the first confirmation prompt.
  --workspace <name> The name of the workspace to use when downloading and validating all documents.
  --dataset <name> Override the dataset used. By default, this is derived from the given workspace.
  --file <filepath> Provide a path to either an .ndjson file or a tarball containing an .ndjson file.
  --format <pretty|ndjson|json> The output format used to print the found validation markers and report progress.
  --level <error|warning|info> The minimum level reported out. Defaults to warning.
  --max-custom-validation-concurrency <number> Specify how many custom validators can run concurrently. Defaults to 5.
  --max-fetch-concurrency <number> Specify how many `client.fetch` requests are allowed concurrently. Defaults to 25.

Examples
  # Validates all documents in a Sanity project with more than one workspace
  sanity documents validate --workspace default

  # Override the dataset specified in the workspace
  sanity documents validate --workspace default --dataset staging

  # Save the results of the report into a file
  sanity documents validate > report.txt

  # Report out info level validation markers too
  sanity documents validate --level info
```



# Exec

```markdown
usage: sanity exec SCRIPT

   Runs a script in Sanity context

Options
  --with-user-token Prime access token from CLI config into getCliClient()
  --mock-browser-env Mocks a browser-like environment using jsdom

Examples
  # Run the script at some/script.js in Sanity context
  sanity exec some/script.js

  # Run the script at migrations/fullname.ts and configure `getCliClient()`
  # from `sanity/cli`to include the current user's token
  sanity exec migrations/fullname.ts --with-user-token

  # Run the script at scripts/browserScript.js in a mock browser environment
  sanity exec scripts/browserScript.js --mock-browser-env

  # Pass arbitrary arguments to scripts by separating them with a `--`.
  # Arguments are available in `process.argv` as they would in regular node scripts
  # eg the following command would yield a `process.argv` of:
  # ['/path/to/node', '/path/to/myscript.js', '--dry-run', 'positional-argument']
  sanity exec --mock-browser-env myscript.js -- --dry-run positional-argument
```





# Functions

The `functions` CLI command enables managing and testing functions. It's used alongside the `blueprints` command to create and deploy functions.

[Functions introduction](/docs/compute-and-ai/functions-introduction)

[Blueprints introduction](/docs/compute-and-ai/blueprints)



```text
usage: npx sanity functions [--default] [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   dev   Start the Sanity Function emulator
   env   Add or remove an environment variable or list environment variables for a Sanity function
   logs  Retrieve or delete logs for a Sanity Function
   test  Invoke a local Sanity Function

See 'npx sanity help functions <command>' for specific information on a subcommand.
```

## Commands

### dev

```text
usage: npx sanity functions dev [--port <port>]

   Start the Sanity Function emulator

Options
  --port <port> Port to start emulator on

Examples
  # Start dev server on default port
  sanity functions dev

  # Start dev server on specific port
  sanity functions dev --port 3333
```

### env

```text
usage: npx sanity functions env <add|list|remove> <name> [key] [value]

   Add or remove an environment variable or list environment variables for a Sanity function

Commands
  add    Add or update an environment variable
  list   List the environment variables
  remove Remove an environment variable

Arguments
  <name> The name of the function
  <key> The name of the environment variable
  <value> The value of the environment variable

Examples
  # Add or update an environment variable
  sanity functions env add echo API_URL https://api.example.com/

  # Remove an environment variable
  sanity functions env remove echo API_URL

  # List environment variables
  sanity functions env list echo
```

### logs

```text
usage: npx sanity functions logs <name> [--limit <number>] [--json] [--utc] [--delete [--force]]

   Retrieve or delete logs for a Sanity Function

Arguments
  <name> The name of the Function to retrieve logs for

Options
  --limit <limit> The number of log entries to retrieve [default 50]
  --json If set return json
  --utc Use UTC dates in logs

Examples
  # Retrieve logs for Sanity Function
  sanity functions logs echo

  # Retrieve the last two log entries for Sanity Function
  sanity functions logs echo --limit 2

  # Retrieve logs for Sanity Function in json format
  sanity functions logs --name echo --json

  # Delete all logs for Sanity Function
  sanity functions logs --name echo --delete
```

### test

```text
usage: npx sanity functions test <name> [--data <json>] [--file <filename>] [--timeout <seconds>] [--api <version>] [--dataset <name>] [--project-id] <id>]

   Invoke a local Sanity Function

Arguments
  <name> The name of the Sanity Function

Options
  --data <data> Data to send to the function
  --file <file> Read data from file and send to the function
  --timeout <timeout> Execution timeout value in seconds
  --api <version> Sanity API Version to use
  --dataset <dataset> The Sanity dataset to use
  --project-id <id> Sanity Project ID to use


Examples
  # Test function passing event data on command line
  sanity functions test echo --data '{ "id": 1 }'

  # Test function passing event data via a file
  sanity functions test echo --file 'payload.json'

  # Test function passing event data on command line and cap execution time to 60 seconds
  sanity functions test echo --data '{ "id": 1 }' --timeout 60
```



# GraphQL

## Available commands

```markdown
usage: sanity graphql [--default] [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   deploy    Deploy a GraphQL API from the current Sanity schema
   list      Lists all the GraphQL endpoints deployed for this project
   undeploy  Remove a deployed GraphQL API

See 'sanity help graphql <command>' for specific information on a subcommand.
```

> [!WARNING]
> Gotcha
> Dataset names with dashes - in the name currently list incorrectly in the sanity graphql list command. If this causes issues for you please use a different delimiter in your dataset names. This is something we are aware of and looking to fix in the future. 

## Deploying/updating an API

```markdown
usage: sanity graphql deploy

   Deploy a GraphQL API from the current Sanity schema

Options
  --dry-run Validate defined APIs, exiting with an error on breaking changes
  --force Deploy API without confirming breaking changes
  --api <api-id> Only deploy API with this ID. Can be specified multiple times.

The following options will override any setting from the CLI configuration file
(sanity.cli.js/sanity.cli.ts) - and applies to ALL defined APIs defined in that
configuration file. Tread with caution!

  --tag Deploy API(s) to given tag (defaults to 'default')
  --dataset <name> Deploy API for the given dataset
  --generation <gen1|gen2|gen3> API generation to deploy (defaults to 'gen3')
  --non-null-document-fields Use non-null document fields (_id, _type etc)
  --playground Enable GraphQL playground for easier debugging
  --no-playground Disable GraphQL playground
  --with-union-cache *Experimental:* Enable union cache that optimizes schema generation for schemas with many self referencing types

Examples
  # Deploy all defined GraphQL APIs
  sanity graphql deploy

  # Validate defined GraphQL APIs, check for breaking changes, skip deploy
  sanity graphql deploy --dry-run

  # Deploy only the GraphQL APIs with the IDs "staging" and "ios"
  sanity graphql deploy --api staging --api ios

  # Deploy all defined GraphQL APIs, overriding any playground setting
  sanity graphql deploy --playground
```

## Deleting/undeploying an API

```markdown
usage: sanity graphql undeploy undefined

   Remove a deployed GraphQL API

Options
  --dataset <dataset> Delete GraphQL API for the given dataset
  --tag <tag> Delete GraphQL API for the given tag (defaults to 'default')

Examples
  sanity graphql undeploy
  sanity graphql undeploy --dataset staging
  sanity graphql undeploy --dataset staging --tag next
```



# Help

```markdown
usage: sanity help [COMMAND]

   Displays help information about Sanity

With no options and no COMMAND given, the synopsis of the sanity command and a
list of the most commonly used commands are printed on the standard output.

If a command is given, the help page for that command is printed to standard
output. This will usually be more in-depth than the brief description shown in
the command list.
```



# Hook

```markdown
usage: sanity hook [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   attempt  Print details of a given webhook delivery attempt
   create   Create a new hook for the given dataset
   delete   Delete a hook within your project
   list     List hooks for a given project
   logs     List latest log entries for a given hook

See 'sanity help hook <command>' for specific information on a subcommand.
```

## Commands

### Attempt

```markdown
usage: sanity hook attempt ATTEMPT_ID

   Print details of a given webhook delivery attempt

```



### Create

```markdown
usage: sanity hook create

   Create a new hook for the given dataset

```





### Delete

```markdown
usage: sanity hook delete [NAME]

   Delete a hook within your project
```

### List

```markdown
usage: sanity hook list

   List hooks for a given project
```



### Logs

```markdown
usage: sanity hook logs [NAME]

   List latest log entries for a given hook
```





# Init

```markdown
usage: sanity init

   Initialize a new Sanity studio project

Options
  -y, --yes Use unattended mode, accepting defaults and using only flags for choices
  --project <projectId> Project ID to use for the studio
  --organization <organizationId> Organization ID to use for the project
  --dataset <dataset> Dataset name for the studio
  --dataset-default Set up a project with a public dataset named "production"
  --output-path <path> Path to write studio project to
  --template <template> Project template to use [default: "clean"]
  --bare Output only the project id and dataset to stdout
  --env <filename> Write environment variables to file [default: ".env"]
  --provider <provider> Login provider to use
  --visibility <mode> Visibility mode for dataset (public/private)
  --create-project <name> Create a new project with the given name
  --project-plan <name> Optionally select a plan for a new project
  --coupon <name> Optionally select a coupon for a new project (cannot be used with --project-plan)
  --no-typescript Do not use TypeScript for template files
  --package-manager Specify a package manager

Examples
  # Initialize a new project, prompt for required information along the way
  sanity init

  # Initialize a new project with a public dataset named "production"
  sanity init --dataset-default

  # Initialize a project with the given project ID and dataset to the given path
  sanity init -y --project abc123 --dataset production --output-path ~/myproj

  # Initialize a project with the given project ID and dataset using the moviedb
  # template to the given path
  sanity init -y --project abc123 --dataset staging --template moviedb --output-path .

  # Create a brand new project with name "Movies Unlimited"
  sanity init -y \
    --create-project "Movies Unlimited" \
    --dataset moviedb \
    --visibility private \
    --template moviedb \
    --output-path /Users/espenh/movies-unlimited
```



# Install

```markdown
usage: sanity install

   Installs dependencies of the current project
```





# Login

```markdown
usage: sanity login

   Authenticates against the Sanity.io API

```

The `sanity login` process requires a browser. To run a command that requires authentication but where a browser is not available, such as on a server, you can login locally, run `sanity debug --secrets` to get a personal auth token, and then precede the command requiring authentication with `SANITY_AUTH_TOKEN=<token>`.

```markdown
SANITY_AUTH_TOKEN=ab97ae7...0f9ff sanity init -y \
  --create-project "Movies Unlimited" \
  --dataset moviedb \
  --visibility private \
  --template moviedb \
  --output-path /path/to/folder
```

## Login with SAML SSO

> [!NOTE]
> SAML SSO Prerequisites
> SAML SSO requires an organization with a business or enterprise plan and an external identity provider that supports SAML authentication, such as Okta, Azure AD, or Google.

Users configured with [SAML SSO](https://www.sanity.io/docs/sso-saml) can use the `--sso` flag when logging in to pass their slug and log into a project using their third-party identity provider. The slug is set via the [Sanity Management Console](https://www.sanity.io/manage) and is configured under the Settings tab for the Organization.

```markdown
usage: sanity login --sso <slug>

   Authenticates against a third-party identity provider

```



# Logout

```markdown
usage: sanity logout

   Logs out of the Sanity.io session

```





# Manage

```markdown
usage: sanity manage

   Opens the Sanity project management UI


```



# Manifest

```
usage: sanity manifest extract

   Extracts the studio configuration as one or more JSON manifest files.

**Note**: This command is experimental and subject to change. It is currently intended for use with Create only.

Options
  --path Optional path to specify destination directory of the manifest files. Default: /dist/static

Examples
  # Extracts manifests
  sanity manifest extract

  # Extracts manifests into /public/static
  sanity manifest extract --path /public/static
```



# Media

Interact with Media Library with the `npx sanity media` command.

```text
usage: npx sanity media [--default] [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   create-aspect  Create a new aspect definition file.
   delete-aspect  Undeploy an aspect.
   deploy-aspect  Deploy an aspect.
   export         Export an archive of all assets and aspect data from the target media library.
   import         Import a set of assets to the target media library.

See 'npx sanity help media <command>' for specific information on a subcommand.
```



The `media` command must be run from within a directory that contains a valid `santy.cli.ts` configuration file. We recommend running it from within an existing Sanity project. [Learn more about configuring Media Library](/docs/media-library/configure-library).

## Commands

### `create-aspect`

Usage details are available in the [aspects guide](/docs/media-library/create-aspect).

```text
usage: npx sanity media create-aspect 

   Create a new aspect definition file.

Examples
  # Create a new aspect definition file.
  sanity media create-aspect
```

### `delete-aspect`

```text
usage: npx sanity media delete-aspect [ASPECT_NAME]

   Undeploy an aspect.

Options
  --media-library-id The id of the target media library.

Examples
  # Delete the aspect named "someAspect".
  sanity media delete-aspect someAspect
```

### `deploy-aspect`

```text
usage: npx sanity media deploy-aspect [ASPECT_NAME]

   Deploy an aspect.

Options
  --media-library-id The id of the target media library.
  --all              Deploy all aspects.

Examples
  # Deploy the aspect named "someAspect".
  sanity media deploy-aspect someAspect

  # Deploy all aspects.
  sanity media deploy-aspect --all
```

### `export`

```text
usage: npx sanity media export [FILE]

   Export an archive of all assets and aspect data from the target media library.

Options
  --media-library-id The id of the target media library.

Examples
  # Export all assets and aspects.
  sanity media export
```

### `import`

```text
usage: npx sanity media import [FILE | FOLDER]

   Import a set of assets to the target media library.

Options
  --media-library-id The id of the target media library.
  --replace-aspects  Replace existing aspect data. All versions will be replaced (e.g. published and draft aspect data).

Examples
  # Import all assets from the "products" directory.
  sanity media import products

  # Import all assets from "gallery" archive.
  sanity media import gallery.tar.gz

  # Import all assets from the "products" directory and replace aspects.
  sanity media import products --replace-aspects
```





# Migration

### Available commands

```text
usage: sanity migration [--default] [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   create  Create a new content migration within your project
   list    List available migrations
   run     Run a migration against a dataset

See 'sanity help migration <command>' for specific information on a subcommand.
```

### Creating a content migration

```text
usage: sanity migration create [TITLE]

   Create a new content migration within your project

Create a new migration within your project

Examples:
    # Create a new migration, you will be prompted to provide a type
    sanity migration create

    # Create a new migration, specifying the title
    sanity migration create "Rename field from location to address"
```

### Listing content migrations

```
usage: sanity migration list [NAME]

   List available migrations
```

### Running content migrations

```
usage: sanity migration run ID

   Run a migration against a dataset

Options
  --dry-run <boolean> Whether or not to dry run the migration. Default to true, to actually run the migration, pass --no-dry-run
  --concurrency <concurrent> How many mutation requests to run in parallel. Must be between 1 and 10. Default: 6.
  --no-progress Don't output progress. Useful if you want to debug your migration script and only see the output of console.log() statements.
  --dataset <dataset> Dataset to migrate. Defaults to the dataset configured in your Sanity CLI config.
  --project <project id> Project ID of the dataset to migrate. Defaults to the projectId configured in your Sanity CLI config.
  --no-confirm Skip the confirmation prompt before running the migration. Make sure you know what you're doing before using this flag.


Examples
  # dry run the migration
  sanity migration run <id>

  # execute the migration against a dataset
  sanity migration run <id> --no-dry-run --project xyz --dataset staging
```



# Preview



```text
usage: sanity preview [BUILD_OUTPUT_DIR] [--port <port>] [--host <host>]

   Starts a local web server for previewing production build

Notes
  Changing the hostname or port number might require a new entry to the CORS-origins allow list.

Options
  --port <port> TCP port to start server on. [default: 3333]
  --host <host> The local network interface at which to listen. [default: "127.0.0.1"]

Examples
  sanity preview --host=0.0.0.0
  sanity preview --port=1942
  sanity preview some/build-output-dir
```



# Projects



```markdown
usage: sanity projects [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   list  Lists projects connected to your user

See 'sanity help projects <command>' for specific information on a subcommand.
```



# Schema

## Available commands

```sh
usage: npx sanity schema [--default] [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   delete    Delete schemas by their IDs.
   deploy    Deploy schemas into workspace datasets.
   extract   Extracts a JSON representation of a Sanity schema within a Studio context.
   list      Lists all schemas in the current dataset.
   validate  Validates all schema types specified in a workspace.

See 'sanity help schema <command>' for specific information on a subcommand.
```

## Delete a schema

```sh
usage: npx sanity schema delete 

   Delete schemas by their IDs.

Options
  --ids <schema_id_1,schema_id_2,...> comma-separated list of schema IDs to delete
  --dataset <dataset_name> delete schemas from a specific dataset
  --manifest-dir <directory> directory containing your manifest file if it's not in the default location
  --no-extract-manifest disables manifest generation – the command will fail if no manifest exists

Examples
  # Delete single schema
  sanity schema delete --ids <schema_id>

  # Delete multiple schemas
  sanity schema delete --ids <schema_id_1,schema_id_2,...>
```

## Deploy schemas

```sh
usage: npx sanity schema deploy 

   Deploy schemas into workspace datasets.

Options:
  --workspace <workspace_name> deploy schema for a specific workspace
  --manifest-dir <directory> directory containing your manifest file if it's not in the default location
  --no-extract-manifest disables manifest generation – the command will fail if no manifest exists
  --id-prefix <prefix> add a prefix to the schema ID
  --schema-required fail if schema file is not found
  --verbose print detailed information during store

Examples
  # if no options are provided all workspace schemas will be deployed
  sanity schema deploy
  # Deploy the schema for only the workspace 'default'
  sanity schema deploy --workspace default
```

## Extract a schema

```
usage: npx sanity schema extract

   Extracts a JSON representation of a Sanity schema within a Studio context.

**Note**: This command is experimental and subject to change.

Options
  --workspace <name> The name of the workspace to generate a schema for
  --path Optional path to specify destination of the schema file
  --enforce-required-fields Makes the schema generated treat fields marked as required as non-optional. Defaults to false.
  --format=[groq-type-nodes] Format the schema as GROQ type nodes. Only available format at the moment.

Examples
  # Extracts schema types in a Sanity project with more than one workspace
  sanity schema extract --workspace default
```

## List schemas

```sh
usage: npx sanity schema list 

   Lists all schemas in the current dataset.

Options
  --json get schemas as json
  --id <schema_id> fetch a specific schema by its ID
  --manifest-dir <directory> directory containing your manifest file if it's not in the default location
  --no-extract-manifest disables manifest generation – the command will fail if no manifest exists

Examples
  # Get full json schemas
  sanity schema list --json

  # Get a specific schema by ID
  sanity schema list --id <schema_id>
```

## Validate a schema 

```
usage: npx sanity schema validate

  Validates all schema types specified in a workspace.

Options
  --workspace <name> The name of the workspace to use when validating all schema types.
  --format <pretty|ndjson|json> The output format used to print schema errors and warnings.
  --level <error|warning> The minimum level reported out. Defaults to warning.

Examples
  # Validates all schema types in a Sanity project with more than one workspace
  sanity schema validate --workspace default

  # Save the results of the report into a file
  sanity schema validate > report.txt

  # Report out only errors
  sanity schema validate --level error
```



# Start

```markdown
usage: sanity start [BUILD_OUTPUT_DIR] [--port <port>] [--host <host>]

   Alias of `sanity preview`

Notes
  Changing the hostname or port number might require a new CORS-entry to be added.

Options
  --port <port> TCP port to start server on. [default: 3333]
  --host <host> The local network interface at which to listen. [default: "127.0.0.1"]

Examples
  sanity start --host=0.0.0.0
  sanity start --port=1942
  sanity start some/build-output-dir
```



# Telemetry



```markdown
usage: sanity telemetry [--default] [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   disable  Disable telemetry for your logged in user
   enable   Enable telemetry for your logged in user
   status   Check telemetry consent status for your logged in user
```

See the [Telemetry overview page](/telemetry) for more details.



# TypeGen

> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

```text
usage: sanity typegen [--default] [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   generate  Generates TypeScript types from schema types and GROQ queries

See 'sanity help typegen <command>' for specific information on a subcommand.
```



# Undeploy

```text
usage: sanity undeploy

Removes the deployed studio from .sanity.studio

Examples
sanity undeploy
```

Once a studio is undeployed, the name (e.g., `<your-studio-name>`) becomes publicly available. Local instances of the studio are not affected.



# Users



```markdown
usage: sanity users [--default] [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   invite  Invite a new user to the project
   list    List all users of the project

See 'sanity help users <command>' for specific information on a subcommand.
```

## Commands

### Invite

```markdown
usage: sanity users invite [EMAIL]

   Invite a new user to the project

Options
  --role Role to invite the user as

Examples
  # Invite a new user to the project (prompt for details)
  sanity users invite

  # Send a new user invite to the email "pippi@sanity.io", prompt for role
  sanity users invite pippi@sanity.io

  # Send a new user invite to the email "pippi@sanity.io", as administrator
  sanity users invite pippi@sanity.io --role administrator
```



### List

```markdown
usage: sanity users list

   List all users of the project

Options
  --no-invitations Don't include pending invitations
  --no-robots Don't include robots (token users)
  --sort <field> Sort users by specified column: id, name, role, date
  --order <asc/desc> Sort output ascending/descending

Examples
  # List all users of the project
  sanity users list

  # List all users of the project, but exclude pending invitations and robots
  sanity users list --no-invitations --no-robots

  # List all users, sorted by role
  sanity users list --sort role
```



# Versions

```markdown
usage: sanity versions

   Shows the installed versions of Sanity CLI and core components

Shows a list of installed Sanity modules and their respective versions, and
checks the npm registry for the latest available versions.
```



# Store and query structured content

#### Querying and retrieving content

[How Queries Work – GROQ](/docs/content-lake/how-queries-work)

[GraphQL](/docs/content-lake/graphql)

[Perspectives for Content Lake](/docs/content-lake/perspectives)

[Libraries and clients](/docs/libraries)



#### Real-time & integration features

[Live Content API](/docs/content-lake/live-content-api)

[API CDN](/docs/content-lake/api-cdn)



#### Document storage

[Drafts and versions](/docs/content-lake/drafts)

[IDs and Paths](/docs/content-lake/ids)

[Datasets](/docs/content-lake/datasets)



#### Content operations

[Migrating your schema and content](/docs/content-lake/schema-and-content-migrations)

[Connected Content](/docs/studio/connected-content)





# Technical limits

This article describes various limits in the data store. Note that a project may have additional limits depending on its plan. An API call that causes any of these limits to be exceeded will be rejected with an error. If you have needs beyond these limits then get in touch and we'll work something out.

We use standard SI units, so 1 MB is 1 000 000 bytes.

> [!TIP]
> Protip
> An attribute here is considered to be any unique attribute/datatype combination, so an attribute attr containing a string, integer, and null value (in different documents) counts as 3 attributes. Additionally, arrays count as 1 extra attribute per unique datatype they contain, so the array [2.718, 3.14, "abc", "def", true] counts as 4 attributes (1 for the array itself, and 3 for the datatypes float, string, and boolean).

## Datasets

- Maximum number of documents: 1 million (customization possible for enterprise customers)
- Maximum total size of JSON documents: 10 GB
- Maximum number of unique attributes across all documents:- Free plan: 2 000 attributes
- Growth plan: 10 000 attributes
- Enterprise plan: Custom # attributes


- Maximum dataset name length: 64 characters

## Documents

- Maximum JSON document size: 32 MB
- Maximum number of attributes:- Free and Growth plan: 1000 attributes
- Enterprise plan: 8000 attributes


- Maximum attribute nesting depth: 20 levels
- Maximum searchable term length: 1024 UTF-8 characters (silently ignored without error)

## Listeners

Max *concurrent* listeners for the various project plans:   

- Free plan: 1000 listeners
- Growth plan: 5000 listeners
- Enterprise plan: 10000 listeners

> [!TIP]
> Editor experience
> In the event you hit the maximum concurrent listener limit, you will get the error Max listener limit exceeded at <limit>.

## API Calls

- Maximum working set retrieved from data backend: 500 MB
- Maximum query execution time: 1 minute
- Maximum mutation execution time: 3 minutes
- Maximum export execution time: 5 minutes
- Maximum listener connection lifetime: 30 minutes
- Maximum [scheduled publishing](/docs/scheduling-api#dcb47be520d0) execution time: 1 minute

## API Rate Limits

We have two rate-limits in place: one per source IP and one for number of concurrent queries. 

### API Rate Limits per IP

API rate limits are enforced per client IP address per second. Exceeding a rate limit will result in HTTP `429` responses for any further requests of that type until the period ends.

- Maximum mutation rate: 25 req/s (`POST` to `/data/mutate`)
- Maximum upload rate: 25 req/s (`POST` to `/assets/`)
- Maximum global API call rate: 500 req/s
- Maximum global API CDN call rate: unlimited for cached responses

### API Concurrent Rate Limits

We also rate limit concurrent API requests for each dataset:

- Maximum concurrent queries to API: 500
- Maximum concurrent mutations to API: 100

## HTTP Requests

- Maximum combined request headers size: 15 KB
- Maximum request body size: 100 MB
- Maximum mutation request body size: 4 MB

> [!TIP]
> Protip
> This implies that if your request exceeds 15KB in headers you probably want to switch from GET to POST and put the payload in body. See Queries - The POST form.

## API CDN HTTP Requests

For the /data endpoints (not assets):

- Maximum POST size: 300 KB

## Assets

### Animated Images with Transforms

Maximum size of animated images with transforms: 256 megapixels

- Calculated as: Width × Height × Frame Count = Total Pixels 
- Divide total pixels by 1,000,000 to get megapixels

This limit applies to on-demand transforms for supported animated image formats.

## Users

- Maximum number of users per project: 1000

[Please get in touch](https://sanity.io/contact) if you need more than 1000 users on a single project.



# API Versioning

From time to time, we may need to make breaking changes to our API functionality. We try very hard to avoid this, but it is *sometimes* necessary, e.g., to fix bugs when some users may have code that relies on incorrect behavior.

> [!NOTE]
> Changelog
> Looking for the latest changes? Check them out in the changelog.

To improve our APIs while also preventing existing code from breaking, we offer a versioned API. The URL of every API call includes an explicit version as the first path segment – the following call uses API version `v2021-10-21`:

`https://example.api.sanity.io/v2021-10-21/data/query/production?query=*`

To the best of our ability, we try to ensure that old versions of the API don't change, even as our services evolve. This may sometimes include "wrong" behavior that we may choose to maintain if we deem that the fix might negatively impact users.

As long as we don't see the change as a breaking change, we may choose to introduce it also in older versions of the API.



> [!WARNING]
> Gotcha
> API Versioning for v1 GraphQL queries. 
> 
> /v2023-03-01/graphql/**  endpoints use the same GROQ resolution as /v1/graphql/**.
> 
> GraphQL API v2023-08-01 introduced breaking changes to v1. You can opt-in to new features or continue using the v1 API. See the GraphQL documentation for additional GraphQL endpoint and usage details.

## Version scheme

[Stripe inspired us](https://stripe.com/blog/api-versioning) to use dates instead of incremental version numbers (although our initial version is `v1`). We much prefer to release frequent small improvements rather than saving them up for a huge `v2` release. This allows us to get fixes into the hands of our users much sooner and makes it easier for our users to upgrade incrementally. With new versions released regularly, we believe it is more informative to use dates rather than rapidly increasing numbers.

Version dates are ISO 8601-formatted and use the UTC time zone. You don't have to use specific dates, any past or present date is valid, and today's date will always give you the latest version - no need to check release history.

## Experimental API version

The special version `X` is used to test experimental changes. This version may change at any time in any way and is used at your own risk - not only will it be backward-incompatible, but it may also cause data loss and other problems.

When using a version that isn't considered completely stable, the API will return a warning message in the `X-Sanity-Warning` header.

## Client configuration and API upgrades

Clients should be configured with an explicit, static API version - see the [individual clients' documentation](/docs/client-libraries) for details on how to do this. When starting new projects, clients should typically be given today's UTC date to get the most recent bugfixes and improvements. Older clients which do not support versioning will default to `v1`, our initial and outdated API version.

> [!WARNING]
> Gotcha
> While it's tempting to use a dynamic date (e.g. the current date) as a version, this can be a risky idea. Using a static date, you pin your project to a specific version of the API, which prevents any sudden changes that can break your implementation.

To upgrade an application to a newer API version, first read our list of API changes to determine which (if any) modifications must be made to the application. Then, with a local instance of the application, set the newer API version for the client, make the necessary changes to the code, and then test the application either locally or in a staging environment. Once you are confident that the application correctly handles the new API version you can deploy it to production.

We recommend making multiple smaller upgrades rather than a single larger upgrade, to reduce the chance of anything breaking and make the job more manageable, but this is up to you to decide.

### Example client configuration with the JavaScript client

The [official Sanity JavaScript client](/docs/js-client) supports API versioning. In the following example, the client will use the most recent API version released on or before June 7, 2021.

> [!WARNING]
> Gotcha
> The apiVersion property of the JavaScript client is optional. If no value is provided, the client will issue a deprecation warning and default to using v1 of the API.

> [!WARNING]
> Gotcha
> When using the HTTP API, the version number is prefixed with the v character. In the JavaScript client, no prefix is needed.

```javascript
const {createClient} = require('@sanity/client')

const client = createClient({
  projectId: 'your-project-id',
  dataset: 'production',
  apiVersion: '2021-10-21', // use a UTC date string
  token: 'sanity-auth-token', // or leave blank for unauthenticated usage
  useCdn: true, // `false` if you want to ensure fresh data
})
```

## Deprecation and removal

The burden of supporting older versions of the API grows as our services evolve. At times we will have to deprecate and then remove certain older versions of the API. We will always give appropriate notice when this is necessary.

In addition to notices on our website and to your registered email address, these versions will receive a warning through the `X-Sanity-Warning` header and also be tagged as deprecated via an `X-Sanity-Deprecated: true` HTTP header. Once an API version is removed, all calls to that version will return errors with code `410`.

## Backward-compatible changes

We consider the following changes to be backward-compatible and may therefore introduce them retroactively in old API versions:

- Adding new object attributes in JSON responses (outside of documents or query results)
- Changing the order of object attributes in JSON responses
- Adding new `_`-prefixed metadata attributes to stored or modified documents
- Adding new functionality to GROQ, e.g., operators, functions, and data types
- Adding new, optional parameters to API calls
- Adding new HTTP headers in responses
- Adding new endpoints to the API, or new methods to existing API endpoints

Note that this list is not exhaustive.



# API CDN

Sanity Content Lake provides two APIs for content delivery:

2. `api.sanity.io`: the live, uncached API. This is the default and will always give you the freshest data, but requests will be slower because they need to reach our back end on every request. Requests are also more costly because they will trigger more computation on our servers.
2. `apicdn.sanity.io`: the CDN-distributed, cached API, an opt-in feature that will give you very fast responses to requests that have been cached. We encourage most users to use this API for their front ends if you expose your API to end users. You are better off using the live uncached API for static builds to get the latest version.

To use the API CDN, simply use `apicdn.sanity.io` instead of `api.sanity.io`. Most clients provide a `useCdn` option that makes this switch seamless.

> [!WARNING]
> Gotcha
> The API CDN supports /data/query for GROQ queries and /<version>/graphql for GraphQL queries.

> [!TIP]
> Protip
> Make sure to pick the right tool for your workload.
> 
> If you are going to fetch content from a browser, we recommend the API CDN so your requests can scale.
> 
> When building integrations with Sanity or responding to webhooks, we recommend using the API to capture the latest saved content.

## Cache Policy

The API CDN is primarily meant to cache query results for end users:

- We cache GET, HEAD, and OPTION requests.
- We cache POST for `/graphql` and `/data/query` as these endpoints are read-only.
- Maximum HTTP POST size is 300 KB.
- We reject all other POST requests since they can contain mutations.
- Responses larger than 10 MB are not cached.
- Non-200 responses are not cached.
- Cookies are ignored when identifying cache hits.
- We cache authenticated requests. Caching is segmented for each unique authentication token.
- Listeners, including the Live Content API, are redirected to the API and do not query cached content.

During periods of high content traffic (mutations or requests), we prioritize the cache invalidation queue to ensure consistent caching windows for customers with our High Frequency CDN.

If Sanity's Content Lake is unavailable, the API CDN will return the last cached content for up to two hours.

All official clients will automatically fall back to using the live API where appropriate.

> [!TIP]
> Protip
> Caches are based on the URLs, including query parameters and other URL fragments, of your requests. Optimize your performance by ensuring that your request URLs will be shared across your traffic and will benefit from caching.

## Locations

Sanity currently has CDNs for the API in these locations:

- Asia- Hong Kong
- Mumbai, India


- Oceania- Sydney, Australia


- Europe- Saint-Ghislain, Belgium


- South America- São Paulo, Brazil


- North America- Oregon, United States
- Iowa, United States
- Northern Virginia, United States



In addition, we utilize a short-lived global CDN in front of our own CDN. This extra CDN has points of presence in multiple locations on all continents. The short-lived global CDN does not cache private datasets or queries using the POST method.

## IP addresses in use

For environments where outgoing connectivity filtering restrictions apply (egress filtering), we recommend adding [the following IPs](https://storage.googleapis.com/sanity-customer-facing-ips/all-ips) to your network configuration to allow the Sanity Studio connectivity to work well for you.

The IP addresses will not change often, but we recommend that you use [the source file](https://storage.googleapis.com/sanity-customer-facing-ips/all-ips) to ensure you have the latest list.



# Datasets

A dataset is a collection of JSON documents that can be of different types and have references to each other. You can think of a dataset as a “database” where all of your content is stored, whereas the document‘s types would constitute “tables”. Using GROQ or GraphQL you can always query and join data across documents within a dataset, but not across them. Typical applications of datasets are:

- operate with different environments for testing, staging, and production
- localization and segmentation across all content types
- different purpose content, but with same user access and billing

```
https://<projectId>.api.sanity.io/v2021-06-07/data/query/<dataset>?query=*
```

You can also specify which dataset to use with the [client libraries](/docs/client-libraries) (configured when initializing a client) and [Sanity Studio](/docs/sanity-studio) (configured in [sanity.config.ts](/docs/studio/config-api-reference) or using [environment variables](/docs/studio/environment-variables)).



![Explainer: Projects, Users, Datasets](https://www.youtube.com/watch?v=hgMl5dofhoU)

## Dataset management

Datasets can be created and managed using the `sanity` [command-line tool](/docs/cli-reference/dataset), e.g. by running `sanity dataset create <name>` or `sanity dataset list`. To see all dataset-related subcommands, run `sanity dataset`. 

Datasets can also be created and deleted in the project's [management console](https://manage.sanity.io), under the "Datasets" tab.

A dataset name must be between 1 and 64 characters long. It may only contain lowercase characters (`a-z`), numbers (`0-9`), hyphens (`-`), and underscores (`_`), and must begin and end with a lowercase letter or number.

## Add-on datasets

Some features automatically create "add-on" datasets and pair them to your dataset. These are complimentary and don't count toward your plan's dataset limit.

![Comments add-on dataset for the production dataset](https://cdn.sanity.io/images/3do82whm/next/4958f8aaa9759681a7e3df864abc13d1c0fd1951-1790x474.png)

You can manage these as you would any other dataset. Learn more about [configuring comments and the comments dataset](/docs/studio/configuring-comments).

## Dataset migration

You can [export](/docs/http-reference/export) and [import](/docs/content-lake/importing-data) content to datasets, as well as performing [mutations](/docs/http-reference/mutation) and [patches](/docs/content-lake/http-patches) to documents in them.

## Advanced Dataset Management

_This is a paid feature, available on the Enterprise plan._

You can initiate dataset copying directly in the cloud and create aliases to hot swap between datasets without changing the underlying code for your project.

- [Full documentation for cloning datasets in the cloud](/docs/content-lake/how-to-use-cloud-clone-for-datasets)
- [Full documentation for hot swapping your datasets without changing your code ](/docs/content-lake/how-to-use-hot-swapping-for-datasets)



# Hot swap

_This is a paid feature, available on the Enterprise plan._

![Flow diagram showcasing a dataset being changed beneath a single Alias used by the frontend codebase](https://cdn.sanity.io/images/3do82whm/next/58a6d2df8dbca8ca674a181dacddb2cd6e668100-590x590.svg)

Dataset Hot Swapping allows a codebase to reference a single, named entity, which can then point to different datasets depending on the need of the project.

By utilizing a Dataset Alias in a codebase, the underlying dataset referenced can be swapped without having to copy or migrate data. When preparing a large content change or a new feature, the data can be prepared in a staging or feature dataset and then Hot Swapped behind the "production" Alias with no code or data changes when ready and approved.

> [!WARNING]
> Gotcha
> Dataset aliases are read-only references to the underlying dataset. Write actions will fail when run against an alias. For example, any Studio deployments must use the underlying dataset name to edit content.

## Using Dataset Hot Swapping

This enterprise feature is included in [the Sanity CLI](/docs/apis-and-sdks/cli). By running a series of commands, an alias can be created, linked, unlinked, or deleted.

```sh
# List all aliases and datasets
sanity dataset list

# Create a new alias
sanity dataset alias create <new-alias> <dataset-to-alias>

# Change what dataset an alias points to
sanity dataset alias link <alias-to-point> <dataset-to-point-to>

# Unlink an alias
sanity dataset alias unlink <alias-to-unlink>

# Delete an Alias
sanity dataset alias delete <alias-to-delete>

# List all aliases and datasets
sanity dataset list
```

## Creating a new alias 

### `sanity dataset alias create`

Before a dataset can be swapped, a new alias must be created and referenced by code. To create a new alias for use as a `production` alias run the following code:

```sh
# Creates a dataset alias and prompts for an alias name
sanity dataset alias create

# Creates an unlinked dataset alias named "production"
sanity dataset alias create production

# Creates a dataset alias named "production" linked to "productionDataset"
sanity dataset alias create production productionDataset
```

Anywhere the code references the `productionDataset` it can now be swapped for `~production`. 

In order to identify incoming alias requests from a client, an alias should be prefixed with the special character `~`. In this example, even though the `create` command was given an alias name of `production`, when referencing it in requests, the string `~production` should be used.

> [!WARNING]
> Gotcha
> When creating a new alias, the name should not include the ~ character. This special character is only used to reference the alias in your code.

```javascript
// Example from sanity.js file
// This code creates a connection to the datastore
// and connects to productionDataset through the production alias

import {createClient} from '@sanity/client'

const client = createClient({
  projectId: 'abc123',
  dataset: '~production',
  apiVersion: '2021-03-25',
  useCdn: false,
})
```

## Hot-swapping the dataset behind an alias 

### `sanity dataset alias link`

Once an alias is created, the dataset behind it can be hot-swapped at any time. In this example, the codebase references the `~production` alias. When a new release is ready, a dataset of `featureDataset` can be created via [Cloud Clone](/docs/content-lake/how-to-use-cloud-clone-for-datasets) from the current dataset behind the `production` alias. Any data changes can be made, then the `production` alias can be Hot Swapped to the new `featureDataset`.

```sh
# Hot swap the production Alias to point to the featureDataset
sanity dataset alias link production featureDataset
```

> [!TIP]
> Protip
> A single dataset can be linked to multiple aliases. When the dataset in this example is linked to the production alias, it can still be linked to a staging or development alias, as well.

## Unlink an alias 

### `sanity dataset alias unlink`

When an alias is not currently being used, but should be included in the alias list, it can be unlinked from a dataset.

```sh
# Unlinks the development alias.
# The development alias will still appear 
# in the sanity dataset alias link command
sanity dataset alias unlink development
```

## Delete an alias

### `sanity dataset alias delete`

When an alias is no longer needed, it can be deleted from the alias list. The `delete` command only deletes the alias and not any datasets associated with it.

```sh
# Deletes the outdated alias
sanity dataset alias delete outdated
```

## List all aliases and datasets on a project

### `sanity dataset list`

The `list` command provides a list of all Datasets and aliases associated with the current project and shows the linked dataset for each alias.

```sh
# Returns a list of datasets and aliases with their associated datasets
sanity dataset list
```



# Cloud clone

> [!NOTE]
> Enterprise Feature
> This feature is part of our Advanced Dataset Management offering on the enterprise plan. Contact us if you need this feature and want to discuss this plan.

Cloud Clone provides a more efficient way of duplicating datasets and is ideal for situations when:

- you want to run tests against real production data in a CI flow
- you regularly copy datasets from production for developing new features

Instead of [exporting](/docs/cli-reference/cli-config) and [importing](/docs/content-lake/importing-data) a dataset with the CLI, you can have that process happen inside of Sanity's infrastructure which will be more efficient and reliable.

There are two methods of initiating and monitoring the cloning of datasets in the cloud: through [the Sanity CLI](/docs/apis-and-sdks/cli) or with [the HTTP API](/docs/copy).

Once a copy is successful, the new dataset will appear in [Manage](https://www.sanity.io/manage). Depending on the size of a dataset this may take hours, so we encourage monitoring the outcome of a copy using the `jobId`, discussed below.

## Copying a dataset with the CLI

The quickest way to begin developing with a freshly-copied dataset is to use the CLI. 

> [!WARNING]
> Gotcha
> As with other project-specific CLI commands, this command will only work from within a configured Sanity project.

By default, the CLI command runs the copy synchronously. If you don't want to wait for the process to be completed, you can use the `--detach` flag to skip the progress. It will log a job ID that you can use to watch the progress again with the `--attach <jobId>` flag.

Depending on the size of the dataset, skipping document history with the `--skip-history` flag can make the copy process significantly faster. In cases where document history is not important in the target dataset, this may be a flag worth considering.

```sh
# Syntax:
# sanity dataset copy
# sanity dataset copy <source-dataset>
# sanity dataset copy <source-dataset> <target-dataset>

# This command will ask for which dataset to copy and what to call the new dataset
sanity dataset copy

# This command will copy the production dataset and request a name for the new dataset
sanity dataset copy production

# This command will copy the production dataset into a new dataset named new-feature
sanity dataset copy production new-feature

# This command will initiate the copy between production and new-feature
# It will run in the background and not display progress while it works
sanity dataset copy production new-feature --detach

# This command will initiate the copy between production and new-feature
# It does not copy document history, speeding the copy action 
# at the expense of the history retention
sanity dataset copy production new-feature --skip-history
```

> [!WARNING]
> Gotcha
> This process creates a new dataset given the specified name. If a dataset already exists with that name—or if a copy job is in progress and a copy is re-attempted using the same dataset name—the command will throw an error Target dataset <name> already exists.

It's encouraged to use this feature instead of exporting/importing your data to another dataset. In most cases, this will be a faster method. On large datasets or datasets with a large number of assets and/or large assets, the process will take some time to complete.

## Copying a dataset with the HTTP API

If you'd prefer to use the HTTP API instead of the CLI, there are API endpoints for copying datasets and for monitoring copy completion.

`PUT /v2021-06-07/projects/:projectId/datasets/:datasetName/copy`

In order to start a copy, a PUT request is sent to the specific dataset's `/copy` endpoint.

```text
https://api.sanity.io/v2021-06-07/projects/<project-id>/datasets/<dataset-name>/copy
```

The request needs to be authorized via a Bearer token, which can be generated from the [Manage dashboard](https://www.sanity.io/manage).

The body of the request must be an object containing the following fields:

1. `targetDataset`: Property to name the new dataset. The value must be consistent with [dataset name requirements](https://www.sanity.io/docs/datasets#881c6960c8a4).

2. `skipHistory`: Boolean property which allows skipping document history while copying the dataset. It potentially reduces copying duration on datasets with large amount of edit history. Check the [retention period](/docs/history-experience#1d9c1651cd13) to know how long a dataset's history is kept for.

```json
{
    "targetDataset": "production-copy",
    "skipHistory": true
}
```

### The full request

```text
curl --location --request PUT 'https://api.sanity.io/v2021-06-07/projects/<project-id>/datasets/<dataset-name>/copy' \
  -H 'Authorization: Bearer <token-here>' \
  -H 'Content-Type: application/json' \
  --data-raw '{
    "targetDataset": "production-copy",
    "skipHistory": true
  }'
```

### The JSON response

```json
{
    "datasetName": "production",
    "message": "Starting copying dataset production to production-copy...",
    "aclMode": "public",
    "jobId": "jobIdString"
}
```

> [!WARNING]
> Gotcha
> When copying a dataset, documents‘ _createdAt and _updatedAt date time fields in the target dataset will remain the same as documents in the source dataset.

## Getting the current status of a copy

`GET /v2021-06-07/jobs/:jobId`

When you run a copy via the HTTP API, you'll receive a Job ID. This ID can be used to query the status of the clone job.

### The Full Request

```text
curl --location --request GET 'https://api.sanity.io/v2021-06-07/jobs/<jobid>' \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer <token here>'
```

### The JSON response

```json
// Running
{
    "id": "jacsfsmnxp",
    "state": "running",
    "authors": [
        "authorId"
    ],
    "created_at": "2020-11-09T17:34:28.071123Z",
    "updated_at": "2020-11-09T17:34:28.144826Z"
}

// Completed
{
    "id": "jarrwsdptf",
    "state": "completed",
    "authors": [
        "authorId"
    ],
    "created_at": "2020-11-09T17:07:41.304227Z",
    "updated_at": "2020-11-09T17:08:30.457692Z"
}

```

## Listening for copy status

`GET /v2021-06-07/jobs/:jobId/listen`

Each job has a `/listen` endpoint to allow you to monitor its status programmatically. Much like the static status endpoint, this endpoint accepts the Job ID that is returned by starting a copy action.

### The full request

```text
curl --location --request GET 'https://api.sanity.io/v2021-06-07/jobs/<jobid>/listen' \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer <token here>'
```

### The response

While listening, event data will be sent back at intervals providing updates on the status of your copy. The response contains the event name as well as a JSON object containing information about the current status of the copy.

```json
event: welcome
data: {"listener_id": "ladaicdbdo"}

event: job
data: {"job_id":"jacsfsmnxp","state":"running","progress":60}

event: job
data: {"job_id":"jacsfsmnxp","state":"running","progress":80}

event: job
data: {"job_id":"jacsfsmnxp","state":"completed","progress":100}
```





# Backups

Sanity offers a backup feature that provides a robust solution for disaster recovery and content history auditing, ensuring your data's safety and integrity. With the ability to restore your production environment seamlessly and to inspect historical data states, this feature is a powerful tool for maintaining data continuity and compliance.

_This is a paid feature, available on the Enterprise plan._

[Backup dataset with CLI](/docs/cli-reference/backup)

[Export dataset with CLI](/docs/cli-reference/dataset)

[Dataset Cloud Clone](/docs/content-lake/how-to-use-cloud-clone-for-datasets)

[Import dataset with CLI](/docs/content-lake/importing-data)



## What is a backup?

A “backup” in the context of this article is an archived snapshot of the state of your dataset (documents and assets) at a specific time. You can use it to audit the history of your content or roll it back to a known safe state in case of data loss or unintended changes.

Each backup contains all documents and assets from your dataset in their state from when the backup job ran. This includes hidden documents used for settings and configuration by the studio and installed plugins. [Comments](/docs/studio/comments) and [document history](/docs/http-reference/history) (the timeline shown when you select **Review changes** in the studio) are not included in the backup.

Once you enable the backup service (as described in the next section of this article) Sanity will perform a backup of your dataset daily.

> [!WARNING]
> Gotcha
> The backup service runs at set regular intervals, so your initial backup may take up to 24 hours to become available.

Your backups are managed by Sanity in an offsite third-party storage location for data redundancy and security. Daily backups are stored for 365 days. Weekly backups are stored for an additional 2 years on top of the 1 year of daily backups.

When you download a backup, the resulting file contains an archive with all your documents exported into a single [NDJSON](https://github.com/ndjson/ndjson-spec) file alongside your files and images in separate folders, neatly collected into a single gzip-compressed archive file with a `.tar.gz` file type - colloquially known as a “tarball.”

```bash
production-backup-2024-02-23-a9bfa2d7-9ba1-42cc-beb2-f9f448bec656/
├── data.ndjson
├── files
│   └── file.txt
└── images
    └── image.png
```

## Enabling and disabling backups

Enabling and disabling the backup service is mainly done with the Sanity CLI.

### Prerequisites

- The relevant project is on a supported plan
- The backup feature is enabled for the project
- The Sanity CLI is up to date (v3.31.0 or later is required)
- The user has administrator permissions for the project

### Enable backups

To enable backups for a dataset, use the `sanity backup enable` CLI command in your project folder:

```sh
sanity backup enable [DATASET_NAME]
```

You should see a confirmation message in your CLI, and your first backup should be available within 24 hours.

### Disable backups

To disable backups for a dataset, use the `sanity backup disable` CLI command in your project folder:

```sh
sanity backup disable [DATASET_NAME]
```

No further backups will be scheduled. Your existing backups will continue to exist and be available for downloading.

## Listing available backups

To list all available backups for a dataset, use the following CLI command in your project folder:

```sh
sanity backup list [DATASET_NAME]
```

Running this command will list the available backups for the dataset in question.

```bash
┌──────────┬─────────────────────┬─────────────────────────────────────────────────┐
│ RESOURCE │ CREATED AT          │ BACKUP ID                                       │
├──────────┼─────────────────────┼─────────────────────────────────────────────────┤
│ Dataset  │ 2024-02-21 16:57:34 │ 2024-02-21-cf51334d-4caa-4487-a746-75a49b078e82 │
│ Dataset  │ 2024-02-22 02:40:30 │ 2024-02-22-c66adb69-cbed-4e4f-88a2-f97b5feeb464 │
│ Dataset  │ 2024-02-23 01:43:28 │ 2024-02-23-e1b1dcd3-fa9b-45a2-ab58-9f1c1eae45c7 │
└──────────┴─────────────────────┴─────────────────────────────────────────────────┘
```

By default, this command will list the 30 most recent backups. You can use the `limit` parameter to increase the listing threshold to a maximum of 100, or you can use the `after` and `before` parameters to target a specific time period from which to list backups.

```sh
sanity backup list production --after 2024-01-31 --before 2024-01-10 --limit 10
```



## Downloading backups

To download a specific backup for a dataset, use the following CLI command in your project folder:

```sh
sanity backup download [DATASETNAME] --backup-id [BACKUPID] --out [FILE_NAME]
```

`[BACKUP_ID]` needs to match the ID of an existing backup, and `[FILE_NAME]` should be a valid file name for the resulting downloaded file. A more realistic example is shown below.

```sh
sanity backup download production --backup-id 2024-02-23-a9bfa2d7-9ba1-42cc-beb2-f9f448bec656 --out backup_2024.tar.gz
```

If you don’t specify the file name in the `--out` flag, the backup file will follow the `[dataset name]-backup-[backup ID].tar.gz` convention.

To learn about all the options for this command, refer to the CLI reference article, or run `sanity backup download --help` in the CLI.

## Restoring from a backup

You can use the `sanity dataset import` CLI command to restore from a downloaded backup.

```sh
sanity dataset import ~/Downloads/backup_2024.tar.gz production
```



> [!WARNING]
> Gotcha
> If you are importing a backup into a different dataset than the one the backup originated from, you will have to use the --allow-assets-in-different-dataset option on import. Read about this and other parameters and options available for this command in the relevant CLI reference article, or by running sanity dataset import --help in the CLI.

Downloaded backups are structured to be ready for importing, both in their original compressed file state and in their decompressed file structure.

## Deleted datasets

If you delete a dataset then no new backups will be created. Any existing backups will continue to be accessible. If you later create a new dataset with the same name then:

47. You will need to actively enable backups for this new dataset to start the backup service up again.
47. Backups for the older, deleted, dataset will no longer be accessible directly through the CLI. Contact support if you require them.

### Conclusion

The backup feature from Sanity offers a straight-forward and easy to use solution for securing your content, providing data redundancy, means of compliance, and peace of mind. Contact your account manager to have backups enabled for your enterprise project, or visit our [pricing page](https://www.sanity.io/pricing), to learn more about Sanity's enterprise plan offerings.



# IDs and Paths

## IDs

Every document in a Sanity dataset must have an ID that identifies it, an arbitrary string of maximum 128 characters made up of the characters `a-zA-Z0-9._-`.  Note that an ID cannot start with a `-` (dash) character, and must not have more than one consecutive `.` character. E.g. `-abcde-12345` would be an invalid ID, as would -`a..bcde-12345`.

The ID is specified in the document’s `_id` property, and must be unique within the dataset. The ID cannot be modified once a document is created, since it is used to track the document’s history and relations.

The Sanity Studio automatically generates a random [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier) for new documents (e.g. `189bc292-e41b-42a0-91b5-bfaa33a34af2`), and does not allow you to specify an ID yourself.

> [!WARNING]
> Gotcha
> For technical reasons, every document ID ever written to a dataset will be retained in our systems until the dataset is deleted, even if the document itself is deleted. For this reason, we strongly recommend you never put personal data or other sensitive data in document IDs.

IDs with multiple segments separated by periods must not include the segment `versions` unless it is the first segment. When the first segment is `versions` the ID must have at least three segments. For example, `versions.abc.xyz` is permitted, but `versions.abc` and  `abc.versions.bar` are not. These restrictions are due to internal implementation requirements.

> [!WARNING]
> Gotcha
> We advise against using our APIs to create document IDs prefixed with drafts. or versions. as these are used internally. Such documents may react with platform functionality in unexpected ways. 
> See the chapter on Using custom IDs for more.



## Paths

IDs are also considered *paths*, separated by periods. 

Sanity also uses paths for storing various internal data in your datasets. For example, internal objects like groups are stored under the `_.` path, and the content studio stores draft documents under the `drafts.` path and Content Release version documents under the `versions.` path.

GROQ provides a `path()` function that allows you to filter documents by path, such as fetching all drafts with `_id in path("drafts.*")` or fetching all versions with `_id in path("versions.**")`. In path expressions, `*` is taken to mean “anything up to the next period”, while `**` means “anything including periods”. The `path()` function currently only works with the `_id` attribute, since it requires special indexing.

To work with drafts, versions, and document ID paths we recommend using the [@sanity/id-utils](https://github.com/sanity-io/id-utils) helper library.

> [!WARNING]
> Gotcha
> The default, fixed access control rules give unauthenticated users read access to documents under the root path only, which means that it is not possible to make documents under a sub-path (i.e. containing a . in the ID) publicly available. 

## Using custom IDs

The Content Lake and Studio automatically generate unique identifiers for documents. These system-generated `_id` values are designed to ensure consistency and prevent conflicts across your dataset. 
There is **no way to override the default ID logic**, but you can theoretically set custom ID strings for documents created via our APIs or the client.

While it might be tempting to create custom IDs for documents, we recommend a more flexible approach, which offers several key advantages and generally scales better.

**Instead of attempting to override the native ID system, create a custom field in your schema to store your preferred identifier. **

```typescript
// product.ts
import { defineType, defineField } from 'sanity'

export default defineType({
  name: 'product',
  title: 'Product',
  type: 'document',
  fields: [
    defineField({
      name: 'customId',
      title: 'Custom Identifier',
      type: 'string',
      initialValue: () => yourCustomIdGenerator()
    })
  ]
})
```

By following these guidelines, you can effectively manage document identifiers while maintaining flexibility and adhering to Sanity.io's best practices.

> [!NOTE]
> Any document ID containing a dot is considered private and has restricted accessibility
> All documents that contain a . in their _id can only be accessed when a user is logged in or a valid authentication token is provided for client and HTTP API calls (minimum read permission required).
> 
> The root path (also known as the published ID) is accessible without authentication, while all subpaths are private (such as drafts drafts.<publishedId> or releases versions.<release-name>.<publishedId>).

### Generating Sanity UUIDs

If you need to generate IDs in a script or function which are compatible with Sanity's system, you can use `uuid()` from the  `@sanity/uuid` package.

```typescript
import {uuid} from '@sanity/uuid'

// Generate a unique ID compatible with Sanity's system
const newDocumentId = uuid()
```





# Drafts and versions

In Sanity Studio, when you create a new document or edit one that has already been published, a *draft document* is created. Drafts capture in-flight updates while the original published document remains intact. This enables keeping changes separated from what is presented to users until those changes are ready to be explicitly rolled-out.

A draft document does not appear on the APIs to unauthenticated users. While you may refer to it as a reference, you can only publish a document that references a draft if that reference field is a weak reference.

When you publish a document it becomes available on the public APIs and you may (strongly) reference it from other documents.

When you start working on a published document a new draft gets created. This creates a new event in the [document history](/docs/user-guides/history-experience). You can access the document history from the context menu:

![Screenshot from Sanity Studio](https://cdn.sanity.io/images/3do82whm/next/bd2bcff0a9e3b951ce43206a3965dfdd965a2bca-658x335.png)

## Behind the scenes

Drafts are saved in a document with an id beginning with the path `drafts.`. When you publish a document it is copied from the draft into a document without the `drafts.`-prefix (e.g. `drafts.ca307fc7-4413-42dc-8e38-2ee09ab6fb3d` vs `ca307fc7-4413-42dc-8e38-2ee09ab6fb3d`). When you keep working a new draft is created and kept read protected in the drafts document until you publish again.

#### Timestamps

The published and draft document both have `_createdAt` and `_updatedAt` fields.

- `_createdAt` is the same value for **both** and reflects the time when the document was first created.
- `_updatedAt` on the **draft** is the time of when it was last edited
- `_updatedAt` on the **published** is the time when it last  got published

### A matter of perspectives

When querying your content from a frontend it's common to face a situation where you are interested in *either* drafted changes *or* published content, and specifically *not* both at the same time. You can use Content Lake's [Perspectives](/docs/content-lake/perspectives) feature to have your queries return with all in-flight changes applied – useful for previewing – or with all changes ignored entirely – useful for production deployments. Visit the article [Presenting and Previewing Content](/docs/content-lake/presenting-and-previewing-content) to learn more about how Perspectives can be used in your presentation layers.

## Disable draft documents

Sometimes you might not need drafts at all. Say you're using listeners to update a ranking list in an app and you just want the changes in the studio to go out in real-time on the wire as mutations as the order is being changed.

To disable drafts for a data type simply include `liveEdit: true` in the schema definition:

```javascript
export default {
  name: 'author',
  title: 'Author',
  type: 'document',
  liveEdit: true,
  …
  

```

> [!NOTE]
> Live Edit differs from the Live Content API
> Live Edit is the "published only" mode where drafts are disabled. It doesn't change how rendering works in your apps, but rather how Sanity Studio handles edits. Your applications and front ends need to render these changes as they happen. The Live Content API tooling will work out of the box with live mode, or you can rely on traditional rendering modes to serve changes on new visits or refreshes.



# Attribute limit

## What is the attribute limit?

The attribute limit determines how many unique combinations of attribute and datatype you can have in your dataset. Depending on what plan your project is on, your limit is one of the following:

- Free: 2k attributes
- Growth: 10k attributes
- Enterprise: Custom # attributes

> [!WARNING]
> Gotcha
> The attribute limit is a hard technical limit right now. For this reason, we do not currently offer a pay-as-you-go option for extra attributes.

## What counts as an attribute?

As shown above, an attribute is officially defined as *a unique combination of attribute and datatype*. An alternative way to think about them is as the different paths through your content.

Let's take a basic data structure:

```json
{
  "foo": [
    {
      "bar":…,
      "baz":…
    },
    {
      "bar":…,
      "baz":…
    },
    {
      "bat": {
        "bar":…
      }
    }
  ]
}
```

This structure contains six unique paths or attributes:

10. foo -> an array
10. foo[] -> an object
10. foo[].bar -> a string
10. foo[].baz -> a string
10. foo[].bat -> an object
10. foo[].bat.bar -> a string

Paths only count towards your attribute limit when they hold actual content. Solely changing your schema definitions will not affect the attribute count. Schema definitions define the structure of your content, a bit like a blueprint defines the structure of a building. Until you add or remove content using the Sanity Studio or the HTTP API, your attribute count will remain unchanged.

Each unique path is counted once, no matter how often it is used. Removing a path from your attribute count requires deleting every piece of content on that path across all documents.

In short, your attribute count:

- goes up when you first add content on a path
- goes down when a path no longer holds any content
- stays the same regardless of whether a path is used once or many times

## Best practices

When structuring your content, there are a few pitfalls to keep in mind to avoid hitting the attribute limit. Although this is not an exhaustive list, following the best practices below should go a long way in keeping your attribute count in check.

### Use arrays for page building

A common use case for Sanity is using structured content for [page building](https://www.sanity.io/guides/how-to-use-structured-content-for-page-building). In setting up a page builder, it may be tempting to use the block content type as the editor gives a lot of flexibility and allows adding any number of custom objects that can then be used inline.

However, a block content field has quite an extensive data structure by default:
• a `blockContent` array, with inside of it:
• `blocks` objects, with inside of them:
• `markDefs` and `children` arrays, with inside of them:
• `span` types, with inside of them:
• a `marks` array and a `text` field

This nested structure is further extended by any custom types you add to it, all with their own unique paths. A block content field with many custom objects may therefore lead to a hefty amount of attributes.

Another issue with this approach is that people sometimes want to use block content fields *inside* of custom objects. This is likely to lead to even more attributes as a result of now having the above structure embedded in the same structure. Moreover, when the exact same block content component is used, allowing this type of nesting basically gives editors the freedom to nest to an arbitrarily deep level, which can then drag a project over the attribute limit.

To avoid any of these challenges and keep the attribute count as low as possible, we recommend using arrays for page building. In addition to fewer attributes, greater control over the exact content structure, and reduced risk of getting into nesting situations, this approach has the added advantage of not having to deal with serializers for complex custom objects. 

### Avoid excessive nesting and recursive data structures

Things get worse when subsequently the same block content configuration is used for any block content fields inside the custom objects, so editors can endlessly nest the entire page builder inside itself.

### Focus on meaning, not presentation

Before responsive web made its entrance and people started optimizing for different devices, it was customary to mix content with presentation. A headline could be blue, have font size 24px, line-height 30px, and a bottom padding of 10px. Although it may still be tempting today to offer that same level of control to editors, there are several downsides to this approach. For one, whenever you want to change your front-end's design, editors will have to review all relevant content.

Most importantly for this guide, adding all these presentational attributes is likely to boost your attribute count significantly as they would exist for nearly every piece of content.

Instead of mimicking CSS properties in your schema definitions, we recommend a separation of concerns. Leave the presentational aspects to wherever you implement your content and instead stick to semantics in your content structure - in other words, focus on the *meaning* of your content.

### Beware of multipliers in translation/localization

There is a variety of i18n/l10n approaches out there, some of which have a greater impact on your attribute count than others. For example, one approach suggests wrapping all your fields inside a language object, so you get the following structure:

```json
{
 "de": {
  ...
 }
 "en": {
  ...
 }
}
```

This basically multiplies the number of attributes by the number of languages added, as all fields get duplicated on a language path. Adding more than a few languages this way means trouble.

Instead of duplicating the fields inside a document, thereby creating all these extra paths, a more frugal approach is to duplicate the *document* instead. To differentiate between the different languages and more easily query for them, you can consider adding a (hidden) internationalization field to your document type and/or add the language to the document ID. As you will be reusing the same fields across different documents, adding an extra language no longer affects your attribute count at all.

## What to do if you hit the limit?

If you inadvertently hit the attribute limit on one of your datasets, you will see the following error when opening your Sanity Studio: `Total attribute count exceeds limit`.



### Export your data

Before deleting any content or changing your data structure, we highly recommend running a full export of your dataset to prevent any unintended data loss. To do so, you can run the [dataset export](https://www.sanity.io/docs/migrating-data#c4665bde1f66) command in your terminal. For example:

`sanity dataset export production production.tar.gz`

### Get unblocked

The first step after exporting your data is to get unblocked so you and other users on your project can work in the studio again. In other words, the challenge is to get back below the attribute limit.

Perhaps there is a heavily nested structure with block content *and* translations that could be optimised. Or maybe you have singletons for different pages that could be folded into a single page type instead to further reduce the number of unique paths.

A final note is that it also helps to remove any unused content from schema revisions. For example, if you used to have a particular document type with a bunch of documents, but later removed that type, or even some fields within a type, make sure to clean up the content so there are no leftovers in the datastore that will count towards the attribute limit.

### Restructure your content

How to restructure your content depends on your content model and is therefore different per project. However, there are a bunch of examples to get you started. Please note that in all cases, it is highly recommended to run a full dataset export *before *proceeding. 

### Track your progress

To keep an eye on your attribute limit while restructuring your content, you can use this URL: `https://<projectId>.api.sanity.io/v1/data/stats/<datasetName>`

The attribute count is the value of `fields.count.value` and the limit is inside `fields.count.limit`.

## Closing remarks

Although this guide was specifically about the attribute limit, the principles outlined above are best practices that are likely to lead to a more solid, flexible, and future-proof content model in any situation.



# Perspectives

> [!NOTE]
> Note on data privacy, drafts, and authentication
> Sanity offers a range of tools for granularly managing access to your content. The rest of this article presumes that all requests are made from an authenticated client with permissions to see both drafts and published content. 
> 
> You can learn more about data security, the drafts model, or how Sanity limits access to content in public datasets using IDs and paths at the following destinations.
> 
> Security overview - Keep your data safe and access it securely
> 
> Drafts - How they work and how to disable them
> 
> IDs and Paths - How document IDs work

The Perspectives feature allows you to query your datasets from a different viewpoint with minimal configuration. You can use the `drafts` perspective to treat all drafts as published, a perspective stack of release IDs to view a custom perspective with [Content Releases](/docs/studio/content-releases-configuration), or the `published` perspective to exclude all unpublished changes from your results. The `raw` perspective will returns all drafts, versions, and published content side by side for authenticated requests.

```typescript
// Example JS/TS client configuration
import {createClient} from '@sanity/client'

const client = createClient({
  ...config,
  perspective: 'published', // 'raw' | 'drafts' | 'published' | ['release-id1', 'release-id2']
})
```

[Introducing Perspectives: See your content from any angle](https://www.sanity.io/blog/introducing-perspectives-sanity-previews)

[Configuring Perspectives using the Sanity JS/TS client](https://github.com/sanity-io/client#using-perspectives)



> [!WARNING]
> Gotcha
> With the release of API version 2025-02-19, the default perspective changed from raw to published.

## Look at your content from a different point of view

Core to the idea of composable, structured content is the ability to weave content from any number of independent but interconnected documents into whatever shape is required on the consuming end. Sanity’s Content Lake lets you write queries that can filter, combine, merge, expand references, and apply transformations to the original content in your dataset.

This flexibility also means that often your query results will be made up of bits and pieces from a multitude of source documents and that sometimes previewing what your app, website, or experience will look like after you hit that publish button can become complicated and cumbersome. Previewing content changes before committing to production in an environment that is as realistic as possible is vital to a smooth editorial experience.

### Using the `drafts` and `published` perspectives

Perspectives allows your GROQ queries to run against an alternate view of the content in your dataset – very similar to how “views” work in traditional databases. The perspective is set as an additional parameter in the client config or API call so that your queries can remain identical between different implementations, such as a preview and production deployment. In addition to the default perspective, which can be explicitly set by using `perspective: 'raw'`, two new built-in perspectives are initially available:

- The `drafts` perspective, in which queries return your content “as if” all draft documents (i.e., unpublished changes in Studio) were published
- The `published` perspective, in which queries return your content “as if” no in-flight unpublished changes existed

> [!TIP]
> Protip
> The drafts perspective used to be called previewDrafts. They both work, but if you're using the latest APIs you should transition to drafts. You may see both mentioned throughout the documentation.

Requesting either of these alternative perspectives is a matter of adding one line to your [client configuration](https://github.com/sanity-io/client#using-perspectives) or passing a URL parameter if you’re using the [HTTP API](/docs/http-reference/query). 

```typescript
// Example JS/TS client configuration
import {createClient} from '@sanity/client'

const client = createClient({
  ...config,
	useCdn: false, // must be false when using 'drafts'
  perspective: 'drafts', // 'raw' | 'drafts' | 'published'
})
```

```
// Example using HTTP API
/data/query/production?query=*[]&perspective=drafts
```

> [!WARNING]
> Gotcha
> Queries using the drafts perspective are not cached in the CDN, and will return an error if useCdn is not set to false. You should always explicitly set useCdn to false when using drafts!

## Example output from different perspectives

Let’s look at a very minimal example dataset with different perspectives applied.

### `raw`

```typescript
import {createClient} from '@sanity/client'

const client = createClient({
  ...config,
  perspective: 'raw', // default value, optional 
})

const authors = await client.fetch('*[_type == "author"]')
```

Making our initial query with the default `raw` perspective (explicitly set in this example but can safely be omitted) reveals that we are looking at a dataset of authors that contains the following documents:

- a published document (Ursula Le Guin)- with unpublished changes in a corresponding draft document (Ursula K. Le Guin)


- a draft document that has never been published (Stephen King)
- a published document with no pending changes (Terry Pratchett)

```json
[
  {
    "_type": "author",
    "_id": "ecfef291-60f0-4609-bbfc-263d11a48c43",
    "name": "Ursula Le Guin"
  },
  {
    "_type": "author",
    "_id": "drafts.ecfef291-60f0-4609-bbfc-263d11a48c43",
    "name": "Ursula K. Le Guin"
  },
  {
    "_type": "author",
    "_id": "drafts.f4898efe-92c4-4dc0-9c8c-f7480aef17e2",
    "name": "Stephen King"
  },
  {
    "_type": "author",
    "_id": "6b3792d2-a9e8-4c79-9982-c7e89f2d1e75",
    "name": "Terry Pratchett"
  }
]
```

### `published`

```typescript
import {createClient} from '@sanity/client'

const client = createClient({
  ...config,
  perspective: 'published',
})

const authors = await client.fetch('*[_type == "author"]')
```

Running the same query with the `published` perspective specified yields a result where all drafted changes and unpublished documents have been excluded. This perspective is useful for ensuring that unpublished content never ends up in a production deployment.

```json
[
  {
    "_type": "author",
    "_id": "ecfef291-60f0-4609-bbfc-263d11a48c43",
    "name": "Ursula Le Guin"
  },
  {
    "_type": "author",
    "_id": "6b3792d2-a9e8-4c79-9982-c7e89f2d1e75",
    "name": "Terry Pratchett"
  }
]
```

### `drafts`

```typescript
import {createClient} from '@sanity/client'

const client = createClient({
  ...config,
	useCdn: false, // must be false, required for this perspective
  perspective: 'drafts',
})

const authors = await client.fetch('*[_type == "author"]')
```

Viewed through the `drafts` perspective, our content is returned with all drafts applied. Documents are deduped in favor of the draft version, and unpublished draft documents are returned as if published. Note also that each document now has an `_originalId` property which identifies its origin.

```json
[
  {
    "_type": "author",
    "_id": "ecfef291-60f0-4609-bbfc-263d11a48c43",
    "_originalId": "drafts.ecfef291-60f0-4609-bbfc-263d11a48c43",
    "name": "Ursula K. Le Guin"
  },
  {
    "_type": "author",
    "_id": "f4898efe-92c4-4dc0-9c8c-f7480aef17e2",
    "_originalId": "drafts.f4898efe-92c4-4dc0-9c8c-f7480aef17e2",
    "name": "Stephen King"
  },
  {
    "_type": "author",
    "_id": "6b3792d2-a9e8-4c79-9982-c7e89f2d1e75",
	  "_originalId": "6b3792d2-a9e8-4c79-9982-c7e89f2d1e75",
    "name": "Terry Pratchett"
  }
]
```

[Presenting and previewing content](/docs/content-lake/presenting-and-previewing-content)



## Perspective layers

The Content Releases feature introduces the concept of the perspective laying. This allows you to create a custom perspective containing documents from multiple Content Releases, as well as published and even draft content.

To create a custom layered perspective, pass a list of release names anywhere you would normally set the perspective. For example, in the client it looks like this:

```typescript
// Example JS/TS client configuration
import {createClient} from '@sanity/client'

const client = createClient({
  ...config,
  useCdn: false, // must be false when using preview content
  perspective: ['release-a', 'release-b', 'release-c'], 
})
```

Layers are prioritized from left to right. In the example above, updates in release "a" will override release "b", updates in release "b" will override release "c", and so on. The published perspective is automatically added to the end of the list. 

In this diagram, you can see releases mixing with draft and published content. Note that while the published perspective is automatically added to the end of a stack, the drafts perspective is not.

![Diagram showing documents overriding one another based on perspective order.](https://cdn.sanity.io/images/3do82whm/next/22b1e851041ed4cb3a3663f326d375828fdc3ac6-794x1273.png)

For more on using perspective with releases, see the [Content Releases API documentation](/docs/apis-and-sdks/content-releases-api).



# Introduction

The idea behind our query language GROQ (Graph-Relational Object Queries) is to be able to describe exactly what information your application needs, potentially joining together information from several sets of documents, then stitching together a very specific response with only the exact fields you need.

If you need help setting up a client to perform these queries in your front end, you should check out the documentation for the client for [JavaScript](/docs/js-client) or [PHP](/docs/php-client). You can also check out the [GROQ Arcade](https://groq.dev) if you want to query any JSON source and get familiar with the language.

## Introduction

Let us start with the basics. We will take this simple query and pick it apart:

```groq
*[_type == 'movie' && releaseYear >= 1979] 

```

A query typically starts with `*`. This asterisk represents every document in your dataset. To do any useful work this is typically followed by a *filter* in brackets. The filter above has two terms:

### The filter

First, we filter by document type. Every document in Sanity is required to have a type, and the type is always in the `_type` field. (We prefix any Sanity-specific fields with an underscore in an attempt to avoid clashing with any of *your *field names.) So `_type == 'movie'` limits this query to documents of the type ‘movie’. `&&` is the operator “and”.

The second term `releaseYear >= 1979` assumes that the movies have a field called `releaseYear` that contains numbers. It will match any document where this number is larger than or equal to 1979.

### Projections

So if we run this query, the result will be an array containing all movies from the year 1979 onwards in the dataset. Nice! However in a typical application movies might be huge documents containing information on actors, staff, posters, tag-lines, show-times, ratings, and whatnot. If our goal is to render a list of movies in an overview, we are wasting bandwidth. *Projections* to the rescue.

The typical projection is wrapped in braces and describes the data we want to see for each movie. A nice and simple projection for this query would give us the id, title, and release year for each movie. It could look like this: `{_id, title, releaseYear}`. Putting it all together:

```groq
*[_type == 'movie' && releaseYear >= 1979]{ _id, title, releaseYear } 
```

### Basic sorting

Now there is another problem. Our movies appear in some unspecified order. Let’s say we want to sort our movies by year. For this, we use the `order`-function. Order takes a number of fields and sort directions and orders your documents accordingly. We wanted to sort our movies by `releaseYear`. This is easily accomplished with `order(releaseYear)`, like this:

```groq
*[_type == 'movie' && releaseYear >= 1979] | order(releaseYear) {
  _id, title, releaseYear 
} 

```

*(We need the *`|`* operator here in front of the order()-function, we'll discuss that more later.)*

We think of GROQ statements as describing a data flow from left to right. First everything (`*`) flows through the filter` [_type == 'movie' && …]`, then all those movies flow through the `order()`-function which is then all mapped through the projection `{_id, title, ...}` which picks out the bits we want to be returned.

The order function accepts a list of fields, and optionally you can specify the sort direction for each field. If you wanted to sort the movies by year, and then within each year we want them alphabetical by title, we could use this ordering: `order(releaseYear, title)` And if we wanted the newest movies first, we could reverse the direction like this: `order(releaseYear desc, title)`.

> [!TIP]
> Protip
> asc means “ascending” and desc means descending in this context. If you leave out the sort-direction, Sanity will assume you want the ascending order.

### Slicing the result set

This brings us to our final problem for this query: There are many movies in the world. Maybe our dataset contains tens of thousands. We need a way to describe which slice of that list we want to show. This is done using a *selector*. Let’s say we just wanted the first movie, we could add a `[0]` at the end. This works exactly like an array accessor and would return only the first element. If we want a slice, we can add [the range operator](/docs/specifications/groq-operators) like this: `[0...100]`. This would return the first hundred movies from index 0 through 99. We can just as well ask for `[1023...1048] `or any other slice we desire. So there we are, our first basic query with filtering, ordering, projections, and selector:

```groq
*[_type == 'movie' && releaseYear >= 1979] | order(releaseYear) {
  _id, title, releaseYear
}[0...100]

```

### References and joins

A reference in Sanity is a link from one document to another. Standard references are “hard” meaning when a document references another document, the target document *must* exist, and is actually prevented from being deleted until the reference is removed. (There are also weak-references that do not "hold on to" the target. You make them by adding a `_weak`-key to the reference object like this: `{_ref: "<document-id>", _weak: true}`)

Let’s say we have “person”-type documents that looks something like this:

```javascript
{
  _id: "ridley-scott",
  _type: "person",
  name: "Ridley Scott"
}

```

Keeping it simple, maybe our movies had a field `director` that contained a reference to a person. It could look something like this:

```javascript
{
  _id: "alien",
  _type: "movie",
  title: "Alien",
  releaseYear: 1979,
  director: { _ref: "ridley-scott" }
}
```

Remember Sanity-specific fields are prefixed with an underscore, and an object containing a `_ref` key appearing anywhere in the document becomes a hard reference.

### Expanding references

Now we can do a number of useful things with this reference. The most basic thing is expanding the reference in place. Let’s revisit our movie queries from the introduction.

```groq
*[_type == 'movie' && releaseYear >= 1979]{
  _id, title, releaseYear
}

```

Let’s say we wanted to include the director in the returned result. If we didn't know any better, we'd perhaps try something like this:

```groq
*[_type == 'movie' && releaseYear >= 1979]{
  _id, title, releaseYear,
  director
}

```

But if we just naïvely include the director in like this, we will just get whatever is in the director field on this document, which is the literal reference description:

```javascript
[
  {
    _id: "alien",
    title: "Alien",
    releaseYear: "1979",
    director: {
      _ref: "ridley-scott"
    }
  },
  … (more movies)
]


```

This is not what we wanted, we wanted to follow that reference! By adding the dereferencing operator `->` we ask Sanity to follow the reference and replace it with the actual content of the document referenced:

```groq
*[_type == 'movie' && releaseYear >= 1979]{
  _id, title, releaseYear,
  director->
}

```

Now, this is useful. We’d get something like this:

```javascript
[
  {
    _id: "alien",
    title: "Alien",
    releaseYear: "1979",
    director: {
      _id: "ridley-scott",
      _type: "person",
      name: "Ridley Scott"
    }
  },
  … (more movies)
]


```

Then maybe we didn’t want all that metadata with our director? We can add a separate projection for our director:

```groq
*[_type == 'movie' && releaseYear >= 1979]{
  _id, title, releaseYear,
  director->{name}
}

```

Our query now returns the director with just the name property we wanted:

```javascript
{
  _id: "alien",
  title: "Alien",
  releaseYear: "1979",
  director: {
    name: "Ridley Scott"
  }
}

```

But we can do one better. We are not limited to the existing fields in the document in our projections, we can actually declare new fields. Let’s say we are building our compact movie list and we wanted just the title, year, and director name. We can get minimal cruft by extracting just the name and putting it in a new field, like this:

```groq
*[_type == 'movie' && releaseYear >= 1979]{
  _id, title, releaseYear,
  "directorName": director->name
}

```

Now our query returns exactly what we want in the form we want it:

```javascript
{
  _id: "alien",
  title: "Alien",
  releaseYear: "1979",
  directorName: "Ridley Scott"
}

```

#### Expanding an array of references

The example above shows how to expand a reference, but sometimes you'll be working with an *array* of references. In the above example, let's say we wanted to add producers. Details on how to set this up in your schema can be found in the [Array](https://www.sanity.io/docs/array-type#R7Awwxtw) documentation, but we'll consider how you might query that data.

In this revised example, let's look at a query like this:

```groq
*[_type == 'movie' && releaseYear >= 1979]{
  _id, title, releaseYear, director,
  producers[]
}

```

We use square brackets after `producers` because it's an array. Note that we used `producers` with an `s`. The naming convention of your schema doesn't matter to GROQ (as long as you get the name right); it is our recommendation to [use the plural form for arrays](https://www.sanity.io/docs/naming-things#0c567e82634b).

Now, you might get this:

```javascript
[
  {
    _id: "alien",
    title: "Alien",
    releaseYear: "1979",
    director: {
      _ref: "ridley-scott"
    }
    producers: [
      {
        _key: "<uniqueKey1>",
        _type: "reference",
        _ref: "gordon-carroll"
      },
      {
        _key: "<uniqueKey2>",
        _type: "reference",
        _ref: "david-giler"
      },
      {
        _key: "<uniqueKey3>",
        _type: "reference",
        _ref: "walter-hill"
      },
    ]
  },
  … (more movies)
]
```

Like before, this isn't returning the details for each producer. We're getting references like we did at the beginning of the single reference example (and a `_key`, which [ensures uniqueness](https://www.sanity.io/docs/array-type#92296c6c45ea)). To expand references in an array, we will use the dereferencing operator (`->`) again. However, the square brackets are mandatory to traverse the array.

```groq
*[_type == 'movie' && releaseYear >= 1979]{
  _id, title, releaseYear, director,
  producers[]->
}

```

This will return the full details for each of the three producers referenced. Projections and [naked projections](https://www.sanity.io/docs/how-queries-work#dd66cae5ed8f) can be used just as with single references (the projection would go *after* the dereference operator).

> [!WARNING]
> Gotcha
> It would be easy to forget the square brackets when expanding an array of references (i.e., querying producers-> instead of producers[]->, with the former returning a single null value). This is perhaps complicated by the fact that both producers and producers[] will return the array (albeit with unexpanded references). This is the nature of how GROQ traversals work.

### Filtering by references

When dealing with references, we have a useful function called `references()` which can be used in filters to select only documents that reference specific other documents. Let’s say we want to list every movie Ridley Scott has been involved in. It looks like this:

```groq
*[_type == 'movie' && references('ridley-scott')]
```

### Our first join

It is time to write our first proper join: Say we wanted to list people and include all the movies they were involved in? We’ll be querying the “person”-type documents, but in the projections for each person, we’ll ask for the movies they have been involved in. To do this we have to briefly cover the parent-operator `^`. Let’s look at the query first:

```groq
*[_type == "person"]{
  _id, name,
  "movies": *[_type == "movie" && references(^._id)].title
}

```

In a join, the parent operator is a way to reference the “parent” document. In this example the outer query for “person”-type documents fetches a bunch of people, and for each person, it returns the `_id` and `name`. Then we want to fetch the movies referencing that person. 

Now we declare the new field “movies” where we start a new query for “movie”-type documents, but for each person, we want to limit our movie query to movies referencing that person. To achieve this we need the _id of the person, but if we just wrote `_id` in the movies-query we’d reference the _id of the movie. 

To get to the fields of the person record we go “up” one level using the parent operator `^`. So `^` means the specific “person”-document that our movie query is about, and then `^._id` is the _id of that person, just as `^.name` would be her name. So when we say `references(^._id)` in the query above, we limit our movies to movies referencing the current person.

### Naked projections

There is one more new thing we haven’t talked about in this query. We could have written the movies-sub-query like this:

```groq
*[_type == "movie" && references(^._id)]{title}
```

Our list of movies would have looked something like this:

```javascript
”movies”: [{title: “Alien”}, {title: “Blade Runner”}, …]
```

Since we just wanted the titles, we can use a “naked projection”. By naming the field we want, like this:

```groq
*[_type == "movie" && references(^._id)].title 
```

We get a nice, simple array of values, like this:

```javascript
”movies”: [“Alien”, “Blade Runner”, …]
```

So, for completeness, the result of the full person w/movies query above could look something like this:

```javascript
[
  {
    _id: "river-phoenix",
    name: "River Phoenix",
    movies: ["My Own Private Idaho", "Stand By Me", …]
  },
  {
    _id: "ridley-scott",
    name: "Ridley Scott",
    movies: ["Alien", "Blade Runner", …]
  },
  …
]

```

## More ways to filter

Sanity supports a growing number of ways to filter your documents. We have shown simple attribute comparisons with `_type == ‘movie’` and  `releaseYear >= 1979`. We have shown filtering by references using the `references()`-function. In addition, we support:

- Text search using the match operator, e.g. `*[title match "Alien*"]`
- Filtering by the presence of a field, e.g. `*[defined(status)]` which only match documents that have the status property set to any value.
- The `in`-operator which matches values in arrays, as in `*["sci-fi" in genres]`, that matches all documents where `genres` is an array and that array contains the value `"sci-fi"`.
- You can of course combine these filters using the boolean operators `&&` (and), `|| `(or), `!` (not), like this `*[_type == "movie" && (!("sci-fi" in genres) || releaseYear >= 1979)]`.

We are working on a full reference for the GROQ feature set. In the meantime, you'll find a comprehensive set of examples in the [cheat sheet](/docs/content-lake/query-cheat-sheet).

## Queries in projections

A useful thing in GROQ is that filtering and projections also can be used inside your projections. Let’s say you work for an architect and every project has a number of milestones. A document might look something like this:

```javascript
{
  _id: "timmerhuis"
  _type: "project",
  title: "Timmerhuis",
  milestones: [
    {status: "competition", year: 2009},
    {status: "design-development", year: 2011},
    {status: "breaking-ground", year: 2013},
    {status: "completed", year: 2015}
  ]
}

```

And let’s say the view we are producing is about showing the current status of the project. We could achieve this by finding the latest milestone and extracting its status tag. This can be done in GROQ like this:

```groq
*[_type == "project"]{
  _id, title,
  "status": milestones|order(year desc)[0].status
}

```

Let’s pick apart the status query `milestones|order(year desc)[0].status` in some detail:

First, we take the field `milestones` which contain the (potentially unordered) list of milestones for the project. Using the pipe-operator `|` we send the contents of this array to the order function, which is instructed to sort the array by year in descending order `order(year desc)`. Then we take only the first element `[0]` (which is the latest milestone) and return the value of its `status` field. So now our project list would look something like this:

```javascript
[
  {
    _id: "timmerhuis",
    title: "Timmerhuis",
    status: "completed"
  },
  …
]

```

Let’s try another clever trick querying the contents of this object. Instead of a status field, we just want a boolean flag telling whether the project is completed. We could achieve this like this:

```groq
*[_type == "project"]{
  _id, title,
  "completed": count(milestones[status == 'completed']) > 0
}

```

Here we take the milestones, but select only the ones having the status “completed”. Then we `count()` the number of milestones matching this filter. If that count is `> 0` the result is `true`. So now our result would look something like this:

```javascript
[
  {
    _id: "timmerhuis",
    title: "Timmerhuis",
    completed: true
  },
  …
]

```

## Some comments on the pipe operator

In the project-status example above we used the pipe operator `|` for a second time. Let's explore that in some detail:

```groq
*[_type == "project"]{
  _id, title,
  "status": milestones | order(year desc)[0].status
}

```

The pipe operator takes the output from its left-hand side and sends it to the operation to its right. "But isn’t this what all GROQ statements do?", I hear you ask. And you’d be right.

In some situations, like when using pipe functions (e.g., `order()` in the project-status example), an explicit pipe operator is required. `milestones order(year desc)` would be a syntax error, so pipe functions must be preceded by a pipe operator, like this: `milestones | order(year desc)`. `score()` is another example of a pipe function, which must therefore be preceded by a pipe operator.

Projections may be preceded by a pipe operator, though it is optional. `Expression { Projection }` and `Expression | { Projection }` are equally valid.

The pipe operator is not valid in any other contexts and will return an error.

## Some fine points on arrays and projections

Let’s consider this document with some deep structure:

```javascript
{
  _id: "alien",
  _type: "movie",
  title: "Alien",
  poster: {
    asset: {_ref: "image-1234"}
  },
  images: [
    {
      caption: "Sigourney Weaver and the cat Jones on set",
      asset: {_ref: "image-1235"}
    },
    {
      caption: "Bolaji Badejo suiting up for the role of the Alien",
      asset: {_ref: "image-1236"}
    },
  ]
}

```

So we have a movie with a poster image and an array of other images. Each image has some metadata represented here by a caption, then a reference to an asset record containing all the metadata on the specific image including its URL. A simplified asset record could look something like this:

```javascript
{
  _id: "image-1234",
  _type: "sanity.imageAsset",
  url: "http:///cdn.sanity.io/images/…"
}

```

Now we can retrieve the poster image url and attach it to our result for each movies like this:

```groq
*[_type == "movie"]{
  title,
  "posterImage": poster.asset->url
}

```

But what if we wanted to do the same thing for the other images? Since the `images` field is an array, we can’t just `images.asset->url`. We somehow have to apply the `asset->url`-part to each member of the array. This is accomplished by adding a blank filter, like this: `images[].asset->url` which will return the image URLs as a simple array. So the full query would look like this:

```groq
*[_type == "movie"]{
  title,
  "imageUrls": images[].asset->url
}

```

This would yield something like this:

```javascript
[
  {
    title: "Alien",
    imageUrls: ["http://cdn.sanity.io/…", "http://cdn.sanity.io/…"]
  },
  …
]

```

If you wanted a richer data-set with your images you could use a normal projection like this (taking care to add the blank filter to apply the projection to every array member):

```groq
*[_type == "movie"]{
  title,
  "images": images[]{
    caption,
    "url": asset->url,
  }
}

```

Now your result looks something like this:

```javascript
[
  {
    title: "Alien",
    images: [
      {
        caption: "Sigourney Weaver and the cat Jones on set",
        url: "http://cdn.sanity.io/…"
      },
      {
        caption: "Bolaji Badejo suiting up for the role of the Alien",
        url: "http://cdn.sanity.io/…"
      }
    ]
  },
  …
]

```

## The ellipsis operator

Sometimes you might want to compute some properties of a document, but still want the entire set of attributes returned. This can be a problem since the moment you specify a projection, you'll have to list all the fields you want to be included. Let's say we wanted to count the actors in a movie doing something like this:

```groq
*[_type == "movie"]{
  "actorCount": count(actors)
}
```

There is a problem with this. We just wanted to add a custom field, but since we needed a projection to do it, now all we got is something like this:

```javascript
[
  {actorCount: 3},
  {actorCount: 27},
  {actorCount: 15}
]
```

What we wanted was our custom field in *addition* to the normal fields. This can be achieved with the ellipsis operator. By appending it like this, we effectively say we want the fields we just specified, but also everything else:

```groq
*[_type == "movie"]{
  "actorCount": count(actors),
  ...
}
```

Which brings us a result that could look something like this:

```javascript
{
  {
    title: "Alien",
    releaseYear: 1979,
    actorCount: 23,
    // And loads more fields, probably
  },
  // and many more movies
}
```

### Placement of the ellipsis operator

In `v1` of the GROQ API, the placement of the ellipsis operator didn't matter. An explicit property would override the ellipsis even when the ellipsis comes last in the projection.

Consider a case where a projection returns some number of properties, with one being `age`. Let's say that `age` is equal to `23`.

```groq
// GROQ API v1

*[]{
  ...,
  'age': 45 // This will override the age property
            // returned from the ellipsis, so age == 45
}


*[]{
  'age': 45,
  ... // The age value returned from the ellipsis does *not*
      // override the explicitly set value, so age == 45
}
```

As of `v2021-03-25` of the GROQ API, the placement of the ellipsis operator matters. An explicitly-set property will only override the property returned by the ellipsis if it comes after the ellipsis. In other words, as of `v2021-03-25`, the property that comes last in the projection wins, even if it's returned by the ellipsis.

```groq
// GROQ API v2021-03-25 or later

*[]{
  ...,
  'age': 45 // This will override the age property
            // returned from the ellipsis, so age == 45
}


*[]{
  'age': 45,
  ... // The age value returned from the ellipsis *does*
      // override the explicitly set value, so age == 23
}
```

Such a distinction might be observed when [dereferencing](https://www.sanity.io/docs/groq-operators#dae298fc7952). In `v1`, the explicit dereference operator could be placed before *or* after the ellipsis operator in a projection, and the reference would be followed in either case. As of `v2021-03-25`, an explicit dereference after the ellipsis would give expected behaviour, returning the contents of the document that was referenced. However, placing the ellipsis last will actually cause the original (non-dereferenced) property to win, returning just the `_ref` and `_type`.

> [!TIP]
> Protip
> When using the ellipsis operator, you will want to list it first in your projection. Any explicitly-listed properties that follow will overwrite that same property that would have been returned by the ellipsis, which is likely the behaviour you're after.

## Queries that don't start with an `*`

We said initially that most GROQ queries start with the asterisk, but they don't have to. Any valid GROQ expression can be the entire query. This is a valid query:

```groq
count(*)

```

It will return the number of documents in the dataset. This is also valid:

```groq
count(*[name match "sigourney"]) > 0

```

It will return `true` if any document in the entire dataset has a `name`-field containing the word "sigourney".

More usefully, you can actually have a projection be your outer statement. Like this:

```groq
{
  "mainStory": *[_id == "story-1234"],
  "campaign": *[_id == "campaign-1234"],
  "topStories": *[_type == "story"] | order(publishAt desc) [0..10]
}
```

This combines three completely separate queries into one query and returns an object containing the result of all of them. This can be a useful way to speed up page loads. By combining queries in this manner you can often get all of the core content for a web page to load in a single, cacheable query.

## Query Optimization

Like with any query language, it's important to be aware of performance as you develop and iterate on your GROQ queries. Learn more about optimizing your queries in [High Performance GROQ](/docs/developer-guides/high-performance-groq).

## Finally

So there you go, this should cover most of what you need to understand in the day-to-day use of GROQ. You should now check out our [Query Cheat Sheet](/docs/content-lake/query-cheat-sheet), [the GROQ Arcade](https://groq.dev), and [the reference docs](/docs/groq-reference) which contain examples of all operators and functions currently supported.

[High performance GROQ](/docs/developer-guides/high-performance-groq)

[Paginating with GROQ](/docs/developer-guides/paginating-with-groq)

[Syntax reference](/docs/specifications/groq-syntax)





# Query cheat sheet

Here are some typical queries in GROQ. You can also check out [our introduction to GROQ](/docs/content-lake/how-queries-work) and [the complete reference documentation](/docs/groq). To actually run queries you can:

- Hit your content lake's query [HTTP endpoint](/docs/http-reference/query) directly
- Use the [JavaScript](/docs/js-client) or [PHP](/docs/php-client) SDKs, or [another client](https://www.sanity.io/exchange/type=plugins/solution=apis)
- Install the [Vision plugin](/docs/content-lake/the-vision-plugin) that runs queries right inside Sanity Studio
- Go to [groq.dev](https://groq.dev) to run queries against any JSON dataset

> [!WARNING]
> Gotcha
> If your query doesn't work as expected, it might be related to:
> 
> API versioning
> 
> Perspectives

## Filters

> [!TIP]
> Protip
> You will get null as a value on a query if the key you ask for doesn't exist. That means you can filter on key != null to check if it exists with a value or not.

```groq
* // Everything, i.e. all documents
*[] // Everything with no filters applied, i.e. all documents
*[_type == "movie"] // All movie documents
*[_id == "abc.123"] // _id equals
*[_type in ["movie", "person"]] // _type is movie or person
*[_type == "movie" && popularity > 15 && releaseDate > "2016-04-25"] // multiple filters AND
*[_type == "movie" && (popularity > 15 || releaseDate > "2016-04-25")] // multiple filters OR
*[popularity < 15] // less than
*[popularity > 15] // greater than
*[popularity <= 15] // less than or equal
*[popularity >= 15] // greater than or equal
*[popularity == 15]
*[releaseDate != "2016-04-27"] // not equal
*[!(releaseDate == "2016-04-27")] // not equal
*[!(releaseDate != "2016-04-27")] // even equal via double negatives "not not equal"
*[dateTime(_updatedAt) > dateTime('2018-04-20T20:43:31Z')] // Use zulu-time when comparing datetimes to strings
*[dateTime(_updatedAt) > dateTime(now()) - 60*60*24*7] // Updated within the past week
*[name < "Baker"] // Records whose name precedes "Baker" alphabetically
*[awardWinner == true] // match boolean
*[awardWinner] // true if awardWinner == true
*[!awardWinner] // true if awardWinner == false
*[defined(awardWinner)] // has been assigned an award winner status (any kind of value)
*[!defined(awardWinner)] // has not been assigned an award winner status (any kind of value)
*[title == "Aliens"]
*[title in ["Aliens", "Interstellar", "Passengers"]]
*[_id in path("a.b.c.*")] // _id matches a.b.c.d but not a.b.c.d.e
*[_id in path("a.b.c.**")] // _id matches a.b.c.d, and also a.b.c.d.e.f.g, but not a.b.x.1
*[!(_id in path("a.b.c.**"))] // _id matches anything that is not under the a.b.c path or deeper
*["yolo" in tags] // documents that have the string "yolo" in the array "tags"
*[status in ["completed", "archived"]] // the string field status is either == "completed" or "archived"
*["person_sigourney-weaver" in castMembers[].person._ref] // Any document having a castMember referencing sigourney as its person
*[slug.current == "some-slug"] // nested properties
*[count((categories[]->slug.current)[@ in ["action", "thriller"]]) > 0] // documents that reference categories with slugs of "action" or "thriller"
*[count((categories[]->slug.current)[@ in ["action", "thriller"]]) == 2] // documents that reference categories with slugs of "action" and "thriller"
  // set == 2 based on the total number of items in the array
```

## Text matching

> [!WARNING]
> Gotcha
> The match operator is designed for human-language text and might not do what you expect!

```groq
// Text contains the word "word"
*[text match "word"]

// Title contains a word starting with "wo"
*[title match "wo*"] 

// Inverse of the previous query; animal matches the start of the word "caterpillar" (perhaps animal == "cat")
*["caterpillar" match animal + "*"] 

// Title and body combined contains a word starting with "wo" and the full word "zero"
*[[title, body] match ["wo*", "zero"]] 

// Are there aliens in my rich text?
*[body[].children[].text match "aliens"] 

// Note how match operates on tokens!
"foo bar" match "fo*"  // -> true
"my-pretty-pony-123.jpg" match "my*.jpg"  // -> false
```

## Slice Operations

> [!TIP]
> Protip
> There is no default limit, meaning that if you're not explicit about slice, you'll get everything.

```groq
*[_type == "movie"][0] // a single movie (an object is returned, not an array)
*[_type == "movie"][0..5] // first 6 movies (inclusive)
*[_type == "movie"][0...5] // first 5 movies (non-inclusive)
*[_type == "movie"]{title}[0...10] // first 10 movie titles
*[_type == "movie"][0...10]{title} // first 10 movie titles
*[_type == "movie"][10...20]{title} // first 10 movie titles, offset by 10
*[_type == "movie"] // no slice specified --> all movies are returned
```

**Also note**: The above queries don't make much sense without also specifying an order. E.g. the "first 6 movies" query only returns "first" movies in the sense that these are the first six movies the backend happens to pull out.

## Ordering

> [!TIP]
> Protip
> Documents are returned by default in ascending order by _id, which may not be what you're after. If you're querying for a subset of your documents, it's usually a good idea to specify an order.
> 
> No matter what sort order is specified, the ascending order by _id will always remain the final tie-breaker.

```groq
// order results
*[_type == "movie"] | order(_createdAt asc)

// order results by multiple attributes
*[_type == "movie"] | order(releaseDate desc) | order(_createdAt asc)

// order todo items by descending priority,
// where priority is equal, list most recently updated
// item first
*[_type == "todo"] | order(priority desc, _updatedAt desc) 

// the single, oldest document
*[_type == "movie"] | order(_createdAt asc)[0]

// the single, newest document
*[_type == "movie"] | order(_createdAt desc)[0]

// oldest 10 documents
*[_type == "movie"] | order(_createdAt asc)[0..9]

// BEWARE! This selects 10 documents using the default
// ordering, and *only the selection* is ordered by
// _createdAt in ascending order
*[_type == "movie"][0..9] | order(_createdAt asc)

// limit/offset using external params (see client documentation)
*[_type == "movie"] | order(_createdAt asc) [$start..$end]

// order results alphabetically by a string field
// This is case sensitive, so A-Z come before a-z
*[_type == "movie"] | order(title asc)

// order results alphabetically by a string field,
// ignoring case
*[_type == "movie"] | order(lower(title) asc)
```

## Joins

```groq
// Fetch movies with title, and join with poster asset with path + url
*[_type=='movie']{title,poster{asset->{path,url}}}

// Say castMembers is an array containing objects with character name and a reference to the person:
// We want to fetch movie with title and an attribute named "cast" which is an array of actor names
*[_type=='movie']{title,'cast': castMembers[].person->name}

// Same query as above, except "cast" now contains objects with person._id and person.name
*[_type=='movie']{title,'cast': castMembers[].person->{_id, name}}

// Using the ^ operator to refer to the enclosing document. Here ^._id refers to the id
// of the enclosing person record.
*[_type=="person"]{
  name,
  "relatedMovies": *[_type=='movie' && references(^._id)]{ title }
}

// Books by author.name (book.author is a reference)
*[_type == "book" && author._ref in *[_type=="author" && name=="John Doe"]._id ]{...}

```

## Objects and Arrays

```groq
// Create your own objects
// https://groq.dev/lcGV0Km6dpvYovREqq1gLS
{
  // People ordered by Nobel prize year
  "peopleByPrizeYear": *[]|order(prizes[0].year desc){
  	"name": firstname + " " + surname,
    "orderYear": prizes[0].year,
    prizes
  },
  // List of all prizes ordered by year awarded
  "allPrizes": *[].prizes[]|order(year desc)
}

// Get all Nobel prizes from all root person documents
// https://groq.dev/v8T0DQawC6ihbNUf4cUeeS
*[].prizes[]

array::join(tags, ", ")                    // tags = ["Rust", "Go", null, "GROQ"] => "Rust, Go, <INVALID>, GROQ"
array::join(["a", "b", "c"], ".")          // "a.b.c"
array::join(year, ".")                     // year = 2024 => null (not an array)
array::join(values, 1)                     // values = [10, 20, 30] => null (separator must be a string)
array::compact(numbers)                    // numbers = [1, null, 2, null, 3] => [1, 2, 3]
array::unique(items)                       // items = [1, 2, 2, 3, 4, 5, 5] => [1, 2, 3, 4, 5]
array::unique(records)                     // records = [[1], [1]] => [[1], [1]] (arrays are not comparable)
array::intersects(firstList, secondList)   // firstList = [1, 2, 3], secondList = [3, 4, 5] => true
array::intersects(tags, keywords)          // tags = ["tech", "science"], keywords = ["art", "design"] => false
```

## Object Projections

```groq
// return only title
*[_type == 'movie']{title} 

// return values for multiple attributes
*[_type == 'movie']{_id, _type, title} 

// explicitly name the return field for _id
*[_type == 'movie']{'renamedId': _id, _type, title} 

// Return an array of attribute values (no object wrapper)
*[_type == 'movie'].title
*[_type == 'movie']{'characterNames': castMembers[].characterName}

// movie titled Arrival and its posterUrl
*[_type=='movie' && title == 'Arrival']{title,'posterUrl': poster.asset->url} 

// Explicitly return all attributes
*[_type == 'movie']{...} 

// Some computed attributes, then also add all attributes of the result
*[_type == 'movie']{'posterUrl': poster.asset->url, ...} 

// Default values when missing or null in document
*[_type == 'movie']{..., 'rating': coalesce(rating, 'unknown')}

// Number of elements in array 'actors' on each movie
*[_type == 'movie']{"actorCount": count(actors)} 

// Apply a projection to every member of an array
*[_type == 'movie']{castMembers[]{characterName, person}} 

// Filter embedded objects
*[_type == 'movie']{castMembers[characterName match 'Ripley']{characterName, person}} 

// Follow every reference in an array of references
*[_type == 'book']{authors[]->{name, bio}}

// Explicity name the outer return field
{'threeMovieTitles': *[_type=='movie'][0..2].title}

// Combining several unrelated queries in one request
{'featuredMovie': *[_type == 'movie' && title == 'Alien'][0], 'scifiMovies': *[_type == 'movie' && 'sci-fi' in genres]}

```

## Special variables

```groq
// *
*   // Everything, i.e. all documents

// @
*[ @["1"] ] // @ refers to the root value (document) of the scope
*[ @[$prop]._ref == $refId ] // Select reference prop from an outside variable.
*{"arraySizes": arrays[]{"size": count(@)}} // @ also works for nested scopes

// ^
// ^ refers to the enclosing document. Here ^._id refers to the id
// of the enclosing person record.
*[_type=="person"]{
  name,
  "relatedMovies": *[_type=='movie' && references(^._id)]{ title }
}
```

## Conditionals

```groq
// select() returns the first => pair whose left-hand side evaluates to true
*[_type=='movie']{..., "popularity": select(
  popularity > 20 => "high",
  popularity > 10 => "medium",
  popularity <= 10 => "low"
)}

// The first select() parameter without => is returned if no previous matches are found
*[_type=='movie']{..., "popularity": select(
  popularity > 20 => "high",
  popularity > 10 => "medium",
  "low"
)}

// Projections also have syntactic sugar for inline conditionals
*[_type=='movie']{
  ...,
  releaseDate >= '2018-06-01' => {
    "screenings": *[_type == 'screening' && movie._ref == ^._id],
    "news": *[_type == 'news' && movie._ref == ^._id],
  },
  popularity > 20 && rating > 7.0 => {
    "featured": true,
    "awards": *[_type == 'award' && movie._ref == ^._id],
  },
}

// The above is exactly equivalent to:
*[_type=='movie']{
  ...,
  ...select(releaseDate >= '2018-06-01' => {
    "screenings": *[_type == 'screening' && movie._ref == ^._id],
    "news": *[_type == 'news' && movie._ref == ^._id],
  }),
  ...select(popularity > 20 && rating > 7.0 => {
    "featured": true,
    "awards": *[_type == 'award' && movie._ref == ^._id],
  }),
}


// Specify sets of projections for different content types in an array
content[]{
  _type == 'type1' => {
    // Your selection of fields for type1
  },
  _type == 'type2' => {
    // Your selection of fields for type2
    "url": file.asset->url // Use joins to get data of referenced document
  }
}

```

### Handling references conditionally

In cases where an array contains both [references and non-references](https://www.sanity.io/docs/array-type#wT47gyCx), it's often desirable for a GROQ query to conditionally return the inline object (where dealing with non-references) or the referenced document (where dealing with references). This can be done by considering the `_type` of each array item and dereferencing the item (`@->`) if it's a reference or getting the whole object (`@`) if it's not a reference.

```groq
'content': content[]{
  _type == 'reference' => @->,
  _type != 'reference' => @,
}
```

## Functions

```groq
// any document that references the document 
// with id person_sigourney-weaver, 
// return only title
*[references("person_sigourney-weaver")]{title}

// Movies which reference ancient people
*[_type=="movie" && references(*[_type=="person" && age > 99]._id)]{title}

*[defined(tags)] // any document that has the attribute 'tags'

// coalesce takes a number of attribute references
// and returns the value of the first attribute
// that is non-null. In this example used to
// default back to the English language where a
// Finnish translation does not exist.
*{"title": coalesce(title.fi, title.en)} 

// count counts the number of items in a collection
count(*[_type == 'movie' && rating == 'R']) // returns number of R-rated movies

*[_type == 'movie']{
  title, 
  "actorCount": count(actors) // Counts the number of elements in the array actors
}

// round() rounds number to the nearest integer, or the given number of decimals
round(3.14) // 3
round(3.14, 1) // 3.1


// score() adds points to the score value depending 
// on the use of the string "GROQ" in each post's description 
// The value is then used to order the posts 
*[_type == "post"] 
  | score(description match "GROQ") 
  | order(_score desc) 
  { _score, title }

// boost() adds a defined boost integer to scores of items matching a condition 
// Adds 1 to the score for each time $term is matched in the title field
// Adds 3 to the score if (movie > 3) is true
*[_type == "movie" && movieRating > 3] | 
  score(
    title match $term,
    boost(movieRating > 8, 3)
  )

// Creates a scoring system where $term matching in the title
// is worth more than matching in the body
*[_type == "movie" && movieRating > 3] | score(
  boost(title match $term, 4),
  boost(body match $term, 1)
)

// Returns the body Portable Text data as plain text
*[_type == "post"] 
  { "plaintextBody": pt::text(body) }

// Get all versions and drafts of a document. Use with the raw perspective or a perspective stack to ensure accurate results.
*[sanity::versionOf('document-id')]

// Get all documents that are part of a release. Use with the raw perspective to ensure accurate results.
*[sanity::partOfRelease('release-id')]
```

## Geolocation

```groq
// Returns all documents that are storefronts
// within 10 miles of the user-provided currentLocation parameter
*[
  _type == 'storefront' &&
  geo::distance(geoPoint, $currentLocation) < 16093.4
]

// For a given $currentLocation geopoint and deliveryZone area
// Return stores that deliver to a user's location
*[
  _type == "storefront" &&
  geo::contains(deliveryZone, $currentLocation)
]

// Creates a "marathonRoutes" array that contains
// all marathons whose routes intersect with the current neighborhood
*[_type == "neighborhood"] {
  "marathonRoutes": *[_type == "marathon" && 
                      geo::intersects(^.neighborhoodRegion, routeLine)  
                    ]
}
```

## Arithmetic and Concatenation

```groq
// Standard arithmetic operations are supported
1 + 2  // 3 (addition)
3 - 2  // 1 (subtraction)
2 * 3  // 6 (multiplication)
8 / 4  // 2 (division)
2 ** 4 // 16 (exponentiation)
8 % 3  // 2 (modulo)

// Exponentiation can be used to take square- and cube-roots too
9 ** (1/2)  // 3 (square root)
27 ** (1/3) // 3 (cube root)

// + can also concatenate strings, arrays, and objects:
"abc" + "def" // "abcdef"
[1,2] + [3,4] // [1,2,3,4]
{"a":1,"b":2} + {"c":3} // {"a":1,"b":2,"c":3}

// Concatenation of a string and a number requires the number be
// converted to a string. Otherwise, the operation returns null
3 + " p.m."         // null
string(3) + " p.m." // "3 p.m."
```





# Custom functions

Sometimes you find yourself repeating the same portion of a GROQ query across multiple queries, or even within a single complex query. Custom functions for GROQ allow you to create modular, reusable sub-queries.

Prerequisites:

- Custom GROQ functions are available on all API versions except v1.

## Function anatomy

Custom functions look similar to other GROQ functions, but with some limitations. They include a namespace and accept a parameter. Let's look at an example function that follows a reference and returns a projection that combines an author's first and last name.

```groq
fn ex::name($author) = $author-> { "name": firstName + " " + lastName };

*[_type == 'post']{
  "author": ex::name(author)
}
```

All functions start with the `fn` keyword and contain a namespace, name, parameter, and function body. In the example above:

- `ex` is the namespace.
- `name` is the function name.
- `$author` is the parameter.
- `$author-> { "name": firstName + " " + lastName }` is the body.

Custom function declarations must happen at the start of the GROQ query and each declaration must end with a semicolon (`;`). You can use them anywhere you'd normally send a GROQ query, such as a Sanity client, the [HTTP query API](/docs/http-reference/query), or [the Vision tool](/docs/content-lake/the-vision-plugin). For example, in `@sanity/client`:

```
const QUERY = `
fn ex::name($author) = $author-> { "name": firstName + " " + lastName };

*[_type == "post"]{
  "author": ex::name(author)
}`

const posts = await client.fetch(QUERY)

```

See the [GROQ functions reference](/docs/specifications/groq-functions) for additional details.

## Examples

Custom functions support a limited set of formats at this time:

- `$param{...}`
- `$param->{...}`
- `$param[]{...}`
- `$param[]->{...}`

Let's use the following documents as an example to explore each format. There is a `person` document, an `occupation` document, and two `pet` documents.

```json
{
  "_id": "a",
  "_type": "person",
  "name": [
    {
      "first": "Jane",
      "last": "Doe"
    }
  ],
  "age": 99,
  "occupation": { "_ref": "developer" },
  "belongings": [
    {"name": "laptop"},
    {"name": "badge"},
    {"name": "backpack"}
  ],
  pet: [
    {"_ref": "dog"},
    { "_ref": "dog2" }
  ]
}
``````json
{
  "_id": "developer",
  "_type": "occupation",
  "title": "Software Engineer"
}
``````json
{
  "_id": "dog",
  "_type": "pet",
  "name": "Pookie"
}
``````json
{
  "_id": "dog2",
  "_type": "pet",
  "name": "Snookie"
}
```

### Basic projection

First we'll define a function that returns a basic projection. This function, `ex:: details`, takes a `$person` parameter and returns a projection containing their `name` and `age`. To use the function, we pass in `@` to represent the person returned by the filter.

```groq
fn ex::details($person) = $person{name, age}; 
*[_type == "person"] { "info": ex::details(@) }

``````json
[{
  "info": {
    "age": 99,
    "name": {
      "first": "Jane",
      "last": "Doe"
    }
  }
}]
```

### Follow references

It's common to follow references to include part or all of their contents in the referencing object. This function follows the person's `occupation` reference and returns a projection with their title.

```groq
fn ex::title($ref) = $ref->{title};
*[_type == "person"] { "occupation": ex::title(occupation) }
``````json
[{
  "occupation": {
    "title": "Software Engineer"
  }
}]
```

### Array projection

This function iterates through the person's `belongings` to display their names.

```groq
fn ex::items($arr) = $arr[]{name}; 
*[_type == "person"] { "stuff": ex::items(belongings) }
``````json
[{
  "stuff": [
    {"name": "laptop"},
    {"name": "badge"},
    {"name": "backpack"},
  ]
}]
```

### Array of references

This function follows each reference in the person's `pet` key.

```groq
fn ex::pets($items) = $items[]->{name};
*[_type == "person"] {"pet": ex::pets(pet)}

``````json
[{
  "pet": [
    {"name": "Pookie"},
    {"name": "Snookie"}
  ]
}]
```

## Limitations

At this time, functions are limited to the formats displayed above. Additionally, custom functions do not yet support:

- Recursion.
- Accessing the parent scope.
- Passing multiple parameters.
- Accessing the function parameter more than once in the function body.
- Calling a custom function from within a custom function.
- TypeGen does not support custom functions. At this time, use the `// @sanity-typegen-ignore` comment before the function to exclude it from TypeGen, then manually create types. [See the ignoring individual queries example](https://www.sanity.io/docs/apis-and-sdks/sanity-typegen#c4ec4db7a627).



# Query playground

Vision is a plugin that lets you quickly test your GROQ queries right from the Studio. It shows up as a tool in the navigation bar when installed, and is part of the default Studio setup when running in development mode.

![Sanity Studio using the Vision plugin](https://cdn.sanity.io/images/3do82whm/next/7f4903586477a14beabf92fca429c92730b77fb8-2430x1352.png)

## Installing the plugin

New projects should have the plugin installed already. For existing projects, or if it is not part of your studio configuration, you can install it by adding `@sanity/vision` as a dependency of your project (adding it to `package.json` and reinstalling dependencies).

With the plugin installed, you should add it to your Sanity configuration (`sanity.config.js` / `sanity.config.ts`):

```typescript
import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'
import {visionTool} from '@sanity/vision'

export default defineConfig({
  // ...
  plugins: [structureTool(), visionTool()],
})
```

Should you want to only include the plugin in development mode, you can import and use the `isDev` boolean and conditionally add the plugin:

```typescript
import {defineConfig, isDev} from 'sanity'
import {structureTool} from 'sanity/structure'
import {visionTool} from '@sanity/vision'

export default defineConfig({
  // ...
  plugins: isDev
    ? [structureTool(), visionTool()]
    : [structureTool()],
})

```

## Configuring the plugin

The plugin can be configured by passing it an object of options. 

```typescript
// sanity.config.js / sanity.config.ts
import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'
import {visionTool} from '@sanity/vision'

export default defineConfig({
  // ...
  plugins: [
    structureTool(),
    visionTool({
      defaultApiVersion: 'v2021-03-25',
      defaultDataset: 'development',
    }),
  ],
})

```

Currently supported properties are:

- `defaultApiVersion` - The default API version for queries, unless the user specifically selects a different version. Allowed values are `v1`, `vX`, `v2021-03-25`, or `v2021-10-21`.
- `defaultDataset` - The default dataset to use unless a specific one has been chosen in the user interface.

Additionally, these base properties are available should you want to customize what the item appears as in the navigation bar:

- `name` - Name used to identify the tool in URLs. Defaults to `vision`.
- `title` - Title that appears in the navigation bar. Defaults to `Vision`.
- `icon` - React icon that appears in navigation bar. Defaults to an eye icon (`EyeOpenIcon` from `@sanity/icons`)

## Using the Vision plugin

### Getting familiar with Vision

The Vision plugin allows you to quickly test a GROQ query against any of the datasets in your [Content Lake](https://www.sanity.io/docs/datastore). At the top of the tool, you'll find dropdowns to select your dataset, API version and perspective.

![Dataset, API version and perspective dropdowns in Vision.](https://cdn.sanity.io/images/3do82whm/next/f2234ee7e0c483e090c75f4d6cd1c56df49157a6-994x318.png)

Each time you run a query (we'll see how in a moment), you'll see a fourth field at the top containing a URL for your query. This URL contains the API call to the Content Lake that's querying for your data.

If your dataset is public, that URL can be run in a browser, cURL, or an app like Postman or Insomnia, and it will return the same JSON as you see in Vision. If your dataset is private, the request must be authenticated in order to return data. The decision on [dataset visibility](https://www.sanity.io/docs/keeping-your-data-safe#5c2e941ea03c) is up to you, and can be changed if necessary.

On the left side of the Vision plugin are two panes: query and params. In the query pane, you can enter any valid [GROQ query](https://www.sanity.io/docs/how-queries-work).

Query parameters works the same way as with Sanity client libraries. Given an object `{minSeats: 2}` in the Params field, you may use the keys in the object as parameters in the query: `*[_type == "bike" && seats >= $minSeats] {name, seats}`. Note that every param key is prefixed with `$` in the *query*, but does *not* have a prefix in the parameters object.

> [!WARNING]
> Gotcha
> You can only use Vision to test queries and listeners. You cannot use it for mutations.

To learn more about how to write queries, read [How Queries Work - GROQ](/docs/content-lake/how-queries-work).



# Syntax reference

> [!TIP]
> Protip
> If you are just getting started with the Sanity query language GROQ, you should probably read the how-to first.

A typical GROQ query has this form:

```groq
*[ <filter> ]{ <projection> }
```

4. `*`  returns all documents in the dataset that the current user has permissions to read. 
4. The documents are passed to a filter (`[]`), which retains documents for which the expression evaluates to `true`. 
4. The retained documents are passed to an optional projection. The projection determines how the result should be formatted. If no projection is specified, all data is returned.

A GROQ query of this form operates as a query pipeline, where the results from each component are passed as inputs to the next. The filter and projection are optional, and a query can have any number of them in any order.

In pipeline components, document attributes can be accessed by name. For example, this query would fetch directors born since 1970 and return their name, year of birth, and a list of their movies:

```groq
*[ _type == "director" && birthYear >= 1970 ]{
  name,
  birthYear,
  "movies": *[ _type == "movie" && director._ref == ^._id ]
}
```

For a complete introduction to GROQ, please see the [how-to](/docs/content-lake/how-queries-work).

## JSON Superset

GROQ's syntax is a superset of JSON, so any valid JSON value is a valid GROQ query (that returns the given value). Below are a few examples of JSON values:

```json
"Hi! 👋"
```

```json
["An", "array", "of", "strings"]
```

```json
{
  "array": ["string", 3.14, true, null],
  "boolean": true,
  "number": 3.14,
  "null": null,
  "object": {"key": "value"},
  "string": "Hi! 👋"
}
```

For more information on JSON syntax, see the [JSON specification](https://tools.ietf.org/html/rfc8259).

## Whitespace

Whitespace is not significant in GROQ, except for acting as a token separator and comment terminator. Any sequence of the following characters is considered whitespace, with Unicode code points in parenthesis:

- Tab (`U+0009`)
- Newline (`U+000A`)
- Vertical tab (`U+000B`)
- Form feed (`U+000C`)
- Carriage return (`U+000D`)
- Space (`U+0020`)
- Next line (`U+0085`)
- Non-breaking space (`U+00A0`)

Whitespace inside a string literal is interpreted as-is.

## Comments

Comments serve as query documentation and are ignored by the parser. They start with `//` and run to the end of the line:

```groq
{
  // Comments can be on a separate line
  "key": "value" // Or at the end of a line
}
```

Comments cannot start inside a string literal.

## Expressions

An expression is one of the following:

- A literal, attribute lookup, parameter, or constant.
- An operator invocation (and, by extension, a pipeline).
- A function call.

Expressions can be used anywhere that a value is expected, such as object values, array elements, operator operands, or function arguments. The expression is in effect replaced by the value which it evaluates to.

> [!WARNING]
> Gotcha
> Due to parser ambiguity with filters, the following access operators can only take literals, not arbitrary expressions: array element access (e.g. array[0]), array slices (e.g. array[1..3]), and object attribute access (e.g. object["attribute"]).

### Selectors

A selector is a subset of an expression used to search for fields inside a document. You can only use them in certain functions—at this time, Delta GROQ functions, to select part of a document. See the [Delta GROQ functions](/docs/specifications/groq-functions) and the Selectors section for a list of available functions and selectors.

## Literals

Literals are inline representations of constant values, e.g., `"string"` or `3.14`. GROQ supports all JSON literals, with a few enhancements and additional data types.

For more information on the data types themselves, see the [data types](/docs/specifications/groq-data-types) reference.

### Boolean and Null Literals

The constants `true`, `false`, and `null`.

### Integer Literals

A sequence of digits, e.g., `42`. Leading zeroes are ignored.

### Float Literals

Floats have an integer part, a fractional part, and an exponent part. The integer part is required, and at least one of the fractional or exponent parts must be given.

The integer part is equivalent to an integer literal. The fractional part is a decimal point `.` followed by a sequence of digits. The exponent part is `e` or `E`, followed by an optional `+` or `-` sign followed by an integer specifying base-10 exponentiation.

The following are examples of float literals:

```json
3.0
3.14
3e6      // Equivalent to 3000000.0
3.14eE0  // Equivalent to 3.14
3.14e-2  // Equivalent to 0.0314
```

### String Literals

A sequence of zero or more UTF-8 encoded characters surrounded by single or double quotes, e.g., `"Hello world! 👋"`. The following escape sequences are supported (mirroring JSON), all of which are valid in both single- and double-quoted string literals:

- `\\`: backslash
- `\/`: slash
- `\'`: single quote
- `\"`: double quote
- `\b`: backspace
- `\f`: form feed
- `\n`: newline
- `\r`: carriage return
- `\t`: tab
- `\uXXXX`: UTF-16 code point, where `XXXX` is the hexadecimal character code
- `\uXXXX\uXXXX`: UTF-16 surrogate pair

### Array Literals

A comma-separated list of values enclosed by `[]`, e.g. `[1, 2, 3]`. An optional trailing comma may follow the final element.

### Object Literals

A comma-separated list of key-value pairs enclosed by `{}`, where the key and value of each pair is separated by `:`, e.g. `{"a": 1, "b": 2}`. Keys must be strings. An optional trailing comma may follow the final pair.

### Pair Literals

Two values separated by `=>`, e.g. `"a" => 1`.

### Range Literals

Two values separated by `..` (right-inclusive) or `...` (right-exclusive), e.g. `1..3` or `1...3`.

## Identifiers

Identifiers name query entities such as attributes, parameters, functions, and some operators. Identifiers must begin with `a-zA-Z_`, followed by any number of characters matching `a-zA-Z0-9_`. Parameters are prefixed with `$`.

### Reserved Keywords

The following keywords are reserved and cannot be used as identifiers:

- `false`
- `null`
- `true`

## Attribute Lookup

A bare identifier looks up the value of the corresponding attribute in the document or object at the root of the current scope. For example, the following query `category` returns the value of the `category` attribute of the document currently being considered by the filter:

```groq
*[ category == "news" ]
```

If the attribute does not exist, or if the root value of the scope is not a document or object, then the identifier will return `null`.

> [!TIP]
> Protip
> JSON allows attribute keys to be any arbitrary UTF-8 string. In cases where the key is not a valid GROQ identifier, it can instead be accessed by using the @ operator (typically returning the current document) and the [] attribute access operator, e.g. @["1 illegal name 🚫"].

### Attribute Scope

Attribute lookups are scoped such that the same identifier may refer to different attributes in different contexts. New scopes are created by pipeline components, typically by iterating over the piped array elements and evaluating an expression in the scope of each element.

#### @ Operator – Access current scope

The `@` operator can be used to access the root value of the current scope.

```groq
// @ refers to the current number being evaluated
// Returns numbers in the array if they're greater than or equal to 10
numbers[ @ >= 10 ]

// @ refers to the myArray value
// This query returns the number of items in the myArray array
*{"arraySizes": myArray[]{"size": count(@)}} 
```

#### ^ Operator – Access the parent scope

Scopes can also be nested, in which case the `^` operator can be used to access the root value of the parent scope. Consider the following query:

```groq
*[ _type == "movie" && releaseYear >= 2000 ]{
  title,
  releaseYear,
  crew{name, title},
  "related": *[ _type == "movie" && genre == ^.genre ]
}
```

In the filter, `_type` and `releaseYear` access the corresponding attributes of each document passed from `*`. Similarly, in the projection, `title`, `releaseYear`, and `crew` access the corresponding attributes from each document passed from the filter. However, in the nested `crew` projection, `name` and `title` access the attributes of each object passed from the `crew` object - notice how the outer and inner `title` identifiers refer to different attributes (one is from the movie, the other is from the crew member).

The `related` pipeline components also create new scopes where `_type` and `genre` refer to the attributes of each document fetched from the preceding `*` operator, not those of the surrounding projected document. Notice how the `^` operator is used to access the document at the root of the parent (outer) scope and fetch its `genre` attribute.

## Operators

GROQ supports nullary, unary, and binary operators, which return a single value when invoked. Unary operators can be either prefix or postfix (e.g. `!true` or `ref->`), while binary operators are always infix (e.g. `1 + 2`). Operators are made up of the characters `=<>!|&+-*/%@^`, but identifiers can also be used to name certain binary operators (e.g., `match` which case they are considered reserved keywords.

## Functions

GROQ function calls are expressed as a function identifier immediately followed by a comma-separated argument list in parentheses, e.g., `function(arg1, arg2)`. An optional trailing comma may follow the final argument. Functions can take any number of arguments (including zero), and return a single value.

## Pipe Functions

Pipe functions ([order()](https://www.sanity.io/docs/groq-pipeline-components#9a5a019b1465) and [score()](https://www.sanity.io/docs/groq-functions#be9f618a7086)) must be preceded by the [pipe operator](https://www.sanity.io/docs/groq-operators#80749b4f431a) (`|`). The left-hand expression will be an array that the pipe operator will pass to the right-hand pipe function, returning a new array.

`*[_type == "post"] | order(_createdAt desc)` will pass an array of all documents with a `_type` of `post` into the `order()` function, returning a new array of those documents sorted by the `_createdAt` property in descending order.



# Introduction

Sanity has powerful APIs for [querying](/docs/content-lake/how-queries-work), [patching](/docs/content-lake/http-patches), and [mutating](/docs/http-reference/mutation) data in the [real-time](/docs/http-reference/listen) [Content Lake](/docs/content-lake). In addition to our [GROQ](/docs/content-lake/how-queries-work) API, we also support deploying GraphQL APIs to query your content. 

GraphQL APIs are deployed [using our command-line interface](#04501f1778aa). The command inspects your studio's schema definitions and generates a GraphQL schema that closely resembles it (type names have their first letter capitalized – *bookAuthor* becomes *BookAuthor*), then adds queries allowing you to find and filter the documents stored in your Sanity dataset. 

To use GraphQL with your project, you need to make sure that you follow the [strict schema conventions](#33ec7103289a).

## Queries

For each document type in your Sanity schema, two top-level query fields are added:

- `all<TypeName>` - used to fetch all documents of the given type. You can add additional filters, sorting, limits, and offsets. Read more about filters below.
- `<TypeName>` - used to fetch a specific document of the given type by specifying its document ID.

### Filters

For each object and document type in your Sanity schema, an equivalent *filter* type is generated. This can be used to constrain which documents are returned for a given query, much like an SQL query.

Most fields in your schema type will have a corresponding field in the filter. For instance, a book schema type may have a `title` field, which would then have a title *filter*:

Input

```text
{
  allBook(where: {title: {eq: "A Game of Thrones"}}) {
    title
    author {
      name
    }
  }
}
```

Result

```json
{
  "allBook": [
    {
      "title": "A Game of Thrones",
      "author": {
        "name": "George. R. R. Martin"
      }
    }
  ]
}
```

In a similar fashion, the `author` field would also have a filter type:

Input

```text
{
  allBook(where: {author: {name: {eq: "George R.R. Martin"}}}) {
    title
    author {
      name
    }
  }
}
```

Response

```json
{
  "allBook": [
    {
      "title": "A Game of Thrones",
      "author": {
        "name": "George R. R. Martin"
      }
    },
    {
      "title": "A Storm of Swords",
      "author": {
        "name": "George R. R. Martin"
      }
    }
  ]
}
```

Which comparator functions exist depend on the field type. For instance, a number field will have the comparators `eq`, `neq`, `gt`, `gte`, `lt` and  `lte`, while a boolean field will only have `eq` and `neq`.

In addition to filtering on a per-field basis, document types have additional filters available under the `_` field: `references` and `is_draft`:

Input

```text
{
  allBook(where: {_: {references: "jrr-tolkien"}}) {
    title
    author {
      name
    }
  }
}
```

Response

```json
{
  "allBook": [
    {
      "title": "The Lord of the Rings",
      "author": {
        "name": "J. R. R. Tolkien"
      }
    }
  ]
}
```

For a full overview of the available filters, see the [GraphQL filter reference](#ba117ddb05ce) section down below.

### Sorting

You can sort on multiple fields on your top-level documents. You can also sort on your nested objects.

Input

```text
{
  allBook(sort: [ { title: ASC }, { published: DESC } ]) {
    title
  }
}
```

Result

```json
{
  "allBook": [
    {
      "title": "A Game of Thrones",
      "author": {
        "name": "George. R. R. Martin"
      }
    },
    {
      "title": "The Fellowship of the Ring",
      "author": {
        "name": "J. R. R. Tolkien"
      }
    }
  ]
}
```

### Pagination

We support pagination in the form of the take and skip concept. Pagination can easily be achieved like this:

Input

```text
{
  allBook(limit: 10, offset: 10) {
    title
  }
}
```

Result

```json
{
  "allBook": [
    {
      "title": "The Two Towers",
      "author": {
        "name": "J. R. R. Tolkien"
      }
    },
    {
      "title": "The Return of the King",
      "author": {
        "name": "J. R. R. Tolkien"
      }
    }
  ]
}
```

## Strict schemas

The schemas for Sanity Studio are a bit more flexible than what GraphQL is able to represent. That means that we can't promise that you'll be able to deploy a GraphQL API without any changes to your Sanity projects. Therefore, you may have to do a few changes (usually these are backward-compatible and do not require any data migration).

You may find that “anonymous“ object types have to be given a name and declared in the top-level scope. Take this example:

```javascript
// schemas/blogPost.js
import {defineType} from 'sanity'

export default defineType({
  name: 'blogPost',
  title: 'Blog post',
  type: 'document',
  fields: [
    // ... other fields ...
    {
      name: 'sponsor',
      title: 'Sponsor',
      type: 'object',
      fields: [
        {
          name: 'name',
          title: 'Name',
          type: 'string'
        },
        {
          name: 'url',
          title: 'URL',
          type: 'url'
        }
      ]
    }
  ]
})
```

In the code above, the `sponsor` field is an object type declared inline. This means it cannot be used outside of the `blogPost` type. This is not compatible with GraphQL – all object types have to be defined in a global scope. To fix this, you should move the sponsor declaration to a separate file and import it into your schema explicitly, then have the `sponsor` field refer to it by name. 

Example:

```javascript
// schemas/blogPost.js
import {defineType} from 'sanity'

export default defineType({
  name: 'blogPost',
  title: 'Blog post',
  type: 'document',
  fields: [
    // ... other fields ...
    {
      name: 'sponsor',
      title: 'Sponsor',
      type: 'sponsor'
    }
  ]
})

// schemas/sponsor.js
import {defineType} from 'sanity'

export default defineType({
  name: 'sponsor',
  title: 'Sponsor',
  type: 'object',
  fields: [
    {
      name: 'name',
      title: 'Name',
      type: 'string'
    },
    {
      name: 'url',
      title: 'URL',
      type: 'url'
    }
  ]
})

```

> [!TIP]
> Protip
> While "lifting"/"hoisting" the type to the top-level scope, it can be helpful to consider whether the type should be altered to make it more reusable in other contexts. If you think the type is only relevant to the specific schema type, consider prefixing it to make it clearer (e.g., blogPostSponsor in the above case).

> [!WARNING]
> Gotcha
> The type names reference and crossDatasetReference are considered reserved words by the Sanity CLI and cannot be used as the value of the name field in a document.

## Deploying GraphQL APIs

GraphQL APIs are deployed using the Sanity CLI tool. In the simplest case, running `sanity graphql deploy` in your Sanity Studio project folder is enough to get started - it will use the default settings and deploy the API to the project ID and dataset configured in your `sanity.config.ts` file.

You can deploy multiple APIs per project/dataset with different API configurations. To do so, you will want to either edit or create a `sanity.cli.ts` file (or `sanity.cli.js` if you prefer not to use TypeScript) in your Sanity Studio project folder.

The configuration file should export a configuration object containing a `graphql` key, which is an array of GraphQL API definitions. Here is an example configuration file:

```typescript
// sanity.cli.ts
import {defineCliConfig} from 'sanity/cli'

export default defineCliConfig({
  graphql: [
    {
      playground: false,
      tag: 'experiment',
      workspace: 'staging',
      id: 'schema-experiment',
    },
  ]
})
```

In the example above, we are telling the CLI:

- We do not want a playground to be deployed for this API.
- We want to use the custom tag "experiment," which allows us to deploy multiple APIs for a single dataset.
- We want to use the [workspace](/docs/studio/workspaces) with the id "staging" from the studio configuration file. This allows us to use different project IDs, datasets, schemas, and similar.
- We want the ID of this GraphQL API to be "schema-experiment." If multiple GraphQL APIs are defined, this lets us deploy specific ones by using the `--api` flag.

Running `sanity graphql deploy` from your Sanity Studio project folder will now deploy all of the configured APIs from the CLI configuration. 

> [!TIP]
> Protip
> The Sanity CLI for deploying GraphQL APIs come with a range of flags that can be set in order to customize your deployments, such as the experimental --with-union-cache flag which can greatly optimizeschema generation for schemas with many self-referencing types. 
> 
> To learn about all these flags, visit the reference article.

> [!WARNING]
> Gotcha
> Keep in mind that changing the schema in your local Sanity studio does not automatically change the GraphQL API – you'll have to run sanity graphql deploy to make the API reflect the changes.

> [!WARNING]
> Gotcha
> Note that deploying multiple GraphQL APIs is not an atomic operation. While the CLI tool attempts to validate/verify the API configuration and schemas ahead of time, there is a theoretical possibility that some APIs might be deployed and some might fail. This may be improved/fixed in the future.

> [!WARNING]
> Gotcha
> Dataset names with dashes - in the name currently list incorrectly in the sanity graphql list command. If this causes issues for you please use a different delimiter in your dataset names. This is something we are aware of and looking to fix in the future. 

### Tagged endpoints

We also support deploying multiple endpoints of the GraphQL schema to the same dataset by using the `tag` option in the CLI configuration file. This tag will be the last segment in the endpoint URL. This will let you test schema changes without breaking existing applications. If you don't specify any tag, the tag will be `default`.

Since we provide a way to deploy multiple GraphQL endpoints, you can use this CLI command to list all your existing endpoints:

```sh
sanity graphql list
```

## The playground

GraphQL APIs have the option to deploy a "playground". This is an interactive GraphQL user interface that will allow you to more easily run/test queries. This is handy for development, but might not necessarily be something you want to deploy in production - which is why it is configurable. Do note that users can still run an [introspection query](https://graphql.org/learn/introspection/) to discover the properties of the schema without the playground being deployed, however.

If you want to enable/disable this feature, it can be done by using the boolean `playground` flag in the GraphQL CLI configuration.

## GraphQL endpoints

> [!NOTE]
> GraphQL API Versioning
> This documentation describes the latest version of the GraphQL API: 2025-02-19. If you're running previous versions, please see the changelog entries for details on what differs.
> 
> On 2023-08-01 the first major upgrade to the Sanity GraphQL API with breaking changes was released. If you are working in a project that queries the legacy v1 API, you can safely continue to do so until you are ready to upgrade. To learn about the new features and breaking changes introduces in v2023-08-01, refer to the release notes.
> 
> You can tell which API version you are targeting by looking at the version segment of the endpoint URL, as shown below:
> 
> Dated endpoints, for example:
> https://<yourProjectId>.api.sanity.io/v2023-08-01/graphql/<dataset>/<tag>
> 
> 
> 
> Legacy v1 endpoint:
> https://<yourProjectId>.api.sanity.io/v1/graphql/<dataset>/<tag>
> 

GraphQL queries can be executed against the [API or API CDN](/docs/content-lake/api-cdn).

- **API** is recommended in development environments or for use cases where you need the latest content to be immediately available.
- **API CDN** is recommended for most use cases to return faster results and scale for high-volume traffic.

The subdomain of your query URL directs the request to the API or API CDN:

`https://<yourProjectId>.apicdn.sanity.io/v2023-08-01/graphql/<dataset>/<tag>`

## Query parameters

> [!WARNING]
> Gotcha
> The query parameters listed below requires version v2023-08-01 or later of the Sanity GraphQL API. Refer to the info box in the previous section to learn about API versioning for GraphQL.

### Perspectives

[Perspectives](/docs/content-lake/perspectives) allow your GraphQL queries to run against an alternate view of the content in your dataset. You can set a perspective by adding the query parameter `perspective` to your request. The available options are:

- `published`: The default option if no perspective is set. Excludes all unpublished changes from your results.
- `raw`: Returns drafts, versions, and published content side-by-side for authenticated requests.
- `previewDrafts`: Treats all draft documents and in-flight changes as if they were published.

```
https://<yourProjectId>.apicdn.sanity.io/v2025-02-19/graphql/<dataset>/<tag>?perspective=raw
```

[Perspectives for Content Lake](/docs/content-lake/perspectives)



### Content Source Maps

[Content Source Maps](/docs/visual-editing/content-source-maps) (CSM) is an [open specification by Sanity](https://github.com/sanity-io/content-source-maps) that enables the embedding of source metadata with your content, and lays the foundation for powerful features such as [Visual Editing](/docs/visual-editing/vercel-visual-editing). 

[Content Source Maps](/docs/visual-editing/content-source-maps)

[Visual Editing](/docs/visual-editing/vercel-visual-editing)



To use CSM with GraphQL, add the query parameter `resultSourceMap=true` to your request.

```
https://<yourProjectId>.apicdn.sanity.io/v2025-02-19/graphql/<dataset>/<tag>?resultSourceMap=true
```

The CSM metadata will then be returned in the `sanitySourceMap` extension in the response:

```json
{
  "data": {
    "allPost": [
      {
        "title": "GraphQL CSM"
      }
    ]
  },
  "extensions": {
    "sanitySourceMap": {
      "documents": [
        {
          "_id": "75bbbd60-0aa9-4b20-9c00-0b40cb010ff6"
        },
      ],
      "paths": [
        "$['title']"
      ],
      "mappings": {
        "$['allPost'][0]['title']": {
          "source": {
            "document": 0,
            "path": 0,
            "type": "documentValue"
          },
          "type": "value"
        }
      }
    }
  }
}
```

For an example of how to use Content Source Maps to implement Visual Editing using GraphQL you can visit these repositories which demonstrates how to set things up in Next.JS:

- [Sanity Presentation with Next.JS and GraphQL – App router](https://github.com/sanity-io/demo-graphql-presentation-nextjs)
- [Sanity Presentation with Next.JS and GraphQL – Pages router](https://github.com/sanity-io/demo-graphql-presentation-nextjs/tree/pages-router)

## Security

The GraphQL API generally has the same rules as the GROQ API – dataset visibility is respected. Authenticated users see only the documents they have access to.

However, remember that your GraphQL schema is public, so all types and fields will be [introspectable](https://graphql.org/learn/introspection/) by anonymous users.

## Mutations

Mutations are not exposed through the GraphQL API but rather through our powerful [Mutation API](/docs/http-reference/mutation).

## Filters reference

### Scalars

#### ID, String, Datetime, Date

- Equals: `field { eq: "" }`
- Not equals: `field { neq: "" }`
- In: `field { in: [ "apple", "banana", "pineapple" ] }`
- Not in: `field { nin: [ "apple", "banana", "pineapple" ] }`
- Matches: `field { matches: "" }`

#### Int

- Equals: `field { eq: "" }`
- Not equals: `field { neq: "" }`
- Greater than: `field { gt: 42 }`
- Greater than or equal: `field { gte: 42 }`
- Lesser than: `field { lt: 42 }`
- Lesser than or equal: `field { lte: 42 }`

#### Float

- Equals: `field { eq: 42.0 }`
- Not equals: `field { neq: 42.0 }`
- Greater than: `field { gt: 42.0 }`
- Greater than or equal: `field { gte: 42.0 }`
- Lesser than: `field { lt: 42.0 }`
- Lesser than or equal: `field { lte: 42.0 }`

#### Boolean

- Equals: `field { eq: true|false }`
- Not equals: `field { neq: true|false }`

### Types

The schema generator will generate filtering types for your documents. It will provide filtering options for most fields defined in your schema. On top-level documents, it provides some special filters which can be accessed through `_`.

#### Document

- References: `field { references: "jrr-tolkien" }`
- Is draft: `field { is_draft: true }`

#### Array

Unfortunately, we don't provide any filtering for your array fields yet.

#### Portable Text

The schema generator will expose a `<your-type-name>Raw` field, which gives you all Portable Text content in raw JSON. It will not resolve references by default, but if you use one of our source plugins for [Gatsby](https://github.com/sanity-io/gatsby-source-sanity/) or [Gridsome](https://github.com/sanity-io/gridsome-source-sanity/), there are arguments you can pass to resolve references.

> [!WARNING]
> Gotcha
> Since Portable Text by nature is somewhat loosely typed, the generation doesn't take into account all the types you provide for it, yet.

## Deprecated fields

You can explicitly deprecate fields in your GraphQL APIs by using the `deprecated` property in [schema-type definitions](/docs/schema-types):

Input

```typescript
export const name = defineField({
  name: 'firstName',
  type: 'string',
  description: `The person's first name`,
  deprecated: {
    reason: 'Use the name field instead'
  }
})
```

GraphQL schema

```json
{
  "name": "type",
  "description": "The person's first name",
  "args": [],
  "type": {
    "kind": "SCALAR",
    "name": "String",
    "ofType": null
  },
  "isDeprecated": true,
  "deprecationReason": "Use fullName and lastName instead"
}
```

## Schema generation issues

Since the schema is generated in Node.js instead of in a browser environment, certain imported modules might cause issues. Things that reference the `window` in a global context are a prime example. If you encounter issues, we'd be interested in hearing which modules cause problems to see if we can work around them. We invite you to reach out to us in our [Discord community](https://discord.com/servers/sanity-1304483263171264613).

## Breaking/dangerous changes 

When a GraphQL API has already been deployed, and you want to deploy a new version, the Sanity CLI tool will generate a new API definition and compare it with the previously deployed version. If any changes are considered breaking or dangerous, the CLI will warn and ask for confirmation before deploying. In a CI environment, the CLI will exit with a non-zero exit code and fail the build. You can use the `--dry-run` flag to only check for breaking/dangerous changes (that is, without deploying the changes), and the -`-force` flag if you are sure you want to deploy even with breaking changes. The rules for determining breaking/dangerous changes are defined in the `findBreakingChanges` and `findDangerousChanges` of the [graphql npm package](https://github.com/graphql/graphql-js). Note that this is currently considered an implementation detail and may change.






# GROQ and GraphQL

Sanity's primary API for querying is GROQ, but we also offer a [GraphQL API](/docs/content-lake/graphql). Here we look at both of them and how they compare.

[Check out the GraphQL API](/docs/content-lake/graphql)

[GraphQL](http://graphql.org/) is a pattern for designing flexible APIs to structured, connected data. GraphQL is very popular, and there are loads of nice tooling and libraries for it appearing every week. Naturally, we are often asked why we did not design Sanity with GraphQL as the core query language, but rather added it as a mapper.

Take this GROQ-query:

```text
*[_type == "author" && name match "Edgar" && debutYear < 1900]{
  name,
  debutYear
}
```

(Fetch authors, whose name contains the string "Edgar" and debuted before 1900, return the name, year of debut, and the number of books for each author. The results would perhaps include `{"name": "Edgar Allan Poe", "debutYear": 1827}`)

Using our GraphQL mapper you would express it like his:

```text
{
  authors(where: {
      debutedBefore_lt: "1900-01-01T00:00:00Z", 
      name_matches: "Edgar*"
  ) {
    name,
    debutYear,
  }
}
```

This is just one example. We see a lot of different uses of Sanity. Some people prefer GraphQL so they can tie the API to other services and enjoy the tooling of the GraphQL ecosystem.  

Sometimes they run up against what you can reasonably express in the built-in GraphQL mapper and build their own APIs. Then it's really good to have GROQ with its free form queries that translate well to GraphQL.

Others simply prefer GROQ for its expressive range and what you can accomplish without any configuration whatsoever.

## What about mutations?

GraphQL mutations are not currently supported. You can learn more about [how to mutate and patch data here](/docs/http-reference/mutation).







# Introduction

Files, like JPGs and PDFs, which exist alongside your data, are considered *assets*. The Sanity Studio provides a hassle-free UI for uploading assets, and an unsurprising API for dealing with all asset concerns such as storage, resizing and deletion.

## Asset types

We currently support [image](/docs/image-type) and [file](/docs/file-type) types. `image` is used for all kinds of images, including SVGs, and `file` is used for all other file types such as documents, audio files, zip or the like. In the future, we might provide specific support for audio and video files, but currently all non-image files belong in the `file` category.

### Images

Though Sanity lets you import a wide range of image formats like TIFF and SVG our currently supported image formats for processed output are: JPEG, PNG and WebP. 

GIF and SVG pass through the image pipeline untouched, but if you want them scaled they will need to be output as one of the three formats above.

To upload assets, use the `/assets/images/<dataset>` endpoint. Once you have the image URL you can request resized versions of the original image by appending query parameters. 

See the [Presenting Images](/docs/apis-and-sdks/presenting-images) documentation for more details.

### Files

When you need to upload a non-image file (say a PDF or a zip file) use the `/assets/files/<dataset>` endpoint. The response is similar to the one you get from an image, but the type will be `fileAsset`.

## Browse and manage assets

To browse assets from the Studio interface, install the [Sanity Media plugin](/plugins/sanity-plugin-media). This adds a new tool to the toolbar and enables Studio users to browse and manage assets. 

You can also query all images with GROQ using the following query:

```groq
*[_type == "sanity.imageAsset"]
```

This GROQ query returns all image assets. Replace `imageAsset` with `fileAsset` to query all files. Run GROQ queries through [Vision](/docs/content-lake/the-vision-plugin), the [client](/docs/js-client), or with the [Query API](/docs/http-reference/query).

## Upload an asset

> [!WARNING]
> Gotcha
> The path of an asset is in part determined by the result of hashing the content of the asset. As such, if the same asset is uploaded multiple times, but with different filenames, only one asset will be created. For example, if image.jpg and image-copy.jpg are the same image, uploading both will only create one asset.

In some cases, uploading assets using the UI is impractical. Say you want to upload a ton of images. If so, you'll want to use the assets API directly.

To upload an image, do a POST request to

`myProjectId.api.sanity.io/v2021-06-07/assets/images/myDataset`

with your file in the request body. E.g.:

```sh
curl \
  -X POST \
  -H 'Content-Type: image/jpeg' \
  --data-binary "@/Users/mike/images/bicycle.jpg" \
  'https://myProjectId.api.sanity.io/v2021-06-07/assets/images/myDataset'

```

```javascript
import {createClient} from '@sanity/client'
import {basename} from 'path'
import {createReadStream} from 'fs'

const client = createClient({
  projectId: 'myProjectId',
  dataset: 'myDatasetName',
  apiVersion: '2021-08-29',
  token: 'myToken'
})

const filePath = '/Users/mike/images/bicycle.jpg'

client.assets
  .upload('image', createReadStream(filePath), {
    filename: basename(filePath)
  })
  .then(imageAsset => {
    // Here you can decide what to do with the returned asset document. 
    // If you want to set a specific asset field you can to the following:
    return client
      .patch('some-document-id')
      .set({
        theImageField: {
          _type: 'image',
          asset: {
            _type: "reference",
            _ref: imageAsset._id
          }
        }
      })
      .commit()
  })
  .then(() => {
    console.log("Done!");
  })
```

### Example of returned asset document

```javascript
{
  "_id": "image-abc123_0G0Pkg3JLakKCLrF1podAdE9-538x538-jpg",
  "_type": "sanity.imageAsset", // type is prefixed by sanity schema
  "assetId": "0G0Pkg3JLakKCLrF1podAdE9",
  "path": "images/myproject/mydataset/abc123_0G0Pkg3JLakKCLrF1podAdE9-538x538.jpg",
  "url": "https://cdn.sanity.io/images/myproject/mydataset/abc123_0G0Pkg3JLakKCLrF1podAdE9-538x538.jpg",
  "originalFilename": "bicycle.jpg",
  "size": 2097152, // File size, in bytes
  "metadata": {
    "dimensions": {
      "height": 538,
      "width": 538,
      "aspectRatio": 1.0
    },
    "location":{ // only present if the original image contained location metadata
      "lat": 59.9241370,
      "lon": 10.7583846,
      "alt": 21.0
    }
  }
}

```

## Downloading an asset

In order to download an asset from your front-end you need to append `?dl=<asset-of-your-choice.jpg>` to the asset URL. If you leave the filename blank, the original filename will be used if present. If the original filename is not available, the id of the file will be used instead.

```javascript
// GROQ query

*[_type == "post"] {
  title,
  mainImage{
    asset->url
  }
}
// Then you can use the URL in HTML for example like this:
// <a href={`${mainImage}?dl=`}>Hero Image</a>
```

## Deleting assets

Deleting an asset can be performed by deleting the associated asset document. 

```javascript
import {createClient} from '@sanity/client'
const config = {
  projectId: 'myProjectID',
  dataset: 'mydataset',
  apiVersion: '2021-08-29',
  token: 'myToken'
}
const client = createClient(config)
client.delete('image-abc123_0G0Pkg3JLakKCLrF1podAdE9-538x538-jpg')
  .then(result => {
    console.log('deleted image asset', result)
  })

```

It's important to note that while the file is deleted, the CDN might have your asset cached so it may not disappear immediately.



This is an `_id = example` where we start a new `_bleh`



# Metadata

The `metadata` option for image fields warrants a closer look. It takes an array of strings describing which types of metadata Sanity should attempt to extract or generate from uploaded images and save alongside the asset. An example of an image field with every metadata option specified looks as follows:

```javascript

  {
      name: 'metaImage',
      title: 'Image with metadata',
      type: 'image',
      options: {
        metadata: [
          'blurhash',   // Default: included
          'lqip',       // Default: included
          'palette',    // Default: included
          'image',      // Default: not included
          'exif',       // Default: not included
          'location',   // Default: not included
        ],
      },
    },

```

There are three additional metadata options that are always included and cannot be disabled: `dimensions`, `hasAlpha`, and `isOpaque`. Specifying an invalid option in the metadata array—including any of those three terms—will throw an error.

The metadata fields fall into one of three "default behaviors": **Always included**, **included by default**, and **not included by default**. We'll look at each default setting and the metadata fields that adhere to it.

> [!WARNING]
> Gotcha
> Metadata is added to your assets asynchronously after uploading! If your query for image metadata returns unexpectedly empty, wait a moment and try again!

> [!WARNING]
> Gotcha
> Metadata is applied to an image asset when the image is uploaded and based on the schema settings at that time. If a metadata array is set to include exif or location data, changing the schema later will not remove those details. If removing those details is desired, you can do so with a script or using the Media browser plugin, among other options. Likewise, adding options to the metadata array will not add those details to images previously uploaded.

## Dimensions, Alpha Channel, and Opaqueness

> [!NOTE]
> Always included
> These values are always available and you do not need to ask for them. In fact, they are not valid options in the options.metadata array so including them will throw an error.

### `hasAlpha`

`hasAlpha` will return `true` if the image has an alpha channel, even if unused.

### `isOpaque`

`isOpaque` returns `true` if the image is fully opaque (i.e., has no transparency).

### `dimensions`

The `dimensions` object contains the numeric values: **aspectRatio**, **height,** and **width,** which together describe the physical features of the image. A photo taken in portrait mode might yield the following payload:

```json
{
  "dimensions" : {
    "_type": "sanity.imageDimensions",
    "aspectRatio": 0.75,
    "height": 4032,
    "width": 3024
  }
}
```

## Placeholders and Colors

> [!NOTE]
> Included by default
> These values are available by default. If you don't ask for any metadata at all (that is, if you don't specify a metadata array), you will get these values. Beware though: If you do specify a metadata array and explicitly leave these out, they will not be returned.

### `lqip` and `blurHash`

Sanity will generate low-fidelity representations of your images automatically. These are useful for creating placeholders for loading images in your front end. These downsampled previews come in two different flavors: LQIP and blurhash.

**LQIP** (Low-Quality Image Preview) is a 20-pixel wide version of your image (height is set according to aspect ratio) in the form of a base64-encoded string and can be used as-is in your front end, as shown below. A typical value for lqip might look like this:

```json

"lqip": "data:image/png;base64,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"

```

And can be used like this:

```html

		<!-- 
			The LQIP value is actual image data
			encoded into a base64-string which can be 
			used directly as the src property of an img tag!
			Remember to set the height and width 
			properties, though, or it'll be very small
		-->
		<img
      height="100"
      width="100"
      src="data:image/png;base64,iVBORw0KGgo[...50 lines of this stuff omitted for brevity...]Jggg=="
    />

```

**BlurHash** is a more [advanced method](https://blurha.sh/) of creating a lightweight image preview that can give a superior result and comes in a more concise format. The trade-off is that you'll need to decode the value using a [helper library](https://github.com/woltapp/blurhash) before use. A blurHash value might look something like this:

```json

"blurHash" : "d79Z$I-o4:IoxaofR*WC00Io?GxtM{Rkt7s:~VxaNGRk"

```

Example of use in a JavaScript project: 

```javascript
import { decode } from "blurhash";

const pixels = decode("LEHV6nWB2yk8pyo0adR*.7kCMdnj", 32, 32);

const canvas = document.createElement("canvas");
const ctx = canvas.getContext("2d");
const imageData = ctx.createImageData(width, height);
imageData.data.set(pixels);
ctx.putImageData(imageData, 0, 0);
document.body.append(canvas);
```

### `palette`

Sanity will generate a color palette by analyzing your image. Along with the dominant swatches, a collection of suggestions for colors that contrast nicely with them is returned, as well as a numeral indication of how prominently each color is represented in the image. A palette object might look like:

```json
{
  "_type": "sanity.imagePalette",
  "darkMuted": {
    "_type": "sanity.imagePaletteSwatch",
    "background": "#653a2d",
    "foreground": "#fff",
    "population": 3.8,
    "title": "#fff"
  },
  "darkVibrant": {
    "_type": "sanity.imagePaletteSwatch",
    "background": "#c4850b",
    "foreground": "#fff",
    "population": 0.08,
    "title": "#fff"
  },
  "dominant": {
    "_type": "sanity.imagePaletteSwatch",
    "background": "#d5c3ba",
    "foreground": "#000",
    "population": 7.17,
    "title": "#fff"
  },
  "lightMuted": {
		// [...] truncated for brevity
  },
  "lightVibrant": {
		// [...] truncated for brevity
  },
  "muted": {
		// [...] truncated for brevity
  },
  "vibrant": {
		// [...] truncated for brevity
  }
}
```

> [!TIP]
> Protip
> If lqip, blurHash, or palette values are absent from your image asset, it's likely that at the time the image was uploaded, a metadata array was specified and the value in question was not included in the array.

## Camera and Location

> [!NOTE]
> Excluded by default
> These values are not included in your image metadata unless a metadata array is specified and these values are specifically requested. This is because camera and location data generally contain private or identifying information.

### `image`

This field contains basic information about the image such as camera make and model, resolution, and orientation. For more detailed information, use the `exif` field. The following is an example readout:

```json
{
  "_type": "sanity.imageExifTags",
  "Make": "Apple",
  "Model": "iPhone 6",
  "Orientation": 1,
  "XResolution": 72,
  "YResolution": 72,
  "ResolutionUnit": 2,
  "Software": "Photos 1.0",
  "ModifyDate": Sat Feb 28 2015 17:13:57 GMT-0800 (PST),
  "ExifOffset": 198,
  "GPSInfo": 1008
}
```

### `exif`

Short for [Exchangeable Image File](https://en.wikipedia.org/wiki/Exif) format, this field contains information about the image file itself and the conditions under which it was produced – typically camera settings. Exactly what data is contained here depends on the origins of the file. Below is an example readout of the Exif object for a photo taken with an iPhone camera:

```json
{
  "_type": "sanity.imageExifMetadata",
  "ApertureValue": 1.6959938128383605,
  "BrightnessValue": 1.7619172145845785,
  "DateTimeDigitized": "2020-03-19T12:25:17.000Z",
  "DateTimeOriginal": "2020-03-19T12:25:17.000Z",
  "ExposureBiasValue": 0,
  "ExposureMode": 0,
  "ExposureProgram": 2,
  "ExposureTime": 0.020833333333333332,
  "FNumber": 1.8,
  "Flash": 16,
  "FocalLength": 4.25,
  "FocalLengthIn35mmFormat": 26,
  "ISO": 250,
  "LensMake": "Apple",
  "LensModel": "iPhone 11 Pro back triple camera 4.25mm f/1.8",
  "LensSpecification": [
    1.5399999618512084,
    6,
    1.8,
    2.4
  ],
  "MeteringMode": 5,
  "PixelXDimension": 4032,
  "PixelYDimension": 3024,
  "SceneCaptureType": 0,
  "SensingMethod": 2,
  "ShutterSpeedValue": 5.586024712398807,
  "SubSecTimeDigitized": "900",
  "SubSecTimeOriginal": "900",
  "SubjectArea": [
    2323,
    710,
    1410,
    1412
  ],
  "WhiteBalance": 0
}
```

### `location`

This field, as you might expect, returns geographical data, usually representing the coordinates where the photo was taken. It conforms to the specification of the [geopoint](https://www.sanity.io/docs/geopoint-type) schema type, and might look like this:

```json
{
  "_type": "geopoint",
  "alt": 168.32554596241746,
  "lat": 59.948811111111105,
  "lng": 10.867780555555557
}
```



## In conclusion

Image assets in your Sanity Content Lake may include a range of helpful metadata. 

- **Always included:** Essential facts about your image: height, width, aspect ratio, and information about transparency.
- **Included by default:** Useful information generated from the image on upload: minified placeholders and palette values.
- **Excluded by default: **Potentially private information about the place and circumstances under which the image was created: exif and location values.



# Transformations

## The anatomy of the image URL

This article provides a detailed rundown of all the options for transforming images with Sanity. A general introduction to our image pipeline and tools can be found [here](/docs/apis-and-sdks/presenting-images).


Let's start by dissecting this Sanity image URL:

```
https://cdn.sanity.io/images/zp7mbokg/production/G3i4emG6B8JnTmGoN0UjgAp8-300x450.jpg
```

- `https://cdn.sanity.io/images/` is the common base for all Sanity image URLs. 
- `zp7mbokg` is the project ID 
- `production` is the dataset name
- `G3i4emG6B8JnTmGoN0UjgAp8` is the asset ID and the asset metadata document `_id`
- `300x450` is the width and height of the original image
- `jpg` is the file format of the *original* asset file

The image URLs can always be found in the asset metadata document referred to in an asset reference. Still, you don't have to fetch this document as the asset document ID contains all the information and represents a stable, documented interface you can trust.

The asset ID corresponding to the URLs above looks like this: `"image-G3i4emG6B8JnTmGoN0UjgAp8-300x450-jpg"`.  It provides the name, dimensions, and format. Given the project ID and dataset name, you have every piece you need to assemble the URLs without fetching the asset document:

`https://cdn.sanity.io/images/<project id>/<dataset name>/<asset name>-<original width>x<original height>.<original file format>`

This represents the base URL. If you fetch this, you will be served the original asset. This potentially uses a lot of bandwidth as content managers are advised to upload full-resolution assets. With the Sanity image pipeline, you can scale, crop, and process images on the fly based on URL parameters. E.g. by appending `?h=200` to the base URL, you instruct Sanity to scale the image to be 200 pixels tall:

`https://cdn.sanity.io/images/zp7mbokg/production/G3i4emG6B8JnTmGoN0UjgAp8-300x450.jpg?h=200`

You can specify any number of parameters. This will extract a rectangle from the image starting at 70 pixels from the left and 20 pixels from the top at a width of 120 pixels and a height of 150 pixels, scale it to 200 pixels tall, and blur it:

`https://cdn.sanity.io/images/zp7mbokg/production/G3i4emG6B8JnTmGoN0UjgAp8-300x450.jpg?rect=70,20,120,150&h=200&blur=10`

Even though the Sanity image backend is fast, you get a tremendous performance boost if your front end limits the number of sizes and crops you ask for. Sanity will cache the result in the global CDN, and if we see the same URLs again, we serve the same data directly from the edge cache closest to the user.

> [!WARNING]
> Gotcha
> Non-integer values for parameters expecting integers may cause performance issues or timeouts. It is recommended that you always use integer values when the parameter calls for it (e.g., w and h), including when returning calculated values.
> 
> &h=200 - Correct
> 
> &h=200.0 - May be problematic

## Supported image types

While the [Image schema type](/docs/image-type) supports a wide range of [image formats](/docs/content-lake/assets), transformations are limited to JPEG, PNG, WebP, PJPG, TIFF, AVIF, and GIF. For all other formats, you should convert the image to one of the supported file types before performing additional transformations.

> [!TIP]
> Protip
> The image pipeline now supports transforming animated GIFs.

## The URL parameters

> [!WARNING]
> Gotcha
> Small images get scaled up to the width or height you specify. To avoid this use &fit=max.

#### Properties

| Property | Description |
|----------|-------------|
| auto | Set auto=format to automatically return an image in in the most optimized format supported by the browser as determined by its Accept header. To achieve the same result in a non-browser context, use the fm parameter instead to specify the desired format, for example fm=webp. |
| bg | Fill in any transparent areas in the image with a color. The string must be resolve to a valid hexadecimal color (RGB, ARGB, RRGGBB, or AARRGGBB). E.g. bg=ff00 for red background with no transparency. |
| blur | Blur 1-2000. |
| crop | Use with fit=crop to specify how cropping is performed:

top, bottom, left and right: The crop starts from the edge specified. crop=top,left will crop the image starting in the top left corner.

center: Will crop around the center of the image

focalpoint: Will crop around the focal point specified using the fp-x and fp-y parameters.

entropy: Attempts to preserve the "most important" part of the image by selecting the crop that preserves the most complex part of the image. |
| dl | Configures the headers so that opening this link causes the browser to download the image rather than showing it. The browser will suggest to use the file name you provided. |
| dlRaw | As dl but requests the original file/image asset. Requires authentication. |
| dpr | Specifies device pixel ratio scaling factor. From 1 to 3. |
| fit | Affects how the image is handled when you specify target dimensions.

clip: The image is resized to fit within the bounds you specified without cropping or distorting the image.

crop: Crops the image to fill the size you specified when you specify both w and h

fill: Like clip, but any free area not covered by your image is filled with the color specified in the bg parameter.

fillmax: Places the image within box you specify, never scaling the image up. If there is excess room in the image, it is filled with the color specified in the bg parameter.

max: Fit the image within the box you specify, but never scaling the image up.

scale: Scales the image to fit the constraining dimensions exactly. The resulting image will fill the dimensions, and will not maintain the aspect ratio of the input image.

min: Resizes and crops the image to match the aspect ratio of the requested width and height. Will not exceed the original width and height of the image. |
| flip | Flipping. Flip image horizontally, vertically or both. Possible values: h, v, hv |
| fm | Convert image to jpg, pjpg, png, or webp.

Note that avif is not a valid option for this parameter as AVIF transformations are generated asynchronously. See the AVIF format details below.

This property also accepts a value of json, which does not convert the image but returns information about the image including width, height, frame count, content length, and content type. |
| fp-x | Focal Point X. Specify a center point to focus on when cropping the image. Values from 0.0 to 1.0 in fractions of the image dimensions. (See crop) |
| fp-y | Focal Point Y. Specify a center point to focus on when cropping the image. Values from 0.0 to 1.0 in fractions of the image dimensions. (See crop) |
| frame | The frame of an animated image. The only valid value is 1, which is the first frame. |
| h | Height of the image in pixels. Scales the image to be that tall. |
| invert | Invert the image. |
| max-h | Maximum height. Specifies size limits giving the backend some freedom in picking a size according to the source image aspect ratio. This parameter only works when also specifying fit=crop. |
| max-w | Maximum width in the context of image cropping. Specifies size limits giving the backend some freedom in picking a size according to the source image aspect ratio. This parameter only works when also specifying fit=crop. |
| min-h | Minimum height. Specifies size limits giving the backend some freedom in picking a size according to the source image aspect ratio. This parameter only works when also specifying fit=crop. |
| min-w | Minimum width. Specifies size limits giving the backend some freedom in picking a size according to the source image aspect ratio. This parameter only works when also specifying fit=crop. |
| or | Orientation. Possible values: 0, 90, 180 or 270.Rotate the image in 90 degree increments. |
| pad | The number of pixels to pad the image.  Applies to both width and height. |
| q | Quality 0-100. Specify the compression quality (where applicable). Defaults are 75 for JPG and WebP. |
| rect | Crop the image according to the provided coordinate values (left, top, width, height). 

left: Number of pixels from the left of the image

top: Number of pixels from the top of the image

width: Width, in pixels, of the crop from the left value

height: Height, in pixels, of the crop from the top value |
| sat | Saturation. Currently the asset pipeline only supports sat=-100, which renders the image with grayscale colors. Support for more levels of saturation is planned for later. |
| sharp | Sharpen 0-100. |
| w | Width of the image in pixels. Scales the image to be that wide. |


## AVIF transformations

Images that have the query parameter `auto` set to `format` and are requested from a browser that supports the AVIF format will potentially get an AVIF returned. 

There are a few exceptions/quirks:

The first few requests for an AVIF may get the "second best option" (WebP if supported, otherwise PNG/JPG depending on the source image). Subsequent requests will eventually get an AVIF back. This is done to ensure a speedy response, since encoding AVIFs is a slow process. 

Image requests made prior to the AVIF rollout may already be cached in our CDN and will not return an AVIF response until they expire/fall out of the cache. In other words: if you are not seeing AVIF images being returned, don't worry — they should eventually return AVIF. 

You can use `curl` to verify the behavior:

```sh
# Replace the URL with an actual URL from your project.
# Remember to include `?auto=format`!
curl -sS -I \
  -H 'accept: image/avif,image/webp,image/*' \
  'https://cdn.sanity.io/images/:projectId/:dataset/:filename?auto=format' \
  | grep 'content-type:'
```

On the first request, you will likely see `image/webp` returned. After waiting 30 seconds, run the same command again, and you should see `image/avif`. If you don't, wait a little longer and retry. If you still do not see AVIF, ensure that the accept header includes `image/avif` (before other formats) and that the query parameters includes `auto=format`.

> [!WARNING]
> Gotcha
> Because AVIF transformations are generated asynchronously, you cannot explicitly request AVIF transformations using the fm query parameter. Instead, use the accept header as described above.

## Read more

[Client library for generating urls](https://github.com/sanity-io/image-url)







# Transactions

Document updates in Sanity are called mutations, and a group of one or more mutations are executed as a single unit called a transaction. Transactions are submitted via the [HTTP mutation API](/docs/http-reference/mutation), and may look like the following:

```json
{ 
  "mutations": [
    {"create": {
        "_id": "alien",
        "_type": "movie",
        "title": "Alien"
    }},
    {"patch": {
        "id": "alien",
        "set": {
            "year": 1979,
            "genre": "Science Fiction"
        }
    }},
    {"delete": {
        "id": "blade-runner"
    }}
  ]
}
```

Transactions are atomic: either all of the mutations succeed or they all fail. More details on transaction semantics can be found below, and details on available mutations can be found in the [mutation API reference](/docs/http-reference/mutation).

All transactions are recorded in an internal transaction log. This log is available through the  [document history API](/docs/http-reference/history).

Once a transaction is committed, any [real-time listeners](/docs/content-lake/realtime-updates) will be notified about the changes.

## Eventual Consistency

Internally, the Sanity data store consists of two main components: a document store where transactions are executed, and a search store where GROQ queries are executed. Document changes are continuously synced between the document and search stores, but this happens outside of transactions, so there is a delay between a transaction being committed and the changes being visible to queries.

As a result, transactions are strongly consistent (they always see the latest data), but queries are eventually consistent (they may see outdated data, but will eventually see the latest data given enough time). Under normal circumstances the convergence time for queries is generally short (less than 1 second), but during operational anomalies such as network failures or heavy load it can be much longer.

> [!WARNING]
> Gotcha
> Transactions using the query parameter are not strongly consistent, since the query is first executed against the search store, which may see outdated data.

When submitting transactions, the `visibility` parameter can be used to control how documents should be synced to the search store. `sync` (the default) causes the transaction request to return only after both the transaction has been committed and the changes have been synced to the search store. `async` causes the request to return once the transaction has been committed, and then syncs the changes to the search store afterwards (typically within a second). `deferred` causes the request to return once the transaction has been committed, but does not trigger syncing to the search store at all, and instead relies on a background process to sync the changes at a later time (within seconds to minutes) - this allows for much higher throughput when submitting a large number of mutations.

> [!WARNING]
> Gotcha
> By default, real-time listeners receive change notifications as soon as a transaction has been committed, but before changes have been synced to the search store. This means that a listening client running a GROQ query in response to a change will usually not see the updated document in the query result. The client can specify visibility=query for the listener to receive notifications after they have been synced to the search store, when possible.

## ACID Compliance

Sanity transactions are ACID-compliant, which means that they have the following properties:

- **Atomicity:** the transaction constitutes a single unit, such that either all of its mutations succeed or they all fail.
- **Consistency:** if a transaction succeeds then the resulting documents are guaranteed to satisfy all data store constraints, i.e. the transaction cannot leave the data in an inconsistent state. For example, this guarantees that there cannot exist two documents with the same ID. Note that this is a different concept than eventual consistency as described above.

> [!WARNING]
> Gotcha
> Sanity schemas are currently only enforced client-side by the Sanity studio, and thus the consistency guarantees do not extend to constraints specified in the schema. Non-studio clients may submit data which does not satisfy the schema, and schema changes may leave old data which no longer satisfies the new schema.

- **Isolation:** transactions have [repeatable read isolation](https://en.wikipedia.org/wiki/Isolation_(database_systems)#Repeatable_reads) via exclusive locks. When a document is first accessed by a transaction it is locked, blocking concurrent transactions from both reading and writing the document until the initial transaction completes. Since locks are acquired on first access and not on transaction start, it is possible for a mutation to see the effects of a concurrent transaction that was committed after the current transaction began but before the document was accessed and locked.

> [!WARNING]
> Gotcha
> When using the query option for mutations, the mutation first executes the given GROQ query against the search store, and then executes mutations against the matching documents. Since the search store is eventually consistent, it is possible for the query to return outdated results, which can cause the mutations to incorrectly affect or ignore documents that have recently been modified. This effectively reduces the transaction isolation level to read committed, and can cause multiple data anomalies including lost updates, non-repeatable reads, phantom reads, and write skew.

- **Durability:** once a transaction succeeds, it is guaranteed to have been written to disk. However, it is not guaranteed to have been replicated to other servers. This means it is possible to lose a transaction in the rare scenario where a primary server crashes and is replaced by a replica server after the transaction has been committed but before it has been replicated.

## Concurrency Control

Transactions use exclusive locks to prevent concurrent transactions from interfering with each other (see description of transaction isolation above). However, clients often use read-write cycles that run a GROQ query and then submit transactions based on the results. This pattern does not have the same isolation guarantees as transactions. For example, if a different client writes a value after our client has read a document but before our client writes its new value, then the value that the other client wrote may be lost (an anomaly known as a lost update).

Clients can use optimistic locking to prevent these kinds of data anomalies. `patch` mutations take an optional `ifRevisionID` parameter containing a document revision ID (typically from the document's `_rev` attribute), and are only accepted if the given revision ID matches the document's current revision ID. If a different client has modified the document in the meanwhile then the mutation will be rejected with a `409 Conflict` HTTP status code, allowing the client to fetch the updated document and retry the operation with fresh data. Optimistic locking will also guard against submitting mutations based on outdated query results caused by the data store's eventual consistency model.



# Importing Data

> [!NOTE]
> Media Library available
> This guide outlines details for importing documents, including images and files, into a dataset. The Media Library allows images and files to be used in any dataset in your organization.
> 
> For details on importing assets to a centralized library, review our guide on importing assets.

There are two ways to import data into your Sanity project. 

The recommended way of importing data is to use the [Command Line Interface](/docs/apis-and-sdks/cli). You can run  `sanity dataset import --help` for a quick summary of syntax and options. Your other option is to use one of our client libraries and handle it yourself. 

> [!WARNING]
> Gotcha
> Consider disabling any webhooks you might have that could cause high volumes of traffic to the receiving endpoint on importing data.

## Import using the CLI

The Sanity import tool operates on [newline-delimited JSON](https://github.com/ndjson/ndjson-spec) (NDJSON) files. Basically, each line in a file is a valid JSON-object containing a document you want to import.

Documents should follow the structure of your [data model](/docs/archive/content-modelling) – most importantly, the requirement of a `_type` attribute. The `_id` field is optional – but helpful – in case you want to make references or be able to re-import your data replacing data from an old import. `_id`s in Sanity are usually a [GUID](http://guid.one/guid), but any string containing only letters, numbers, hyphens, and underscores are valid.

During import, all references are automatically set to *weak*, then flipped to *strong* after all documents are in place. This ensures that you can import documents that reference other documents in any order you like.

Assets (images and files) are stored using references in Sanity. To make it easy to import these and refer to them within your documents, you can use a special `_sanityAsset` property where you would normally put a `_ref`. For instance, let's say you want your document to end up like this:

```javascript
{
  "_id": "movie_123",
  "_type": "movie",
  "title": "Rogue One",
  "poster": {
    "_type": "image",
    "asset": {
      "_ref": "image_234",
      "_type": "reference"
    }
  }
}
```

This is what your ready-to-import document should look like:

```javascript
{
  "_id": "movie_123",
  "_type": "movie",
  "title": "Rogue One",
  "poster": {
    "_type": "image",
    "_sanityAsset": "image@file:///local/path/to/rogue-one-poster.jpg",
  }
}
```

However, ndjson uses the newline character as delimiter (NDJSON == Newline Delimited JSON), therefore your ndjson file must be structured with one document on each line, like this:

```json
{"_id": "movie_123", "_type": "movie", "title": "Rogue One", "poster": {"_type": "image", "_sanityAsset": "image@file:///local/path/to/rogue-one-poster.jpg"}}
{"_id": "another_movie", "_type": "movie"}
{"_id": "yet_another_movie", "_type": "movie"}

```

Note that you need to prefix the asset URL with a type declaration – either `image@` or `file@`.

If your asset is on the Internet use `image@https://example.com/path/to/rogue-one-poster.jpg` instead of `image@file:///local/path/to/rogue-one-poster.jpg`.

> [!WARNING]
> Gotcha
> File URIs are absolute so include the entire path.

Once you have prepared your ndjson file, you can run the import using the Sanity CLI.

> [!NOTE]
> What should I import?
> In some cases you will want to import your ndjson file, such as when you've exported your dataset, made changes to the ndjson file, and are importing it back into the same dataset.
> 
> In other cases you will want to compress your dataset back into a tarball / tar file (.tar, .tar.gz, or .tgz), which includes the ndjson file and your assets. You might take this approach when migrating data to a new dataset, as you'll want to maintain references to assets.
> 
> If you're getting an import error like Error: Error while fetching asset from "file://./images/<image-name>.<ext>": File does not exist at the specified endpoint, you can either (1) make the filenames absolute or (2) import a tarball (including assets) rather than an ndjson file.

```bash
sanity dataset import <file> <targetDataset>
```

E.g.:

```bash
sanity dataset import my-data-dump.ndjson production

// or

sanity dataset import staging.tar.gz production
```

> [!TIP]
> Protip
> The import will fail if an incoming document already exists in the dataset. A couple of options allow you to amend this:
> 
> --replace Overwrite existing documents. If you specify _id in the imported data, this flag can be very useful. It will let you reimport stuff that you got wrong in an earlier pass.
> --missing Only create documents which don't exist, leave the rest alone.
> 
> The import will also fail if an asset is unavailable. This typically happens if the file isn't at the given path on your local system or the asset URL returns 404. You can tell the import not to fail on a missing asset by passing the --allow-failing-assets option.

> [!TIP]
> Protip
> Check out our reference-type docs page for more ways on how to reference different documents.

## Import using a client library

If you prefer not to use our CLI import tool, you may of course do the import yourself with help from one of our client libraries.

There are some common pitfalls to keep in mind:

- *Concurrency*. While you may have thousands of documents to import, you shouldn't trigger thousands of requests in parallel. This is going to exceed API rate limits and might fail. We advise you to use a queue with a reasonably low concurrency.
Use a library to keep your import below our [API rate limit](/docs/content-lake/technical-limits):

```javascript
const {default: PQueue} = require('p-queue')
const queue = new PQueue({
  concurrency: 1,
  interval: 1000 / 25
})

queue.add(() => client.create(...))
queue.add(() => client.patch('id').inc('visits').commit())
```

- *API usage limits*. Importing large data sets can quickly cause a lot of requests, especially if you import a single document per request. It is usually a good idea to send [multiple mutations within a single transaction](https://www.sanity.io/docs/js-client#multiple-mutations-in-a-transaction).
- *Mutation size limits*. While it's a good idea to do multiple mutations per transaction, you need to make sure that the size of the request is [within our limits](https://www.sanity.io/docs/technical-limits#c854fda72658), in terms of byte size.
- *Mutation visibility*. A Sanity client will use the [visibility mode](https://www.sanity.io/docs/http-mutations#visibility-937bc4250c79) of `sync` by default, which means that it will wait for the documents to be searchable before returning. This should not be necessary when importing large datasets, so we recommend you use `deferred`. If you have a lot of documents, it can take a little while for them to be searchable, but the import job will move along much faster.
- *References*. If you are referring to one document from another, they either need to be imported in the right order, or the reference needs to be flagged as *weak* by setting the `_weak` property to `true`. After importing, you probably want to remove the weak property in order to prevent referenced documents from being deleted.

> [!WARNING]
> Gotcha
> When a weak reference is desired, you should use the weak property when defined in the schema but _weak when set up using a client. Using the weak property with the client will likely return the error: key "weak" not allowed in ref.
> 
> weak in the schema, _weak in the JSON.

- *Assets*. Since assets (e.g., files and images) in Sanity are stored using references, you'll need to upload the assets first and put the returned document ID in your reference.

With this in mind, do check out our [client libraries](/docs/client-libraries) documentation to see how to perform mutations.



# Migrating your schema and content

Most projects will require changes to [the schema](/docs/schema-types), that is, your content model. At the start of a project, these changes are often additive and only involve building out the schema with new document and field types. There will be no actual content that needs to be changed.

However, sometimes, you must change the existing schema and content in your Sanity Studio and Content Lake dataset (and maybe for your GraphQL API). Maybe you want to rename a field, add a new validation rule, check how existing documents will be affected, or move fields into an array or object type. There are many valid and necessary reasons to change and evolve your schema and the content that goes with it.

While changing a Sanity Studio schema is most often straightforward, if you have content in documents that assumes a certain structure, you will want to migrate these to match your updated schema. This is where our tools for schema change management come in.

[Introduction to Schema Change Management](https://www.sanity.io/learn/course/handling-schema-changes-confidently)

[Content migration cheat sheet](/docs/content-lake/content-migration-cheatsheet)

[Important considerations for schema change management](/docs/content-lake/important-considerations-for-schema-and-content-migrations)

[CLI tooling for validations](/docs/cli-reference/documents)

[CLI tooling for content migrations](/docs/cli-reference/cli-migration)



## What is schema change management?

The process of changing your schema and existing content is called “schema change management.” This is comparable to what other content management systems call “content migrations,” but it goes beyond that. Schema change management is about what you must think about when changing the structure of your content or the validation rules of your field and document types.

In some contexts, “content migrations” might refer to cases where you go from one content management system to another. While the following can be useful as part of that process, this article and the features we address focus on migrations *within* a dataset in a Sanity project.

Common examples of schema change management:

- You need to change a schema for a Sanity Studio workspace and wish to update existing content to validate against the new schema update
- You have imported content from another content management system (CMS) and wish to change and improve its structure
- You have added new validation rules and wish to introspect and list documents that need to be updated by a content team or a content migration script
- You are crafting an NDJSON import file and want to validate the documents before importing them into your dataset
- You just want insight into the validation status across your whole dataset (as opposed to per document in the Studio) to troubleshoot implementation bugs

> [!WARNING]
> Gotcha
> It’s important to note that changing the schema for a Sanity Studio workspace will not automatically change or delete existing content in your dataset. 
> 
> We consider this a feature that helps prevent unintended breakage for applications that rely on your content model having a certain shape. As with databases, you want changes to the content model (or data model) to be intentional, preferably reproducible, and part of your development workflow.

Sanity offers tooling to bring existing content up to date so you have flexibility in adding, removing, and changing the shape of your content.

## Tooling for schema and content migrations in a dataset

Sanity offers tooling and capabilities that support schema migrations:

- The [deprecated](/docs/schema-types) property lets you mark document and field types that shouldn’t be used anymore with a defined `reason`. This configuration will appear visually in the Sanity Studio and part of the [GraphQL API schema](/docs/content-lake/graphql) for those using this feature.
- Command Line Interface (CLI) commands: - [sanity documents validate](/docs/cli-reference/documents) lets you check the validation status of documents in a dataset or export file
- [sanity schema validate](/docs/cli-reference/cli-schema) lets you identify potential errors in your schema configuration (is also run by `sanity documents validate`)
- [sanity migration [command]](/docs/cli-reference/cli-migration) lets you create, list, and run content migrations (defined in code) with helper functions for defining how documents should change
- [sanity dataset import|export](/docs/cli-reference/dataset) (or using [Advanced Dataset Management](/docs/content-lake/how-to-use-cloud-clone-for-datasets)) lets you export/import datasets to test and validate a migration in a non-production environment


- If you have needs beyond what the current tooling gives you, you can also use the [Sanity Content Lake APIs](/docs/http-api) for directly querying and mutating content.

## Deprecating document and field types

While you can add, change, and remove schema types freely without cascading changes to existing content, chances are you want to be more considered, especially for projects running in production. Often, you want content teams to be able to see and (sometimes) edit) a deprecated field and explain why and how a schema type has been changed.

All user-configurable schema types support explicit deprecation through a configuration property called `deprecated` where the value is an object with a defined `reason` as a string (required):

```typescript
export const person = defineType({
  name: 'person',
  type: 'document',
  deprecated: {
    reason: 'Use the Author document type instead.'
  },
  fields: [],
  readOnly: true // to prevent further edits
})

```

You can use the `readOnly: true` configuration to prevent deprecated fields and documents from being edited.

```tsx
export const name = defineField({
  name: 'firstName',
  type: 'string',
  description: `The person's first name`,
  deprecated: {
    reason: 'Use the name field instead.'
  },
  readOnly: true, // to prevent further edits
})

```

This configuration will show up visually in the studio:

![](https://cdn.sanity.io/images/3do82whm/next/ef1e94c0d63a200fcf0d53837871e848b1e60b27-2000x903.png)

Our GraphQL API also supports the deprecation property, which translates the property and reason into the [directives for deprecations in the GraphQL specification](https://spec.graphql.org/October2021/#sec--deprecated).

## Checking validating status across all documents in a dataset

Validation rules for document and field types are primarily shown within a document form for users of Sanity Studio. However, when working with schema and content migration, it is useful to review the validation status for all documents in a dataset, especially for quickly getting insight into the state of your datasets and helping you decide what migrations jobs to create.

The Sanity CLI offers this functionality with `sanity documents validate`. This command runs all `validation` rules in your current schema configuration on documents in a virtual browser environment from where the command is run. In addition, it can also report on document types, that is, documents with a `_type` that is not covered in your schema configuration.

Without any extra flags, `sanity documents validate` will output a pretty formatted list of validation errors and warnings from the project and dataset defined in `sanity.cli.ts`. It will also give you actionable Studio links, provided you have configured a Studio URL in the project settings on [sanity.io/manage](https://www.sanity.io/manage).

You can use flags to specify specific Studio workspace (if it has more than one), dataset name, validation level (`error`, `warning`, `info`), and output formats (`pretty`, `json`, `ndjson`).

You can also use the `--file [file path]` flag to run validations against a dataset import/export file (supports both `filename.ndjson` or `filename.tar.gz`).

### The anatomy of the validation output

When exporting to a JSON format, the output of the CLI gives you actionable data such as the document ID (the same as `_id`) and type (the same as `_type`), the revision ID, the URL to find the document in a deployed Studio, as well as an array with all validation notices that it can have. The `level` property on the root of this object will always reflect the most severe level in the markers array, from “error,” “warning,” to “info.

```tsx
{
    "documentId": "person_robin-sachs",
    "documentType": "person",
    "revision": "GspWPjs815p7KTxv2q3x76",
    "intentUrl": "https://schema-change-management-demo.sanity.studio/intent/edit/id=person_robin-sachs;type=person",
    "markers": [
      {
        "path": ["fullName"],
        "level": "warning",
        "message": "Field 'fullName' does not exist on type 'person'"
      }
    ],
    "level": "warning"
  }

```

### A lot of validation errors? Pipe the output to a file!

If you have a sizeable dataset and/or a bunch of validation errors, the output will likely overflow your terminal’s buffer. In these cases, piping the output to a file in the terminal can be useful. For most shell environments, it will look something like this:

```tsx
sanity documents validate -y --format ndjson > documentValidations.ndjson

```

A pro tip is that you can then use [GROQ CLI](https://github.com/sanity-io/groq-cli) to parse this file. Let’s say you wanted to have a list of Studio URLs of all documents that have a validation error (and not just a `warning`):

```tsx
# npm install --global groq-cli
cat documentValidations.ndjson|groq -n "*[level == 'error'].intentUrl"

```

## Working with content migrations in the CLI

The Sanity CLI has tooling for creating and running content migrations against a dataset. Content migrations are described in JavaScript (or TypeScript) as files inside a `migrations` folder in your Sanity Studio project. You can also automate and run content migration as part of a CD/CI pipeline.

### Creating new content migrations

You can use the CLI command `sanity migration create` to create a new content migration file. The CLI will prompt you for a human-friendly title, which document types you want to filter on, and a content migration template to start from. You can also go to [the content migration cheat sheet](/docs/content-lake/content-migration-cheatsheet) to find starting points for common migration patterns.

### File and folder structure

The sanity migration CLI command will create and look for migration files in a `migrations` folder, when run from the Studio project root. You can write migration files in JavaScript (`.js`, `.mjs`, `.cjs`) and TypeScript (`.ts`).

You can store migration script in two patterns, that can also be combined:

- `studioFolder/migrations/my-content-migration.ts`
- `studioFolder/migrations/my-content-migration/index.ts`

The latter is useful when you have a complex migration and want to split up the code in more files.

### The anatomy of a content migration file

A migration file should export a `defineMigration` (using `default export`) with the following configuration:

- `title`: A reader-friendly description of what the content migration does
- `documentTypes`: an array of document types to run the content migration on. If you don’t define this, the migration type will target all document types.
- `filter`: A simple GROQ-filter (doesn’t support joins) for documents you want to run the content migration on
- `migrate`: an object of named helper functions corresponding to the primary schema type of the content you want to migrate. You can also run these functions as async and return the migration instructions as promises if you need to fetch data from elsewhere

```tsx
// migrations/example-migration/index.ts
import {defineMigration} from 'sanity/migrate'

export default defineMigration({
  title: 'A human-friendly description of what this content migration does',
  documentTypes: ['aDocumentType'],
  migrate: {
    document(doc, context) {
      // this will be called for every document of the matching type
    },
    node(node, path, context) {
      // this will be called for every node in every document of the matching type
    },
    object(node, path, context) {
      // this will be called for every object node in every document of the matching type
    },
    array(node, path, context) {
      // this will be called for every array node in every document of the matching type
    },
    string(node, path, context) {
      // this will be called for every string node in every document of the matching type
    },
    number(node, path, context) {
      // this will be called for every number node in every document of the matching type
    },
    boolean(node, path, context) {
      // this will be called for every boolean node in every document of the matching type
    },
    null(node, path, context) {
      // this will be called for every null node in every document of the matching type
    },
  },
})

```

[Content migrations cheat sheet](/docs/content-lake/content-migration-cheatsheet)



### Understanding `node` in the context of content migrations

We refer to “node” in the code example above and that there is a mutation creator function for `node(node, path, context)`. Here, a “node” refers to any value in a document, including nested ones.

The `object`, `array`, `string`, `number`, `boolean`, and `null` functions are subsets of the node function to make it easier to access content by its JSON data type.

The `node` as the first argument in these functions will be the value for any given node.

The `path` will tell you where the `node` value comes from in the document. This is where you will find any node's key/property/field name and the `_key` value in array data.

> [!TIP]
> Protip
> The migration tooling will run mutations with autoGenerateArrayKeys enabled. This means that you don't have to manually set a _key value when you work with objects in array data.

Let’s say you log out the values for `node` and `path` (`console.log(node, path)`). The output from a document with a `title` field would then be:

```text
My title [ 'title' ]

```

In the same document, for a slug field:

```text
my-title [ 'slug', 'current' ]

```

And for the first paragraph of a `description` field that is a Portable Text field

```text
This is some text. 
[
  'description',
  { _key: '77b13c0924cb9fa06505215e8d0c8ee6' },
  'children',
  { _key: 'o9Bcm59c' },
  'text'
]

```

As you will see in the next section, you can change the data without authoring complex patches using these arguments even with more complex data structures.

### Getting the right changes in the right places

The content migration tooling has helper functions for defining the changes you want to make. Under the hood, they translate the content migrations you define into transactions of mutations and patches submitted to the Sanity Content Lake.

For example, if you want to change all occurrences of the single string “acme” to uppercase regardless of where it is in your dataset, then the content migration script could look like this:

```typescript
// migrations/uppercase-acme/index.ts
import {defineMigration, at, set} from 'sanity/migrate'

export default defineMigration({
  title: 'Make sure all strings with “acme” is uppercased to “ACME”',
  migrate: {
    string(node, path, context) {
      if (node === "acme") {
        return set(node.toUpperCase())
      }
    },
  },
})

```

When you use the `document` or are in a nested field, then the `at` function can be useful; you can pass the `path` and the operation to `at` to get the change in the right place. Below is an example of how to change the `_type` in an object field:

```typescript
// migrations/movie-to-film/index.ts
import {defineMigration, at, setIfMissing, unset} from 'sanity/migrate'

export default defineMigration({
  title: 'Change the movie object field to film',
  documentTypes: ['screening'],
  migrate: {
    document(doc, context) {
      return [
        at('film', setIfMissing(doc[from])),
        at('movie', unset())
      ]
    }
  }
})

```

As the example shows, you can return multiple patches as well. When changing a field name, you typically want to remove the value from the old field and add it to the new one.

Do note how the `setIfMissing` patch will only apply to documents without an existing `film` field. Usually, it’s good to be defensive when writing these scripts to avoid making unintended changes to existing data. Of course, you could also use the `set` operation to (over)write data to `film` regardless of whether it existed in a document.

### A note on immutable properties and migrating `_type` and `_id`

Before you go on trying to `at('_type', set('myNewDocumentType')`, we must address that some fields (or properties) for documents in the Content Lake are *immutable*. In other words, you can’t change the value when it’s set. These immutable built-in properties all begin with an underscore, including:

- `_type` 
- `_id`
- `_createdAt`
- `_updatedAt` (automatically updated on changes to the document)
- `_rev` (automatically updated on changes to the document)

We realize that there are cases where you want to change the value of these. The best strategy for now is to do this by exporting and unpacking your dataset. Make the changes in the NDJSON file that holds all the documents, delete the documents you want to change the `_type` of, and then import it again.

### How to run and execute content migrations

Once you have defined your content migrations as code and made sure they are nicely organized in a `migrations` folder in your Studio project, that is, alongside your `sanity.cli.ts` configuration, then you’re all set up for a test run.

To quickly list out what migrations the tool can access, run `sanity migration list`.

Now you can copy-paste the content migration ID and run `sanity migration run <migration-id>`

The command will *always* run in dry run mode; unless you add the `--no-dry-run` flag. When running in dry mode, the CLI will output a list of patches and document IDs from your content migration script. You can review this list to catch obvious mistakes.

Before executing a content migration, ensure you have backed up your dataset (`sanity dataset export` or `sanity dataset copy`).

[Important considerations for schema and content migrations](/docs/content-lake/important-considerations-for-schema-and-content-migrations)



When you feel confident that you want to make the changes, you can run the following command:

```sh
sanity migration run your-content-migration --no-dry-run

```

It will output its progress and detail how many documents were processed, mutations were generated, and transactions were committed.

### Rate limits

Migrations adhere to the same [rate limits](https://www.sanity.io/docs/technical-limits#50838b4c19db) as other API interactions with the Sanity Content Lake. If your migrations involve numerous patches, consider regulating the volume of simultaneous mutation requests to manage your API call rate with the **--concurrency** flag. You can run between 1 and 10 transactions in parallel. The default is 6, but you can lower it to avoid rate limits.



# Content migration cheat sheet

Below are content migration code snippets you can copy-paste and fit for your purposes. Requires familiarity with Sanity's schema and content migration tooling.

[Introduction to schema change management](/docs/content-lake/schema-and-content-migrations)

[Important considerations for schema and content migrations](/docs/content-lake/important-considerations-for-schema-and-content-migrations)

[Reference docs for working with content migrations in the CLI](/docs/cli-reference/cli-migration)

[Reference docs for running validations in the CLI](/docs/cli-reference/documents)

[TypeScript reference for sanity/migration](https://www.sanity.io/docs/reference/api/sanity/migrate/append)



## Rename a field in a document

```typescript
import {defineMigration, at, setIfMissing, unset} from 'sanity/migrate'

export default defineMigration({
  title: 'Rename field from "oldFieldName" to "newFieldName"',
  migrate: {
    document(doc, context) {
      return [
        at('newFieldName', setIfMissing(doc.oldFieldName)),
        at('oldFieldName', unset())
      ]
    }
  }
})

```

## Add a field with default value to all documents missing the field

Note: This example uses [an async generator pattern](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/AsyncGenerator) (`*migrate`) to read out the document ID (`_id`) one by one and return the patch. This prevents the script from loading all documents into memory.

```typescript
import {defineMigration, patch, at, setIfMissing} from 'sanity/migrate'

export default defineMigration({
  title: 'Add title field with default value',
  // documentTypes: ['post', 'article'], // only apply to certain document types
  async *migrate(documents, context) {
    for await (const document of documents()) {
      yield patch(document._id, [
        at('title', setIfMissing('Default title')),
      ])
    }
  }
})

```

## Migrate a reference field into an array of references

```tsx
import { defineMigration, at, setIfMissing, append, unset} from 'sanity/migrate'

export default defineMigration({
  title: 'Convert a reference field into an array of references',
	documentTypes: ['product'],
	filter: 'defined(category) && !defined(categories)',
  migrate: {
    document(product) {
      return [
        at('categories', setIfMissing([])),
        // use `prepend()` to insert at the start of the category array
        at('categories', append(product.category)),
        at('category', unset())
      ]
    }
  }
})

```

## Convert a string field into a Portable Text array

```typescript
import {pathsAreEqual, stringToPath} from 'sanity'
import {defineMigration, set} from 'sanity/migrate'

const targetPath = stringToPath('some.path')

export default defineMigration({
  title: 'Convert a string into a Portable Text array',

  migrate: {
    string(node, path, ctx) {
      if (pathsAreEqual(path, targetPath)) {
        return set([
          {
            style: 'normal',
            _type: 'block',
            children: [
              {
                _type: 'span',
                marks: [],
                text: node,
              },
            ],
            markDefs: [],
          },
        ])
      }
    },
  },
})

```

## Convert a Portable Text field into plain text

```typescript
import {pathsAreEqual, stringToPath, type PortableTextBlock} from 'sanity'
import {defineMigration, set} from 'sanity/migrate'

// if the portable text field is nested, specify the full path to it
const targetPath = stringToPath('some.path')

function toPlainText(blocks: PortableTextBlock[]) {
  return (
    blocks
      // loop through each block
      .map((block) => {
        // if it's not a text block with children,
        // return nothing
        if (block._type !== 'block' || !block.children) {
          return ''
        }
        // loop through the children spans, and join the
        // text strings
        return (block.children as {text: string}[]).map((child) => child.text).join('')
      })
      // join the paragraphs leaving split by two linebreaks
      .join('\n\n')
  )
}
export default defineMigration({
  title: 'A Portable Text field into plain text (only supporting top-leve',
  documentTypes: ['pt_allTheBellsAndWhistles'],

  migrate: {
    // eslint-disable-next-line consistent-return
    array(node, path, ctx) {
      if (pathsAreEqual(path, targetPath)) {
        return set(toPlainText(node as PortableTextBlock[]))
      }
    },
  },
})

```

## Migrate inline objects into references

This example shows how to convert an inline object in an array field into a new document and replace the array item with a reference to that new document. 

You can also use this in Portable Text fields and use `.filter({_type}) => _type == "blockType")` to convert only specific custom blocks.

```tsx
// npm install lodash
import {deburr} from 'lodash'
import {at, createIfNotExists, defineMigration, replace, patch} from 'sanity/migrate'

/**
 * if you want to make sure you don't create many duplicated
 * documents from the same pet, you can generate an ID for it 
 * that will be shared for all pets with the same name
 **/
function getPetId(pet: {name: string}) {
  return `pet-${deburr(pet.name.toLowerCase())}`
}

export default defineMigration({
  title: 'Convert an inline object in an array into a document and reference to it',
  documentTypes: ['human'],
  filter: 'defined(pets) && count(pets[]._ref) > 0',
  migrate: {
    document(human) {
      const currentPets = human.pets
      // migrate any pet object to a new document
      if (Array.isArray(currentPets) && currentPets.length > 0) {
        return currentPets
          // skip pets that have already been converted to a reference
          .filter((pet) => !pet._ref)
          .flatMap((pet) => {
            const petId = getPetId(pet)

            // avoid carrying over the array _key to the pet document
            const {_key, ...petAttributes} = pet

            return [
              createIfNotExists({
                _id: petId,
                _type: 'pet',
                ...petAttributes,
              }),
							patch(human._id, at(['pets'], replace([{_type: 'reference', _ref: petId}], {_key}))),
            ]
          })
      }
    },
  },
})

```

## Delete all documents by its type

```typescript
import {at, defineMigration, del, setIfMissing, unset} from 'sanity/migrate'

export default defineMigration({
  title: 'Delete posts and pages',
  documentTypes: ['post', 'page'],
  migrate: {
    document(doc) {
      // Note: If a document has incoming strong references, it can't be deleted by this script.
      return del(doc._id)
    },
  },
})

```

## Migrate a document type

> [!WARNING]
> Gotcha
> The _id and _type attributes/fields on documents are immutable; that is, they can't be changed once they are set; there is no straightforward way to change these using the content migration tooling.
> 
> The simplest and most controlled way of approaching the migration of a document _type and _id, is:
> 
> export your dataset (sanity dataset export <dataset>, add --no-assets if you're not planning to do anything with these)
> 
> Untar the export file (tar -xzvf <dataset>.tar.gz)
> 
> Open the NDJSON of your dataset (<dataset>.ndjson)
> 
> Use whatever method to find and replace all that you find suitable
> 
> Re-import your dataset with the --replace flag (sanity dataset import <dataset>.ndsjon <dataset> --replace)
> 
> Always ensure you have a backup of your dataset and triple-check before changing content in production.

## Delete file assets over a certain file size

This migration will attempt to delete any file asset metadata documents over 50MB in size. Deleting the metadata document will also delete the asset from your dataset.

- Update `documentTypes` to include `sanity.imageAsset` to remove images
- Update `filter` to adjust the maximum file size (in bytes)
- **Note:** The migration will fail if there are any references to the metadata document. The second `filter` example will filter out any large file assets already referenced by other documents.

```typescript
import { defineMigration, delete_ } from "sanity/migrate";

export default defineMigration({
  title: "Delete large files",
  documentTypes: ["sanity.fileAsset"],
  // Size is greater than 50MB
  filter: "size > 50000000",
  // Additionally only target unreferenced assets
  // filter: "size > 50000000 && count(*[references(^._id)]) == 0",

  migrate: {
    document(doc) {
      return delete_(doc._id);
    },
  },
});
```





# Schema migration principles

Content and schema migrations are potentially high-stakes operations, especially for projects that are in production. At the same time, it can be hard to nail a content model on the first try and anticipate all needs and requirements ahead of time. Our aim at Sanity is to enable you to work with content models and content through APIs early in your projects without being penalized for it when these need to change.

The considerations you need to take can differ depending on whether your project is in development or has been put into production. Below are some overarching considerations for both scenarios.

> [!WARNING]
> Gotcha
> Keep in mind that editors may be editing in the Studio while the migration is running. It's good to give them a heads-up before running a content migration on a dataset that is being worked on.

### For projects that haven’t been put into production yet

Most changes to a content model for projects in development that haven’t been put into production are additive; you *add* new document types and fields. Often, you will not have as much content that needs to be changed or updated either. Sometimes, editing documents manually in the Studio might be as efficient as running automated scripts to change them.

That said, there are also cases where you have a lot of content because you have engaged the content team to work in parallel to enhance the design and implementation process or have imported content from another system. You wish to take the opportunity to improve its structure.

In these cases, you should always consider to:

- Export the dataset before migration
- Commit your schema changes to git with updated validation rules and/or deprecated schema types
- Run `sanity documents validate` to check what documents give errors against your schema changes
- Initialize a migration job with `sanity migration create` to scaffold file and boilerplate code
- Dry run `sanity migration <ID>` and validate that the patches seem correct
- Run the `sanity migrate run <ID> --no-dry-run` to make the changes
- Update the queries and down-stream code in the application(s) where the content is used

> [!TIP]
> Protip
> If you aren’t quite ready to change the code that implements your content, you can use the coalesce function in GROQ to “alias” the new patterns to the old variable/shape:
> 
> "oldFieldName": coalesce(newFieldName, oldFieldName)

### For projects in production

Non-additive changes to the content model for projects in production require more diligence — as you might be used to from any database migration, especially if you aim for as little downtime as possible. Migrations like these are easier if you support PR/branch deployments in your CI/CD tooling. We recommend deploying the Studio from a git-based platform if you have more than simple needs.

To prepare a migration for projects in production:

- Export the dataset before migration (or for enterprise: enable dataset backups)
- Export and import (or copy) your production dataset into a staging dataset where you can test your migrations and relevant applications against
- Make your schema changes, and remember to give easy-to-understand instructions when deprecating fields.
- Run `sanity documents validate` to check what documents give errors against your new schema changes
- Initialize a migration job with `sanity migration create` to scaffold file and boilerplate code
- Dry run `sanity migration run <ID>` and validate that the patches seem correct
- Run the `sanity migration run <ID> --no-dry-run` to make the changes in your staging dataset
- Update queries and downstream code paths in applications that depend on the affected content
- The most foolproof way to write “defensive code” that supports both content models.
- Thoroughly test the changes in the branch/PR deployments
- Onboard users/stakeholders of your Sanity Studio to the new changes and let them test out the editorial experience
- When you have confidence that everything works, you can merge the applications to production and then run the migration jobs against your production dataset.
- When you have confirmed that everything works as it should in production, you can clean up the “defensive” code to eliminate the code paths for the old content model.

## Strive for idempotent migrations

An idempotent migration is a migration that can safely be run multiple times. Typically, an idempotent migration will start by checking if a precondition is met before it runs, and if this condition isn’t met, the migration will do nothing.

```typescript
defineMigration({
  title: 'Convert product category from string to array of strings'
	documentTypes: ['product'],
	filter: 'defined(category) && !defined(categories)'
  migrate: {
    document(doc) {
      return [
        at('categories', setIfMissing([])),
        at('categories', insert(doc.category])),
        at('category', unset())
      ]
    }
  }
})

```

Example of an idempotent operation:

`at('name', set(person.name.toUpperCase())`

This will produce the same result no matter how often you run it.

Example of a non-idempotent operation:

`at('members', insert({name: 'Some One'}))`

This inserts a new member into the array every time it’s run, giving different results every time it’s run.

### Providing an idempotence key

If there’s no way to write your migrations idempotent, you can instead write an idempotence marker to your document along with the migration.

```typescript
const idempotenceKey = 'xyz' // should be unique for the migration but never change

export default defineMigration({
  name: 'Convert product from reference to array of references'
	filter: 'defined(product) && !defined(products)'
  migrate: {
	  document(doc) {
			if ((document?._migrations||[]).includes(idempotenceKey) {
	      // Document already migrated, so we can skip
	      return
	    }
	    return [
	      // migration
				at('members', insert({name: 'Some One'}))
	      //… add idempotence key
				at('_migrations', setIfMissing([]),
	      at('_migrations', insert(idempotenceKey)
	    ]
		}
  }
})

```



# Perspectives for preview and presentation

By presenting all in-flight changes together within the real experience, content creators, reviewers, and stakeholders can get a realistic view of what the experience will look like when these changes are published. High-fidelity content previews enable valuable and accurate feedback, validation, and approvals before rolling out updates.

Likewise, ensuring that your "production" or "public" experience or application only presents published content is critical to ensuring that what is presented to your consumers is complete, validated, and approved.

Sanity offers a range of tools and [Perspectives](/docs/content-lake/perspectives) for Content Lake to help you build first-class preview experiences.

> [!TIP]
> Protip
> This article focuses on preview environments outside the studio. To learn more about the various ways of previewing content changes within Sanity Studio, please visit this article.

A key feature of composable content is that you can shape data from a variety of independent but interconnected documents into whatever shape is required on the consuming end. For content authored in Sanity Studio, all unpublished changes are tracked in *draft documents* that co-exist with the published version of the same document. With content spread over multiple documents, each with its own publishing state, creating robust preview tooling can become cumbersome. 

Perspectives is an out-of-the-box solution that does away with the heavy lifting of creating engaging preview experiences.

> [!NOTE]
> Defining "published"
> For simplicity, this article assumes that the publish status on the consuming end matches that of the Content Lake. That is, a document that is published in your studio should be visible in the front-end, while unpublished changes in draft documents are what’s most relevant for previewing purposes.

## Content Lake Perspectives

[Perspectives](/docs/content-lake/perspectives) let you run your queries against an alternate "view" of the documents in your dataset. Use the `drafts` perspective to preview what your content would look like if all drafts were published, or set the `published` perspective in your production environment to make sure no unfinished draft content is accidentally made public.

If you use the [Content Releases](/docs/studio/content-releases-configuration) feature, you can also pass a perspective stack to view a custom perspective containing versions of documents across multiple releases.

The perspective is set as a property of the Sanity client configuration or passed as a parameter to the HTTP API query endpoints, allowing you to reuse queries between preview and production environments.

### Best practice

Web applications often use perspectives to retrieve initial server-side or static content.

- In your production environment, queries use the `published` perspective. 
- In preview environments, queries use the `drafts` perspective or a perspective stack and are then hydrated with live updates via [Loaders](/docs/visual-editing/fetching-content-for-visual-editing).

```typescript
// Example JS/TS client configuration
import {createClient} from '@sanity/client'

const client = createClient({
  ...config,
  useCdn: false, // must be false for 'drafts'
  perspective: 'drafts', // 'raw' | 'published' | 'drafts' 
})
```

```
// Example using HTTP API
/data/query/production?query=*[]&perspective=drafts
```

### `raw`

The `raw` perspective returns drafts, document versions from releases, and published content for authenticated requests. 

> [!WARNING]
> Gotcha
> Prior to API version 2025-02-19, raw was the default perspective value and returned both published and draft documents.

### `drafts`

When using this perspective, your queries will return as if all draft documents in your dataset were published. This means:

- References to draft documents will be resolved/dereferenced normally, exactly as when dereferencing published documents
- Query results will not be cached in the CDN, ensuring you always get the latest most up-to-date version of in-flight changes
- When using [custom permission resources](https://www.sanity.io/docs/roles-reference#b794f620247e), draft documents will be presented only when the user’s permissions grant access to the draft document; otherwise, the published version is presented

### `published`

The default perspective value. When you use this perspective, your queries will operate as if there were no draft or version documents in your dataset.

[Perspectives for Content Lake](/docs/content-lake/perspectives)

[JS/TS Client README](https://github.com/sanity-io/client#readme)



### Release perspectives

The Content Releases feature introduces document versions and allows you to customize the perspective to layer one or more releases in a perspective stack. 

Releases take priority from left to right. For example, in the perspective `a,b,c` you would see changes in `a` take priority over `b` and `c`, and changes in `b` take priority over `c`.

```typescript
// Example JS/TS client configuration
import {createClient} from '@sanity/client'

const client = createClient({
  ...config,
  useCdn: false, // must be false for 'release previews'
  perspective: ['a','b','c']
})
```

The `published` perspective is automatically added to the end, so even if a release only contains changes to one document, the response will include all matching published documents in addition to the release changes. You can also append `drafts` to the list to include draft versions.

## Preview tooling

The Perspectives feature alone can enable running independent preview and production environments, but for truly custom, interactive live-as-you-type preview, Sanity also offers powerful tooling for any front end framework and offers guides on the most popular ones.

[Visual Editing](/docs/visual-editing/introduction-to-visual-editing)

[Guide: Visual Editing with Next.js](https://www.sanity.io/guides/nextjs-app-router-live-preview)

[Guide: Visual Editing with Remix](https://www.sanity.io/guides/remix-run-live-preview)







# Live Content API

The Live Content API allows you to deliver live, dynamic experiences to your users without the complexity and scalability challenges that typically come with building real-time functionality. It is available on all plans, including free plans. See the [pricing page for usage details](https://www.sanity.io/pricing#compare-plans).

![Distribute real-time updates to your applications](https://cdn.sanity.io/images/3do82whm/next/053e032a2de083d666a0815b7634dd302490638d-1200x675.png)

With the Live Content API, you can:

- **Subscribe to changes** and receive notifications whenever documents are created, updated, or deleted.
- **Efficiently query** for the exact content you need, and only receive updates for that content.
- **Scale** to handle high volumes of live updates, even during peak traffic periods.

## When to use the Live Content API

The Live Content API is designed to integrate into your existing application. It provides an interface for subscribing to content changes and receiving real-time updates. 

Most sites and applications benefit from a mix of live and static content. For example, a news organization may want their homepage to use live content while article pages remain statically generated. In other cases you may want islands of dynamic content to use the Live Content API in an otherwise statically generated application.

The Live Content API requires API version `v2021-03-25` or later.

> [!NOTE]
> Usage limits and live connections
> Live connections are part of the Live Content API usage, but new requests contribute to your API usage quota. 
> 
> Live connections don't request data, but instead listen for targeted updates in your dataset data. Your application relies on these tags to make new requests.
> 
> The Live Content API holds on to old events and can replay them for clients when they connect up to a certain retention time. See Live Content API pricing for limits.
> 
> Requests should use site-wide caching techniques to minimize unnecessary requests and prevent unexpected usage spikes. The next-sanity client does this automatically.

## How Live Content works

Taking advantage of the the Live Content API is a two stage process:

12. Clients listen for content identifiers we call sync tags. They map to specific requests, so changes in your dataset only trigger new tags for requests on the new content.
12. Clients query your data, listen for changes, and update content only when it changes.

Our client libraries handle this process for you and provide helpers for building on top of the API. If you're using a framework like the latest version of Next.js, we'll even handle caching to minimize additional request overhead. See the getting started section below to begin using Live Content in your application. For more on the underlying API, see the [Live Content API reference](/docs/http-reference/live) and our example implementations in the resources section.

## Get Started

Start implementing the Live Content API by following one of our guides or experimenting with an example project.

[Set up live content in your app](/docs/developer-guides/live-content-guide)

[Clean Next.js + Sanity Starter](https://www.sanity.io/templates/nextjs-sanity-clean)

[Sanity Learn: Content-driven web application foundations](https://www.sanity.io/learn/course/content-driven-web-application-foundations/)



## Additional resources

[Live Content API reference](/docs/http-reference/live)

[Live content examples on GitHub](https://github.com/sanity-io/lcapi-examples)







# Listening API

> [!TIP]
> Protip
> Searching for ways to update your sites or apps in real-time as your data changes? The Live Content API is a more flexible, efficient choice.
> 
> If you're looking to react to content as it changes, check out GROQ-powered webhooks. 

The Sanity data store supports realtime updates, allowing API clients to listen for content changes. This is used e.g. for collaborative editing in our content studio, where your view of the document is updated as other people make changes. These updates are available to your own apps as well, and have a wide range of uses, such as:

- Alerting end-users of breaking news stories.
- Updating client state in a multiplayer game.
- Transmitting chat messages between users.
- Controlling IoT (Internet-of-Things) devices such as home automation systems.

Listeners use the [Server-Sent Events protocol](https://www.w3.org/TR/eventsource/), by making an HTTPS request to:

`https://<project-id>.api.sanity.io/v2021-06-07/data/listen/<dataset>?query=<GROQ-query>`

The server will keep the connection open and stream events as they occur for any documents matching the provided [GROQ query](/docs/content-lake/how-queries-work). Further parameters and details are listed in the [listeners reference](/docs/http-reference/listen).

> [!WARNING]
> Gotcha
> Listener queries do not support joins, since they operate on individual documents, and will ignore order-clauses and projections.

We recommend using one of our [client libraries](/docs/client-libraries) to listen for updates, which will automatically decode events into native data structures and handle stuff like automatic reconnects. Here's an example using our [JavaScript library](/docs/js-client):

```javascript
const query = '*[_type == "comment" && authorId != $ownerId]'
const params = {ownerId: 'myUserId'}

const subscription = client.listen(query, params)
  .subscribe(update => {
    const comment = update.result
    console.log(`${comment.author} commented: ${comment.text}`)
  })
```

## Events

> [!TIP]
> Protip
> Client libraries may hide or automatically handle certain events, refer to its documentation for details.

### `welcome` 

When the listener is set up and ready to serve mutations, you will receive the `welcome` event. It looks like this:

```text
event: welcome
data: {"listenerName": "Ua6BR3GwQ14cnZXrgwCdsF"}

```

You don't need to process this event, but it could be used to kick off other processing. If you are tracking changes to keep a document in sync on the client side, this is a good time to fetch the initial document using the [doc endpoint](/docs/http-reference/doc). Fetching the document after the listener is ready ensures that you receive every subsequent mutation. If you fetch the initial document before setting up the listener, you may miss one or more mutations in the intervening time.

### `mutation` 

The most common event is the `mutation` event, which looks like this:

```text
event: mutation
id: lqgiok-skp-eja-k6z-9wrng7k5e#38123cba-286c-45a0-a6d1-3cc4dc43748a
data: <JSON-payload on a single line>
```

The payload is a single line of JSON. The [listener reference](/docs/http-reference/listen) has a complete list of fields and descriptions, but some of the most useful fields are summarized below:

- `documentId`: the ID of the modified document
- `transition`: type of event - `update`, `appear`, or `disappear`
- `identity`: the user making the changes
- `mutations`: an array of mutations as submitted to the [mutate endpoint](/docs/http-reference/mutation)
- `result`: the complete document after the mutations are applied
- `previousRev`: the document revision ID before the mutation
- `resultRev`: the document revision ID after the mutation
- `timestamp`: time when the mutation was applied
- `visibility`: whether the change is visible to queries yet (`query`), or only to subsequent transactions (`transaction`).

> [!WARNING]
> Gotcha
> Due to the distributed nature of the Sanity backend, mutation events may be sent out of order. A meticulous client would reassemble mutation events as an unbroken chain by comparing previousRev and resultRev, or use the most recent result document as determined by timestamp.

### `channelError` 

Errors during processing will appear as `channelError` events. These are typically caused by syntax errors in the query, and look like this:

```
event: channelError
message: {"message": <the error message>}

```

### `disconnect` 

Normally, if you are disconnected from a listener endpoint you should just immediately reconnect. However, if you receive the `disconnect` event, you should disconnect and stay away. Typically this means you just got a `channelError` that is considered fatal (e.g. a syntax error) and reconnecting will just repeat the ordeal. The event looks like this:

```
event: disconnect
data: {"reason": <a string describing the reason>}

```





# GROQ-powered webhooks

## Webhooks at a glance

Webhooks are a way to integrate applications with automated HTTP requests. Typically you use it to connect services by creating a special URL that accepts incoming requests. What happens when the request resolves depends on the application or service.

> [!NOTE]
> Webhooks are typically used for, but not limited to:
> 
> 
> Setting up notifications to systems like Slack, Discord, or email services
> 
> Keep external logs and update auditing systems
> 
> Update content in other services
> 
> Trigger automation and workflows

Some services only support receiving webhooks; others can both receive and send them. The Sanity Content Lake supports both sophisticated outgoing webhooks, and receiving incoming webhooks to any appropriate API endpoint, provided they have the proper payload and authentication.

## Webhooks in your Sanity Content Lake

You can create and manage outgoing webhooks in the API section of your project settings, which you'll find at [sanity.io/manage](https://www.sanity.io/manage). Webhooks can also be managed through the CLI or directly through the project APIs.

![API settings with the webhook overview showing a “Trigger site rebuild” webhook.](https://cdn.sanity.io/images/3do82whm/next/13bdd207a065dbdc51d06151b1a170d40cc26378-1053x624.png)

## Configuration

The following fields are available for webhooks. You can find them all under the webhooks section under the API in your project's settings on [sanity.io/manage](https://www.sanity.io/manage).

### Name and description

You can name your webhooks and give them a description. The description field, while optional, is a useful way to add helpful context about your webhook.

### URL

The URL field is where you specify the endpoint to which the webhook request is sent. If you want to test the webhook before entering the production endpoint, you can use services like [webhook.site](https://webhook.site), or [Beeceptor](https://beeceptor.com/). You can also use [ngrok](https://ngrok.com/) or [Localtunnel](https://localtunnel.github.io/www/) to test a hook against your local environment.

### Trigger on

Webhooks can be triggered when a document is **created**, **updated**, **deleted**, or any combination of these.

- **Create** - triggers on the creation of a new document.
- **Update** - triggers on every change to a document once created.
- **Delete** - triggers on the deletion of a document

Between these, you'll be able to react to all major interactions with the documents in question.



> [!NOTE]
> Pro tip!
> By default, your webhooks will not trigger on draft or version events. They will only trigger when changes to the document are published and not for every single occurrence while you edit. Triggering on draft and version events can be enabled, but be careful or you may end up causing huge amounts of traffic to your endpoint!

### Filter

A GROQ filter specifying which documents will, when changed, trigger your webhook. A filter is what you commonly see between the `*[` and `]` in a GROQ query. This field supports all the GROQ functions you'd expect and has additional support for functions in the [delta::](https://www.sanity.io/docs/groq-functions#a64594a50318) namespace, as well as [before() and after()](https://www.sanity.io/docs/groq-functions#bbcf50816968).

If left empty, it will apply to all documents (`*[]`).

Webhook filter does not support the following kind of queries and will just yield to `false`:

- Sub-queries, e.g. `_type == "book" && author._ref in *[_type=="author" && name=="John Doe"]._id`
- Cross dataset references: `_type == "book" && author->featured` where author is a [cross-dataset reference](https://www.sanity.io/docs/cross-dataset-references).

See our [Intro to Filters](https://www.sanity.io/guides/filters-in-groq-powered-webhooks) guide for tips on using filters in webhooks.

### Projection

A GROQ projection defining the payload (or body) of the outgoing webhook request. This field supports GROQ functions in the [delta::](https://www.sanity.io/docs/groq-functions#a64594a50318) namespace, as well as [before() and after()](https://www.sanity.io/docs/groq-functions#bbcf50816968).

If left empty, it will include the whole document *after* the change that triggered it.

> [!WARNING]
> Gotcha
> “Sub-queries” are not supported for webhook projections. For example, the following query will not work: { "relatedPost": *[^._id in related[]._ref]{_id, title, slug}}

See our [Intro to Projections](https://www.sanity.io/guides/projections-in-groq-powered-webhooks) guide for tips on using projections in webhooks.

### Status

Enable or disable your webhook.

> [!NOTE]
> Disabling webhooks
> When a webhook is disabled all pending requests will be canceled.

### HTTP method

This field configures the webhook's [HTTP request method](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods). It can be set to POST, PUT, PATCH, DELETE, or GET. Some endpoints require incoming requests to use a specific method to work.

### HTTP headers

Additional HTTP headers. You can add multiple headers. A common example is adding an `Authorization: Bearer <token>` header to authenticate the webhook request.

> [!WARNING]
> Gotcha
> Be mindful if you share webhooks that header configuration will be included with sensitive information if you don't remove it before sharing the link.

A webhook will always include the following headers and values:

- [connection](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Connection): close
- [accept-encoding](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Encoding): gzip
- idempotency-key: <a unique key> See documentation below.
- [content-type](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Type): application/json
- [content-length](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Length): <the length of the payload in bytes>
- [user-agent](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent): [Sanity.io](http://Sanity.io) webhook delivery
- [host](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Host): <the endpoint URL host>

As well as the following Sanity-specific headers that can be useful for logging and debugging your webhooks:

- `sanity-transaction-id`: ID of transaction.
- `sanity-transaction-time`: Timestamp of transaction.
- `sanity-dataset`: Name of dataset (also available in projection today as `sanity::dataset()`).
- `sanity-document-id`: Document ID being notified about.
- `sanity-project-id`: ID of project (also available in projection today as `sanity::projectId()`).
- `sanity-webhook-id`: ID of webhook.
- `sanity-operation`: Either create, update or delete

> [!NOTE]
> Info
> The projection will always be returned as JSON. If you for some reason need it to be another content type, you’ll have to pass it through a serverless function or a custom endpoint and do the transformation there.

### API version

Defaults to the `v2021-03-25` of the query API. Can be overridden using the [Webhooks API](/docs/http-reference/webhooks) in cases where you want to create webhooks with old behavior that might have been deprecated.

### Drafts and versions

By default, documents in the `drafts.` and `versions.` ID-namespace will be automatically ignored. Enable the drafts or version setting if you want the triggers and filter to apply to draft or version documents. Note: version support was added in version 2025-02-19.

> [!WARNING]
> Gotcha
> This might cause a lot of webhooks to trigger whenever someone is working inside Sanity Studio, since almost every keystroke represents an update. Webhooks are limited to 1 concurrent request, but you should also make sure that your endpoint is able to handle the incoming events.

### Secret

To let receiving services verify the origin of any outgoing webhook, you may add a secret that will be hashed and included as part of the webhook request's headers. You may find our [webhook toolkit library](https://github.com/sanity-io/webhook-toolkit) helpful for working with secrets. If you want to roll your own; we model the encryption and decryption of secrets on the same standard as [Stripe](https://stripe.com/docs/webhooks/signatures#verify-manually).

## Idempotency-key

Requests include a new header that can be used to de-duplicate deliveries: `idempotency-key`.

This is necessary because webhooks will sometimes be retried, and our system has *at least one* delivery. Using the unique idempotency key lets the receiver ignore messages it has already received.

We follow [this new draft standard](https://datatracker.ietf.org/doc/draft-ietf-httpapi-idempotency-key-header/) for idempotency.

## Sharing webhooks

Webhook configurations can be shared with a URL. This is practical if you want to quickly repurpose webhooks across projects or share with the community. You can generate a share URL by going to [sanity.io/manage/webhooks/share](https://www.sanity.io/manage/webhooks/share) or by finding the share button in the three-dot-menu in the webhooks overview. 

> [!WARNING]
> Gotcha
> Note that all the configuration is stored as part of the URL. Be mindful of any sensitive information that might be part of the configuration and that it will be shared in plain text. It can be wise to replace secret tokens and so on with capitalized placeholder text.

## Debugging webhooks

### Attempts log

> [!TIP]
> Protip
> Use the attempts log to determine whether your webhooks are being successfully delivered.

You can find the attempts log if you click the three-dotted menu for a given webhook. The log will include information about the response a webhook request got. The attempts log is available as an API endpoint at:

```
https://api.sanity.io/v2021-10-04/hooks/projects/${projectId}/${id}/attempts
```

### Message log

> [!TIP]
> Protip
> Use the message log if you want to know whether all outstanding messages for a webhook have been delivered. 

The message log is available as an API endpoint at:

```
https://${projectId}.api.sanity.io/v2021-10-04/hooks/${id}/messages
```

The log contains a list of messages in the queue and any delivery attempts for each:

- If all the messages returned have the status `queued` then your processing has fallen behind. This may indicate that your webhook processing is too slow and/or that your webhook filter is too broad and is generating a vast number of messages. 
- If your webhook request handler takes longer to process a message than the rate at which you are generating changes that trigger the webhook then the queue will never be cleared.

### HTTP Status codes

The HTTP status codes are used to determine if delivery is successful:

- 200-range will be treated as a success
- 400-range will be treated as undeliverable, as the server said it was a client error (with one exception—see next item)
- 429 will be retried using an exponential back-off pattern
- 500-range will be retried using an exponential back-off pattern

## Technical limits, retries, and timeouts

Webhooks are limited to 1 concurrent request.

We will retry sending a Webhook request for up to 30 minutes with an exponential back-off. This limit is subject to changes in the future.

A webhook request will time out after 30 seconds.

## **Webhooks origin IP addresses**

The full list of IP addresses that Sanity webhooks calls originate from, can be found on this file:

[https://www.sanity.io/files/webhooks-egress-ips.txt](https://www.sanity.io/files/webhooks-egress-ips.txt)

The IP addresses generally don’t change but they may be updated from time to time, on planned or unplanned/emergency maintenance. For planned changes, we aim to announce upcoming changes 7 days in advance on Sanity’s status page feed here: [https://www.sanity-status.com/](https://www.sanity-status.com/#). Unplanned maintenance changes will happen without notice, but the URL file will be immediately up-to-date.

> [!WARNING]
> Gotcha
> If you’re aiming to use these addresses for IP filtering/security purposes, make sure you keep your tooling up-to-date with the URL above in an automated/unattended way.





# Webhook Best Practices

The below is our best practice guidelines on how webhooks should be used and how your system should handle them.

## Configuration

GROQ webhooks should be configured to trigger on the narrowest possible set of changes. Make sure the filter is as specific as possible and avoid triggering webhooks on draft changes unless absolutely necessary. Drafts can change frequently as content is being edited, which could result in a high volume of webhooks that may be costly or overwhelming for your systems.

## Delays

In rare circumstances there can be delays in the delivery of webhooks. If receiving timely updates is critical to your app, this should be considered in webhook handling. For example, you could check the `sanity-transaction-time` header and compare this to the current date and time - if you see times over a certain age you might trigger a catch up via API calls.

Delays could also mean webhooks can potentially be received out of order. Therefore is can be useful to check the `_updatedAt` value on a document to ensure you're using the latest data. It can sometimes be worth considering whether you're best to use the data in a webhook or use the webhook to trigger a query.

## Recovery from downtime

It's important to consider that downtime can occur with any webhook setup, whether it's on the side of your application or the provider itself. 

To mitigate the impact of potential downtime, it's recommended to implement a mechanism for recovering missed data via API calls. This ensures your application can stay up-to-date even if webhooks are temporarily unavailable.

## Idempotence

Idempotence in the context of webhooks refers to the ability to process the same webhook payload multiple times without adverse effects.

For example, if a webhook is delivered and processed successfully, but the acknowledgement response fails to reach the sender due to a network issue, the sender might retry sending the same payload. In an idempotent system, receiving and processing the same payload again would not result in duplicate data or unintended side effects.

Sanity provides an `idempotency-key` header which you can use to ignore messages that might be in a state of being processed or that have been processed already. By checking the `idempotency-key`, you can ensure that your application processes each unique webhook payload only once, even if it is delivered multiple times.

## Reconciliation

Relying solely on webhooks isn't recommended in any application - delivery can't always always guaranteed due to network issues, application downtime, or other factors.

You might want to run regular sync jobs – at hourly, daily or other intervals – to make sure everything updated between syncs is reconciled. This sync could filter using the `_updatedAt` field on Sanity documents to find everything which has changed since the last sync.

## Scalability

As the volume of webhooks received by your application increases, it can become challenging to process all of them in real-time. To handle this scalability issue, it's recommended to implement a queueing system for incoming webhooks.

When a webhook is received, instead of processing it immediately, your application should add it to a queue for asynchronous processing. This allows your webhook endpoint to quickly acknowledge receipt of the webhook and return a response within the 30-second timeout window Sanity implements.

It's important to note that the response returned by your webhook endpoint should indicate that the webhook was received successfully, not that it was fully processed. This distinction is crucial because the actual processing of the webhook happens asynchronously through the queue.

By decoupling the receipt and processing of webhooks using a queue, you can ensure that your application remains responsive and can handle a high volume of incoming webhooks without overwhelming your system. The queue acts as a buffer, allowing you to process webhooks at a pace that your application can handle, while still acknowledging their receipt in a timely manner.

Implementing a robust queueing system for webhook processing is a best practice for building scalable and reliable applications that can handle increasing webhook traffic as your system grows.

## Security

When setting up webhooks, it's important to consider security measures to protect your application and data. Webhooks are sent across HTTP and you will want to ensure the data you receive is sent from Sanity.

Here are a few key points to keep in mind:

- **Secrets**: Sanity allows you to configure a secret token for your webhooks. This secret should be a unique, random string that is only known to your application and Sanity. When Sanity sends a webhook payload to your endpoint, it includes this secret in the request headers. Your application should verify the secret to ensure that the webhook is coming from a trusted source (Sanity) and not from a malicious actor.
- **Sanity Webhook Toolkit**: Sanity offers a [webhook toolkit](https://github.com/sanity-io/webhook-toolkit), which is a set of utilities for handling webhooks in a secure and reliable manner. The toolkit includes features like signature verification, which helps ensure the integrity and authenticity of the webhook payloads you receive. Although the toolkit is written in TypeScript, the concepts and principles it promotes are language-agnostic.
- **IP Whitelisting**: Sanity provides a [specific set of IP addresses](https://www.sanity.io/files/webhooks-egress-ips.txt) from which webhooks are sent. You can configure your application to only accept webhook requests originating from these trusted IP addresses. This adds an extra layer of security by preventing unauthorized sources from sending fake webhook payloads to your endpoint.

By implementing these security measures, you can protect your application from potential threats and ensure that the webhooks you receive are genuine and trustworthy.





# Access Your Data (CORS)

For security reasons, your project is configured by default to only respond to queries from `localhost:3333` (i.e. your laptop) and the hostname you used when deploying (if you used `sanity deploy`). If you want to open up your project to any other origins, you need to add the host name to your allowed CORS origins (you can read more on [browser security & CORS](/docs/content-lake/browser-security-and-cors) or [the technicalities of CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)).

Typical reasons you'd want to add a new CORS origin include:

3. You are using a non-default port when developing, so you'd open up to `http://localhost:<your port>`
3. You are deploying a front end, so you'd open up to `https://the-public-host.com`
3. You are deploying a studio outside the Sanity infrastructure (i.e. not using the `sanity deploy` command)
3. You want to try something out on [JSFiddle](https://jsfiddle.net/), you'd open to `https://fiddle.jshell.net`

It's good practice to limit your origins to the smallest possible set, and never open a sensitive dataset to public playgrounds like JSFiddle. A JSFiddle example will be able to access projects you open to it with *your credentials *when you run it*.*

## Defining a CORS origin

A CORS origin will be defined using the following format: `protocol://hostname[:port]`. The protocol and host name are required while the port is optional. Wildcards (`*`) are allowed.

Some valid examples include: `https://example.org`, `https://*.example.org`, `https://fiddle.jshell.net`, `http://localhost:3000`, and `http://localhost:3333`.

> [!WARNING]
> Gotcha
> Allowing credentials from wildcard origins is dangerous. Any domain that matches the given pattern will be able to send requests on the user's behalf if they are logged in to your studio. Tread carefully!

When adding a CORS origin, you will also need to decide whether or not to allow credentials. If you allow credentials, the origin will be allowed to send authenticated requests using the token or session of a logged in user.



> [!TIP]
> Protip
> If this origin hosts a studio, you will need to allow credentials. Otherwise, you should probably select not to allow credentials.

> [!WARNING]
> Gotcha
> Are you getting one of these errors in your browser console when trying to access your studio?
> 
> Firefox: Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at https://PROJECT_ID.api.sanity.io/v1/users/me. (Reason: expected ‘true’ in CORS header ‘Access-Control-Allow-Credentials’)
> 
> Chrome: Access to XMLHttpRequest at 'https://PROJECT_ID.api.sanity.io/v1/auth/providers' from origin '<STUDIO_URL>' has been blocked by CORS policy: The value of the 'Access-Control-Allow-Credentials' header in the response is '' which must be 'true' when the request's credentials mode is 'include'
> 
> Safari: XMLHttpRequest cannot load https://PROJECT_ID.api.sanity.io/v1/users/me due to access control checks. Credentials flag is true, but Access-Control-Allow-Credentials is not "true".
> 
> Try allowing credentials on your CORS origin.

## How to add a CORS origin

You can add a CORS origin from your management console or using the command line interface (CLI).

### Via your management console

To add a CORS origin from your management console:

16. Go to [https://www.sanity.io/manage ](https://www.sanity.io/manage)
16. Pick your project from the list
16. Go to **Settings**, and then to **API settings**
16. Under **CORS Origins**, click the **Add CORS origin** button
16. Enter your **Origin**, select whether or not to **Allow credentials**, and click **Save**. If your origin was added successfully, it will appear at the top of your CORS origins list.

### Via the command line interface

To add a CORS origin from the CLI:

19. Navigate to your project's folder in your terminal
19. Run the command `sanity cors add [ORIGIN]`, where `[ORIGIN]` meets the requirements listed above
19. When prompted, select whether or not to allow credentials

You can confirm your origin was added with the statement `CORS origin added successfully` or by consulting the list returned by the command `sanity cors list`.

> [!WARNING]
> Gotcha
> Keep in mind that in order to add a CORS origin, you will need the proper permissions. If you are unable to add a CORS origin, please speak to your project Administrator.



# Browser security & CORS

> [!TIP]
> Protip
> Read this article to learn more about the use of tokens in your API client.

CORS is a technique to relax the strict browser security model for certain trusted domains.

CORS makes it possible to specify which origins (i.e. webpages) that may issue requests and read response data from the API of your project. Let's say you have the Sanity Content Studio open in a browser tab and you're logged in. Behind the scenes, you now have an active session cookie at `https://yourproject.api.sanity.io`. Blissfully unaware, you click a link you receive in a mail, that opens evil-site.com in another browser tab. Now, if browser vendors did not care about security at all, the JavaScript running on `evil-site.com` would be able to make requests to `https://yourproject.api.sanity.io`. These requests would even be authenticated, passing whatever cookies the browser had previously set for yourproject.api.sanity.io, meaning that nothing would prevent the maintainer of `evil-site.com` from including a script that deletes all the contents in your projects dataset.

Luckily, browser vendors are extremely concerned about security and have all implemented something called the Same-origin policy, which denies all requests from a web page to another web page that does not share the same origin (origin is the combination of URI scheme, hostname, and port number). The Same-origin policy is what prevents your bank account from becoming emptied when you're logged in to your online bank while also browsing random web pages at the same time. If you're unlucky and enter `evil-site.com`, there's no way any browser will allow scripts running on `evil-site.com` to send a request to `your-bank.com/transfer` with instructions to transfer all your money to the maintainer of evil site.

However, the Same-origin policy is bad news for you as a Sanity user, since your Content Studio (and possibly your single page app too) doesn't run on the same origin as the Sanity API. Typically the Content Studio runs on either `http://localhost:3333` or `https://yourproject.sanity.studio`, while your content is located on `https://yourproject.api.sanity.io`. Due to the Same-origin policy, any script running on `http://localhost:3333` is denied from making requests to yourdataset.api.sanity.io. So how can you still run your studio and edit your data at `https://yourproject.sanity.studio`? This is where CORS comes to the rescue. CORS stands for Cross-Origin Resource Sharing and is a way to bypass the Same-origin policy for trusted pages. 

CORS provides a way for the browser to first check with the server whether the origin of a page is allowed to perform a specific request (e.g. a request to delete some content). Explained simplified: if a script loaded on a web page tries to request `https://yourproject.api.sanity.io`, the browser will issue a *preflight* request to the Sanity API, providing information about the origin of the web page (along with some additional metadata about the request it is about to perform). If the origin is in the list of trusted origins for your project, then the Sanity API will respond with a “YAY”, and the browser will continue with the actual request. If the origin is *not* found in the list of trusted origins, the Sanity API will respond with a “NOPE” and the browser will *not* perform the request at all. To add or remove a trusted origin, go to the [management console](https://manage.sanity.io), select a project, and find the list under the settings tab.

In addition to simply allowing or *not* allowing an origin access to your dataset, there's also an additional setting for whether or not to allow *credentials* to be sent. In this context, credentials means either a session cookie or an authorization token. If you are creating a single-page application that talks to the Sanity API directly, you usually want to *disallow* sending credentials. 

> [!WARNING]
> Gotcha
> CORS is only about browser security, and does not apply when requesting from e.g. Node.js or curl.

In order to modify data, the API request must be authenticated with an access token that has write access. This token is either coming from the auth session set by the browser when the user logged in, or from an access token added in the [project management console](https://manage.sanity.io/) under project settings. A public dataset is only publicly *readable*, and can only be written to by users invited to the project, or by tokens configured with write access enabled.



# Keeping your data safe

## Take good care of your access tokens

An access token (also known as a robot token) is a credential that can give access to read or write data to a Sanity project. You can read more about access control and tokens in the [authentication docs](/docs/content-lake/http-auth).

Access tokens are project-specific and you can create them from the project settings in the [management console](https://manage.sanity.io).

> [!WARNING]
> Gotcha
> Access tokens should not be confused with user tokens, which is a personal token that identifies a logged in user and is generated at the time the user logs in.

The single most important thing you can do to keep your data safe is to make sure never to disclose access tokens to unauthorized users. There are several ways to accidentally leak an access token, the most common being that it is gets bundled together with a frontend JavaScript bundle.

As a rule of thumb, you should:

- **Never** add an access token to JavaScript that is bundled for client-side use and served publicly unless you take extra precautions (described below).
- **Never** commit access tokens to public code repositories or open source projects.
- **Never** share access tokens through unsecured or public channels.



> [!WARNING]
> Gotcha
> Be extra careful with access tokens that grant write access to your data. Everyone with access to that token can delete all of your data.

### What to do if an access token gets compromised?

If you find that your token has been leaked or accidentally made public, you should consider it forever lost and delete it immediately, no matter how quickly you manage to make it private again. 

To delete a token, go to the management console at [https://manage.sanity.io](https://manage.sanity.io), select your project, and navigate to project settings. From there, select the API settings and delete the token in question.

## Submitting data from a frontend

If you want users of your website or app to be able to submit data, we recommend creating a small proxy server or cloud function that validates the received data, transforms it to a Sanity document and submits it using a [sanity client](https://www.sanity.io/docs/client-libraries) that is configured with a token that has write access to the dataset. 

## Dataset visibility

When creating datasets, you may choose whether it should be:

- **Public** - everyone can query for content in the dataset without being authorized - great for single page applications
- **Private** - only authenticated users or requests with authorization tokens can read from the dataset

You may change the visibility mode for your dataset either by running `sanity dataset visibility set <datasetName> <public/private>` or by using the management console at [https://manage.sanity.io/](https://manage.sanity.io/)

> [!WARNING]
> Gotcha
> Asset files are not private, so even images uploaded to a private dataset can be viewed by unauthenticated users.

> [!TIP]
> Protip
> Private datasets can be cached in our API Content Delivery Network (API CDN), it is cached with your access token as the key. 
> 
> See API CDN documention for details.

Customers with the custom access control feature can specify fine-grained rules for configuring which users can create, delete and update documents. See the [access control](/docs/archive/access-control-deprecated) documentation for details.

## Tokens in browser-side JavaScript

Configuring the Sanity client with an access token should generally be avoided for browser-side JavaScript. Usually, JavaScript for browsers are served publicly, and if it includes an access token, then that token will be available in plain text to everyone.

A common case is wanting to fetch data from a private dataset in a public frontend. If this is done by including an access token in JavaScript code that is shipped to the browser of the visitors of the site, the whole dataset will in effect be made public, since it takes little technical insight to inspect the JavaScript source code and find the token there. Even worse, if the access token grants write permission, you have in effect made your data writeable by everyone.

To avoid this, you could consider:

- Making the JavaScript private by serving it only to authorized users.
- Making the backend fetch the data from the Sanity APIs, filtering out only the data that should be available to the general public.

If you are making a frontend for a private intranet, make sure that also static assets are served only to authorized users as long as it includes an access token.



# Activity Feed

The Activity Feed lets you investigate what happened in your Sanity projects. If you are uncertain how a scenario took place, you can use the Activity Feed to investigate what actually happened.

![](https://cdn.sanity.io/images/3do82whm/next/e54be0b039dcf6cb7d215e5473bd5efc9315bc1f-1790x1364.png)

## What is an event?

An event is created when various actions are performed in the system. This can be by a user, by Sanity, or even by a robot token. An event contains information about what happened and when. Events differ by action, and each contains a unique ID.

### List of team events

- user creates team
- user changes team’s name
- user changes billing address
- user changes payment method
- user changes EU Representative
- user changes Data Protection Officer
- user changes user’s role
- user removes user
- user invite user(s)
- user joins team
- user revoked invitation

### List of project events

- user creates project
- user changes project’s name
- user changes project’s custom studio URL
- user changes project’s plan
- user adds CORS origin
- user removes CORS origin
- user adds webhook
- user removes webhook
- user adds API token
- user removes API token
- user changes user’s permissions
- user removes user
- user invites user(s)
- user joins project
- user revokes invitation
- user creates dataset
- user deletes dataset
- user edits dataset
- user duplicates dataset

## Exporting

Actions for projects and teams are available as a CSV export from the [manage dashboard](https://sanity.io/manage) for each project. The export can be customized by the date when created.

### Data provided in the export

- action
- actorEmail
- actorId
- actorName
- correlationId
- datasetName
- description
- documentId
- id
- metadata.email
- metadata.invitedBy
- metadata.role
- organizationDisplayName
- organizationId
- projectDisplayName
- projectId
- timestamp
- transactionId
- userEmail
- userId
- userName
- version



# Roles, grants, and permissions

The Sanity [Roles system](/docs/user-guides/roles) is a granular way of attaching specific capabilities to specific groups of users. It is designed to function in a structured and flexible way. The goal of the Roles system is to provide a set of strong default permissions groups with an API for creating, managing, and using custom roles built the way your organization works with content.

The Roles system is comprised of five main entities:

- Grants
- Resources
- Permission Resource Schema
- Roles
- Members (users) 

## Grants and Resources

A **grant** is the root item in the roles system. They represent the ability to perform a specific action or give access to a specific resource. 

A **resource** defines an element of a Sanity project or organization on which a user can have special grants. 

These permission pairs are defined in the related Permission Resource Schema. In addition, each grant may also have parameters, further limiting the scope of that permission.

## Permission Resource Schema

A Permission Resource Schema (or simply "Permission") defines a set of possible actions that defines what can be performed on specific resources. These can be accessed via the `/projects/${project_id}/permissionResourceSchemas` endpoint. Each provides a template for use in the creation of new roles and custom permissions.

> [!NOTE]
> Use cases
> A grant to read and edit (but not publish) documents of blogPost on the production dataset
> 
> A grant to create project API tokens

### Syntax

```json
[
  {
    "id": "srp-bvl4pkml",
    "title": "Project Tokens",
    "description": "Manage authorization tokens for a project",
    "name": "sanity.project.tokens",
    "permissions": [
      {
        "name": "read",
        "title": "Read",
        "description": "Ability to read project tokens",
        "params": []
      },
      {
        "name": "create",
        "title": "Create",
        "description": "Ability to create project tokens",
        "params": []
      },
      {
        "name": "delete",
        "title": "Delete",
        "description": "Ability to delete project tokens",
        "params": []
      }
    ]
  },
  {
    "id": "srp-ysohuysj",
    "title": "Project CORS",
    "description": "Manage CORS settings for a project",
    "name": "sanity.project.cors",
    "permissions": [
      {
        "name": "read",
        "title": "Read",
        "description": "Ability to read project cors",
        "params": []
      },
      {
        "name": "create",
        "title": "Create",
        "description": "Ability to create project cors",
        "params": []
      },
      {
        "name": "delete",
        "title": "Delete",
        "description": "Ability to delete project cors",
        "params": []
      }
    ]
  },
  {
    "id": "srp-2ve36erw",
    "title": "Sanity Document Filter",
    "description": "Defines a permission resource for a filtered collection of Sanity documents",
    "name": "sanity.document.filter",
    "permissions": [
      {
        "name": "create",
        "title": "Create",
        "description": "Create documents matching the filter",
        "params": [
          {
            "name": "datasetPolicyId",
            "type": "string",
            "title": "Dataset Policy Id",
            "description": "A dataset policy id to scope the permission"
          }
        ]
      },
      {
        "name": "read",
        "title": "Read",
        "description": "Read documents matching the filter",
        "params": [
          {
            "name": "datasetPolicyId",
            "type": "string",
            "title": "Dataset Policy Id",
            "description": "A dataset policy id to scope the permission"
          }
        ]
      },
      {
        "name": "update",
        "title": "Update",
        "description": "Update documents matching the filter",
        "params": [
          {
            "name": "datasetPolicyId",
            "type": "string",
            "title": "Dataset Policy Id",
            "description": "A dataset policy id to scope the permission"
          }
        ]
      },
      {
        "name": "manage",
        "title": "Manage",
        "description": "Manage documents matching the filter",
        "params": [
          {
            "name": "datasetPolicyId",
            "type": "string",
            "title": "Dataset Policy Id",
            "description": "A dataset policy id to scope the permission"
          }
        ]
      },
      {
        "name": "history",
        "title": "History",
        "description": "Read the history of documents matching the filter",
        "params": [
          {
            "name": "datasetPolicyId",
            "type": "string",
            "title": "Dataset Policy Id",
            "description": "A dataset policy id to scope the permission"
          }
        ]
      },
      {
        "name": "editHistory",
        "title": "Edit History",
        "description": "Edit the history of documents matching the filter",
        "params": [
          {
            "name": "datasetPolicyId",
            "type": "string",
            "title": "Dataset Policy Id",
            "description": "A dataset policy id to scope the permission"
          }
        ]
      }
    ]
  },
]
```

## Role

Roles define a set of grants which project members can have assigned to them. A project member can have many roles and grants, even within the same organization. 

### Default Roles

By default, there are specifically defined roles available for each plan type. Custom roles are available for Enterprise customers.

#### All plans

- Administrator: Read and write access to all datasets, with full access to all project settings. 
- API Tokens- Editor Token (read+write)
- Viewer Token (read-only)



#### Free

- Administrator
- Viewer: Can view all documents in all datasets within the project

#### Growth

- Administrator
- Editor
- Viewer
- Developer
- Contributor: Read and write access to draft content within all datasets, with no access to project settings.

## Members

A member is a person making use of a Sanity resource. A member always references the system via a role and is defined by an associated email.

## Token

A token is a unique string that can be used to authorize requests towards your content lake. A token can be given a role that defines its access to the system. Tokens should generally be considered secret.

## Creating a custom role

_This is a paid feature, available on the Growth plan._

To create a custom role, you need to create a new role, create the grants for the role, and assign the role to individual users.

### Create the role object

A custom role is an object containing a `title`, `name`, and `description` properties. In this example, a custom role with a name of `custom-role`, a title of `My Custom Role`, and a description of `Custom role` is created by sending that data to a given projects `/roles` API endpoint.

```sh
curl \
	-X POST \
	-H "Authorization: Bearer ${apiBearerToken}"
	-H "Content-Type: application/json"
	-d '{"title": "My Custom Role", "name": "custom-role", "description": "Custom role"}'
	https://api.sanity.io/v2021-06-07/projects/${projectId}/roles
```

This is now a custom role, but it currently has no granted permissions. To add permissions, you connect each permission to the role via the `/grants` API endpoint.

### Inspecting and adding grants

To see the available permission resources for the current project, you can send a `GET` request to the `/permissionResources` API endpoint for the current project.

```sh
curl \
	-X GET \
	-H "Authorization: Bearer ${apiBearerToken}"
	https://api.sanity.io/v2021-06-07/projects/${projectId}/permissionResources
```

Once you select the resources from this list you wish to add to the role, you can send a `POST` request to the `/grants` endpoint for the project for each addition.

```sh
curl \
	-X POST
	-H "Authorization: Bearer ${apiBearerToken}"
	-H "Content-Type: application/json"
	-d '{"roleName": "custom-role", "permissionName": "${permissionName}", "permissionResourceId": "${permissionResourceName}"}'
	https://api.sanity.io/v2021-06-07/projects/${projectId}/grants
```

## Creating a custom permission resource

_This is a paid feature, available on the Growth plan._

The above examples create a new role using pre-made permission resources. However, when you need to create granular permissions for specific roles, you can create a custom permission resource.

For Sanity to understand your resource, you need to have it correspond to a specific permission resource schema. You can see all the schema by sending a GET request to the `/projects/${projectId}/permissionResourceSchemas` endpoint. These function much like a configurable template for your custom permission.

To create a custom resource around a specific set of documents, you can use the `sanity.document.filter` schema. This schema provides a `config` option for adding a GROQ filter to specify a specific set of documents. The `permissions` array gives you the name of all the permissions that Sanity will understand with this template (e.g. "update," "read," etc.).

```json
{
  "id": "srp-2ve36erw",
  "title": "Sanity Document Filter",
  "description": "Defines a permission resource for a filtered collection of Sanity documents",
  "name": "sanity.document.filter",
  "config": [
    {
      "name": "filter",
      "type": "string",
      "title": "Filter",
      "description": "GROQ filter limiting the document collection"
    }
  ],
  "permissions": [
    {
      "name": "update",
      "title": "Update",
      "description": "Update documents matching the filter",
      "params": [
        {
          "name": "datasetPolicyName",
          "type": "string",
          "title": "Dataset Policy Name",
          "description": "A dataset policy name to scope the permission",
          "defaultValue": "default"
        }
      ]
    },
    {
      "name": "read",
      "title": "Read",
      "description": "Read documents matching the filter",
      "params": [
        {
          "name": "datasetPolicyName",
          "type": "string",
          "title": "Dataset Policy Name",
          "description": "A dataset policy name to scope the permission",
          "defaultValue": "default"
        }
      ]
    },
    {
      "name": "manage",
      "title": "Manage",
      "description": "Manage documents matching the filter",
      "params": [
        {
          "name": "datasetPolicyName",
          "type": "string",
          "title": "Dataset Policy Name",
          "description": "A dataset policy name to scope the permission",
          "defaultValue": "default"
        }
      ]
    },
    {
      "name": "history",
      "title": "History",
      "description": "Read the history of documents matching the filter",
      "params": [
        {
          "name": "datasetPolicyName",
          "type": "string",
          "title": "Dataset Policy Name",
          "description": "A dataset policy name to scope the permission",
          "defaultValue": "default"
        }
      ]
    },
    {
      "name": "create",
      "title": "Create",
      "description": "Create documents matching the filter",
      "params": [
        {
          "name": "datasetPolicyName",
          "type": "string",
          "title": "Dataset Policy Name",
          "description": "A dataset policy name to scope the permission",
          "defaultValue": "default"
        }
      ]
    }
  ]
```

 To create the resource, you need to pass an object containing its details to the `/permissionResources` endpoint.

```sh
curl \
	-X POST
	-H "Authorization: Bearer ${apiBearerToken}"
	-H "Content-Type: application/json"
	-d '{"permissionResourceType": "sanity.document.filter", "title": "Awesome Documents!", "description": "Our awesome documents", "config": {"filter": "_type == \"awesome\""}}'
	https://api.sanity.io/v2021-06-07/projects/${projectId}/permissionResources
```

When the permission resource is created, an `id` will be returned. This ID can be used to create a grant based on this resource. Just like in the above examples, only one grant will be created at a time.

```sh
# Create read grant for custom-role
curl \
	-X POST
	-H "Authorization: Bearer ${apiBearerToken}"
	-H "Content-Type: application/json"
	-d '{"roleName": "custom-role", "permissionName": "read", "permissionResourceId": "${idReturned}"}'
	https://api.sanity.io/v2021-06-07/projects/${projectId}/grants

	
# Create Update grant for custom-role
curl \
	-X POST
	-H "Authorization: Bearer ${apiBearerToken}"
	-H "Content-Type: application/json"
	-d '{"roleName": "custom-role", "permissionName": "update", "permissionResourceId": "${idReturned}"}'
	https://api.sanity.io/v2021-06-07/projects/${projectId}/grants
```

At this point, any user with the custom role will have this specific set of permissions around documents of a `_type == "awesome"`.

## All permissions by default role

The following are the granular permission grants for the default roles available for various plans. You can view your default permission resources by sending a `GET` request to a [project's /roles endpoint](https://www.sanity.io/docs/roles-reference?skipCdn=true#11c07f756785).

```json
[
  {
    "name": "administrator",
    "title": "Administrator",
    "description": "Administrate projects",
    "isCustom": false,
    "projectId": "xxxxxx",
    "appliesToUsers": true,
    "appliesToRobots": false,
    "grants": {
      "sanity.document.filter.mode": [
        {
          "grants": [
            {
              "name": "mode",
              "params": {
                "mode": "publish",
                "history": true,
                "datasetPolicyName": "default"
              }
            }
          ],
          "config": {
            "filter": "_id in path(\"**\")"
          }
        }
      ],
      "sanity.project": [
        {
          "grants": [
            {
              "name": "createSession",
              "params": {}
            },
            {
              "name": "delete",
              "params": {}
            },
            {
              "name": "deployStudio",
              "params": {}
            },
            {
              "name": "read",
              "params": {}
            },
            {
              "name": "update",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.cors": [
        {
          "grants": [
            {
              "name": "create",
              "params": {}
            },
            {
              "name": "delete",
              "params": {}
            },
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.datasets": [
        {
          "grants": [
            {
              "name": "create",
              "params": {}
            },
            {
              "name": "delete",
              "params": {}
            },
            {
              "name": "read",
              "params": {}
            },
            {
              "name": "update",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.graphql": [
        {
          "grants": [
            {
              "name": "manage",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.members": [
        {
          "grants": [
            {
              "name": "delete",
              "params": {}
            },
            {
              "name": "invite",
              "params": {}
            },
            {
              "name": "read",
              "params": {}
            },
            {
              "name": "update",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.roles": [
        {
          "grants": [
            {
              "name": "create",
              "params": {}
            },
            {
              "name": "delete",
              "params": {}
            },
            {
              "name": "read",
              "params": {}
            },
            {
              "name": "update",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.tokens": [
        {
          "grants": [
            {
              "name": "create",
              "params": {}
            },
            {
              "name": "delete",
              "params": {}
            },
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.usage": [
        {
          "grants": [
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.webhooks": [
        {
          "grants": [
            {
              "name": "create",
              "params": {}
            },
            {
              "name": "delete",
              "params": {}
            },
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ]
    }
  },
  {
    "name": "contributor",
    "title": "Contributor",
    "description": "Read and write to select datasets within the project",
    "isCustom": false,
    "projectId": "xxxxxx",
    "appliesToUsers": true,
    "appliesToRobots": true,
    "grants": {
      "sanity.document.filter.mode": [
        {
          "grants": [
            {
              "name": "mode",
              "params": {
                "mode": "create",
                "history": true,
                "datasetPolicyName": "default"
              }
            }
          ],
          "config": {
            "filter": "_id in path(\"**\")"
          }
        }
      ],
      "sanity.project.members": [
        {
          "grants": [
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.roles": [
        {
          "grants": [
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ]
    }
  },
  {
    "name": "create-session",
    "title": "Create Session",
    "description": "Create third-party sessions, manage third-party user profiles",
    "isCustom": false,
    "projectId": "xxxxxx",
    "appliesToUsers": false,
    "appliesToRobots": true,
    "grants": {
      "sanity.document.filter": [
        {
          "grants": [
            {
              "name": "create",
              "params": {
                "datasetPolicyName": "default"
              }
            },
            {
              "name": "history",
              "params": {
                "datasetPolicyName": "default"
              }
            },
            {
              "name": "manage",
              "params": {
                "datasetPolicyName": "default"
              }
            },
            {
              "name": "read",
              "params": {
                "datasetPolicyName": "default"
              }
            },
            {
              "name": "update",
              "params": {
                "datasetPolicyName": "default"
              }
            }
          ],
          "config": {
            "filter": "!(_id in [\"_.groups.create-session\", \"_.groups.administrator\", \"_.groups.write\", \"_.groups.read\", \"_.groups.public\"] || _id in path(\"_.groups.sanity.**\")) && _id in path(\"**\")"
          }
        }
      ],
      "sanity.project": [
        {
          "grants": [
            {
              "name": "createSession",
              "params": {}
            },
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.members": [
        {
          "grants": [
            {
              "name": "update",
              "params": {}
            }
          ],
          "config": {}
        }
      ]
    }
  },
  {
    "name": "deploy-studio",
    "title": "Deploy Studio",
    "description": "A role that is only allowed to deploy the studio",
    "isCustom": false,
    "projectId": "xxxxxx",
    "appliesToUsers": false,
    "appliesToRobots": true,
    "grants": {
      "sanity.project": [
        {
          "grants": [
            {
              "name": "deployStudio",
              "params": {}
            },
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.graphql": [
        {
          "grants": [
            {
              "name": "manage",
              "params": {}
            }
          ],
          "config": {}
        }
      ]
    }
  },
  {
    "name": "developer",
    "title": "Developer",
    "description": "Develop the projects",
    "isCustom": false,
    "projectId": "xxxxxx",
    "appliesToUsers": true,
    "appliesToRobots": true,
    "grants": {
      "sanity.document.filter.mode": [
        {
          "grants": [
            {
              "name": "mode",
              "params": {
                "mode": "publish",
                "history": true,
                "datasetPolicyName": "default"
              }
            }
          ],
          "config": {
            "filter": "_id in path(\"**\")"
          }
        }
      ],
      "sanity.project": [
        {
          "grants": [
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.cors": [
        {
          "grants": [
            {
              "name": "create",
              "params": {}
            },
            {
              "name": "delete",
              "params": {}
            },
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.datasets": [
        {
          "grants": [
            {
              "name": "create",
              "params": {}
            },
            {
              "name": "delete",
              "params": {}
            },
            {
              "name": "read",
              "params": {}
            },
            {
              "name": "update",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.graphql": [
        {
          "grants": [
            {
              "name": "manage",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.members": [
        {
          "grants": [
            {
              "name": "invite",
              "params": {}
            },
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.roles": [
        {
          "grants": [
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.tokens": [
        {
          "grants": [
            {
              "name": "create",
              "params": {}
            },
            {
              "name": "delete",
              "params": {}
            },
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.usage": [
        {
          "grants": [
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.webhooks": [
        {
          "grants": [
            {
              "name": "create",
              "params": {}
            },
            {
              "name": "delete",
              "params": {}
            },
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ]
    }
  },
  {
    "name": "editor",
    "title": "Editor",
    "description": "Editor can make changes to all datasets within the project",
    "isCustom": false,
    "projectId": "xxxxxx",
    "appliesToUsers": true,
    "appliesToRobots": true,
    "grants": {
      "sanity.document.filter.mode": [
        {
          "grants": [
            {
              "name": "mode",
              "params": {
                "mode": "publish",
                "history": true,
                "datasetPolicyName": "default"
              }
            }
          ],
          "config": {
            "filter": "_id in path(\"**\")"
          }
        }
      ],
      "sanity.project": [
        {
          "grants": [
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.datasets": [
        {
          "grants": [
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.members": [
        {
          "grants": [
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.roles": [
        {
          "grants": [
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.usage": [
        {
          "grants": [
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ]
    }
  },
  {
    "name": "viewer",
    "title": "Viewer",
    "description": "Viewer can view all documents in all datasets within the project",
    "isCustom": false,
    "projectId": "xxxxxx",
    "appliesToUsers": true,
    "appliesToRobots": true,
    "grants": {
      "sanity.document.filter.mode": [
        {
          "grants": [
            {
              "name": "mode",
              "params": {
                "mode": "read",
                "history": true,
                "datasetPolicyName": "default"
              }
            }
          ],
          "config": {
            "filter": "_id in path(\"**\")"
          }
        }
      ],
      "sanity.project": [
        {
          "grants": [
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.datasets": [
        {
          "grants": [
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.members": [
        {
          "grants": [
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.roles": [
        {
          "grants": [
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ],
      "sanity.project.usage": [
        {
          "grants": [
            {
              "name": "read",
              "params": {}
            }
          ],
          "config": {}
        }
      ]
    }
  }
]
```



# URL Format

Take this generic document query URL: 

```
https://zp7mbokg.api.sanity.io/v2021-06-07/data/query/production?query=*[0]
```

Each project has its own private hostname, which is always `<projectId>.api.sanity.io` for requests and `<projectId>.apicdn.sanity.io` for the [API CDN (cache) endpoint](/docs/content-lake/api-cdn). The path (what comes after the hostname) is always preceded by the [API version](/docs/content-lake/api-versioning) (you can set the present ISO date, `YYYY-MM-DD`, for the latest version) then the path to the API endpoint. So, to sum it up, these are the URL prefixes:

- API: `https://<projectId>.api.sanity.io/v<YYYY-MM-DD>/<path>`
- API CDN: `https://<projectId>.apicdn.sanity.io/v<YYYY-MM-DD>/<path>`

In the rest of this document we will generally only refer to the `<path>` part of the URL.

> [!NOTE]
> Note about the API CDN
> The API CDN only supports the /data/query path—its purpose being to cache query results across the globe for the benefit of your end users.

## How do I find my project ID?

In a configured studio, you find the project ID in the `sanity.json` file at the root of your project. Otherwise, you can always find it by locating your project on [https://manage.sanity.io](https://manage.sanity.io) or running `sanity debug` in the terminal in your studio folder.

## URL encoding

For clarity, we have opted to write URLs with their component in cleartext. In actual use they will all have to be encoded (using [encodeURIComponent](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/encodeURIComponent) or equivalent) so that this:

```text
https://zp7mbokg.api.sanity.io/v2021-06-07/data/query/production?query=*[_id == $id]&$id="myId"
```

Becomes this:

```text
https://zp7mbokg.api.sanity.io/v2021-06-07/data/query/production?query=*%5B_id%20%3D%3D%20%24id%5D&%24id=%22myId%22
```

> [!WARNING]
> Gotcha
> If you encode a URL that contains more than just a query string (i.e., it includes params as well), encodeURIComponent() will encode the & between query and your params, which is probably not what you want. Consider encoding the query and parameter strings separately or using encodeURI() instead.



# Authentication

> [!TIP]
> Protip
> If you are just going to read public documents, and edit them in the studio, you don't need to worry about authentication.

By default, unauthenticated users have read access to published documents (with certain [exceptions](/docs/archive/access-control-deprecated)). However, if you want to access draft documents or make modifications you will need to authenticate yourself as a project member with [write access](/docs/archive/access-control-deprecated). 

Sanity uses tokens for authentication, which are generated when you log in and then attached to all API requests in the HTTP `Authorization` header - e.g.:

```text
Authorization: Bearer skE5UXUmBEy7U50jcG4In4v4xoHZTlduDxQYet8Y84tsTqAZxp2reIPJsA1JzqXJno2qcpauGwPfjHpU
```

The content studio handles this for you automatically when you log in, and the command-line tool will generate and store a personal token when you run `sanity login`.

> [!WARNING]
> Gotcha
> Without intervention, personal tokens will last for one year (if using SAML SSO, the lifetime of the token is shorter). After logging out of the Sanity CLI, the subsequent login will generate a new personal token, and in doing so, invalidate the old one.

If you want to run authenticated API requests manually with e.g. `curl`, you can find your personal API token by running `sanity debug --secrets`, and look for the "Auth token" value under "Authentication". You then place this in an `Authorization` header:

```sh
curl -H "Authorization: Bearer <token>" https://<project>.api.sanity.io/v2021-06-07/data/query/production?query=*
```

> [!TIP]
> Protip
> Your API token is personal, and gives complete access to the Sanity API as your user. Take care not to share it with anyone, and use robot tokens instead to authenticate from applications and third-party services.

## Securing your API token

After setting up your token, it's important to keep this secure and not in a publicly-visible space -- such as GitHub or Bitbucket. When deploying code that needs your API token, many hosting companies provide ways of creating environment variables. These variables are stored securely on your host's server and are not stored in plain text in a repository.

- [Setting up environment variables in Netlify](https://docs.netlify.com/configure-builds/environment-variables/)​ 
- [Setting up environment variables in Vercel](https://vercel.com/blog/environment-variables-ui)​ 

## Robot tokens

If you need to authenticate with the Sanity API from an application or third-party service, you should generate a dedicated robot token for it, with appropriate [permissions](/docs/archive/access-control-deprecated). To create a robot token, open your project's [management console](https://manage.sanity.io), go to *Settings* > *API* > *Tokens*, and use the *Add new token *button to open the token creation dialog. Using a separate token for each application makes it easier to replace it or revoke access, if necessary.

Once a token is generated, it will be displayed exactly once - be sure to make a secure copy of it, since it is not possible to recover the token later (although you can create a new one). You can then use the token in API requests as outlined above.





# Patches

It is good practice to use **patches** when modifying Sanity documents through the API. Ideally, you make the smallest, most specific patch possible for your changes so that if multiple scripts or users are modifying the same documents at the same time, Sanity is able to merge those changes in a sensible way.

A patch is a special mutation submitted through the [mutations API  endpoint](/docs/http-reference/mutation). Since this endpoint is transactional you may submit one or several patches at once, potentially changing any number of documents in one single transaction. Here is an example of a full transaction submitting two patches at once (This sets the name property of the document with id "person-123" to "Remington Steele" and adds a reference to it to the end of the people-array of the document with the id "remingtons":

```json
{
  "mutations": [
    {
      "patch": {
        "id": "person-1234",
        "set": {
          "name": "Remington Steele"
        }
      }
    },
    {
      "patch": {
        "id": "remingtons",
        "insert": {
          "after": "people[-1]",
          "items": [
            {
              "_type": "reference",
              "_ref": "person-1234"
            }
          ]
        }
      }
    }
  ]
}
```

**Note:** Generally the keys of the patches use [JSONMatch syntax](/docs/content-lake/json-match) to target values for change. This syntax generally allows for paths like `some.array[8].attribute`, but can also do pattern matching, recursive search, and target multiple values at once. The full syntax is [documented here](/docs/content-lake/json-match).

> [!TIP]
> Protip
> JSONMatch is a variant of JSONPath that simplifies the syntax and eliminating to the maximum extent the number of special characters required to express a path.

## set

`set` performs a shallow merge of its argument into the document. Each key in the argument is either an attribute or a JSON path.

### Usage

`{ "set": { attributeOrJSONPathExpression: any } }`

### Examples

**Object properties**

Set the field `name` to the value `Bob` and the nested field `personalMetrics.height` to `201`:

`{ "set": { "name": "Bob", "personalMetrics.height": 201 } }`

**Arrays**

Set the `text` property to the value `Do the thing!` in all objects in the `body` array where the `_type` is `cta`:

`{ "set": { "body[_type==\"cta\"].text": "Do the thing!" } }`

> [!WARNING]
> Gotcha
> Notice that the array filter ([_type == \"cta\"]) must use double quotes. If you are in JSON, you must escape them (\"). 

## setIfMissing

`setIfMissing` is like `set`, except existing keys will be preserved and not overwritten.

## unset

Deletes one or more attributes. Each entry in the argument is either an attribute or a JSON path. Missing attributes are ignored. Unset can also be used to delete elements of an array.

### Usage

`{ "unset": [ attributeOrJSONPathExpression, ... ] }`

### Example

`{ "unset": ["foo", "bar"] }`

## insert

`insert` provides methods for modifying arrays, by inserting, appending and replacing elements via a JSONPath expression.

### Appending

Inserts the string `"a"` at the end of the array `some.array:`

```json
{
  "insert": {
    "after": "some.array[-1]",
    "items": ["a"]
  }
}
```

### Insertion

Inserts the string `"a"` at index 2 before whatever was there:

```json
{
  "insert": {
    "before": "some.array[2]",
    "items": ["a"]
  }
}
```

### Prepend

This inserts the string `"a"` at the beginning of the array.

```json
{
  "insert": {
    "before": "some.array[0]",
    "items": ["a"]
  }
}
```

### Replace

This removes index 2 through the end of the array, replacing the content with `"a"`.

```json
{
  "insert": {
    "replace": "some.array[2:]",
    "items": ["a"]
  }
}
```

### Advanced use of JSONMatch 

*NOTE: see the article on JSONMatch for more details*

Finds the element that has `key == 'abc-123'` and inserts `"a"` after it.

```json
{
  "insert": {
    "after": "some.array[key == \"abc-123\"]",
    "items": ["a"]
  }
}
```

Finds any object that has `key == 'list-123'` adds `"a"` at the beginning of its items array:

```json
{
  "insert": {
    "before": "blocktext..[key=\"list-123\"].items[0]",
    "items": ["a"]
  }
}
```

> [!WARNING]
> Gotcha
> Since single quotes are used to denote field names, regular strings must be enclosed in double quotes. When defining patches in JSON, the double quotation marks needs to be escaped (\").

## inc/dec

`inc` and `dec` change a numeric value. Each entry in the argument is either an attribute or a JSON path. For each entry, the attribute is looked up, modified and stored. The value may be a positive or negative integer or floating-point value. The operation will fail if target value is not a numeric value, or doesn't exist.

`inc` increments; `dec` is the same as `inc`, except the value is decremented.

### Examples

```json
{
  "inc": {
    "stats.visitorCount": 1
  }
}
```

If it's not certain whether the attribute exists, you can provide a default with `setIfMissing`:

```json
{
  "setIfMissing": {
    "stats.visitorCount": 0
  },
  "inc": {
    "stats.visitorCount": 1
  }
}
```

## diffMatchPatch

This operation supports robust incremental text patches according to the [Google diff-match-patch algorithm](https://code.google.com/p/google-diff-match-patch/), which has wide library support in practically all programming languages in common use. Given the document:

```json
{
  "_id": "dog-1",
  "_type": "someType",
  "aboutADog": "The rabid dog"
}
```

The following patch applies a diff-match-patch patch to the string:

```json
{
  "patch": {
    "id": "dog-1",
    "diffMatchPatch": {
      "aboutADog": "@@ -1,13 +1,12 @@\n The \n-rabid\n+nice\n  dog\n"
    }
  }
}
```

The document is transformed to read:

```json
{
  "_id": "dog-1",
  "_type": "someType",
  "aboutADog": "The nice dog"
}
```

The beauty of diff-match-patch patches is that they allow you to modify huge strings with small patches, and that they compose well, generally giving sane results even when several users or scripts are modifying the same text.







# Using JSONMatch

JSONMatch is widely used in the `patch` mutation type when updating documents. All mutations types support JSONMatch at the root key level when targeting the operations. This means that a single `set`, `unset`, `append` or `inc` operation can easily target one or more values of the document, or use the powerful recursive filtering of JSONMatch to find the desired value of the document automatically.



## General format

A JSONMatch path is an expression that, when evaluated, resolves to one or more locations in JSON document. A path can traverse object keys and arrays.

## Examples

In this reference we will use the following example JSON object to extract data from:

```javascript
{
  "name": "fred",
  "friends": [
    {
      "name": "mork",
      "age": 40,
      "favoriteColor": "red"
    },
    {
      "name": "mindy",
      "age": 32,
      "favoriteColor": "blue"
    },
    {
      "name": "franklin",
      "favoriteColor": "yellow"
    }
  ],
  "roles": ["admin", "owner"],
  "contactInfo": {
    "streetAddress": "42 Mountain Road",
    "state": {
    "shortName": "WY",
    "longName": "Wyoming"
    }
  }
}

```

Given the example document, these expressions can be evaluated:

```javascript
"name" → "fred" 
"friends[*].name" → ["mork", "mindy", "franklin"] 
"friends[age > 35].name" → ["mork"] 
"friends[age > 30, favoriteColor == "blue"].name" → ["mork", "mindy"] 
"friends[age?].age" → [40, 32] 
"friends[0, 1].name" → "mork" 
"friends[0, 1].name" → ["mork", "mindy"] 
"friends[1:2].name" → ["mindy", "franklin"] 
"friends[0, 1:2].name" → ["mork", "mindy", "franklin"] 
"contactInfo.state.shortName" → "WY" 
"contactInfo.state[shortName, longName]" → ["WY", "Wyoming"] 
"friends.age[@ > 35]" → [35] 
"roles" → ["admin", "owner"] 
"roles[*]" → ["admin", "owner"] 
"roles[0]" → "admin" 
"roles[-1]" → "owner" 
"contactInfo..shortName" → "WY" 
"[contactInfo.state.shortName, roles]" → ["WY", ["admin", "owner"]] 

```

## Keys

A single key matches that key in an object. For example, `name` returns `"fred"`. If keys contain special characters the key name can be surrounded in single quotes, so `'name'` also returns `"fred"`.

> [!WARNING]
> Gotcha
> Since single quotes are used to denote field names, regular strings must be enclosed in double quotes.

## Descent operator

The `.` operator descends into a key and selects a nested key. It has the format: `key1.key2`

For example:

```javascript
friend.name
```

This will match the `name` attribute in:

```javascript
{
  "friend": {
    "name": "mork"
  }
}

```

## Recursive descent

The `..` operator matches every value below the current selection descending through any objects, iterating over every array. Typical usage is to find a sub-object regardless of where it resides in an object. `content.blocks..[key == "abc123"]` will find the object having the `attribute` key equal to "abc123" wherever it resides inside the object or array at `content.blocks`.

## Arrays

Arrays can be subscripted with the `[]` operator. It has the formats:

```
"array[2]" → The second element of the array
"array[2, 3, 9]" → the second, third and ninth array element
"array[-1]" → the last array element
"array[1:9]" → array element 1 through 9 (non-inclusive)
"array[4:]" → array element 4 through to the end of the array
"array[:4]" → array elements from the start to element 4 of the array (non-inclusive)
"array[1, 4, 5:9, 12]" → union of array elements 1, 4, 5 to 9 and 12

```

## Constraints

Arrays can be filtered with constraints, e.g. `friends[age == 32]`. Constraints are separated by comma and are always a union ("or"), not an intersection. 

## Boolean operations

In its current implementation, JSONMatch do not support boolean operators `&&` or `||`, BUT essentially a union is the same as boolean `or`, and chaining constraints work the same as boolean `and`:

`"numbers[@ < 50, @ > 60]"`: Select numbers that are < 50 OR > 60.

`"numbers[@ > 20][@ < 30]"`: Select number that are > 20 AND < 30.

`'employees[name == "John Smith", name == "Granny Smith"]'`: employees that have the name "John Smith" OR "Granny Smith".



# Setting up your studio

## Create a new Studio with Sanity CLI

![Video](https://stream.mux.com/wIMs3CS7T4pP7hRArpQZsBZ01Be02vCjbK)

Run the command in your Terminal to initialize your project on your local computer.

See the documentation if you are [having issues with the CLI](/docs/help/cli-errors).

```sh
npm create sanity@latest -- --dataset production --template clean --typescript --output-path studio-hello-world
cd studio-hello-world
```

## Run Sanity Studio locally

Inside the directory of the Studio, start the development server by running the following command.

```sh
npm run dev
```

## Log in to the Studio

**Open** the Studio running locally in your browser from [http://localhost:3333](http://localhost:3333).

You should now see a screen prompting you to log in to the Studio. Use the same service (Google, GitHub, or email) that you used when you logged in to the CLI.



# Defining a schema

## Create a new document type

![Video](https://stream.mux.com/IfVfAwxfwOKN2khdGCQ3cs5IuF1rYte1)

Create a new file in your Studio’s `schemaTypes` folder called `postType.ts` with the code below which contains a set of fields for a new `post` document type.

```
import {defineField, defineType} from 'sanity'

export const postType = defineType({
  name: 'post',
  title: 'Post',
  type: 'document',
  fields: [
    defineField({
      name: 'title',
      type: 'string',
      validation: (rule) => rule.required(),
    }),
    defineField({
      name: 'slug',
      type: 'slug',
      options: {source: 'title'},
      validation: (rule) => rule.required(),
    }),
    defineField({
      name: 'publishedAt',
      type: 'datetime',
      initialValue: () => new Date().toISOString(),
      validation: (rule) => rule.required(),
    }),
    defineField({
      name: 'image',
      type: 'image',
    }),
    defineField({
      name: 'body',
      type: 'array',
      of: [{type: 'block'}],
    }),
  ],
})
```

## Register the `post` schema type to the Studio schema

Now you can import this document type into the `schemaTypes` array in the `index.ts` file in the same folder.

```
import {postType} from './postType'

export const schemaTypes = [postType]
```

## Publish your first document

When you save these two files, your Studio should automatically reload and show your first document type. Click the `+` symbol at the top left to create and publish a new `post` document.



# Displaying content in Next.js

## Install a new Next.js application

![Video](https://stream.mux.com/QSs5g22NaumIiAkggFufaDtEpCumej1j)

If you have an *existing* application, skip this first step and adapt the rest of the lesson to install Sanity dependencies to fetch and render content.

**Run** the following in a new tab or window in your Terminal (keep the Studio running) to create a new Next.js application with Tailwind CSS and TypeScript.

```sh
# outside your studio directory
npx create-next-app@latest nextjs-hello-world --tailwind --ts --app --src-dir --eslint --import-alias "@/*" --turbopack
cd nextjs-hello-world
```

You should now have your Studio and Next.js application in two separate, adjacent folders:

```text
├─ /nextjs-hello-world
└─ /studio-hello-world
```

## Install Sanity dependencies

**Run** the following inside the `nextjs-hello-world` directory to install:

- [next-sanity](https://github.com/sanity-io/next-sanity) a collection of utilities for integrating Next.js with Sanity
- [@sanity/image-url](https://www.sanity.io/docs/image-url) helper functions to take image data from Sanity and create a URL

```sh
npm install --legacy-peer-deps next-sanity @sanity/image-url
```

## Start the development server

**Run** the following command and open [http://localhost:3000](http://localhost:3333) in your browser.

```sh
npm run dev
```

## Configure the Sanity client

To fetch content from Sanity, you’ll first need to configure a Sanity Client.

**Create** a directory `nextjs-hello-world/src/sanity` and within it create a `client.ts` file, with the following code:

```
import { createClient } from "next-sanity";

export const client = createClient({
  projectId: "YOUR-PROJECT-ID",
  dataset: "production",
  apiVersion: "2024-01-01",
  useCdn: false,
});
```

## Display content on the homepage

Next.js uses server components for loading data at specific routes. The current home page can be found at `src/app/page.tsx`.

**Update** it to render a list of posts fetched from your Sanity dataset using the code below.

```tsx
import Link from "next/link";
import { type SanityDocument } from "next-sanity";

import { client } from "@/sanity/client";

const POSTS_QUERY = `*[
  _type == "post"
  && defined(slug.current)
]|order(publishedAt desc)[0...12]{_id, title, slug, publishedAt}`;

const options = { next: { revalidate: 30 } };

export default async function IndexPage() {
  const posts = await client.fetch<SanityDocument[]>(POSTS_QUERY, {}, options);

  return (
    <main className="container mx-auto min-h-screen max-w-3xl p-8">
      <h1 className="text-4xl font-bold mb-8">Posts</h1>
      <ul className="flex flex-col gap-y-4">
        {posts.map((post) => (
          <li className="hover:underline" key={post._id}>
            <Link href={`/${post.slug.current}`}>
              <h2 className="text-xl font-semibold">{post.title}</h2>
              <p>{new Date(post.publishedAt).toLocaleDateString()}</p>
            </Link>
          </li>
        ))}
      </ul>
    </main>
  );
}
```

## Display individual posts

**Create** a new route for individual post pages.

The dynamic value of a slug when visiting `/[slug]` in the URL is used as a parameter in the GROQ query used by Sanity Client.

Notice that we’re using [Tailwind CSS Typography](https://github.com/tailwindlabs/tailwindcss-typography)’s `prose` class name to style the post’s `body` block content. Install it in your project following their documentation.

```tsx
import { PortableText, type SanityDocument } from "next-sanity";
import imageUrlBuilder from "@sanity/image-url";
import type { SanityImageSource } from "@sanity/image-url/lib/types/types";
import { client } from "@/sanity/client";
import Link from "next/link";

const POST_QUERY = `*[_type == "post" && slug.current == $slug][0]`;

const { projectId, dataset } = client.config();
const urlFor = (source: SanityImageSource) =>
  projectId && dataset
    ? imageUrlBuilder({ projectId, dataset }).image(source)
    : null;

const options = { next: { revalidate: 30 } };

export default async function PostPage({
  params,
}: {
  params: Promise<{ slug: string }>;
}) {
  const post = await client.fetch<SanityDocument>(POST_QUERY, await params, options);
  const postImageUrl = post.image
    ? urlFor(post.image)?.width(550).height(310).url()
    : null;

  return (
    <main className="container mx-auto min-h-screen max-w-3xl p-8 flex flex-col gap-4">
      <Link href="/" className="hover:underline">
        ← Back to posts
      </Link>
      {postImageUrl && (
        <img
          src={postImageUrl}
          alt={post.title}
          className="aspect-video rounded-xl"
          width="550"
          height="310"
        />
      )}
      <h1 className="text-4xl font-bold mb-8">{post.title}</h1>
      <div className="prose">
        <p>Published: {new Date(post.publishedAt).toLocaleDateString()}</p>
        {Array.isArray(post.body) && <PortableText value={post.body} />}
      </div>
    </main>
  );
}
```





# Deploying Studio and inviting editors

## Deploy your Studio with Sanity

![Video](https://stream.mux.com/CvYhCQr8e1oZt98NW202BZLLNv376VVKc)

In your Studio directory (`studio-hello-world`) run the following command to deploy your Sanity Studio.

```sh
npm run deploy
```

## Invite a collaborator

Now that you’ve deployed your Studio, you can optionally invite a collaborator to your project. Navigate to: `https://www.sanity.io/manage/project/{{PROJECT_ID}}/members`.

They will be able to access the deployed Studio, where you can collaborate together on creating content.





# Administer organizations, projects, datasets, and users

#### Manage your team

[Roles](/docs/user-guides/roles)

[Projects, organizations, and billing](/docs/platform-management/projects-organizations-and-billing)



#### Understand pricing

[Plans and Payments](/docs/platform-management/plans-and-payments)

[Pricing plans](https://www.sanity.io/pricing)





# Platform terminology

## Organizations

![Illustrative graphic showing two separate boxes with connected user avatars within](https://cdn.sanity.io/images/3do82whm/next/2afb547252d5d719a8bba88aa7dbe2257eeac243-3840x2160.png)

*Organizations collate billing for multiple projects, which may be queried by the same or unique frontends.*

An organization is an entity where multiple projects are grouped to give them a single billing point. It does not need to be a registered company.

Project configuration cannot be shared across projects in an organization, nor can its content be referenced across projects.

A member may be a member of multiple organizations but must be invited to each. The roles in each project are created uniquely.

Organizations are also where Single Sign-On (SSO) is configured so that members can register to your projects using the provider of your choice.

Splitting a Sanity implementation across organizations is rarely required. Only if you require different projects to be billed with different payment methods.

## Projects

![Illustrative graphic showing user avatars connected within a single box](https://cdn.sanity.io/images/3do82whm/next/d77647461411b1ab167d38fe8c9737b1df1e3f9e-3840x2160.png)

*Datasets inside a project can reference one another, be used independently by members, and be queried by one or many API consumers.*

A project is a self-contained collection of datasets, members, and configuration options such as webhooks and tokens. These cannot be shared between projects. A member of one project is not automatically granted access to any other, though an administrator member may invite them.

All administrator members in a project will have access to project-level configurations such as datasets, members, tokens, custom access control, and webhooks. Other members may get read-only or no access to these.

Your various frontends and the Sanity Studio can be configured to query from or write data to multiple projects.

However, content cannot be *referenced* between projects, only between datasets.

With plugins and scripts, it is possible to *migrate* content across projects.

Dividing how you use Sanity across projects is useful when you need absolute separation of developer and author teams with very different content creation goals.

## Roles

Within Sanity, you can control user access by assigning roles. Organization and project roles are 2 different sets of roles to control user access at different levels of granularity:

- **Organization**: global throughout the organization.
- **Project**: specific for each project.

To access organization and project role configuration options:

22. Go to the [management](https://www.sanity.io/manage) page.
22. The top navigation bar features 2 drop-down menus: click the first from the left to select the organization, and the second to select a project within the specified organization.

![The diagram represents the organization and project hierarchy with the respective roles.](https://cdn.sanity.io/images/3do82whm/next/f38b881860683d90654a1fd3c9db0be6c2f82d32-2624x1767.png)

### Organization roles

An **Organization** is an entity that groups multiple projects to provide a centralized location to manage tasks that are common to all projects, such as billing.

The available roles within an [organization](https://www.sanity.io/docs/projects-organizations-and-billing) are:

- **Administrator**: administrators can manage billing details, legal contacts, organization members and manage project ownership. Organization administrators have the ability to manage user access to each project in the organization.
- **Billing manager**: billing managers can manage billing details, legal contacts and attach projects to the organization.
- **Developer**: developers can create and attach projects to the organization. They can also alter the metadata for an organization, such as the informal name.
- **Member**: this is the default role for users in an organization. Members are able to view teammates operating in all projects across the organization.- When a **Project Administrator** is also an **Organization Member**, they are able to autocomplete the name of teammates when inviting users to a project.



> [!TIP]
> Protip
> Organization administrators don't automatically have access to every project in the organization. However, they do have the ability to fully manage user membership in projects (including their own membership).
> 
> If a user needs access to a project, then either an organization or project administrator can add them.

> [!WARNING]
> Gotcha
> Users with the Organization Member role are able to identify any other user that exists in a project across the organization. If you're using your Sanity organization in a multi-tenant setup, you may not want your users to be aware of what exists outside their accessible project.
> 
> To avoid this situation, confirm that your users do not have the Organization Member role. To ensure that newly added users do not automatically inherit this role, you can remove the 'default organization role' in your organization settings.

### Project roles

A **Project** is a self-contained collection of datasets, members, and configuration options such as webhooks and tokens.

[Project roles](https://www.sanity.io/docs/roles#e2daad192df9) include an administrator role as well. Whereas organization administrators can manage users and billing for the whole organization, project administrators have read and write access to all project settings and datasets.

> [!WARNING]
> Gotcha
> Organization administrators and project administrators are different roles and have different scopes.
> 
> Organization members are not automatically granted access to projects owned by the organization.
> 
> You can assign both the organization and project administrator roles to the same user.
> 
> Different Login providers e.g. email-password, Google, GitHub, do not map to a the same Sanity user account even if they use the same email.
> 
> To transfer ownership of a project to another organization, you need either an Administrator or a Billing manager role in both the source and the receiving organizations.

## Members and custom access controls

![Illustrative graphic showing a single user avatar connected to various icons representing different workflows](https://cdn.sanity.io/images/3do82whm/next/3bf653b7198c56f5dea40461f666077e00445971-3840x2160.png)

*A project member may have a different view, create or edit permission depending on document values and dataset tags.*

Members in Sanity can be active across multiple organizations and projects – but will need to be invited to any of them to begin collaborating and have unique roles within each. Their roles in a project will determine their access to datasets.

Members may be invited to a Sanity project via our built-in authentication or with Single Sign-on (SSO) configured, the provider of your choice.

> [!TIP]
> Protip
> Our SAML support includes the ability to map groups from your authentication provider to roles within a Sanity project. Read the docs to find out more.

They can be privileged as administrators to have complete access to all project settings and the ability to modify any data. Using custom access controls, permissions can be scoped to no access. Or, at a minimum, view-only permissions of a single document based on any value within it.

Example: A member with the custom role “Junior Store Manager” may only be able to view documents of the type `product` with a `price` field greater than `100` on the **production** dataset.

For projects with multiple teams or lines of responsibility, member roles ensure that individual content creators and developers do not have their work disturbed unexpectedly.

## Datasets

![Illustrative graphic sthat show connection between user avatars, the Sanity Studio, and an example frontend](https://cdn.sanity.io/images/3do82whm/next/b65714c033677e6485fa010e51d00cdcc43804bb-3840x2160.png)

*Think of the Sanity Studio as just one of the many frontends that interact with Content Lake API’s*

A dataset is a collection of schemaless data stored as JSON and uploaded file and image assets. Members may have access to all datasets in a project by default, but their privileges can be filtered in each dataset using custom access controls.

Your applications can query data from multiple datasets, and your Studio can be configured to switch between them using [workspaces](https://www.sanity.io/docs/workspaces). Content can be referenced between datasets using [cross-dataset references](https://www.sanity.io/docs/cross-dataset-references).

Datasets are often used as “environments.” Many teams have dataset names mapped to **development**, **staging**, and **production** environments.

Predominately in Sanity, multi-tenant configurations are performed by separating content between datasets. You may have individual datasets for teams, brands, or markets, in addition to datasets as global sources of truth, which all other datasets may reference.

## References

- [Projects, organizations, and billing](https://www.sanity.io/docs/projects-organizations-and-billing)
- [Plans and Payments](https://www.sanity.io/docs/plans-and-payments#TXYAJCaS)
- [Roles](https://www.sanity.io/docs/roles)



# Plans and payments

All Sanity projects are on a plan (by default a free plan), which comes with a set of included features and monthly resource quotas. Our available plans are listed on the [pricing page](/pricing), and can be ordered at any time from the project's [management](https://manage.sanity.io) page. Some plans offer additional features for purchase. Any resource usage beyond the quotas will be billed at overage rates, except for free projects without a registered credit card (or on a legacy free plan), which may be temporarily deactivated instead.

Paid plans require that the project belongs to an "organization", which is responsible for payment. This does not have to be an actual company–you can create your own personal organization if you want to. The organization simply holds the billing address and credit card information for one or more projects.

Projects are billed in advance each month, and follow calendar months; all projects in an organization are invoiced and charged together on the first of each month, along with any overage charges accrued during the previous month.

We prorate the price when you change plans or cancel: if you upgrade to a larger plan on the 20th, we will only charge you for the remaining third of the month, and subtract a third of what you may have already paid for the old plan. Similarly, if you cancel a project on the 20th, we will refund one third of the monthly cost. Features and resource quotas are prorated in the same way. Plan changes and cancellations can be performed at any time, and take effect immediately.

From time to time we may make changes to our plan offerings, but you will remain on your original plan unless you choose to switch to a newer plan yourself. We may occasionally move projects to newer plans automatically, but this will generally only happen when it is clearly in your best interest, and you will always be notified of this in advance.

In the case of payment failures, we will notify you by email, and retry the payment for three days. If payment still has not gone through, and we have not been able to contact you, we may temporarily deactivate the project.

## Resource Quotas and Overage

Plans come with a set of resource quotas:

- **API requests:** number of HTTP(S) requests to `<project>.api.sanity.io` during the month, excluding `OPTIONS` requests.
- **API CDN requests:** number of HTTP(S) requests to `<project>.apicdn.sanity.io` during the month, excluding `OPTIONS` requests. (This is typically the requests incurred when serving your content to end-users.)
- **Assets:** total size of all uploaded assets and files at the end of the month
- **Bandwidth:** total outgoing bandwidth for API, API CDN, and asset traffic during the month
- **Datasets:** total number of datasets at the end of the month
- **Documents:** total number of stored documents (including drafts) across all datasets at the end of the month
- **Non-admin users:** total number of non-admin users (excluding service tokens, i.e. robots) at the end of the month
- **Agent Actions**: Total number of API calls available for agent actions. Can be configured through the Manage org level settings.

Resource usage is metered periodically, with statistics available on each project's management page. Free projects without a credit card (or on our legacy free plan) may be temporarily disabled once they reach their quotas, while paid projects are charged additional overage fees. The current overage rates for a project can be found under the "Plan" tab in the [management tool](https://manage.sanity.io)). Project admins may prevent extra overage fees by temporarily disabling the project.

Please be aware that downgrading a project to a smaller plan will also prorate the quotas, which may trigger additional overage charges due to resources that have already been spent earlier in the month. For example: if you are on a plan with a 50.000 API request quota, and have used 35.000 of these so far, then downgrading to a tiny plan with only 10.000 API requests in the middle of the month with leave you with half of 50.000 + half of 10.000, which is 30.000 API requests. Since you used 35.000, you now have 5.000 overage on your effective plan for this month and the rest of this months API requests will be billed as overage.

## Project Transfers

Projects may be transferred between organizations. If you have billing rights in both organizations, this happens instantly. Otherwise, a billing manager in the receiving organization must approve the transfer before it takes effect.

Once a transfer is approved, the sender and receiver are prorated the plan cost; the sender is refunded the already paid amount for the remainder of the month, while the receiver is charged for the remainder of the month at the time of the transfer. The receiver will be responsible for paying any overage charges accrued on the project. Since a transfer does not change the project plan, this has no effect on resource quotas.

> [!NOTE]
> Permissions Required
> To transfer a project, you must have an Administrator role in both the current project and its organization. Specifically, your role must include the following permission grants:
> • sanity.organization.projects.detach
> • sanity.project.update

## Refunds and Credits

Project downgrades, transfers, and cancellations take effect instantly, and the already paid plan cost will be prorated and refunded for the remainder of the month. Any overage will be tallied and charged at the end of the month, unless the entire organization is deleted, in which case overage is tallied and charged immediately.

Refunds are issued to the card that was originally charged, and may take up to 10 business days to complete depending on the card issuer.



# Projects, organizations, and billing

 All projects on Sanity.io can be tied to an organization. An organization holds contact and billing information, and can have administrators, billing managers, and developers added to them. Agencies and freelancers can initiate projects and create organizations for their clients for a smooth hand-over.

> [!WARNING]
> Gotcha
> Looking to add a project to an organization? Projects are always assigned to an individual upon creation and can then be moved to an organization via the Manage interface. To move a project between organizations you must have admin privileges in the organization that currently owns the project.

## Creating a new organization

4. Log into [manage.sanity.io](https://manage.sanity.io)
4. Select a project and go to **Settings**
4. Under **General** you'll find the “Organization” heading
4. Here you can either select an organization you're already a member of, or create a new one from the link
4. Fill in the Payment details and hit **Save**

## Managing an organization

6. Log into [manage.sanity.io](https://manage.sanity.io)
6. Projects will be listed out and sectioned by the organizations you're member of
6. In the organization headings, push the “edit organization” link to go to its settings
6. In the organization settings you get an overview over existing projects, as well as the team, billing information, and GDPR information.

## Adding contact details for EU representative and Data Protection Officer (DPO)

8. Follow the steps for managing your organization
8. Under **Settings** you'll find buttons for adding contact details for your EU representative and Data Protection Officer

## Deleting an organization  

10. Move or delete all projects connected to the organization in the projects’ settings
10. Follow the steps for managing your organization
10. Under **Settings**, click the **Delete Organization **button



# Understanding the Growth plan trial

During the trial period, you’ll have access to additional paid features from the Growth plan including [private datasets](/docs/keeping-your-data-safe#5c2e941ea03c), [user roles](/docs/user-guides/roles), [Comments](/docs/studio/comments), and [Scheduled Publishing](/docs/scheduled-publishing), and [AI Assist](/docs/studio/install-and-configure-sanity-ai-assist).

## Starting the trial

Every new Sanity project created automatically gets free access to additional paid features from the Growth plan for a limited period of time. Here's how to activate the trial on a new project:

- **New Sanity users**: [Create your first project](/get-started?ref=trial-docs) and follow the instructions
- **Existing Sanity users: **Create a new project by running `npx create sanity@latest` in your CLI/terminal and follow the instructions to create a new project

## Trial limitations

The Growth trial unlocks additional features available in the Growth plan, but comes with the same usage limits as the Free plan:

- 20 users available
- 2 datasets
- 2k unique attributes (per dataset)
- 10k documents
- 2 GROQ-powered webhooks

See the [plan comparison table on the Pricing page](/pricing#compare-plans) for more details.

## End-of-trial decision

When the trial period is over, you can choose to either keep the paid features by upgrading to the Growth plan or do nothing and get automatically downgraded to the Free plan.

**We will not charge you for anything unless you upgrade to the Growth plan.**

### Upgrade to Growth plan

If you want to keep access to the additional features of the Growth plan, you'll need to upgrade your project and add a payment method in [Manage](/manage). You can do this both during the trial or after it ends. Here's how to do it:

14. Select your project from the dropdown menu labeled **Select project or organization** 
14. Navigate to the **Plan** tab
14. Click **Upgrade to Growth**
14. Follow the instructions on how to create an Organization and add your payment method

### Downgrade to Free plan

If you don't add your payment details and upgrade to the Growth plan, your project is automatically downgraded to the Free plan when the trial ends. All team members with non-admin roles will be converted to viewers and you'll lose access to the paid features from the Growth plan.

You can upgrade to the Growth plan anytime later by visiting your project page in [Manage](/manage), navigating to the tab labeled **Plan** and clicking **Upgrade to Growth**.



# Extending the Growth plan with paid add-ons

## How to enable add-ons

To enable one of the add-ons for your Growth plan project you can:

3. Log into [Manage](/manage?ref=docs-add-ons)
3. Select your project from the dropdown labeled **Select project or organization**
3. Navigate to the tab labeled **Plan**
3. Scroll down to the **Add-ons** section and click **See details** on the add-on you wish to enable
3. Click **Enable add-on** in the modal

The add-on is now enabled and will be billed in the subsequent billing cycle as a line item on your invoice.

#### Get all add-ons with Enterprise
Every add-on can be made available through our Enterprise plan, with further customization options. Contact our sales team to see if it's a fit for your project.
[Talk to sales](/contact/sales?ref=docs-add-ons)

## Add-ons available

### SAML single sign-on (SSO)

SAML single sign-on (SSO) enables you to control access to your Sanity project through a third-party identity provider, such as Okta, Google, or Azure Active Directory.

When a user logs in, they will be assigned a default role. You can configure which role is the default. Additional role mapping rules are available on the [Enterprise plan](/enterprise).

[Learn how to set up SAML SSO →](/docs/developer-guides/sso-saml)

### Dedicated support

The dedicated support add-on gives you access to direct technical support from Sanity's Support Engineers over email.

If you have questions about this add-on, you can [contact our support team](/contact/billing?ref=docs-add-ons).

### Increased quota

Extend the included quota of the Growth plan to:

- Documents: 50k (up from 25k)
- API CDN requests: 5M (up from 1M)
- API requests: 1M (up from 250k)
- Bandwidth: 500GB (up from 100GB)
- Assets: 500GB (up from 100GB)

Cost of additional usage remains unchanged, as listed on our [Pricing page](/pricing?ref=docs-add-ons).

### Extra datasets

Unlock up to 2 additional datasets for your project, increasing the maximum number of datasets from 2 to 4. Note that we only charge you when you create the additional dataset(s) – not when you enable this add-on.

## Questions or feedback?

Please [reach out to our support team](/contact/billing?ref=docs-add-ons) if you have any questions about the paid add-ons. And if there's another feature you'd like to see here, we'd love to hear from you in our [community Slack](https://slack.sanity.io).



# Sanity's non-profit plan

## The plan

The non-profit plan mirrors the [Growth plan](/pricing), but we offer it for free (no credit card required) as long as you stay within the quotas. Additionally, we've added the following features to the plan:

- 25 users included free of charge, with $15 per additional user without limit
- 3 datasets (+1 from Growth plan)
- [SAML SSO add-on](/docs/growth-plan-add-ons#ec44fec5f1cd) included

Note that additional [add-ons](/docs/platform-management/growth-plan-add-ons) are not available, and you need to add a credit card to pay for additional overages and users.

## Who's eligible?

We offer the non-profit plan to:

- Small and mid-sized organizations that are “organized and operated for a collective, public or social benefit” and where the revenue exceeding expenses goes back into the cause
- Educational and academic institutions of smaller sizes and budgets
- Open-source projects that are based on sponsorships or voluntary effort (so not monetized)

## Who's *not *eligible?

- Organizations that qualify for our [Enterprise plan](/pricing), including large non-profit organizations like global humanitarian operations, universities, etc.
- Organizations that can’t comply with our [Terms of Service](https://www.sanity.io/legal/tos).

## How to apply?

[Fill out the application form](https://forms.gle/xkQstGLFrujT2me39) and you'll hear back from us within 14 business days. Please note:

- If you don't provide a valid Sanity project ID, your application will be ignored.
- You'll receive an email when a decision has been made, but we're not able to provide technical support over email after this. Please join our community on Discord to get help.



# Activity Feed

The Activity Feed lets you investigate what happened in your Sanity projects. If you are uncertain how a scenario took place, you can use the Activity Feed to investigate what actually happened.

![](https://cdn.sanity.io/images/3do82whm/next/e54be0b039dcf6cb7d215e5473bd5efc9315bc1f-1790x1364.png)

## What is an event?

An event is created when various actions are performed in the system. This can be by a user, by Sanity, or even by a robot token. An event contains information about what happened and when. Events differ by action, and each contains a unique ID.

### List of team events

- user creates team
- user changes team’s name
- user changes billing address
- user changes payment method
- user changes EU Representative
- user changes Data Protection Officer
- user changes user’s role
- user removes user
- user invite user(s)
- user joins team
- user revoked invitation

### List of project events

- user creates project
- user changes project’s name
- user changes project’s custom studio URL
- user changes project’s plan
- user adds CORS origin
- user removes CORS origin
- user adds webhook
- user removes webhook
- user adds API token
- user removes API token
- user changes user’s permissions
- user removes user
- user invites user(s)
- user joins project
- user revokes invitation
- user creates dataset
- user deletes dataset
- user edits dataset
- user duplicates dataset

## Exporting

Actions for projects and teams are available as a CSV export from the [manage dashboard](https://sanity.io/manage) for each project. The export can be customized by the date when created.

### Data provided in the export

- action
- actorEmail
- actorId
- actorName
- correlationId
- datasetName
- description
- documentId
- id
- metadata.email
- metadata.invitedBy
- metadata.role
- organizationDisplayName
- organizationId
- projectDisplayName
- projectId
- timestamp
- transactionId
- userEmail
- userId
- userName
- version



# Request logs

Sanity can be set up to deliver detailed logs for all [API requests related to a project](/docs/http-api). This allows you to make informed decisions about how content is requested and interacted with in the Content Lake.

You can use these logs to get insights into what’s driving requests and bandwidth usage, where requests come from, and more.

## Enrich your logs with request tags

Sanity’s Content Lake supports marking your requests with tags as a simple but powerful way of adding some context to your API activities. Visit the [request tags reference article](/docs/platform-management/reference-api-request-tags) to learn more about this feature.

[Request tags reference](/docs/platform-management/reference-api-request-tags)



## Request logs for self-serve plans

You can access request logs on self-serve plans by going to the **Usage** section of [your project settings](https://www.sanity.io/manage). At the bottom of this page, you'll find a button to download up to 1GB of log data from the last 7 days up to the day before you download the data. You can request a new export every 24 hours.

### Analyzing request logs

The request log export will come as a compressed NDJSON file. You use different tools to analyze this like [GROQ CLI](https://github.com/sanity-io/groq-cli) or [jq](https://jqlang.github.io/jq/). You can even convert it to CSV using a package like [json2csv](https://github.com/juanjoDiaz/json2csv):

```sh
gunzip --stdout [compressed logfile].ndjson.gz | npx json2csv --ndjson --output [output].csv
```

Exploring tools like [Jupyter Notebook](https://jupyter.org/) can also be helpful for more extensive analysis.

Another helpful tip is to upload a sample of your log files to AI tools like ChatGPT and ask them to analyze it or provide you with methods for doing so in Python or other programming languages and frameworks. 

Visit the [request log data reference](/docs/platform-management/reference-request-log-data) to learn how the logs are structured and formatted.

[Request log data reference](/docs/platform-management/reference-request-log-data)

[Request logs analysis example](https://github.com/sanity-io/sanity-request-logs-analysis/blob/4e3db114b57c270ab899837d6a72ce6f2930471f/request-logs.ipynb)



## Request logs on enterprise plans

For projects on enterprise plans, logs are delivered as compressed NDJSON files to your [Google Cloud Storage (GCS)](https://cloud.google.com/storage) bucket, which then serves as a staging area for ingesting the reports into a data analysis tool of your choice. 

_This is a paid feature, available on the Enterprise plan._

Visit the [request log data reference](/docs/platform-management/reference-request-log-data) to learn how the logs are structured and formatted.

### Enable and configure log delivery

You can always extract, that is, download, the raw request log file on demand for ad hoc analysis. However, you can save time and make insights more broadly accessible to your team if you load logs into a data lake, such as [BigQuery](https://cloud.google.com/bigquery), and set up pre-defined queries for common reports.

The entire process – from enabling the log delivery service to querying your data for insights – requires a few separate steps to set up and follows the [Extract, Load, and Transform (ELT)](https://www.ibm.com/topics/elt) pattern for data integration. You will find an example implementation below, detailing how to implement Sanity request logs with [Google Cloud Storage](https://cloud.google.com/storage) and [BigQuery](https://cloud.google.com/bigquery).

> [!WARNING]
> Gotcha
> Currently, Google Cloud Storage is the only supported option for delivery. Azure and AWS both offer ways to copy data from GCS; however, Sanity has not yet tested and verified either of these solutions.

### Step 1: Extract (required)

In this step, you will enable log delivery in your Sanity project and connect your GCS bucket in the Sanity project management settings. The setup described in this step is the only part of this guide that is required to use the request log feature, while the subsequent steps are provided as an example implementation.

#### Prerequisites

- A Sanity account with administrator access to [project management settings](https://sanity.io/manage)
- A [GCS account](https://cloud.google.com/storage) with permission to create and administrate GCS buckets
- Optionally: Access to and familiarity with command line tooling like `node`, `npm`, and the [gcloud suite of tools](https://cloud.google.com/sdk/docs/install). While this guide demonstrates how to achieve the necessary setup in GCP using the command line, the same result can be achieved using the GCP web interface.



#### Configure project in `gcloud` CLI

Ensure that the Google Cloud CLI is configured to the correct project for where you want to store your request logs.

```bash
# Replace [PROJECT_ID] with your actual Google Cloud project ID

gcloud config set project [PROJECT_ID]
```

#### Create the bucket

Create a new GCS bucket where your files will be uploaded to.

```bash
# Replace [BUCKET_NAME] with your actual bucket name

gcloud storage buckets create gs://[BUCKET_NAME]
```

#### Give Sanity access to the bucket

For Sanity to deliver files to your GCS bucket, you must give our service account (`serviceAccount:delivery@sanity-log-delivery.iam.gserviceaccount.com`) the `storage.objectCreator` role:

```bash
# Replace [BUCKET_NAME] with your actual bucket name

gcloud storage buckets add-iam-policy-binding gs://[BUCKET_NAME] --member=serviceAccount:delivery@sanity-log-delivery.iam.gserviceaccount.com --role=roles/storage.objectCreator
```



#### Enable log delivery on your Sanity project

Log Delivery is disabled by default. It must be enabled in the Sanity project settings by a project administrator.

- Log in to Manage: [www.sanity.io/manage](https://www.sanity.io/manage)
- Navigate to your project settings. You should see an option to enable the log delivery feature by adding a GCS bucket.

![Shows the interface for enabling log delivery in the Sanity project management area](https://cdn.sanity.io/images/3do82whm/next/063ac57a3f3fc5390af1ef2840269c969b835bb1-767x177.png)

- Click the button labeled **Add bucket** and you should be prompted to add the full URI of your GCS bucket.

![Shows the interface for entering your details for log delivery in the Sanity project management area](https://cdn.sanity.io/images/3do82whm/next/6d1b071c1494d3accf62977ad89c52d0f305bcf7-960x383.png)

- Assuming everything went well, you should now be set to receive API request logs within a couple of minutes. You may read on to see an example implementation or roll your own with the tooling of your choice.



### Step 2: Load (optional)

Once the pipeline for log delivery has been configured, it’s time to hook up your preferred data analysis tool. The process will vary somewhat from tool to tool. The following section will show you how to accomplish this task using [BigQuery](https://cloud.google.com/bigquery) from Google.

You can set up a direct connection between BigQuery and GCS buckets using [External Tables](https://cloud.google.com/bigquery/docs/external-tables). Please read [the documentation to understand the costs and limitations](https://cloud.google.com/bigquery/docs/external-tables#pricing).

Sanity has structured the bucket key in a way that allows for partitioning per project, event type, and date.

We key the object using [Hive partitioning](https://cloud.google.com/bigquery/docs/hive-partitioned-queries#supported_data_layouts) with the following format:

```plaintext
gs://event-logs/project_id=[string]/kind=[event-type:string]/dt=[date:DATE]/[file-name:string].ndjson.gz
```

This allows the log data to be loaded into various data platforms with the project ID, data type, and date used as partitioning properties.

#### Prerequisites

- You should have completed the setup process described in step 1, so you are starting with the log delivery service already enabled and connected to your GCS bucket
- You’ll need `node`, `npm`, and the `gcloud` command line tools installed

#### Define your table schema

Create a JSON file locally named `schema.json` with the nested schema definition for your log data.

> [!TIP]
> Protip
> If you’re working on a Sanity Studio project, we recommend placing this schema file in its folder (for example, /log-delivery/schema.json) to avoid confusion with the Studio schema for your content model.

```json
{
  "sourceFormat": "NEWLINE_DELIMITED_JSON",
  "schema": {
    "fields": [
      { "name": "timestamp", "type": "TIMESTAMP", "mode": "REQUIRED" },
      { "name": "traceId", "type": "STRING", "mode": "REQUIRED" },
      { "name": "spanId", "type": "STRING", "mode": "REQUIRED" },
      { "name": "severityText", "type": "STRING", "mode": "NULLABLE" },
      { "name": "severityNumber", "type": "INT64", "mode": "REQUIRED" },
      {
        "name": "body",
        "type": "RECORD",
        "mode": "REQUIRED",
        "fields": [
          { "name": "duration", "type": "DECIMAL", "mode": "NULLABLE" },
          { "name": "insertId", "type": "STRING", "mode": "NULLABLE" },
          { "name": "method", "type": "STRING", "mode": "NULLABLE" },
          { "name": "referer", "type": "STRING", "mode": "NULLABLE" },
          { "name": "remoteIp", "type": "STRING", "mode": "NULLABLE" },
          { "name": "requestSize", "type": "INT64", "mode": "NULLABLE" },
          { "name": "responseSize", "type": "INT64", "mode": "NULLABLE" },
          { "name": "status", "type": "INT64", "mode": "NULLABLE" },
          { "name": "url", "type": "STRING", "mode": "NULLABLE" },
          { "name": "userAgent", "type": "STRING", "mode": "NULLABLE" }
        ]
      },
      {
        "name": "attributes",
        "type": "RECORD",
        "mode": "NULLABLE",
        "fields": [
          {
            "name": "sanity",
            "type": "RECORD",
            "mode": "NULLABLE",
            "fields": [
              { "name": "projectId", "type": "STRING", "mode": "REQUIRED" },
              { "name": "dataset", "type": "STRING", "mode": "NULLABLE" },
              { "name": "domain", "type": "STRING", "mode": "NULLABLE" },
              {
                "name": "groqQueryIdentifier",
                "type": "STRING",
                "mode": "NULLABLE"
              },
              { "name": "apiVersion", "type": "STRING", "mode": "NULLABLE" },
              { "name": "endpoint", "type": "STRING", "mode": "NULLABLE" },
              { "name": "tags", "type": "STRING", "mode": "REPEATED" },
              { "name": "studioRequest", "type": "BOOLEAN", "mode": "NULLABLE" }
            ]
          }
        ]
      },
      {
        "name": "resource",
        "type": "RECORD",
        "mode": "REQUIRED",
        "fields": [
          {
            "name": "service",
            "type": "RECORD",
            "mode": "NULLABLE",
            "fields": [{ "name": "name", "type": "STRING", "mode": "NULLABLE" }]
          },
          {
            "name": "sanity",
            "type": "RECORD",
            "mode": "NULLABLE",
            "fields": [
              { "name": "type", "type": "STRING", "mode": "NULLABLE" },
              { "name": "version", "type": "STRING", "mode": "NULLABLE" }
            ]
          }
        ]
      }
    ]
  },
  "compression": "GZIP",
  "sourceUris": ["gs://[BUCKET_NAME]/[PREFIX]event-logs/*"],
  "hivePartitioningOptions": {
    "mode": "CUSTOM",
    "sourceUriPrefix": "gs://[BUCKET_NAME]/[PREFIX]event-logs/{project_id:STRING}/{kind:STRING}/{dt:DATE}/"
  }
}

```

> [!WARNING]
> Gotcha
> Make sure to replace [BUCKET_NAME] and [PREFIX] with the appropriate values for your setup.

#### Creating the external table in BigQuery

Run the following command using the `bq` (BigQuery) CLI tool bundled with the `gcloud` CLI:

```bash
# Replace [DATASET_NAME] and [TABLE_NAME] with your details

bq mk --external_table_definition=schema.json [DATASET_NAME].[TABLE_NAME]
```

#### Query your log data in BigQuery

Once the log data is loaded into the table, you can run queries against it to test if the implementation works as expected.

**Example: Get data from yesterday.**

```sql
/* Replace [GCP_PROJECT_NAME], [DATASET_NAME], and [TABLE_NAME] with your details */

SELECT
  *
FROM
  `[GCP_PROJECT_NAME].[DATASET_NAME].[TABLE_NAME]`
WHERE
  project_id = '[SANITY_PROJECT_ID]' AND
  kind = 'request-log' AND
  dt = DATE_ADD(CURRENT_DATE(), INTERVAL -1 DAY)
```



### Step 3: Transform (optional)

Your log data is now ready to provide answers and insights into API and CDN usage. The following section will show how to query your logs using BigQuery and SQL.

> [!TIP]
> Protip
> You can also use AI solutions like ChatGPT to figure out queries for specific questions by giving it the log table schema and specifying that you are working with BigQuery.

#### Prerequisites

At this point, you should have accomplished the following:

- Enabling log delivery in the Sanity project management console
- Connecting your Google Cloud Storage (GCS) bucket, and verifying that logs are being delivered as expected
- Loading your log data into GCS BigQuery, so it’s ready for querying

You will also need the appropriate user privileges to query BigQuery in the Google Cloud Platform.

> [!WARNING]
> Gotcha
> Caution: BigQuery can get expensive when querying large datasets as they have a pay-per-usage model by default. Before running queries on this platform, understand the BigQuery pricing model and how your query will impact cost.

#### Example 1: Which asset is downloaded the most?

Sanity projects are metered on bandwidth usage. A large part of bandwidth usage can come from image and video downloads. Use this BigQuery query to understand which asset is using the most bandwidth.

```mysql
/* Replace [PROJECT], [DATASET], and [TABLE_NAME] with your details */

SELECT body.url, sum(body.responseSize) / 1000 / 1000 AS responseMBs
FROM `[PROJECT].[DATASET].[TABLE_NAME]`
WHERE  attributes.sanity.domain = 'cdn' 
  AND timestamp > TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL -1 DAY)
GROUP BY 1
ORDER BY 2 DESC
LIMIT 10;
```

You can use this information to search your Sanity dataset for the documents using this asset and then optimize to reduce bandwidth.

#### Example 2: What is the average response time for a GROQ query?

```mysql
/* Replace [PROJECT], [DATASET], and [TABLE_NAME] with your details */

SELECT 
  DATE(timestamp) AS date,
  body.method,
  attributes.sanity.groqQueryIdentifier AS groq_query_identifier,
  COUNT(*) as times_called,
  AVG(body.duration) / 1000 AS average_response_time_seconds
FROM 
  `[PROJECT].[DATASET].[TABLE_NAME]`
WHERE 
  body.duration IS NOT NULL
  AND attributes.sanity.groqQueryIdentifier IS NOT NULL
  AND attributes.sanity.groqQueryIdentifier != ""
  AND body.method = "GET"
  AND attributes.sanity.endpoint = "query"
GROUP BY 
  1,2,3
ORDER BY 
  1 DESC,5 DESC,4 DESC
```

Note we cannot create a GROQ query identifier if the query is in a POST body.

#### Example 3: How many requests return user or server errors?

```mysql
/* Replace [PROJECT], [DATASET], and [TABLE_NAME] with your details */

WITH ErrorCount AS (
  SELECT 
    DATE(timestamp) AS date,
    COUNTIF(body.status >= 500) AS server_error_count,
    COUNTIF(body.status >= 400 AND body.status < 500) AS user_error_count,
    COUNT(*) AS total_requests
  FROM 
  `[PROJECT].[DATASET].[TABLE_NAME]`
  WHERE 
    body.status IS NOT NULL
  GROUP BY 
    date
)

SELECT 
  date,
  server_error_count,
  user_error_count,
  total_requests,
  ROUND((server_error_count + user_error_count) / total_requests * 100, 2) AS error_percentage
FROM 
  ErrorCount
ORDER BY 
	  date;
```

#### Example 4: Analyse dataset usage

```sql
/* Replace [PROJECT], [DATASET], and [TABLE_NAME] with your details */

SELECT 
  attributes.sanity.dataset AS dataset_name,
  COUNT(DISTINCT attributes.sanity.groqQueryIdentifier) AS unique_get_queries,
  COUNT(*) AS total_requests,
  SUM(body.responseSize) AS total_response_size
FROM 
  `[PROJECT].[DATASET].[TABLE_NAME]`
WHERE 
  attributes.sanity.dataset IS NOT NULL
GROUP BY 
  dataset_name
ORDER BY 
  total_requests DESC;
```

### Technical details for log delivery

#### Delivery

- **Process: **Logs are delivered to the customer's GCP Cloud Storage bucket in batches contained in compressed NDJSON files.
- **Data Window:** Each file will contain 10K lines of data or 5 minutes worth of data.

#### Guarantees

- **Delivery Assurance**: Logs are guaranteed to be delivered **at least once.** This means that the customer must perform deduplication processes if exact data is required.
- **Consumer Responsibility**: Customers are responsible for deduplication of logs if necessary.

#### Retries

- **Retry Mechanism**: In case of inaccessible customer storage, Sanity will attempt multiple retries with exponential back-off.
- **Retry Limit**: There's a cut-off time after which retries stop. Currently, five attempts will be made in approximately 70 seconds before the service will cease further attempts.
- **Consequences of Failure**: Persistent failure in file transfers will lead to disabling of the integration, requiring customers to reconfigure it.

#### Security

Customers have full control of the data and the security of their systems; the solution has multiple levels of security;

- Customers must allow Sanity access to their GCP environment by giving write access to a Sanity-owned Google service account.
- Sanity will deliver files from a static IP listed in this [file](https://www.sanity.io/files/request-log-delivery-ips.txt). Customers with greater security needs, for example, buckets behind a VPN, should be given a link to this file.
- Customers can whitelist Sanity in their Google environment by adding our `DIRECTORY_CUSTOMER_ID` as an allowed `gcloud` organization. Sanity’s customer ID can be found in the project management area during the setup process.



# Request tags

Request tags are values assigned to API and CDN requests that can be used to filter and aggregate log data within [request logs from your Sanity Content Lake](/docs/platform-management/request-logs). The tagging can be achieved by adding the `tag` query parameter to the request URL, typically in the format:

```text
GET /data/query/<dataset>?query=<GROQ-query>&tag=<custom-defined-tag>
```

### SDK Support

[@sanity/client](https://github.com/sanity-io/client) has out-of-the-box support for tagging every API and CDN request on two levels:

5. **Globally**: Using the `requestTagPrefix` client configuration parameter
5. **Per Request**: Pass the tag option to the SDK’s Request method.

This provides a flexible method for tagging requests:

| requestTagPrefix | tag | result |
| - | - | - |
| - | landing-page | tag=landing-page |
| website | - | tag=website |
| website | landing-page | tag=website.landing-page |


### Code example

The following example will result in a query with `tag=website.landing-page`.

```jsx
const client = createClient({
  projectId: "<project>",
  dataset: "<dataset>",
  token: "",
  useCdn: false, 
  apiVersion: "2024-01-24",
  requestTagPrefix: "website" // Added to every request
});

const posts = await client.fetch('*[_type == "post"]', {
  tag: `landing-page`, // Appended to requestTagPrefix for this individual request
});
```



# Request logs data reference

This article describes the file format and data structure delivered by the [API request log feature](/docs/platform-management/request-logs).

### File Format

Files are GZIPPED in NDJSON format.

### File Content

The logs will contain detailed HTTP event information, with each line representing an individual event.

> [!TIP]
> Protip
> Filter out studioRequest from your cost analysis. 
> 
> Requests from Sanity Studio are not counted towards API or bandwidth usage and do not incur any cost, so it might be useful to remove them from any workflow that involves monitoring or prediction of your billable data consumption.

### Available data

##### Available data

| Field ID | Field name | Type | Description | Mapping |
| --- | --- | --- | --- | --- |
| insertId | Insert ID | String | Unique identifier for log entry. Used in deduplication processes. | body.insertId |
| traceId | Trace ID | String | Can appear for multiple log entries. Useful for Sanity Support. | traceId |
| spanId | Span ID | String | Can appear for multiple log entries. Useful for Sanity Support. | spanId |
| timestamp | Timestamp | String | Time of request in RFC3339 UTC format. | timestamp |
| projectId | Project ID | String | Sanity Project ID associated with the request. | attributes.sanity.projectId |
| datasetName | Dataset Name | String | Sanity Dataset associated with the request. Not all APIs require a dataset name. | attributes.sanity.dataset |
| domain  | Request Type | String | Type of request (API, APICDN, CDN). Useful for understanding API vs API CDN and asset usage. | attributes.sanity.domain |
| requestMethod | Request Method | String | HTTP Verb used (e.g., GET, POST). Useful for differentiating request types. | body.requestMethod |
| requestUrl | Full URL | String | Unaltered URL received by Sanity including query parameters. | body.requestUrl |
| groqQueryIdentifier | GROQ Query Identifier | String | Hashed version of the GROQ query string without parameters. Useful for grouping similar queries. | attributes.sanity.groqQueryIdentifier |
| apiVersion | API Version | String | API version used, formatted as v[number] or v[YYYY-MM-DD]. Not applicable to asset requests. | attributes.sanity.apiVersion |
| tags | Tags | String[] | Array of tags supplied by the caller. Useful for grouping requests by business needs. See  | attributes.sanity.tags |
| referer | Referrer | String | Referrer URL of the request, as defined in HTTP/1.1 Header Field Definitions. | body.referer |
| userAgent | User Agent | String | User agent sent by the client, optional. Example format provided. | body.userAgent |
| remoteIp | Remote IP | String | IP address (IPv4 or IPv6) of the client that issued the HTTP request. Includes port information if available. | body.remoteIp |
| studioRequest | Is Studio Request | Boolean | Indicates if the request was sent from Sanity Studio. | attributes.sanity.studioRequest |
| returnStatus | Return Status | Integer | Response code indicating the status of the response (e.g., 200, 404). | body.responseStatus |
| requestSize | Request Size | String | Size of the HTTP request message in bytes, in int64 format. | body.requestSize |
| responseSize | Response Size | String | Size of the HTTP response message sent back to the client in bytes, in int64 format. Used for metering bandwidth | body.responseSize |
| duration | Response Time | Decimal | Number of millisecond between request and response within the service. Useful for performance analysis. Does not account for network latency. | body.duration |
| endpoint | Endpoint | String | The endpoint used in the request, e.g. graphql is used for GraphQL calls while query or mutate are GROQ calls. | attributes.sanity.endpoint |


### Example output

```json
{
	"timestamp": "2024-01-03T13:36:56.87202961Z",
	"traceId": "b48b918db42f0f0786702fa3ef7f6451"
	"spanId": "be245ae33db3cdaf"
	"severityText": "info", // info = <400, warn = <500, error = >500
	"severityNumber": 9, // info = 9, warn = 13, error = 17
	"body": {
    "duration": 32,
		"insertId": "asdf93n03nasdf",
		"method": "GET",
		"referer": "",
    "remoteIp": "34.79.228.45",
    "requestSize": "421",
    "responseSize": "936",
    "status": 200,
    "url": "https://0ekpuoxg.apicdn.sanity.io/v2022-09-01/data/query/cache-delay?query=%0A%2A%5B_id+%3D%3D+%22cache-delay%22%5D%5B0%5D%7B%0A++++%22timestampUnixMs%22%3A+dateTime%28_updatedAt%29+-+dateTime%28%221970-01-01T00%3A00%3A00Z%22%29%2C%0A++++%22counter%22%3A+counter%0A%7D%0A",
    "userAgent": "python-requests/2.21.0"
	},
	"resource": {
		"service": {
			"name": "Sanity.io",
		},
		"sanity": {
			"type": "http_request",
			"version": "0.0.1",
		},
	}
	"attributes": { // information extracted/parsed from the glb log
		"sanity": {
			"projectId": "exx11uqh",
		  "dataset": "webhook-test",
		  "domain": "api",
			"endpoint": "query",
			"groqQueryIdentifier": "somehash",
			"apiVersion": "2022-09-01",
			"tags": [],
			"studioRequest": false
		}
	}
}

```



## File delivery for projects on enterprise plans

We key the object using Hive partitioning with the following format:

```
gs://event-logs/project_id=[string]/kind=[event-type:string]/dt=[date:DATE]/[file-name:string].ndjson.gz
```

This allows data to be loaded into various platforms with the project ID, data type, and date used as partitioning properties.



# User Guides

#### The Sanity Applications

[Dashboard](/docs/dashboard/dashboard-introduction)

[Media Library](/docs/media-library/interface)

[Canvas](/docs/canvas/canvas-user-guide)



#### Studio Fundamentals

[Tasks for Sanity Studio](/docs/studio/tasks)

[Comments for Sanity Studio](/docs/studio/comments)

[Content releases](/docs/user-guides/content-releases)

[Compare document versions](/docs/studio/compare-document-versions)

[Copy and paste fields](/docs/user-guides/field-copy-and-paste)





# Meet the Dashboard

> [!TIP]
> Find your dashboard
> To find your organization dashboard, visit www.sanity.io/welcome!

## Dashboard at a glance

The Sanity Dashboard is the central hub for your organization's content operations. Here you'll find your deployed [studios](/docs/studio), [custom apps](/docs/app-sdk), and official Sanity apps like [Canvas](undefined) and [Media Library](undefined).

![Shows the dashboard with the home screen active](https://cdn.sanity.io/images/3do82whm/next/acb30da7d5120de7fa8c38ad8fd71235098d76b2-1256x965.png)

Your dashboard is centered around your organization, and gives access to all deployed studios and apps within the organization, across projects and datasets.

> [!NOTE]
> What about the dashboard plugin?
> As the keen reader may have observed, there is already a "dashboard" in the Sanity ecosystem, namely the official dashboard plugin for Sanity Studio. This plugin will continue to be available for your intra-studio dashboard needs.

## Touring the dashboard interface

Your dashboard consists of the main area, which will adapt to the app you are currently working in, with sidebars on either side. We'll further separate each sidebar into their top and bottom groups, as shown in the image below.

![Shows the dashboard with annotations from 1 to 5, where 1 is the main content area, and 2-5 are the sidebar sections listed clockwise from top left](https://cdn.sanity.io/images/3do82whm/next/23295ac6edc7a394b6ec9520b2b93eea6b9cf43f-2560x1984.png)

### 1. Main content area

Unsurprisingly, the main content area takes up the lion's share of the screen real estate. This is where you'll be spending most of your time, after all. The main content area adapts to whatever app you're currently working in. Shown below is a studio active in the dashboard. 

![Shows the dashboard with an example studio active in the main area](https://cdn.sanity.io/images/3do82whm/next/d98acefd1bca5e4dd53d7db7ac1cabd93e8845e8-1253x965.png)

If there is no active app, this area defaults to show you links to your most likely destinations, as well as some nifty insights about your content.

![Shows information about the content from different studios connected to the organization](https://cdn.sanity.io/images/3do82whm/next/5546fe009621d2f7673814f706a00698033b18c3-931x504.png)

### 2. Left sidebar – top

The top section in the left sidebar is where you'll switch between the apps and studios connected to your organization, as well as a menu to switch between different organizations if you belong to more than one.

![a screenshot of the organization switcher dashboard home canvas media library custom application studio and another studioShows icons with labels: Organization Switcher, Dashboard Home, Canvas, Media Library, Custom App, Studio, Another Studio](https://cdn.sanity.io/images/3do82whm/next/3e42e6f4b802b94bd123831ba6bb33b879027c26-638x463.png)

#### Organization Switcher

Click to bring up a list of the organizations available to you.

#### Home

Loads the dashboard home page

#### Canvas & Media Library

Grouped right below the Home button you'll find the official Sanity apps available to your organization, such as [Canvas](/docs/canvas) and [Media Library](/docs/media-library).

#### Studios and custom apps

Below the official Sanity apps, you'll find any [custom apps](/docs/app-sdk) and [studios](/docs/studio) deployed by your organization. If you can't find an app or studio you expected to see here, your studio maintainer might have to do a little [configuration](/docs/dashboard/dashboard-configure) first.

### 3. Left sidebar - bottom

In the bottom left sidebar section you'll find a list of all your deployed studios and apps, as well as options to manage your organization and its members. 

![a screenshot of the application and studios organization settings page](https://cdn.sanity.io/images/3do82whm/next/a9bca236f75172963a257f829d82d17baabcc021-424x158.png)

#### Applications and studios

Selecting this option will load a list of, well, applications and studios, into the main content area. You'll also find some convenients links that'll help you add any missing destinations, or kickstart your own custom application.

![a screenshot of the studios and applications page](https://cdn.sanity.io/images/3do82whm/next/764ae5f9c9e3862cced2bfef1dca651771a1f23b-1102x833.png)

If you want to remove an app or a studio from the sidebar, you can do so by togggling the pin icon to the left of each listing's title, as shown above.



#### Organization settings

![a button that says invite members and organization settings](https://cdn.sanity.io/images/3do82whm/next/eb4d25c6dcb39736708467164dcb95e0ca05bb6a-412x171.png)

This menu option will reveal outbound links to manage your organization and its members.



### 4. Right sidebar – bottom

This is where you'll find some helpful links, as well as an option to submit feedback about your experience.

![a help and feedback button with a speech bubble](https://cdn.sanity.io/images/3do82whm/next/7cb5a5dc546de9a1ba8e2097f411558fc4fc2d59-247x140.png)

#### Help

Clicking the question mark icon will bring up a contextual menu with some fairly straight-forward options. Hope to see you in our community!

![a screenshot of a website that says join the community](https://cdn.sanity.io/images/3do82whm/next/47fbef225e231fc20cde3af0780b7121559a427e-216x264.png)

#### Feedback

Have something to say? We'd love to hear it! 

![Shows a popover feedback form with a send button](https://cdn.sanity.io/images/3do82whm/next/0f6cd3ca05c30d780bfec487a6353d423805516d-616x211.png)

### 5. Right sidebar – top

If the bottom left section was all about the organization, top right is all about you! Here you'll find a link to your account settings, as well as an option to create favorite bookmarks for destinations you visit frequently.

![Shows the account settings popover with a publically displayed email that no one in their right mind would ever abuse](https://cdn.sanity.io/images/3do82whm/next/917b88a4df826c9af84349721bdc9d5e25657df1-453x252.png)









# Introduction

## Canvas at a glance

Sanity Canvas is a powerful writing tool that combines the best of human creativity and AI assistance to help you craft compelling, well-informed content. 

At its core, Sanity Canvas is designed around two key principles:

4. **Context is king**: Attach relevant notes, research, and guidelines to your document. Canvas ensures that your writing is always grounded in the specific context of your project. 
[Read more about notes](#k051cf71039bf)
4. **AI as collaborator**: Canvas' built-in AI is more than just a text generator. It's an intelligent co-author that leverages the power of large language models while staying anchored in the context you provide. 
[Read more about the embedded AI](#k8cde9e5037d7)

Whether you're drafting a blog post, crafting product copy, or putting together a comprehensive report, Canvas empowers you to produce your best work more efficiently. 

## Feature walk-through video 

![Video](https://stream.mux.com/vCtyUNV00Et6kHsOOhqLfn29NgBOTF5rK)

To get started you can watch a short video introducing the most important features, or get into the documentation by [taking a tour of the Canvas interface](#8552fa172bc7), or keep on reading to get your hands dirty straight away with a practical example.

[Canvas – This is how we write now](https://sanity.io/create)

[Content Mapping - Introduction](/docs/canvas/canvas-content-mapping)

[Content mapping - Setup and configuration](/docs/canvas/configure-content-mapping)



## Quickstart Example – Drafting a blog post

In this follow-along example you'll be writing a blog post with some select travel advice for spud aficionados. 

- Go to the Canvas app in your organization dashboard and create a new document ([Help](#k5d8dafd1d613)). Title it *"The best travel destinations for potato lovers"*.
- Start off by making a note. Notes are where you keep all your research, contextual information and other supporting material. In the notes panel on the left side, find the button labeled **Create your first note** and click it.

![The Canvas interface with the Notes panel open](https://cdn.sanity.io/images/3do82whm/next/41eb2a8e6226dac65d16692c6ec0ea0f2e541fd5-5348x1858.png)

- To start off with some solid information, paste this list of potato museums around the world into your note. The AI assistant will draw from your notes when making suggestions.

```markdown
# List of potato museums

## Austria 

Waldviertler Erdäpfelwelt (Waldviertler potato world) is a museum with interactive displays located in the town hall of Schweiggers displaying the history and uses of potatoes to the present day.

## Belgium 

Musée vivant de la pomme de terre ("Living Museum of the Potato") in Genappe is part of the Wallonia Botanical Gardens and also houses a collection of onions from northern Europe.

Frietmuseum in Bruges is dedicated to chips (or fries in American-English) and is located in one of Bruges' oldest buildings, dated 

## Canada

The Canadian Potato Museum in O'Leary, Prince Edward Island, claims to contain the world's largest collection of potato artifacts. It is also home to a Potato Hall of Fame. A 14-foot (4.3 m) high giant potato made of fiberglass stands at the entrance and visitors can learn about the origins of the wild potato up to modern-day agricultural practices.
Potato World is a museum dedicated to the potato. It is located in Florenceville-Bristol,New Brunswick, known as the french fry capital of the world.

## Denmark 

Danmarks Kartoffelmuseum ("Danish Potato Museum") in Otterup is part of the Hofmansgave estate. The Hofmansgaves were responsible for popularising the potato in Denmark where potatoes were known as "German lumps".

## France 

Moulin Gentrey in Harsault (fr) is a former starch mill dating from 1870 which contains a small potato museum as part of a historical tour of starch making for the textile industry.

## Germany 

Deutsches Kartoffelmuseum ("German Potato Museum") in Fußgönheim (de) is housed in a former synagogue next to the Fußgönheim Agricultural Museum. The museum dates from 1987.Das Kartoffelmuseum ("The Potato Museum") in Munich dates from 1996 and is run by the Otto Eckart Foundation on behalf of Pfanni GmbH, a division of Unilever.Vorpommersches Kartoffelmuseum ("Potato Museum of Vorpommern") in Tribsees (de)

## Italy 

Museo della patata ("Potato Museum") in Budrio

## Lithuania

 There is a Bulvės muziejus ("Potato Museum") in Kudirkos Naumiestis, near the border with Kaliningrad.

## United States 

Potato Museum in Albuquerque, New Mexico, was originally housed in the basement of E. Thomas and Meredith Hughes' basement in Washington, D.C., before moving to Albuquerque in 1993. It began in 1975 and is a nonprofit organization.Idaho Potato Museum is in Blackfoot, Idaho, and among the exhibits has the world's largest potato chip (crisp in British-English), a Pringle measuring 25 inches (64 cm)
```

- Doing so, you'll notice that the Canvas AI has helpfully named our note and classified it as a **Fact.** If your result was different, bring up the note category menu and change it to **Fact**.

![Shows the note category menu opened](https://cdn.sanity.io/images/3do82whm/next/d90d6f5af6d37239e981fed52ee19c1e1052774b-5348x1858.png)

- Before moving on to the document editor to do some writing, create one more note with some context for the task at hand. Once again, click **Create note** in the side panel on the left, and paste the following into it:

> `We are writing a blog post featuring exciting destinations for potato-aficionados.`

![Shows an instruction block in Sanity Canvas](https://cdn.sanity.io/images/3do82whm/next/7a3b4605f984b3c65654a59917f6099cd5b3e69f-5348x1858.png)

This will inform the AI assistant about your goal and intention, which in turn will enable it to make better suggestions. Make sure this new note is classified as **Context**.

- With your notes ready, move your focus to the document editor. This huge blank space is sort of intimidating. Time to ask your AI assistant to help out. - Enter the following: "Write an outline for a blog post featuring 3 of the best travelling destinations for potato lovers". 
- With your cursor positioned at the end of the sentence, hit CMD-ENTER (CTRL-ENTER on Windows). This will turn the sentence into an instruction for the writing



![Shows an instruction block in Sanity Create](https://cdn.sanity.io/images/3do82whm/next/9241f89163e6b8a940eb7123a16977c31fe8d7a8-1528x744.png)

> [!TIP]
> Protip
> You can also create a new instruction by hitting the slash key / on an empty line. Slash commands are discussed in greater details further on in this article!

- The assistant, now primed with both the context of the task at hand and a source of information to draw from should be able to do a pretty decent first draft. From here, you can accept the incoming changes or fiddle around with the instruction to further hone the output.

![Shows the AI assistant's suggestion](https://cdn.sanity.io/images/3do82whm/next/7d234f112c4e9179a4912db5782d0bf96069861b-5348x3516.png)

- After accepting or discarding what the assistant provided, place your cursor at the end of a paragraph. Notice the subtle circle icon that follows you around? This is the AI contextual menu, affectionately known as "the Blip". Give it a click.

![a list of resources includes links to museums and attractions](https://cdn.sanity.io/images/3do82whm/next/b5760132cf9354471c70320592177b3e0d13328e-701x437.png)

- You'll be presented with a few options for interacting with the AI capabilities of Canvas. - **Ghostwrite** will tell the assistant to just pick a direction and go, letting you press TAB to tell it to keep writing after each sentence.
- **Show me options** will instruct the assistant to suggest a few different ways to proceed.
- **Rewrite paragraph** will tell the assistant to do another take on the current paragraph.



![Shows the Blip in "Show me options"-mode](https://cdn.sanity.io/images/3do82whm/next/4eeb70b58c8a746c58d2d52f188e53c74324f518-2124x962.png)



#### Next steps: 

- Experiment with notes: - Try to add a **Style** note to influence the tone of the blog post.
- Find a great piece of prose and add it as **Inspiration.**
- [Read more about notes](#b3c8c19dad0e)


- Keep exploring the AI capabilities:- Ask the AI to give you some different options on how to proceed.
- Use `CMD + .` (command period) to start the AI ghostwriter. Hit tab to continue once it pauses.
- [Read more about AI ](#8cde9e5037d7)


- Read the article on [Content Mapping](/docs/canvas/canvas-content-mapping) to learn about linking your Canvas documents to your [Sanity Studio](https://www.sanity.io/studio).
- Read on to learn more about Sanity Canvas.

## Touring the Canvas interface

### Documents

Documents are the core conceptual unit of Canvas. The document editor is where your work is done, providing a familiar, minimalist writing environment for you to craft your content, while the document sidebar is where you'll find all your existing notes, as well as notes shared with you by others and templates for new documents. Here's what you need to know about working with documents in Canvas:

#### Finding your documents

You can browse through all your existing documents with some helpful filters by opening the left sidebar. This is also where you'll find any templates created by you or other people in your organization. Templates are helpful starting points for new documents.

![The Canvas document list showing a list of all documents](https://cdn.sanity.io/images/3do82whm/next/d244d37deac2ff38b8e913cbdb9cc706dc80348d-5348x3516.png)

#### Creating a new document

You can quickly create a new document from either the sidebar or from within the document browser.

![Shows the menu option for creating a new document](https://cdn.sanity.io/images/3do82whm/next/7a3e9d544ed494944b75d5145d1f1033d0915bde-5348x1858.png)

#### Deleting documents

If you need to remove a document, you can do so from within the document editor view. Click the ellipsis menu in the top right corner, and then click delete. Note that this action is permanent, so be sure you truly want to delete the document before confirming.

![Shows the menu option for deleting the selected document](https://cdn.sanity.io/images/3do82whm/next/16487db1c5d09844941503d412fcffe26e75e7b6-5348x1858.png)

### The document editor

Canvas offers a clean, distraction-free writing environment that will feel instantly familiar to anyone who has used a modern word processor or text editor. The interface is designed to put your content front and center, allowing you to focus on getting your thoughts down without any clutter or unnecessary features getting in the way.

![Shows the document editor in distraction free mode](https://cdn.sanity.io/images/3do82whm/next/4a1ecc05023f7cc794c4049650b8b46fdbbe998e-5348x3516.png)

#### Slash commands and Markdown formatting

The writing space in Canvas supports a range of formatting options to help you structure and style your content. You can use familiar slash commands to quickly apply headings, lists, quotes, and more without taking your hands off the keyboard.

![Shows a contextual menu invoked by typing "/". It has options for different headings, blockquotes, and lists. Additionally it has an option labeled "Instruction"](https://cdn.sanity.io/images/3do82whm/next/01bd6f189f361321c828cfb9f307531e4d90a3c7-1528x996.png)

In addition to slash commands, Canvas also supports a subset of standard [Markdown](https://en.wikipedia.org/wiki/Markdown) syntax for formatting text. You can use hyphens or numbers to make lists, and hash symbols to denote headings. For those who prefer a more visual approach, basic formatting options like bold, italic, and underline are also available via buttons in a popover whenever text is selected.

![a text box that says notes are a key feature in sanity create that provide context facts style](https://cdn.sanity.io/images/3do82whm/next/5d65ef45e697d5fc4c7b34f28b929c6c6896c6ed-3246x930.png)

Whether you prefer slash commands, Markdown, or the toolbar, Canvas aims to make formatting your document quick, intuitive, and distraction-free. The goal is to let you focus on your writing while still having easy access to the tools you need to make your content clear, scannable, and visually appealing.

## Notes

Every writer knows that behind every polished piece of prose lies a mountain of notes, research, and insights that inform and guide the work, but never see the light of day in the final draft. Canvas is a writing tool that not only acknowledges, but celebrates and elevates this often unseen but essential part of the process.

![](https://cdn.sanity.io/images/3do82whm/next/570499883691e46fd4332154e5159a10da67259f-2800x1032.png)

Notes are a key feature in Canvas that provide context, facts, style guidelines, and inspiration to inform your writing and enable the built-in AI co-writer to make relevant and informed suggestions. By attaching relevant notes to your document, you give the AI the background knowledge and topical awareness it needs to be of actual help.

#### Show or hide the notes side panel

Your notes live in the side panel to the left of your main content editing area. If you don't see the side panel it might have been closed when someone needed to unclutter a bit. If this is the case you can open the side panel by clicking the button labeled **Notes** in the left sidebar area.

![Shows the note panel in open and closed states](https://cdn.sanity.io/images/3do82whm/next/cc8284912090fc99a1f4ce3988be7f98d902f7fa-5348x3516.png)

To resize the side panel, or hide it altogether, grab the divider between the sidebar and main editor area and drag it all the way to the left to hide the side panel, or resize it to your liking. Clicking the divider will also toggle between hiding or showing the side bar.

![Shows how hovering the divider makes it draggable](https://cdn.sanity.io/images/3do82whm/next/9b02f3bfd631b699ee87645128e8a0f671e626aa-5348x3516.png)

### Creating notes

To create a note, click the button with a **+** icon at the very top of the notes panel. You might proceed give your note a title and select a category, or just start writing. You can add content to your note in the form of text, images, URLs, or even PDF files. Canvas will do its best to determine the type of note (as will be discussed in the next section) and generate a suitable title. You can, of course, override the AI's suggestions in either case.

![Shows the note sidebar and the button to add a new note](https://cdn.sanity.io/images/3do82whm/next/bd8c46f28e5dbe77ad729424370f1325adeeecbd-537x213.png)

#### Different notes for different goals

Canvas offers four distinct types of note, each serving a different purpose:

- **Context Notes**: Provide high-level background information and framing for the document, such as project briefs, target audience details, or internal enablement material.
- **Fact Notes**: Contain specific data points, quotes, or pieces of information that should be treated as factual and incorporated into the content where relevant.
- **Style Notes**: Outline the desired voice, tone, and stylistic guidelines for the document, ensuring the ghostwriter adopts the appropriate tone and style for the piece.
- **Inspiration Notes**: Collect examples, analogies, or creative prompts to inspire the writing and infuse it with engaging elements.The AI assistant uses these notes to gain a deep understanding of the project at hand and tailor its suggestions accordingly.

### How notes inform the AI assistant

When you provide notes, the AI uses this information to guide its content generation. Context notes help the AI understand the big picture and overall purpose of the document. Fact notes ensure accuracy by providing specific data points to incorporate. Style notes allow the AI to adopt the appropriate voice and tone for the piece. And inspiration notes give the AI creative fodder to draw from, helping to make the writing more engaging and colorful.

> [!TIP]
> Protip
> When the AI receives contradictory information in your notes, the note that is located the highest in the side panel is the one that takes presedence. Top notes beat bottom notes.

In essence, notes allow you to have a "conversation" with the ghostwriter, providing it with the knowledge and direction it needs to be a truly useful writing assistant. The more relevant and specific the notes you provide, the better the ghostwriter can tailor its output to your needs. So take the time to curate your notes carefully - it will pay off in the quality of the content Canvas helps you produce.

![Shows a sidebar with a bunch of notes of every type](https://cdn.sanity.io/images/3do82whm/next/c1eff35473b62e248b780744f31d1738e278b272-2346x1858.png)

### Note actions

#### Moving, renaming, duplicating, and deleting notes

**Move** and re-arrange notes by dragging them directly in the notes panel. Right clicking any note in the list will open a contextual menu that will reveal additional options, including those to either **duplicate** or **delete **a specific note.

Once a note has been selected, **rename** it by clicking the title or alternatively do this in each note's dedicated context menu, located in the top right of each selected note.

## AI ghostwriter

Canvas' built-in AI writing assistant is more than just a text generator. It's an informed collaborator that leverages the power of Large Language Models (LLMs) while staying grounded in the specific context of your project. By feeding the AI relevant notes, research, and guidelines, you transform it from a mere automaton into an insightful co-author capable of producing content that aligns with your unique needs and objectives.

What sets the Canvas AI apart is its ability to understand and utilize the notes you provide. Whether it's high-level context about the project's goals, specific facts to incorporate, stylistic guidelines to follow, or creative inspiration to draw from, the AI takes it all into account when generating suggestions. This means the content it produces is not only fluent and coherent but also relevant and tailored to your project.

Think of the AI as your brainstorming partner. It can help you flesh out ideas, expand on sections, and even polish your writing to ensure it hits the right tone and style. But it's not a replacement for your own insights and expertise. The key is to use the AI as a tool to enhance and accelerate your writing process, not to fully automate it.

By combining the efficiency of AI with the directed context of your notes, Canvas empowers you to craft content that is both deeply informed and compellingly written. It's a new way of working that puts you in control while harnessing the power of artificial intelligence to boost your productivity and creativity as a writer.

### Capabilities

- Generating new content based on your notes and existing text
- Expanding on ideas and fleshing out sections
- Refining and polishing your writing
- Adapting to the style and tone specified in your notes
- Incorporating facts and inspiration from your notes

### Meet the Blip – How to use the AI  assistant

The AI assistant in Canvas can be interacted with in a number of ways, but the most readily apparent is the subtle circle icon that follows you around the document, affectionally known as "the Blip".

![Shows the AI contextual menu in its inactive and active state](https://cdn.sanity.io/images/3do82whm/next/b5760132cf9354471c70320592177b3e0d13328e-701x437.png)

Clicking the Blip will open a menu of different instructions to run in the context of your current position in the document.

#### Ghostwrite

The **Ghostwrite** option is your go-to for generating new content or expanding on existing ideas. When you select this option, the AI assistant will analyze your current position in the document, along with any relevant notes and surrounding context, to suggest a continuation of your writing.Depending on where your cursor is placed, the AI may suggest completing the current sentence, starting a new paragraph, or even beginning a new section with a relevant heading. The goal is to provide a seamless and contextually appropriate continuation that flows naturally from your existing content.

#### Show options

**Show options** presents you with a range of alternative suggestions for how to continue your writing. When you click this option, the AI will generate multiple possible paths forward based on your current context and notes.These options might include different ways to complete the current thought, introduce a new idea, or transition to a related topic. By offering a variety of suggestions, the AI allows you to explore different creative directions and choose the one that best fits your vision.

![Shows the AI contextual menu with suggestions on how to proceed](https://cdn.sanity.io/images/3do82whm/next/4eeb70b58c8a746c58d2d52f188e53c74324f518-2124x962.png)

#### Rewrite paragraph

This option will instruct the assistant to do another take on the current paragraph. You will be given the opportunity to give the assistant a short brief on what you'd like.

![Shows an instruction element with the resulting output under](https://cdn.sanity.io/images/3do82whm/next/f93b238a0b0fdb9b85dd894a34d86f1cbd58ec2b-727x741.png)

### It's bots all the way down –  AI assistant in notes 

![Shows the AI contextual menu in a note](https://cdn.sanity.io/images/3do82whm/next/47770fc2cbad4f41869226cccb1ede7c42b06ea9-540x342.png)

In addition to using the AI co-writer to help write your main document, you can also invoke it within individual notes for more targeted assistance.

You can select any text and hit CMD-Return to run the selected text as an instruction, or you can locate the **Blip**-button to access the same menu of options discussed in the previous section.

Some examples of instructions you could run inside a note:

- Summarize the key points of the note
- Identify any potential contradictions or inconsistencies
- Suggest additional facts or examples to include
- Rephrase the note in a different tone or style
- Answer a specific question based on the note's content

Once you submit your prompt, the ghostwriter will process the note and provide a response directly within the note editor. You can then choose to incorporate the ghostwriter's suggestions, modify them, or discard them as you see fit.

This in-note AI assistance can be incredibly useful when you need help refining or expanding on a particular piece of context without disrupting your main writing flow.


![Shows an AI instruction inside a note](https://cdn.sanity.io/images/3do82whm/next/a770bf0764658791dccee7149e14eaef05aef1a0-546x809.png)

#### Best practices

- The more relevant and specific your notes, the better the AI can tailor its output to your needs
- Break down complex topics into smaller sections and use the AI to help flesh out each part
- Don't rely solely on the AI - review, edit, and add your own insights and perspective
- Experiment with different prompts and note combinations to find what works best for your writing style and goals







# Content mapping

> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

Canvas is a great tool for freestyle writing, but when it's time to put your creative output to work, you'll want to move everything into a structured environment where it can enjoy all of the benefits of treating content as data—Sanity Studio! 

> [!WARNING]
> Gotcha
> Some initial setup by a studio maintainer is needed to make content mapping work. Visit the article on configuring content mapping to learn more.

For example, you might sketch out a blog post in Canvas, and then connect your work to a new document in Sanity Studio of a specific content type—like a `blogPost`, with fields like `title`, `excerpt`, `body`, and `tags`. 

A pretty clever mapping agent—from here on lovingly referred to as "the bot"—will go to work in the background identifying which parts of your rich content in Canvas corresponds to which document fields in your studio and automatically mapping content appropriately. Subject to your overrides, of course. 

![A side by side view of Canvas and the Studio form](https://cdn.sanity.io/images/3do82whm/next/12a7a0278863030e0037ae130977b58feaf2ae27-5348x3516.png)

You also have the option of marking certain parts of your document as **context**, to make the bot ignore your "notes to self" and other non-content. You can even include little helpful pointers to the bot, like:  `// slug: my-cool-post` or `!! title below`. The bot will try to infer meaning and decide what is content and what is context. Anything it gets wrong, you can fix!

## Get started

### Locate your project in the studio panel

- In Canvas, look for the button in the top right corner labeled **Studio **or, on smaller screens, with **an icon resembling three boxes arranged in a diagram** (a schema!).

![A side by side view of Studio buttons](https://cdn.sanity.io/images/3do82whm/next/ea28be77f21b02a45d72e5d7642c856326596f16-602x335.png)

- Click the **Studio** button to reveal the **Studio** panel:

![The Studio panel in Canvas](https://cdn.sanity.io/images/3do82whm/next/86ebdb07bb1ab73bd890e6654b05342c1745ba0c-1276x1023.png)

- Find your project in the **Studio** menu. If not automatically selected for you, find your studio deployment and workspace in the appropriate dropdowns. Then, find the document type you want to map your content to. 

> [!WARNING]
> Gotcha
> Can't see any projects in the dropdown? You may have to contact the person or people responsible for maintaining the studio and ask them to enable content mapping in Sanity Studio.

![Shows the studio link panel, now populated with the appropriate details](https://cdn.sanity.io/images/3do82whm/next/a3394db81990857e917df2e629caa7eb26f2284e-963x1023.png)

### Select and apply a document type 

- With your document type selected, click the button labeled **Connect and start mapping ->** to proceed.
- The link panel will change to show a "minimap" of the selected content type, with its fields laid out in a tree structure. Fields with a little arrow on their left can be clicked to expand and reveal their values, or drill down deeper into nested fields.

![A mapped document with the Studio panel open](https://cdn.sanity.io/images/3do82whm/next/db2f76b43ac5950a1f1613979bf103893ed50747-2674x1758.png)

Note also the bottom right status indicator, which shows the mapping agent already making progress. It will keep working in the background, intelligently mapping your content to corresponding fields. 

## Exploring the Link Panel further

As your content is mapped, you'll see the minimap tree of document fields starting to fill out with content. You might also notice the colors changing as the mapping agent finishes with a field.

![A mapped document with the Studio panel open, with all fields unfurled](https://cdn.sanity.io/images/3do82whm/next/5c2ad5fd6ae5911c1c59a92c4e9c7c3f5935853e-2674x1758.png)

### Using colors to discern mapping state

![Green – Automatically mapped. Gray – Treated as context. Yellow – Manually mapped.  Black / white – Not yet analyzed.](https://cdn.sanity.io/images/3do82whm/next/c165edd6bd1216f399d6a998f6377747e4b3e64b-2800x1078.png)

As the content is mapped, you'll notice your screen getting progressively more colorful. Content that was automatically mapped to a field will be tinted **green**, as will the corresponding field in the link panel minimap, while anything the bot has decided is **context** will get a light gray color.

The **yellow**-colored field in the screenshot above indicates a field where an editor has actively overruled the suggested mapping and manually linked a bit of content to a field, while text in **black** or **white** (depending on whether or not the dark mode is active) indicates content that the mapping agent hasn't yet analyzed.

### Adjusting the results

While the automatic mapping is quite good (really!), you may at times want to manually adjust how your content in Canvas matches up with your studio schema. The tools you need to make these changes are at your fingertips.

- To map a content block, like a paragraph or an image, to a specific field, click on the item to reveal its context menu, and find the option to **Map to field... **as shown below.

![](https://cdn.sanity.io/images/3do82whm/next/dd704093b9fbf68888b1055b7cb34bf84a6bdc47-2800x1800.png)

- Selecting **Map to field...** will cause the interface to direct focus to the link panel, where you can select an appropriate field to map your selected content to. In the example below, we mapped the first image in the document to the **Cover Image > Asset** field. User-defined mappings are shown in yellow, instead of green for auto-mapped or gray for context.

![The studio panel when custom field mapping mode is enabled](https://cdn.sanity.io/images/3do82whm/next/73d809751a129e01dd1cbc3fa354a648a5bd5352-2674x1758.png)

- Similarly, if any content is mapped incorrectly, you can unmap it by clicking the **Clear mapping** button. Note that unless you explicitly reassign it as  context, the mapping agent will try to re-map on its next pass until everything has been neatly categorized with a color.

![](https://cdn.sanity.io/images/3do82whm/next/ff639d0b26090e9b2e1e7f827ff50f424c535428-2800x1078.png)

- For more granular control, you can select specific sentences or phrases and map them individually to Studio fields.

![](https://cdn.sanity.io/images/3do82whm/next/3e64eae6f1a1cec4ebf9d4ce94c1579e232eb1d2-719x277.png)

- As also demonstrated in the previous screenshot, leaving some contextual hints for the mapping agent can be quite effective. You can read more about this in the section on [content mapping tips and tricks](#block_40).

With these tools, you can control exactly how your content in Canvas will be translated into structured data in Sanity Studio. If you haven't already, this would be a good time to link your work to a new document in your studio.

## Link your work to a new document in your studio 

> [!TIP]
> Protip
> In this article, we’ve chosen to complete the mapping work first, and then create the studio document for narrative clarity. However, you’re free to do it the other way around—choose the workflow that suits you best!

Once you're happy with your mappings, find the button labeled **+ Link to new studio document** near the top of the studio panel.

![The link to new studio document button in the studio panel](https://cdn.sanity.io/images/3do82whm/next/f4c96172a668aa54a0103e80eb041fbaa40e2f12-1328x489.png)

Clicking it should result in a visual confirmation of success, and the button label changing to **Linked document**. Click it to open the connected studio with your new document selected. 

![A view of the studio once it's been linked to Canvas](https://cdn.sanity.io/images/3do82whm/next/930f33274d3a4b467219a8f98ebdcaca8a8c7bba-2674x1758.png)

You'll notice that your new document in the studio is in a read-only state while linked to its counterpart in Canvas. 

Any further changes you make in Canvas will be synced with the studio document automatically, so you can continue refining and expanding your content without worrying about manually transferring anything.

### Unlink your document from Canvas to edit it in Sanity Studio

As mentioned, your document will appear as **read-only** in your studio while linked to Canvas. You can think of this as the Canvas document being the **source of truth** for both versions while the link persists. In order to edit your document in Sanity Studio, you need to unlink it from its source in Canvas.

When the time comes, locate the **Unlink** button in the contextual menu next to your **Publish** button to sever the connection and edit your document in the studio. 

![The document context menu showing how to unlink a document from Canvas](https://cdn.sanity.io/images/3do82whm/next/4cce0bd87287bbac6d05eecb23608c4a79b1b3e1-817x677.png)

Clicking **Unlink** will cause a dialog to appear, informing you of the consequences. Confirm to dismiss it and unlock the document for editing in the studio.

![](https://cdn.sanity.io/images/3do82whm/next/266989fae1de6484671bbb46c8104b9fa1710a98-2800x1800.png)

> [!TIP]
> Protip
> Unlinking does not delete anything! You can keep on editing your document in Sanity Create, though the changes will no longer sync to the studio version. They are no longer connected.

## Content mapping tips and tricks

- **Procedural discovery!** The bot works procedurally on one bit of content at a time, but it can and will make several passes, so it might re-visit and re-evaluate mappings as it moves through your content. You can use this to your advantage by adding content hints above content blocks to quickly make the bot reconsider its choices. 

![](https://cdn.sanity.io/images/3do82whm/next/cc35c33aab4fa16ddce5b5e0ca7eb6fb63c2c365-720x463.png)

In the example above, the image was originally judged to be part of the blog post `body` field, but remapped to the `coverImage` field after some gentle nudging.

- **Provide some context!** Mark words, lines, or whole blocks as context to make the mapping agent treat your notes as notes that should not be mapped to any field. The bot will also read you context for clues on how to treat content, so feel free to be conversational. `// slug: my-cool-slug` or `[description below]` might do wonders. 

![](https://cdn.sanity.io/images/3do82whm/next/a8c1ee0678d7b9ede0c2007dd05a3fff99ef693b-2800x1078.png)

## Troubleshooting

### Can't find your project?

Make sure your studio has been [configured properly](/docs/canvas/configure-content-mapping) to allow Canvas to connect. This involves configuring and deploying the relevant studio.

### Can't see your content type, or content type is missing some fields

Make sure the relevant types or fields aren't configured to be excluded from content mapping. This, too, involves configuring and deploying the studio in question.

### The bot is making too many mistakes when mapping content

Try leaving some contextual clues to help the bot figure out what's what. There are no hard rules when it comes to what the bot will and will not pick up on, but as a general guideline: If it would be hard for a human co-author to catch your context notes, the bot will probably not do great either. Some examples:

- Partial mapping simple values with a simple inline instruction like `slug: my-cool-slug`
- Using headings as mapping clues for blocks
- Leave a note in plain text. `Note: Use this part for description`
- ... and if all else fails, manually adjust the mapping to get it just right

### I can't seem to map the title of my Canvas document to any field

Mapping the title is currently not possible, due to vague unspecified technical limitations. We're working on it!



# Comments

![Shows a comment about to be posted](https://cdn.sanity.io/images/3do82whm/next/83eeef7375c7da21cd2c3a162ed97e5de6d3c502-352x139.png)

Comments for Sanity Studio enables effective collaboration workflows right where the work is done. Leave comments on specific document fields or even single words in Portable Text, *@mention* your colleagues, and streamline your content workflow without ever leaving the studio.

_This is a paid feature, available on the Growth plan._

[Enable Comments for Sanity Studio](/docs/studio/configuring-comments)

[Enabling Tasks for Sanity Studio](/docs/studio/configuring-tasks)



## Comments workflow

Once Comments has been enabled for your project, open any document in your studio to start exploring how they work. If someone has already left comments on any field in the document you will notice a small speech bubble icon 💬 adorning the input showing how many comments have been posted. If no comments have yet been posted, hover any field to bring up the speech bubble to leave the first!

### Leaving comments

Hover over any comment-enabled field and click on the comment icon 💬 to open a popover dialog, then type your comment in the input field and hit **Send** to post it.

![](https://cdn.sanity.io/images/3do82whm/next/f75b3ef413eb78a529ec0b52e33638e96368272a-614x171.png)

To mention a colleague, type **@** followed by their name. A list of users with access to the document will appear. Click on the user you want to mention, and they will receive an email notification.

Your comment will now be visible to others with access to the document, and any mentioned users will receive a notification by email.

![Shows a string field with an icon indicating it has 1 comment attached](https://cdn.sanity.io/images/3do82whm/next/333aac237fe95c447fa1ea002f0ead1396c31096-501x158.png)

Unlike their closely related cousin [Tasks](/docs/studio/tasks), comments are always directly coupled with a specific piece of content in your studio. Comments can be attached to any compatible field, or even to distinct sentences or words within Portable Text!

![](https://cdn.sanity.io/images/3do82whm/next/3affb600efe308c532305c0b53730f477a24d2e5-536x204.png)

Clicking the 💬 comments icon on a field, will open the comment inbox for the document so you can easily browse through existing comments. Comments are neatly grouped into the fields they correspond to.

![](https://cdn.sanity.io/images/3do82whm/next/a69088d21908c24452a3d7297c77bd81593f9c74-354x608.png)

### Resolving comments

When a comment has been addressed or is no longer relevant, you can mark it as resolved. To do this, click on the **Resolve** option in the popover menu that appears when hovering. Resolved comments will be hidden from the main view but can still be accessed in the **Resolved Comments** list.

![Comment being resolved](https://cdn.sanity.io/images/3do82whm/next/8acabde302218caf03c6d7f865c28bdfaf8e9bef-315x311.png)

### Reactions, editing, and deleting comments

In addition to resolving comments, the popover menu includes a few more options. You can leave a reaction emoji for effective communication, copy a direct link to the comment, and you have options to edit or delete your comment. These options all work as you'd expect.

![Shows options for reacting to, editing, and deleting comments](https://cdn.sanity.io/images/3do82whm/next/d11b21191a257b198bf9616caf0e514dd109e9cb-649x175.png)



# Tasks

Tasks for Sanity Studio are perfect for collaborating on content with your team, or even for solo content creators who need to keep track of their outstanding to-do’s in the same environment where the work is to be done. Assign tasks to the appropriate team member, and they will get a notification alerting them to the new item in their inbox. Keep the discussion going in dedicated comment threads for every task, and tag in those who might be missing out with *@mention*’s.

_This is a paid feature, available on the Growth plan._

[Configuring Tasks](/docs/studio/configuring-tasks)

[Comments in Sanity Studio](/docs/studio/configuring-comments)



## Working with tasks

### Find your tasks inbox

Your tasks inbox is located in the top-right corner of your studio, next to your profile picture. Here, you’ll find any new tasks assigned to you, any in-progress tasks that you’ve subscribed to, and all open tasks for the currently active document, whether or not you’ve been tagged in yet.

![Shows the tasks inbox in Sanity Studio](https://cdn.sanity.io/images/3do82whm/next/a215b192a21a60726137274df4e2e44ffccc6390-360x448.png)

### Create a task

Click the link aptly labeled **+ New task** to create a new task. You can give your task a deadline, and assign it to the appropriate person who will then receive a notification email. You can also *@mention* studio users to notify them that their input is requested.

> [!TIP]
> Protip
> Memo to self? Assigning a task to yourself, or @mentioning yourself in a task will not trigger any notifications, so talking to yourself in the studio is perfectly fine, and won’t flood your inbox. 

![Shows an un-published task with a deadline requesting a review from a colleague on the target article](https://cdn.sanity.io/images/3do82whm/next/b2cb65d7be8edd2e277891c0564c82f6510abeb0-359x642.png)

You can also choose to attach your task to a target document or leave it empty if that’s more appropriate. Adding a target document facilitates discovery and contextualizing, and will also put a handy notice next to the publish button for the relevant document, listing unfinished tasks.

### Comment on tasks

Tasks can have comment threads attached so you can keep related discussions in one easy to find place. Just as with comments elsewhere you can *@mention* your team members to let them know about discussions they should be aware of.

![Shows a comment in a task thread tagging a tam member with a @mention](https://cdn.sanity.io/images/3do82whm/next/bd10f58b6cd0d3683a2d0815aee22df7a7438601-359x254.png)

### Resolve tasks

Once dealt with, a task can be satisfactorily checked off your to-do list. Resolved tasks are still available by accessing the list of **Done** tasks at the bottom of your inbox.

![Shows a popover allowing users to mark a task as done](https://cdn.sanity.io/images/3do82whm/next/bf65a2958195f26ab55fb6dada339e3febca1850-370x217.png)



# Copy and paste for fields

The field copy and paste feature in Sanity Studio enables you to copy and paste field values or entire documents within your studio. This feature can be a significant time saver when you need to duplicate content or move it between different document types.

You can access these specialized copy-and-paste actions in the following ways:

- Through the **Field Actions** menu on individual fields.
- Using the standard **Ctrl/Cmd+C** and **Ctrl/Cmd+V** keyboard shortcuts on supported field types.

## Copy and paste fields

To copy and paste individual fields within a document:

6. Hover over a field to reveal the **Field Actions** menu. 

![](https://cdn.sanity.io/images/3do82whm/next/2885ad78b489aa44a290a88452e726116f2c5e28-492x235.png)

8. Select **Copy field** to copy the contents of that field.
8. Navigate to another field of the same type and select **Paste field** in the **Field Actions** menu to paste the copied content.

Additionally, certain field types support using the standard **Ctrl/Cmd+C** and **Ctrl/Cmd+V** keyboard shortcuts for copying and pasting:

- Array fields
- Object fields
- Reference fields
- Image and File fields 

Using keyboard shortcuts can be a quick way to duplicate content within these field types.

## Copy and paste documents

To copy and paste entire documents:

14. Open the **Document Actions** menu and select **Copy** to copy the current document to your clipboard.

![Shows the Document Actions menu with options for copying and pasting documents](https://cdn.sanity.io/images/3do82whm/next/37300fc59a415e495fe7baaafdcae1bddeeae6d9-491x330.png)

16. Navigate to the document list where you want to create a new document.
16. Create a new document.
16. Select **Paste** from the **Document Actions** menu or use the keyboard shortcut **Ctrl/Cmd+V**.

Another advantage of the copy/paste workflow over using the **Duplicate** action is that you can paste documents across different document types. Sanity Studio will try to map the fields from the source to the destination document.

## Examples

Here are some examples where copy-and-paste for fields can come in handy.

### Copying between array types

There might be cases where it’s more efficient to copy-start existing items from an array into a new one and edit them. For example, if you use array fields to build landing pages, newsletters, etc, and want to keep the same structure or have minor variations between them.

**Note that pasting into an array will replace all the items in it. **However, if you do this accidentally, you can use Review changes and restore to the content you want to keep.

### Copying between object types

Say you have an `object` field of type `bio` with the fields `name`, `image`, and `history`. If you copy that entire object and paste it into an object field of type `author` which has the fields `name` and `image`, Sanity Studio will transfer over the field values that are in common between the two types (`name` and `image`) and discard the field that doesn't exist in the destination (`history`).

### Copying between document types

Similarly, if you copy a whole document of type `author` and paste it into a document of type `person`, Sanity Studio will copy over any fields that the two document types have in common (e.g., `name` and `image`). Fields that do not exist in the destination type (e.g., `publicationsList` in `author`) will be discarded.

If there are no fields in common between the source and destination, you will see a warning listing the fields that were discarded. If there is nothing currently on your clipboard when you try to paste, you'll get a notification informing you there's nothing to paste.

## Limitations

There are a few known limitations to be aware of with the new copy-and-paste feature:

- References cannot be fully validated when pasting except to check that the reference type matches the target field. Complete validation would require making the paste operation asynchronous to fetch the referenced document, adding significant complexity. Full validation will happen after the reference value has been added.
- For images and files, validating the MIME type against the field's accept configuration is impossible without fetching the full asset document.
- When focused inside a text input, copy/paste is handled by the input's own clipboard event management to avoid interfering with the native editing experience, undo/redo functionality, etc.
- Pasting into an array field will overwrite the entire array rather than appending the pasted content. This behavior might be unexpected for users accustomed to appending when pasting in other contexts.



# Preview and page building

The Presentation tool in Sanity Studio is a powerful feature that allows editorial teams to work visually with structured content. It provides a bridge between the content model and the front-end presentation, making it easier for content teams to navigate and manage content in context.

> [!WARNING]
> Gotcha
> The Presentation tool can be customized and configured in considerable detail, so your implementation may not exactly match the examples shown in this article. Talk to your studio maintainer to get the specifics of your setup.

## Key features

### Live previews in your studio

Preview your drafted changes as they'll appear in your front end from within the Sanity Studio editorial interface. See your changes update in the preview area in real-time as you edit your content.

![Shows the studio interface with the Presentation tool active](https://cdn.sanity.io/images/3do82whm/next/b0f669b0d430bea82135cc29d7e56d9b457ab6b5-1459x1110.png)

### Overlays shows the way

Click any element in the preview to navigate to the corresponding field in your studio editor pane, even deep within a Portable Text block! Since pages in your front end can consist of content from any number of documents, these overlays are invaluable for quickly finding exactly what you're looking for.

![](https://cdn.sanity.io/images/3do82whm/next/c12f0abac45bd9404c73e9a9a62948e1ac2d2933-781x160.png)

### Build pages, block by block

Compose entire pages with drag-and-drop-powered page-building features. In addition to simplifying content management, the Presentation tool also supports simple page building using predefined design components, ensuring consistency with brand guidelines and UX best practices.

![](https://cdn.sanity.io/images/3do82whm/next/56d250ba1fe169f47dc31109a4d7d31aee0d9337-600x444.png)

## The anatomy of the Presentation tool

The presentation tool is in the top studio toolbar, alongside other tools you might have access to, such as the Structure and Scheduling tools. 

![](https://cdn.sanity.io/images/3do82whm/next/b088568b2ff975100524de90b2ba3eb1cda8e295-1355x75.png)

Once selected, the Presentation tool will show an interactive preview area side by side with the Sanity Studio editorial interface. When no specific route is defined, the preview area will open a default view—like the homepage or an index of available routes—and the editor pane will show a list of all the documents being used by the current preview.

![](https://cdn.sanity.io/images/3do82whm/next/1864df96c069eca67bb176b1e66bbe3b2eca5755-1459x1110.png)

Interacting with the preview area will reveal blue outlined overlays with labels indicating the source documents for elements in the preview. Clicking one of these will cause the document editor pane to navigate to the appropriate document and field, ready for your editing. Your changes will be updated in real-time in the preview!

![](https://cdn.sanity.io/images/3do82whm/next/5b8a4d329a6f44cbff5f884f7c7113f121958019-1459x1110.png)

The preview area has a toolbar of its own, modeled after a typical web browser address bar. 

![](https://cdn.sanity.io/images/3do82whm/next/c03791b77c6b142f7df3148db8b041e3eb1c0635-1459x1110.png)

From left to right, this toolbar sports the following elements:

![](https://cdn.sanity.io/images/3do82whm/next/00c0bae5dd8a5f872e5f614b5c9a0b95aff8d7d2-678x114.png)

24. An **Edit** toggle button that lets you switch the click-to-edit overlays in the preview area on or off. This is useful for navigating the preview without accidentally switching the form editor to a different context. Pro-tip: Hold down the `Alt` / `Option`-key to temporarily disable click-to-edit for quickly navigating your front end in the Presentation tool!
24. An address field where you may manually enter the route you want to preview, refresh the preview area, and open the current front-end route in a new tab.
24. A button to switch the preview area between desktop and mobile viewport sizes lets you quickly check that your drafted edits will work on all devices.
24. A share button that lets you share a preview of your draft by copying a link or QR code.

We'll discuss each of these in more detail in the upcoming sections.

## Opening a specific page in preview

There are several different ways to open a specific front-end route in the Presentation tool. The most straightforward is to type or paste the path of the page you want to preview into the tool's address field.

![](https://cdn.sanity.io/images/3do82whm/next/64d00d9f744b5eb97fcfd6a4e4ef0059fd76bcf9-884x76.png)

> [!WARNING]
> Gotcha
> Make sure you only copy and paste the relative path! This is typically the stuff that appears after the root domain. In the example below, the relevant part is within the square brackets.
> 
> https://my-cool-site.com[/posts/hello-world]

![](https://cdn.sanity.io/images/3do82whm/next/8eb8c90b83211ca933e2642723f180d166d6df9a-624x257.png)

Another option for a more "studio first" approach is to navigate to the relevant document in the Structure tool. You should see a list of pages where the document content appears below the document title. Clicking one of these will open the corresponding route in the preview area.

> [!WARNING]
> Gotcha
> If you can't see the list of routes, your studio maintainer may have to set up a location resolver first.

## Previewing and editing content

When the **Edit** toggle is active, you can hover any element in the preview area to see a blue outline with a label indicating the source document in Sanity Studio. Clicking the overlay will cause the form editor to navigate to the relevant document and field.  Since any page on your front end can include content from multiple documents, it is very useful for quickly navigating to the appropriate document in your studio. 

![](https://cdn.sanity.io/images/3do82whm/next/14cb72d86eb9771d4e1640f70a34d058bacf05b2-1459x1110.png)

Once you've found the field you want to edit, the preview will update in real time to reflect your changes.

![](https://cdn.sanity.io/images/3do82whm/next/57530396df116563a0b5fb6951c6597af81cdb4c-1459x1110.png)

The overlays will let you navigate to specific bits of content, no matter how deep in your content structure. For example, you can target specific blocks within a Portable Text Field. In the example below, clicking an image within a block of rich content in the preview area opens the details dialog for the relevant image within the Portable Text editor.

![Shows content deeply linked in the Structure tool](https://cdn.sanity.io/images/3do82whm/next/30fd109aa7b8942038d103744a0293c2cfd2733e-1459x1110.png)

> [!TIP]
> Protip
> Make sure your images have alt text! Not only is it important for accessibility, but it also helps the Presentation tool find your images.

## Building pages with drag-and-drop

The Presentation tool can also be set up to accommodate page-building, with a drag-and-drop interface to rearrange content blocks. Drag-and-drop can be enabled for array fields and configured to allow for either vertical or horizontal positioning.

![Shows a block being dragged horizontally](https://cdn.sanity.io/images/3do82whm/next/af06a4742694e9a13e86f6b8c393f4f81d011df2-930x332.png)

When you hover any draggable element, your cursor will turn into a crosshair, indicating that the element can be moved around by clicking and dragging. When you do so, a simplified representation of the block you are repositioning appears.

![](https://cdn.sanity.io/images/3do82whm/next/17080ae0349bed5e4768074fd8f1cde09b492fac-1359x949.png)

In certain cases, like in the example above, it might be useful to zoom out a bit to see the full context of the repositionable area. Hold down **shift** while dragging to enter a helpful minimap mode.

![](https://cdn.sanity.io/images/3do82whm/next/7067b68763fd1a49e3250bec38fbee437f3848d3-1304x836.png)

Draggable blocks also have a context menu, accessible by right-clicking, that offers several helpful options for repositioning, removing, or adding content.

![](https://cdn.sanity.io/images/3do82whm/next/2855a9054763ee1633f8916185e4cee6d61cb70e-324x252.png)

## The preview address bar

![](https://cdn.sanity.io/images/3do82whm/next/4d77e8f94df568e9b9f05bb691b50bf5d51be084-1459x449.png)

Let's examine in more detail the opportunities offered by the address bar in the Presentation tool's preview area.

### Toggle edit mode to navigate inside the preview

You can temporarily disable overlays so you can click links and navigate your front end within the preview area. Do so by toggling the **Edit** switch in the preview address bar.

![](https://cdn.sanity.io/images/3do82whm/next/76a7a0988b934ac36a889cc3efb945f258279d74-777x71.png)

## Switch between draft- and published mode

![](https://cdn.sanity.io/images/3do82whm/next/7c4f5f7fb66674620584c0126144e84e19f7fa96-777x71.png)

To quickly toggle between previewing your drafted changes or your published content, locate the **Perspective** select dropdown to the right of the address field. 

![](https://cdn.sanity.io/images/3do82whm/next/a2bdd3cee12cc9af5077edd6e49c5a340ec5cc3c-236x146.png)

## Change the viewport size to check responsive content

To resize the viewport, click the **Phone** icon second from the right in the preview address bar. 

![](https://cdn.sanity.io/images/3do82whm/next/aad1929926fec7f7efdc9b30dedd225c78130705-777x71.png)

Your preview area will resize to a more mobile format, so you can make sure your content looks good on the go.

![](https://cdn.sanity.io/images/3do82whm/next/bac9ec6135896349598ce949e0a96a95495335a8-1459x1110.png)

## Sharing previews

To quickly share a preview link, click the **Share** icon on the far right in the preview area address bar.

![](https://cdn.sanity.io/images/3do82whm/next/a9d0eb4d73b9aaf7ab5705f3d1c5f45b7d2456b8-777x71.png)

A popover will appear with options to enable or disable sharing. A QR-code is generated for easy access from any device with a camera, or you can opt to copy a link in plain text if you so prefer.

![](https://cdn.sanity.io/images/3do82whm/next/d0de7ef9efae351facea224a37dac476d22c5e7c-244x368.png)



# Content Releases

Content Releases allow teams to organize and schedule updates across multiple documents. Teams can plan, preview, and validate significant changes in advance, ensuring seamless and conflict-free content deployment.

Content Releases provide several key benefits:

- **Coordinate Updates:** Simultaneously manage and publish updates across multiple pages and channels, ensuring consistency.
- **Reduce Manual Effort and Risk:** Automate scheduling to minimize manual tracking and prevent errors or conflicting changes.
- **Gain Confidence with Previews and Validation:** Preview and validate scheduled releases to guarantee readiness before going live

For developer documentation on how to configure, integrate, and interact with Content Releases programmatically, go here:

[Configure content releases](/docs/studio/content-releases-configuration)

[Content Releases API](/docs/apis-and-sdks/content-releases-api)



_This is a paid feature, available on the Growth plan._

## Before you begin

Content Releases requires that your studio and plugins be up to date. If you're experiencing issues using Content Releases, check with your administrator and direct them to the [Studio configuration](/docs/studio/content-releases-configuration).

## The Content Releases workflow

Content Releases introduces the concept of a **release**. Releases are a way to group multiple document changes together into a single unit that can be previewed, validated, scheduled, and published as one.

The most basic workflow is as follows:

12. Create a release.
12. Add documents to a release to create new document versions.
12. Make changes to documents.
12. Publish the release.

### The document view

![content releases document screen](https://cdn.sanity.io/images/3do82whm/next/9b0426bfd894bfeae9393ab1af169efde4bd21fb-2142x1820.jpg)

When you're working on a release, the document screen displays details about **versions** of the document. Document version names correspond to release names. Published and drafts are always enabled, but additional versions are displayed as documents are added to releases.

Select a version name to switch between versions.** **Right click the names to copy versions between releases, or discard a version.

> [!TIP]
> Protip
> The release color highlights the global toolbar and document list to remind you that you're working on a specific release.

### The releases view

![the content releases screen](https://cdn.sanity.io/images/3do82whm/next/30ead86756557f74b0ba1e8b06e46104a9a7d236-2130x1820.jpg)

The releases screen displays any upcoming releases. Bold dates in the calendar indicate releases with date estimates. You can also see the number of changes in each release, and warnings if there are validation errors.

### Release layering

Release layering is the concept of displaying documents based on where a release falls in the release timeline and which perspective is active.

This allows editors to preview document changes across multiple releases. You can see a simplified version of this in how *drafts* override published documents in Presentation. 

In Studio, release layering works on a timeline. The type and time of release indicates where a release falls on the timeline. You already know *published* and *draft*, but there are also *as soon as possible (ASAP)*, *timed*, and *undecided*.

The layer follows this order, starting at 1 and adding changes.

26. Published
26. Draft
26. As soon as possible (ASAP)
26. Timed (e.g., a set time in the future)
26. Undecided

When viewing a release with an undecided release time, you will see all changes from drafts, ASAP releases, and timed releases stacked atop published documents—plus any changes on the undecided release(s). These views of your content in Studio are the *global perspective.*

> [!NOTE]
> Documents override their earlier versions
> The global view and document list will show changes across releases based on the layering order, but multiple changes in the same document across releases will be overridden by the highest selected layer level.

### Global perspective

The global perspective is your view into the state of all documents at a point in time. By selecting a release, you're viewing not only its changes, but all changes in prior releases. You can hide individual releases form view, or view just the Published perspective.

> [!WARNING]
> Gotcha
> Does it seem like all documents are read-only? You might be in the Published perspective. Select Draft or a release from the document screen to make changes to a document.

### How do drafts fit in with releases?

You can work directly on a draft and publish it without needing to create a release. You can also work on a draft, then copy it to a release. 

One important thing to keep in mind. Publishing a release will not reset a draft. If you created a draft and made changes, then copied it to a release, that draft still exists. There are two ways you can keep these leftover drafts in check:

- If you know you're working on a release, start the draft in the release. This way a draft document is never created.
- After copying a draft to a release, return to the draft document and discard the draft version.

## Create a release

To add new documents and changes to a release, you first need to create a release.

![](https://cdn.sanity.io/images/3do82whm/next/9f0e6c6a20bea7f40490ae8e91c4a61a82c43c50-774x758.jpg)

39. Locate the **calendar** icon in the top right corner of the toolbar. 
39. Select the **down arrow** icon to reveal a list of releases.
39. Select **new release** to create a new release.4. Select an approximate time of release.
4. Enter a release title (optional).
4. Enter a description for the release (optional).



![](https://cdn.sanity.io/images/3do82whm/next/dc780f442bb5fa52f63adfeab058d7ecd08bdc82-1410x962.jpg)

You can change these values later by navigating to the release on the **Releases** screen.

> [!TIP]
> Protip
> You can also create a release from the Releases screen by selecting New release in the top right corner.

## Add a document to a release

There are two ways to add a document to an existing release.

### On the main releases screen

46. Navigate to the **releases** screen by selecting the **calendar** icon in the top right of Studio.
46. Select the **release name** to navigate to its detail screen. 
46. At the bottom of the list of documents, select **Add document**.
46. Search for and select a document.

### On the document screen

48. Ensure you are in a release perspective by pinning a release. You'll know you've pinned a release if the release name is next to the **calendar icon** in the toolbar.
48. In a document's editor view, select the **Add to release** button in the top bar. This button and bar should match the color scheme associated with the release perspective.

Once a document is part of a release, you'll be able to edit the release version by ensuring the release is selected at the top of the document.

## Remove a document from a release

Removing a document from a release discards any changes unique to that version. This action won't remove the document from other releases.

There are three ways to remove a document from a release.

### On the main releases screen

54. Select the **release name** to navigate to its details screen.
54. Identify the document you wish to remove and select the **"..." icon** to reveal additional options.
54. Select **Discard version** and confirm the selection when prompted.

### On the document screen

56. Confirm you are in the perspective for the desired release. You should see the release name next to the calendar icon in the toolbar, as well as the highlighted release name at the top of the document.
56. At the bottom right of the document screen, select the **"..." icon**. 
56. Select **Discard version** and confirm the selection when prompted.

or

58. On the document header, find the chip with the version you wish to discard.
58. Right click the chip, a context menu will open.
58. Select **Discard version** and confirm the selection when prompted.

## Copy a document from one release to another

You can copy a document version to a different release from the document view.

![Version action user interface](https://cdn.sanity.io/images/3do82whm/next/7f067c31c8845d0650b1a15320125eb13b5a6bd9-1608x820.jpg)

62. Navigate to the document you want to copy.
62. Right click the release name you want to copy from.
62. Hover over **"Copy version to"**.
62. In the popover menu, select the destination release.

## Unpublish a document as part of a release

Sometimes you want a release to unpublish, or remove a live document. This converts a published document back to a draft once the release is published. 

65. Add the document to a release.
65. In the bottom right corner of the document screen, select the **"..." icon**.
65. Select **Unpublish when releasing** and confirm the selection when prompted.

![Document screen popover menu](https://cdn.sanity.io/images/3do82whm/next/a4f0e9c5e58bcf2fa020b8996ce7e5b9d26f06de-846x400.jpg)

## Discard a draft version

To discard a document version, follow the steps listed in *Remove a document from a release*. 

To discard changes from the **Draft** version, select the **"..." icon **at the bottom right of the document screen and select **Discard changes**.

## Publish a release

After creating a release, you can choose to publish it on-demand or schedule a publish. 

72. Navigate to the **release screen** for the release you want to publish.
72. Select **Publish all documents.**

## Schedule a release

To schedule a release, first set a release time and date. You can do this when creating a release, or by selecting the **release time** label and selecting **At time** from the **release** screen. You can adjust this time later if needed.

![](https://cdn.sanity.io/images/3do82whm/next/75b7b10a61d63b82240f3937c2a8a65774719131-1816x986.jpg)

Next, select **Schedule for publishing** in the bottom left of the **release** screen.

![](https://cdn.sanity.io/images/3do82whm/next/aadb1c9274e77f04d984ada034df5c50e90fb9e3-2134x1232.jpg)

Confirm the release time and date, then select **Yes, schedule for publishing**.

> [!WARNING]
> Gotcha
> Setting a release time alone does not schedule the release. You must set a time, and schedule the release using the Schedule for publishing button.

## Unschedule a release

To unschedule a release, select the **Unschedule** button in the bottom right of the **release** screen.

## Archive a release

Archived releases are releases that were published, but you can also archive unpublished releases to preserve them for future reference.

> [!WARNING]
> Gotcha
> You cannot archive a scheduled release. First, unschedule it. Then follow the steps below to archive the release.

There are two ways to manually archive a release.

### On the main releases screen

87. Select the **"..." icon** for the release you want to archive.
87. Select **Archive release**.

### On the individual release screen

89. In the bottom right, next to the Publish / Schedule button, select the **"..." icon**. 
89. Select **Archive release**.

## Unarchive a release

You may unarchive an archived, unpublished release. Published releases cannot be unarchived.

There are two ways to manually unarchive a release.

### On the main releases screen

94. Select the **"..." icon** for the release you want to unarchive.
94. Select **Unarchive release**.

### On the individual release screen

96. In the bottom right select the **"..." icon**. 
96. Select **Unarchive release**.

## Change the release order

Release order is determined by when the release will be live, with exceptions for *ASAP* and *Undecided*. 

- ASAP releases come first, in order of creation.
- Dated releases come next, ordered by date.
- Undecided releases come last, ordered by creation.

To change the order of a release, change the date and time associated with it.

## Pin a release (global perspective)

Pinning a release sets the global perspective in Studio. This is indicated by the color change in the toolbar, as well as the highlighted release name throughout Studio. 

You can only pin one release at a time.

![](https://cdn.sanity.io/images/3do82whm/next/686434212666f7c5f8175ddb7cb64adedb95d523-2132x1330.jpg)

There are three ways to pin a release.

### Anywhere in Studio

107. In the top toolbar, select the dropdown arrow next to the **calendar icon**. If a release is currently pinned, the arrow will display next to the pinned release.
107. Select the **release name** for the release.

### On the main releases screen

109. Locate the release to pin.
109. Select the **pin** **icon **to the left of the release name.

### On the Release detail screen

111. Navigate to the release that you wish to pin.
111. Select the pin icon on the top left, above the release name

## View Release history

You can view past releases, including unpublished ones, from the **Archived** tab on the **main releases screen**. 

## Edit properties of an existing release

You can edit the name, estimated release time, or description directly on the **release** screen. 

To change the title or description, select the field and begin typing. 

To change the estimated publish time, select the **release time** label and choose a new time.

> [!WARNING]
> Gotcha
> You can edit the name and description of scheduled releases, but in order to change the schedule date or time you first need to unschedule the release.

## Hide releases from the global perspective view

When viewing a future release, you can choose to hide earlier releases from the global perspective view. This lets you hide document changes made by specific releases, while still previewing a subset of changes across releases.

![](https://cdn.sanity.io/images/3do82whm/next/2ff2a670d4b12d4cd311d5170b82e65273622949-1128x734.jpg)

122. To hide versions from a specific release, first set your global perspective.
122. In the release dropdown view, select the **open eye icon** next to any release you wish to hide.
122. To reveal a hidden release, select the **closed eye icon**.

## Preview releases in Presentation

If your team has enabled Presentation, you can preview a release by **pinning it** and then selecting the Presentation tool in Studio. 

Keep the release layering concept in mind, and use the *hide release* feature to customize your preview perspective.



# Compare document versions

Use the document comparison view to compare document versions. This includes drafts, published, and release versions. To get started, open a document in Sanity Studio that contains multiple document versions.

2. In the top left right corner of the document view, select the **"..."** icon.
2. Next, select "Compare versions".

![](https://cdn.sanity.io/images/3do82whm/next/cdb7012fcd612207338ade2426a87d897c623d57-1474x896.jpg)

The document comparison view appears over your studio window. It contains a version selection at the top, and two panels that correspond to each document version on the left and right.

> [!WARNING]
> Gotcha
> This view only works when multiple document versions exist. If you don't see the option, ensure that changes are made to a draft or release version in addition to the published version.

![](https://cdn.sanity.io/images/3do82whm/next/4f834f0e91d3b28a0861c0fad7303e5d3808e982-2962x1710.png)

Differences between the fields in the right version are highlighted in yellow. You may recognize this from other history or "diff component" tools. In the screenshot above, the *Overview* field has a yellow highlight along the right edge indicating changes.

You can adjust the compared versions with the version selector at the top of the window.

![](https://cdn.sanity.io/images/3do82whm/next/5c140b6852622c79f2e4a310db0f211b7729d910-1706x872.png)

> [!TIP]
> Protip
> You cannot leave comments or tasks from within the selector. It's best to save major changes and workflows for the document view.





# History experience

## History retention

In order to make Sanity Studio real-time, it sends edits as patches to the backend. All these patches are stored as transactions. Together they make up your documents’ revision history. 

History retention is the amount of time you have access to these revisions before they are automatically deleted. The latest version of your published and drafted document will always be available.

The retention period on your documents are defined by the plan you are on. We count retention time backward from the current day. 

The retention time for the [different plans](/pricing/compare) are:

- Free: 3 days
- Growth: 90 days
- Enterprise: 365 days, or contact us for custom retention

Revisions that are older than the cutoff will be truncated into one revision item, older transactions will be permanently deleted. The document history is truncated regularly every day.

### GDPR

We introduced history retention to make it possible to use Sanity and be GDPR compliant. You can learn more about our [security and compliance here](https://www.sanity.io/security).

### Upgrading the retention time

If you change retention time by changing plans, or upgrading on your current plan, this will only affect the retention cutoff time by postponing it to however long your retention time is. The retention history for your documents will stay as it was before the upgrade.

### Downgrading the retention time

The revision history for all your documents will be truncated to your new cutoff time when downgrade either by turning off the upgrade on your current plan, or switching to one with less included retention time.

## Exploring history in Sanity Studio

While viewing a single document in the studio editor, you can access the history either by clicking the document status indicator in the very bottom of the editor view, or by opening the contextual menu by clicking the ellipsis icon in the top right corner of the editor and selecting **History**.

![The document editor in the studio showing the context menu with the links for opening document revision history highlighted.](https://cdn.sanity.io/images/3do82whm/next/751920608b2a4ac0a4e60f17b498ea72cac8d1ac-1181x1012.png)

## Document status labels

The labels under the title in the document editor shows whether the content you are looking at is published and/or a draft.

### Published

The content in the editor is the same that is published to the API.

### Draft

The content in the editor has not yet been published, or has been unpublished. 

### Published, Draft

The content has been edited after the document has been published.

### Live

The document is in live edit mode. All changes are published real-time and skip the draft workflow. This is not to be confused with the [Live Content API](/docs/content-lake/live-content-api), which is a way of rendering published content changes instantly.

## History status labels

These are the labels for the revision items in the history view.

### Published

The document was published to the API.

### Unpublished

The document was unpublished from the API.

### Edited

The document was edited.

### Truncated

Revisions before the cutoff date 



# Introduction

AI Assist is the official plugin for Sanity Studio that brings artificial intelligence features to the editorial experience. Beyond running simple text generation prompts, it can interact with your structured content in numerous ways.

This article covers the different capabilities and affordances that this plugin has once installed and configured in your Sanity Studio.

> [!NOTE]
> Paid feature
> This article is about a feature currently available for all projects on the Growth plan and up.

[Install and configure AI Assist](/docs/studio/install-and-configure-sanity-ai-assist)

[Common instructions for AI Assist](/docs/user-guides/ai-assist-cheat-sheet)

[Content translation with AI Assist](/docs/studio/ai-assist-content-translation)



## Document- and field-level instructions

You might be familiar with the term *prompt;* it's called *instruction* for AI Assist. AI Assist is not a chat interface (like ChatGPT) but a way to describe tasks that the AI can do to your content. That being said, you can bring techniques and methodologies from prompt engineering and bring them in here, too.

AI Assist lets users of Sanity Studio add instructions for whole documents and specific fields. The instructions can be visible only to those who made them or shared with all users of the Studio.

### Creating document-level instructions

Once the plugin is successfully installed and activated, you will find a new button with a sparkle ✨ icon at the top of every document form (side by side with the ellipsis … button where you find options to inspect the document or review its history). If comments are enabled for your project, this is also where you’ll find the speech bubble 💬 button to open the comments panel.

Clicking the sparkly button will open the AI Instruction editor in a side panel to the right. The editor opens with the entire document as its “target” because it was opened from the root-level sparkle button in the top right corner of the editor.

Create a new instruction by clicking **+ Add item**. Give your instruction an appropriate and informative name, and if you wish, click the sparkle icon on the left to select a fitting icon.

From this view, you can name your instruction, decide whether or not to make it available to other users of this studio once you’re happy with it, and, of course, edit and run your instruction.



### Creating field-level instructions

To target a specific field, hover your cursor over the relevant field to reveal its very own dedicated sparkle button. Then click it to switch the context to that field. This opens up the same panel type as with the document-level instructions described above.

Note that not all field types are supported, as elaborated further in this article.

## Instruction editor

Instructions for AI Assist are written in a normal, human-readable style. These instructions can be enhanced by adding references to fields’ content from the current document, which the assistant can access in real time. You can also prompt the user for input or refer to reusable contextual documents – such as a style guide or a description of your target audience – to further inform the assistant about the expected output.

[Common instructions for AI Assist](/docs/user-guides/ai-assist-cheat-sheet)



### Instruction contexts

AI Assist will include a description of your schema by default but not any content unless you explicitly include references to fields or other contexts in your instructions:

- Document fields: Add a placeholder for a field’s content. AI Assist will include the field name and its content, meaning that you can insert these anywhere in your instruction, inline or on separate lines.
- User input: Opens a box to which the user running the instruction can paste plain text. You can customize the title and instructions given to the user that triggers this box. Only the contents of what’s inserted in the box will be put into the instruction.
- AI Context: Includes the content of an AI Context document that you can also manage in the Studio. This is where you would typically include brand and style guides. Only the contents of what’s inserted in the box will be put into the instruction.

### Allowed fields

Below the instruction editor, there is a collapsed option for **Allowed fields.** This is relevant for document-, object-, and array-level instructions. By default, an instruction will run for all supported fields. But there might be cases where you want to prevent AI Assist from interacting with certain nested fields. You can uncheck field labels for which the AI Assist *should not* add content.

## Sharing instructions

When you create a new instruction, it will only be visible to you by default. Select the **Make visible to all Studio members** switch to share it with other Studio users.

Note that AI Assist runs with the same permissions as the user who runs the instruction. Someone with limited access might still be able to see and trigger an instruction, but it won’t successfully run if it assumes permissions they don’t have.

## Running instructions

There are two ways of running an instruction for AI Assist:

- Using the **Run instruction** button at the bottom of the instruction editor
- By clicking its name in the sparkle menu (✨) once the instruction is added

Once you have triggered AI Assist, it will run the instruction in a real-time collaborative mode as if it’s a user of the Studio. You are then free to continue working on the same document in real time or you can navigate away from the document. You will get a pop-up message telling you that AI Assist is done unless you have closed the Studio, which you can do without disrupting the AI.

How long it will take to run an instruction depends on many factors. AI Assist does quite a lot under the hood to bring your content model and other contexts into its tasks.

## Supported field types

AI Assist can use most fields in your schema as a context in an instruction.

These are the field types it can write content to, including custom schema types based on the following:

- String and text
- Objects and the fields within them
- Arrays with inline objects and references
- Portable Text, including formatting and custom blocks
- Image assets (and image fields)
- References (requires additional configuration)

### Conditionally hidden and read-only fields

Fields and field sets that are conditionally visible or read-only can have instructions and can be written to by an instruction, as long as the field is non-hidden when the instruction is initiated.

> [!WARNING]
> Gotcha
> AI Assist will ignore any field that is hidden or in read-only mode when the instruction starts running. Changes to these conditions that occur while the instruction is running will not alter this behavior. 

### Unsupported fields

There are some field types that AI Assist can use as context but not write content for:

- Booleans
- Numbers
- Slugs
- URLs
- Date and DateTime
- Geolocation
- Cross Dataset References
- File assets

## Working with images

AI Assist can generate image assets based on instructions and generate image descriptions from an uploaded image.

[Install and configure Sanity AI Assist](/docs/studio/install-and-configure-sanity-ai-assist)



### Generating images

There are two ways of working with image generation with AI Assist:

- Write an instruction, as with any other field
- Configure image generation from an “instruction field” (allowing AI-assisted image instruction generation)

### Generating image captions / alternative text

AI Assist can add image descriptions for image asset fields. This feature can typically be used to autogenerate alternative text or image captions.

A developer can enable this as part of [the schema configuration](/docs/studio/install-and-configure-sanity-ai-assist). Note that that instruction will automatically run when an image is uploaded and/or replaced. 

For images that were uploaded before it was enabled, there will also be a "Generate caption" instruction that you can find in the description field’s sparkle (✨) menu.

## Working with references

AI Assist can work on reference fields and pull in relevant articles based on an instruction. This feature requires configuration and works only on documents that have been included in an embeddings index.

It’s good to note that this feature relies on (vector) embeddings and not a regular string search or matching. This means that AI Assist will look for *semantic similarity* between your instruction and what’s indexed in your documents. This means that it also works across languages, which, in some cases, can be powerful but, in other cases, can lead to undesired results.

Preventing certain documents from being referenced can be solved either by adjusting what’s being included in the embeddings index or in content queries, the latter being less transparent for Studio users.

[Embeddings Index API](/docs/compute-and-ai/embeddings-index-api-overview)



## Working with translations

You can use AI Assist to generate content translations. It’s built to be compatible with the localization content models used by the [Document Internationalization](https://github.com/sanity-io/document-internationalization) and [Internationalized Array Field](https://github.com/sanity-io/sanity-plugin-internationalized-array) plugins.

[Content translation with AI Assist](/docs/studio/ai-assist-content-translation)



When the configuration for translation is enabled, users of the Studio will be able to trigger an AI-assisted translation depending on which approach is used:

- Document-level translations can be found in the document-level sparkle menu(✨)
- Field-level translations from the sparkle (✨) menu on localized fields that have AI translation enabled

## AI context documents

When the AI Assist plugin is enabled, it will add a document type called **AI Context** that can be found in the Structure tool (unless intentionally hidden). You can use these documents to centralize parts of instructions that you want to be consistent across your project.

Typical examples are, but not limited to:

- Style guide for text copy and images
- Brand guidelines
- Company descriptions
- Specific instruction tunings

## AI technologies powering AI Assist

AI Assist uses different AI technologies, largely Large Language Models (LLMs), under the hood and is built to be service- and model-agnostic. We will change the underlying models and services to improve and secure the performance of AI Assist.

Technologies we use:

- Models by OpenAI, like GPT and DALL·E
- Models by Google, like VertexAI
- Models by Anthropic, like Claude



# Common instructions

Here, you will find instructions for common tasks that AI Assist can do. Use these as inspiration and a starting point for more specialized instructions. 

> [!NOTE]
> Paid feature
> This article is about a feature currently available for all projects on the Growth plan and up.

[Install and configure Sanity AI Assist](/docs/studio/install-and-configure-sanity-ai-assist)

[Create and run instructions with AI Assist](/docs/user-guides/ai-assist-working-with-instructions)

[Content translation with AI Assist](/docs/studio/ai-assist-content-translation)



## Good to know

Sanity AI Assist knows about your Studio's schema. When running instructions at the document level, for instance, you can still target specific fields by referring to them by title. E.g., *“Create a description based on title, then add a relevant callout to body. Do not add anything to any other fields.”Using explicit commands and clear language like “Important: <command>” can help guide the assistant in a certain direction if it’s being too creative.*

## Write more of a body

**Where***:* `body` *–* Portable Text or plain text field

**Instruction***: *

Given the `title` `body`, keep writing copy.

## Get started on an article

**Where***:* document instruction

**Instruction***: *

Given the following inspiration `User input (What should the article be about?)` Create an article.

## Summarize a field

**Where***:* `summary` *–* Portable Text or plain text field

**Instruction**: 

Summarize `body`

**Instruction variation*** *(to suggest how the summary should be formatted when saving to a formatted text field)*:*

Summarize `body`. Do not use lists, headlines, or quotes.

## List categories

**Where**: On an array field with strings

**Instruction***:*

List the categories relevant to `body`

## Shorten a field

**Where**: On the field to shorten

**Instruction***:*

Shorten `<the field itself>`

## Translate a field

**Where**: On the field where the translation should be stored

**Instruction***:*

Translate `title` to Norwegian.

## Sentiment analysis

**Where***:* On a text field where the sentiment should be stored

**Instruction***:*

Classify the sentiment of the following text `body`

Determine if it is elated, happy, neutral, sad, or angry.

## Create a catchy title

**Where***:* On the title field

**Instruction***:*

Create a catchy and engaging headline about `User input: (What should the title be about)`

## “Smart” paste

**Where***:* On any field or at the document level

**Instruction**:

Given the following User input:

`User input (Paste your stuff here)`

Format the User input so it aligns with the schema. Do not omit sections or paragraphs; use formatting, headings and lists as appropriate.

Infer item types based on context.



# Meet the library

## Media Library at a glance

Media Library is home to your organization's shared assets. It stores assets for use across your projects and datasets, and allows content teams to have a central source of truth for their media.

![a screenshot of a media library showing various images](https://cdn.sanity.io/images/3do82whm/next/cae386064a9678b739ff46b3370a3773a92c6c10-3136x1596.png)

Media Library is an organization-wide application. [You can access it from the dashboard](/docs/dashboard) by selecting the "Media" icon in the left navigation bar. Media Library requires the dashboard.

> [!NOTE]
> Where are my existing assets?
> If you've been using Sanity already, you may have images and other files that you're using in your studios. These files are saved within your datasets, and they are not automatically copied into the media library.
> 
> Soon, we will add the capability to migrate existing assets into the media library and preserve connections to those assets within your studios.



## The library interface

The library adapts based on the assets you have selected.

![a view of the three main panels in the media library](https://cdn.sanity.io/images/3do82whm/next/0771a14ab13fa57617b60a79fda2c3f416825d5b-3128x1596.png)

The core of the interface is split into three sections:

11. The asset list: View existing assets, filter the results, and upload new assets.
11.  The library menu: Narrow your view of the asset list, explore collections, and see recently uploaded assets.
11. The asset sidebar: Edit asset metadata, apply aspects, and view additional details about the asset. 

### Uploading assets

There are two ways to upload assets in the library interface:

14. Select the **Upload** button in the top right of the asset list to upload an asset.
14. Drag-and-drop one or more assets directly into the asset list to start an upload.

As your assets upload, you'll see a status screen showing the progress of each asset.



### Select multiple assets

You can select multiple assets at once by pressing the checkbox in the top-left corner of each asset.

![a screenshot of a media library showing a selection of images](https://cdn.sanity.io/images/3do82whm/next/a970d91d7470f28fd859237ffa75c2a7271705c9-3070x1596.png)

### Delete assets

To delete one or more assets, first select them in the asset list.

Next, select the vertical **"..."** icon from the popover at the bottom of the asset list.

![a screenshot of the popover that says delete 1 asset](https://cdn.sanity.io/images/3do82whm/next/1582b2b41d1a0b92c88cb6742a396be2af2da257-1306x826.png)

Select **"Delete 1 asset"** to delete the asset.

## Aspects

![a screenshot of a media library with an asset detail panel open](https://cdn.sanity.io/images/3do82whm/next/6bb17c72377a527f1b361a8ede94a1877f2360b1-3388x1910.png)

Aspects let you organize your assets with customly defined fields. Aspects are defined programatically with a schema-like syntax.

#### Developing aspects

[Create an aspect](/docs/media-library/create-aspect)

[Aspect patterns](/docs/media-library/aspect-patterns)



You can use aspects to sort and filter results in the asset list, or to store internal metadata.

> [!NOTE]
> Aspect limits
> You can have multiple aspects defined, but depending on your plan, there may be a limit on the number of aspects an asset can have attributed to it. 

### Add aspects to an asset, or edit an aspect

To add aspects to an asset, first select one or more assets in the asset list.

If adding an aspect for the first time, select **"Add aspect"**. If adding additional aspects, select the **"+"** button to the right of the *Aspects* heading.

Once you've made changes to an aspect, select the** "Publish"** button to publish the changes to the asset.

> [!TIP]
> Publishing changes
> Don't forget to publish changes whenever you add or remove aspects, or when you make updates to the asset title.

## Collections

![a screenshot of the media library showing a collection of landscapes](https://cdn.sanity.io/images/3do82whm/next/de4563243f810e5c856444146aa6c22544debc80-3070x1596.png)

_This is a paid feature, available as an addon on the Enterprise plan._

Collections allow further grouping of assets and are not limited to available aspects. You can create new collections while selecting an asset, or from the collection's screen.

### Add an asset to a collection

You can add an asset to a collection in two ways:

42. Navigate to the collection, then select **"Add"** in the top right, where the upload button normally is.
42. In any view, select the asset then, then select the vertical **"..."** icon, then select **"Add to existing collection"** from the popover menu.



# Roles

You can manage access to content and settings in your Sanity Content Lake by setting roles and permissions for project members. Each member may have different roles for granular access control to your Content Lake. All projects have default roles available, but you can also create **custom roles** that define granular access to datasets and project settings. You can also use GROQ to define custom **content resources**. Content permissions are typically set to either all or individual datasets, but you can use **Tags** to group datasets that should share permissions.

Roles and permissions can be [configured through the API](/docs/http-reference/roles), or through the project settings available at [sanity.io/manage](https://sanity.io/manage). This article will focus mainly on the latter option.

## Default roles per plan

Each plan type has access to specifically defined roles. Custom roles are available for Enterprise customers.

#### Properties

| Property | Description |
|----------|-------------|
| Administrator | Read and write access to all datasets, with full access to all project settings. |
| Viewer | Read-only access to all datasets, with no access to project settings. |
| Editor | Read and write access to all datasets, with limited access to project settings.

Editors can modify existing datasets, but cannot create new ones. |
| Developer | Read and write access to all datasets, with access to project settings for developers. |
| Contributor | Read and write access to draft content within all datasets, with no access to project settings. Can write but not publish documents. |
| Custom | Fully custom roles and permissions, with custom access to project settings. |


## Assigning roles to members

To assign roles to users, navigate to the Member section in your project settings at [sanity.io/manage](https://sanity.io/manage). You'll see each project member's roles listed by their name and login info.

![Overview of project members in manage](https://cdn.sanity.io/images/3do82whm/next/554f0afe514787948ac29011ff22443a72e91450-796x385.png)

When using Single Sign-On (SSO), roles can be [automatically assigned](https://www.sanity.io/docs/sso-saml#8b07dd93c2dc) to users using rules that evaluate each user’s group membership in your identity provider. Role assignment can be restricted to be set only through mapping rules, or allow for manual modification. If role assignment is restricted to be set only through mapping rules, you cannot manually change the role of a user in this screen.



![Shgows a popover alerting the user that roles are handled by identity provider and cannot be manually updated](https://cdn.sanity.io/images/3do82whm/next/9a7a942da4941567856316ac1613ac4a72ca57f9-1605x1365.png)

## Creating custom roles

_This is a paid feature, available on the Enterprise plan._

To define custom roles, navigate to the **Access** tab in your project settings. You will see a list of your currently defined roles with a summary of each role's access privileges. To create a new custom role, click the button in the upper right corner.

![Shows the button indicated above](https://cdn.sanity.io/images/3do82whm/next/ee47a8e618a0fbcae78c8dd451ba40f27eded47e-1041x648.png)

You will be asked to provide some basic details for your role.

![Shows dialog for creating new role](https://cdn.sanity.io/images/3do82whm/next/787e50a9c2ed123cabbc98ed5d57ef0feedb799f-648x516.png)

Once created you'll have the option of adding members to the role or proceeding to define permissions and restrictions. These are divided into two main categories: **Content Permissions** and **Management Permissions**.

### About viewer roles

Users with the "Viewer" role are free and don't count toward a plan's available seats. If a user with the viewer role is assigned an additional role, that user will count as a billable user. 

> [!WARNING]
> Gotcha
> Only the built-in viewer role is consider a "free viewer." Any custom role, even if it only grants read-only access, is billed as a regular user.

## Management Permissions

These settings grant a role access to your project's settings which are typically accessed in the project management console at [sanity.io/manage](https://sanity.io/manage). Access to a project's details and usage statistics, members and roles, API settings, and datasets and tags are currently available for configuration.



> [!WARNING]
> Gotcha
> In order for a role to have access to the project, Project Details should be set to read. For content editor roles, also setting Project Members to read will ensure they get the best studio experience with the full advantage of Presence features.

## Content permissions

This is where you define the role's access to your Content Lake. You can grant any role wide-reaching privileges that extend to all your datasets or use GROQ filters to set up granular access to only certain content types.

Once you've navigated to the role you want to configure you'll be presented with a list of your datasets that can be individually configured, as well as the opportunity to set some base permissions for *all datasets*.

![Shows overview of permissions for role](https://cdn.sanity.io/images/3do82whm/next/1934bbd28a3ca4b6cc182beed7d3a04b9b9035e3-910x780.png)

By default, all permissions are set to **No access**. Permissions cascade down from more general contexts to more specific ones, so it's generally better to start restrictive and grant privileges on each dataset separately as any permission granted on **All datasets** will override more restrictive settings in the individual datasets.

![Shows default permissions for all datasets set to "No access"](https://cdn.sanity.io/images/3do82whm/next/8d2232e47247037206b615fbac27a3b0bbebcc2a-677x313.png)

> [!WARNING]
> Gotcha
> Permissions are additive in nature. 
> 
> That means you cannot remove a permission that has been granted to a role elsewhere.
> 
> Example: If you defined that a role has publish rights for all documents in all datasets, it is impossible to define a resource (via GROQ filter) which only grants read access to a specific subset of documents.

This hereditary characteristic of permissions is visualized when you go to edit the permission for a single dataset. The dialog shown below demonstrates how the final permissions for the dataset are derived from both the privileges set generally for all datasets and from the privileges set specifically for this dataset.

![Shows permissions on several levels of specificity](https://cdn.sanity.io/images/3do82whm/next/7e324ed31d419d7136c61cdbe4e9b0830d77db16-826x735.png)

The base set of content resources available for access control are general in nature but powerful enough to cover many use-cases. You may grant privileges to read, create and update, and publish each of the widely encompassing options; **All documents**, **Image assets**, and **File assets**.

> [!WARNING]
> Gotcha
> If your dataset is public all project members will have read access to your published content even if their role is set to no access!

## Content resources

_This is a paid feature, available on the Enterprise plan._

In addition to the basic set of permission scopes that lets you configure access to **All documents**, **Image assets**, and **File assets,** you may also create custom content resources to control access to particular content types, which you may then control the access to with your custom roles. To create a new content resource, find the **Resources** section in the left column menu, under the **Access** tab.

> [!WARNING]
> Gotcha
> The filter does not support dereferencing! This will not work: referenceField->! Instead, check against the _ref property when creating custom resources: referenceField._ref == "my-referenced-doc-id".

![Shows the button described below](https://cdn.sanity.io/images/3do82whm/next/4f324593a0373521a2486807bf47e5afa91483b6-683x470.png)

In our example, we'll be working with the default starter template called *Movie Project*. This gives us a prefilled dataset with content types like `movie`, `person`, and *screening*. Click the button in the top right of the section to create a new content resource.

![Shows the setup dialog for new content resource](https://cdn.sanity.io/images/3do82whm/next/4fae04f825f30382264af0e92d97c8db0308ab40-1292x1512.png)

Content resources leverage the power of GROQ to filter which content types are affected by the privileges you choose to grant. In this example, we're using a simple but powerful GROQ expression to return only documents of type movie.

> [!TIP]
> Protip
> GROQ is Sanity's open-source query language for JSON. Learn more about GROQ in the docs!

Once you hit save, you should see your new content resource added to the list.

![Shows the new content resource in the list of resource definitions](https://cdn.sanity.io/images/3do82whm/next/0707eba5207d5c510e085be4984a7dd6111e73cb-1766x576.png)

Revisiting the **Roles** section in the left column menu, we can now set `movie`-specific privileges on our custom role.

![Shows dialog for specifying permissions on content resource](https://cdn.sanity.io/images/3do82whm/next/926857104bb65dbd336033dadbaf08ef442f51f2-649x728.png)

To test your role, make sure you have actually set the role on a member account and then proceed to log into the studio with the account in question.

![Shows list of project members, one with new role specified](https://cdn.sanity.io/images/3do82whm/next/dd43be021181aa698dcc53d7ed502f17534dd9ee-667x189.png)

Your account should be able to view, create, update and publish any document of the `movie` type, but should be unable to edit documents of any other type.

![Shows a notification stating that current user does not have permissions to update document](https://cdn.sanity.io/images/3do82whm/next/850bcc2e259960e29685bd8742b20f7aab09eaae-640x357.png)

![Shows a notification stating that current user does not have permissions to update document](https://cdn.sanity.io/images/3do82whm/next/2cdd6060d774cc8654bd556f6ef2c79eab88d276-641x287.png)

## Tags

Tags are a useful feature that lets you group datasets with similar characteristics together so that roles and permissions can be conveniently set on multiple datasets in a single operation. You might create tags for different environments, such as `production` and `staging`, or combine tags for different publications and locales, E.g. `elle` `us` or `vogue` `jp`.

_This is a paid feature, available on the Growth plan._

To create a new tag, navigate to the **Datasets** tab in your project settings and find the **Tags** section in the left column menu.

![Shows the button described above](https://cdn.sanity.io/images/3do82whm/next/55c7e0536da439f29d08e05ec42a4f53e1d56042-1540x466.png)

In the example shown below we'll be creating a tag for staging and production datasets for our movie blog, and then assigning editing privileges in both for our `movie-critic` role.

![Shows setup dialog for new dataset tag](https://cdn.sanity.io/images/3do82whm/next/80c860d3162574acd4c512d218d997eefdafec7b-645x639.png)

Once created, we can add datasets to the tag and define permissions to content resources for our custom roles.

![Shows content permissions for dataset tag](https://cdn.sanity.io/images/3do82whm/next/17b14929387db6f537a1b496a87029d25490f0f7-1546x988.png)

The change is reflected and can be edited in the content permissions for the custom role.

![Shows the permission setting as described above](https://cdn.sanity.io/images/3do82whm/next/552ddfb71be659d716500c07b44cfd7ed3d7e64f-1554x1280.png)





# Setting up your studio

## Create a new Studio with Sanity CLI

![Video](https://stream.mux.com/wIMs3CS7T4pP7hRArpQZsBZ01Be02vCjbK)

Run the command in your Terminal to initialize your project on your local computer.

See the documentation if you are [having issues with the CLI](/docs/help/cli-errors).

```sh
npm create sanity@latest -- --dataset production --template clean --typescript --output-path studio-hello-world
cd studio-hello-world
```

## Run Sanity Studio locally

Inside the directory of the Studio, start the development server by running the following command.

```sh
npm run dev
```

## Log in to the Studio

**Open** the Studio running locally in your browser from [http://localhost:3333](http://localhost:3333).

You should now see a screen prompting you to log in to the Studio. Use the same service (Google, GitHub, or email) that you used when you logged in to the CLI.



# Defining a schema

## Create a new document type

![Video](https://stream.mux.com/IfVfAwxfwOKN2khdGCQ3cs5IuF1rYte1)

Create a new file in your Studio’s `schemaTypes` folder called `postType.ts` with the code below which contains a set of fields for a new `post` document type.

```
import {defineField, defineType} from 'sanity'

export const postType = defineType({
  name: 'post',
  title: 'Post',
  type: 'document',
  fields: [
    defineField({
      name: 'title',
      type: 'string',
      validation: (rule) => rule.required(),
    }),
    defineField({
      name: 'slug',
      type: 'slug',
      options: {source: 'title'},
      validation: (rule) => rule.required(),
    }),
    defineField({
      name: 'publishedAt',
      type: 'datetime',
      initialValue: () => new Date().toISOString(),
      validation: (rule) => rule.required(),
    }),
    defineField({
      name: 'image',
      type: 'image',
    }),
    defineField({
      name: 'body',
      type: 'array',
      of: [{type: 'block'}],
    }),
  ],
})
```

## Register the `post` schema type to the Studio schema

Now you can import this document type into the `schemaTypes` array in the `index.ts` file in the same folder.

```
import {postType} from './postType'

export const schemaTypes = [postType]
```

## Publish your first document

When you save these two files, your Studio should automatically reload and show your first document type. Click the `+` symbol at the top left to create and publish a new `post` document.



# Querying content with GROQ

## Write your first GROQ query

![Video](https://stream.mux.com/Mc12Sdeu00ugrGuQyz00Du1G4AQZmT36UV)

Open **Vision** in your Studio's top nav bar and paste this query into the **Query** code block field.

```groq
*[_type == "post"]{
  _id,
  title,
  slug,
  publishedAt
}
```

- `*` represents all documents in a dataset as an array
- `[_type == "post"]` represents a **filter** to only return matching documents
- `{ _id, title, slug, publishedAt }` represents a **projection** which defines the attributes from those documents that you wish to include in the response.

## Run the query

Click **Fetch** to see the JSON output in **Results**. You should see the document you previously published in the results.

Queries run in Vision use your authenticated session, so you will see private documents – which have a `.` in the `_id` key, like `drafts.`. You will not see when queried from your front end in the next step.



# Displaying content in an Astro front end

## Install a new Astro application

![Video](https://stream.mux.com/BRpQTRNc2nAWQweqMyPFw5QoX7019MMOT)

If you have an *existing* application, skip this first step and adapt the rest of the lesson to install Sanity dependencies to fetch and render content.

**Run** the following in a new tab or window in your Terminal (keep the Studio running) to create a new Astro application with Tailwind CSS and TypeScript.

```sh
# outside your studio directory
npm create astro@latest astro-hello-world -- --template with-tailwindcss --typescript strict --skip-houston --install --git
cd astro-hello-world
```

You should now have your Studio and Astro application in two separate, adjacent folders:

```text
├─ /astro-hello-world
└─ /studio-hello-world
```

## Install Sanity dependencies

**Run** the following inside the `astro-hello-world` directory to:

- Install and configure the official Sanity integration [@sanity/astro](https://www.sanity.io/plugins/sanity-astro)
- Install [astro-portabletext](https://github.com/theisel/astro-portabletext) to render Portable Text

```sh
# your-project-folder/astro-hello-world
npx astro add @sanity/astro -y
npm install astro-portabletext
```

## Add Types for Sanity Client

**Update **`src/env.d.ts` with the following additional code for TypeScript support of Sanity Client.

```
/// <reference types="astro/client" />
/// <reference types="@sanity/astro/module" />
```

## Configure the Sanity client

**Update** the integration configuration to configure a Sanity Client to fetch content.

```
import { defineConfig } from "astro/config";
import tailwind from "@astrojs/tailwind";

import sanity from "@sanity/astro";

export default defineConfig({
  integrations: [
    tailwind(),
    // 👇 Update these lines
    sanity({
      projectId: "your-project-id",
      dataset: "production",
      useCdn: false, // for static builds
    }),
  ],
});
```

## Start the development server

**Run** the following command and open [http://localhost:4321](http://localhost:4321) in your browser.

```sh
npm run dev
```

## Display content on a posts index page

Astro performs data fetching inside front-matter blocks (`---`) at the top of `.astro` files

**Create** a route for a page with a list of posts fetched from your Sanity dataset, and visit [http://localhost:4321/posts](http://localhost:4321/posts)

```tsx
---
import type { SanityDocument } from "@sanity/client";
import { sanityClient } from "sanity:client";

const POSTS_QUERY = `*[
  _type == "post"
  && defined(slug.current)
]|order(publishedAt desc)[0...12]{_id, title, slug, publishedAt}`;

const posts = await sanityClient.fetch<SanityDocument[]>(POSTS_QUERY);
---

<main class="container mx-auto min-h-screen max-w-3xl p-8">
  <h1 class="text-4xl font-bold mb-8">Posts</h1>
  <ul class="flex flex-col gap-y-4">
    {posts.map((post) => (
        <li class="hover:underline">
          <a href={`/posts/${post.slug.current}`}>
            <h2 class="text-xl font-semibold">{post.title}</h2>
            <p>{new Date(post.publishedAt).toLocaleDateString()}</p>
          </a>
        </li>
      ))}
  </ul>
</main>
```

## Display individual posts

**Create** a new route for individual post pages.

The dynamic value of a slug when visiting `/posts/[slug]` in the URL is used as a parameter in the GROQ query used by Sanity Client.

Notice that we’re using [Tailwind CSS Typography](https://github.com/tailwindlabs/tailwindcss-typography)’s `prose` class name to style the post’s `body` block content. Install it in your project following their documentation.

```tsx
---
import type { SanityDocument } from "@sanity/client";
import { sanityClient } from "sanity:client";
import imageUrlBuilder from "@sanity/image-url";
import type { SanityImageSource } from "@sanity/image-url/lib/types/types";
import { PortableText } from "astro-portabletext";

const POST_QUERY = `*[_type == "post" && slug.current == $slug][0]`;
const post = await sanityClient.fetch<SanityDocument>(POST_QUERY, Astro.params);

export async function getStaticPaths(): Promise<{ params: { slug: string } }> {
  const SLUGS_QUERY = `*[_type == "post" && defined(slug.current)]{
    "params": {"slug": slug.current}
  }`;
  return await sanityClient.fetch(SLUGS_QUERY, Astro.params);
}

const { projectId, dataset } = sanityClient.config();
const urlFor = (source: SanityImageSource) =>
  projectId && dataset
    ? imageUrlBuilder({ projectId, dataset }).image(source)
    : null;
const postImageUrl = post.image
  ? urlFor(post.image)?.width(550).height(310).url()
  : null;
---

<main class="container mx-auto min-h-screen max-w-3xl p-8 flex flex-col gap-4">
  <a href="/posts" class="hover:underline">&larr; Back to posts</a>
  {
    postImageUrl && (
      <img
        src={postImageUrl}
        alt={post.title}
        class="aspect-video rounded-xl"
        width="550"
        height="310"
      />
    )
  }
  <h1 class="text-4xl font-bold mb-8">{post.title}</h1>
  <div class="prose">
    <p>Published: {new Date(post.publishedAt).toLocaleDateString()}</p>
    {Array.isArray(post.body) && <PortableText value={post.body} />}
  </div>
</main>
```





# Deploying Studio and inviting editors

## Deploy your Studio with Sanity

![Video](https://stream.mux.com/CvYhCQr8e1oZt98NW202BZLLNv376VVKc)

In your Studio directory (`studio-hello-world`) run the following command to deploy your Sanity Studio.

```sh
npm run deploy
```

## Invite a collaborator

Now that you’ve deployed your Studio, you can optionally invite a collaborator to your project. Navigate to: `https://www.sanity.io/manage/project/{{PROJECT_ID}}/members`.

They will be able to access the deployed Studio, where you can collaborate together on creating content.





# Agent Actions

#### Jump right in

[Generate quick start](/docs/agent-actions/generate-quickstart)

[Transform quick start](/docs/agent-actions/transform-quickstart)

[Translate quick start](/docs/agent-actions/translate-quickstart)



#### Fundamentals

[Agent Actions introduction](/docs/agent-actions/introduction)

[Creating instructions](/docs/agent-actions/instructions)

[Operations](/docs/agent-actions/operations)



#### Additional resources

[HTTP reference](/docs/http-reference/agent-actions)





# Introduction

> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

Agent Actions let you programmatically run schema-aware AI instructions to create and modify Sanity documents. You can run instructions from anywhere you can execute code, such as Sanity Functions, custom components, webhook listeners, CI/CD pipelines, migration scripts, and more.

- Add AI-assisted content suggestions.
- Generate draft documents with new content.
- Generate images based on fields within your document.
- Translate documents automatically or on demand.
- See live AI presence so your editors know when the Instruct API works on a document.

You can create powerful AI-driven workflows, by combining Agent Actions with Functions, Content Releases, the Actions API, and the rest of Content Lake's APIs.

#### Get started with the actions

[Transform quick start](/docs/agent-actions/transform-quickstart)

[Generate quick start](/docs/agent-actions/generate-quickstart)

[Translate quick start](/docs/agent-actions/translate-quickstart)

[Prompt quick start](/docs/agent-actions/prompt-quickstart)

[Patch quick start](/docs/agent-actions/patch-quickstart)



## Requirements

- You'll need a place to execute code, such as a custom component, cloud functions, a webhook listener, or any service that can run the JavaScript client or initiate HTTP requests.
- Sanity client (`@sanity/client`) version 7.1.0 or later and API version vX.

### Presence support

Install and [enable the AI Assist plugin](/docs/ai-assist) for Sanity Studio to enable presence support.

### Image and reference support

Some Agent Actions, like Generate, can create images and connect references. To enable automation for image and reference fields, further setup is required. 

[Create images with Agent Actions](/docs/agent-actions/agent-actions-image-generation)

[Enable references in Generate](/docs/agent-actions/generate-add-references)



## Core Concepts

### Get to know the actions

Agent Actions all share the same core, but are specialized for different uses.

#### Generate

When you want to create brand new content, you want Generate. It excels at creating new content based on the information you pull in from your existing Sanity documents.

- Create full, structured documents in a single command.
- Reference multiple documents to use as the source for new documents.
- Generate images and make reference connections to existing documents.

[Generate quick start](/docs/agent-actions/generate-quickstart)

[Generate cheat sheet](/docs/agent-actions/generate-cheatsheet)



#### Transform

When you need to modify existing documents, Transform can walk through your document fields and make changes. It keeps the formatting and style, while only making the changes you tell it to make.

- Change a document's tone.
- Rename a product across your entire site.

[Transform quick start](/docs/agent-actions/transform-quickstart)

[Transform cheat sheet](/docs/agent-actions/transform-cheatsheet)



#### Translate

A specialized version of Transform, Translate is designed with internationalization in mind. It supports both document-level translation and field-level translation. It's a fast way to translate documents into multiple languages.

[Translate quick start](/docs/agent-actions/translate-quickstart)

[Translate cheat sheet](/docs/agent-actions/translate-cheatsheet)



#### Prompt

Need to make prompts to an LLM without reaching for another service? Prompt allows you to use your content in Sanity to make requests, then process that information however you like.

[Prompt quick start](/docs/agent-actions/prompt-quickstart)



#### Patch

Agent Actions are schema-aware, which lets them validate and safely modify your documents. Now you can use this same approach with Patch. No LLMs involved.

[Patch quick start](/docs/agent-actions/patch-quickstart)



### Operations

Should an action create a new document? Should it edit an existing one? Operations tell each Action Action how to act on your data.

[Operations](/docs/agent-actions/operations)



### Instructions

Agent Actions use the concept of instructions to describe what tasks you want them to perform. These are combined with each action's configuration. 

[Creating instructions](/docs/agent-actions/instructions)



### Actions understand your schema

Actions know about your content model, which allow them to map content accurately to your documents and fields. However, you must deploy an up-to-date schema version to make this work.

If you're already hosting Studio at Sanity, you're all set. Run `sanity deploy` from your project to ensure the latest schema is uploaded. If you're hosting Studio elsewhere, you can manually deploy the schema.

Refer to any of the [quick start guides](/docs/agent-actions/generate-quickstart) for instructions on deploying your schema.

### How Agent Actions compare to AI Assist

[AI Assist](/docs/ai-assist) is a plugin for Sanity Studio. The intended user is the content creator or Studio manager. While powerful, the nature of the manual, on-demand triggering of AI workflows is limited for cases where automation would be more efficient. Manual workflows don't scale well for batch operations, can't be triggered by external events, and require human intervention for each execution.

Agent Actions let you trigger, or invoke, AI workflows from anywhere you can make an API call or run the Sanity client. They also allow you to supply additional contextual information beyond what lives in a single Sanity document or field.

## Usage and spending limits

Agent Actions usage is shared across the organization. For details on your plan's limits, see the [pricing page](https://www.sanity.io/pricing). 

You can set spending limits and view your remaining budget in your Organization's settings in Manage. 

49. Navigate to [Manage](https://sanity.io/manage), or run `sanity manage` from the CLI.
49. Select your Organization.
49. Navigate to Settings, then Spending Limits.

![The spending limits settings page located in your Organization settings.](https://cdn.sanity.io/images/3do82whm/next/a1b82171bdfda2cce6213e02166a38d9929c9f15-2576x1120.png)

## Limitations

The following field types are not supported:

- [File](/docs/file-type)

The following field types are supported, but with limitations:

- [Slug](/docs/slug-type): Agent Actions don't perform uniqueness validation to check if the slug conflicts with others.
- [URL](/docs/url-type): Agent Actions only write to this field type if the instruction includes links.

The following types require additional setup:

- [Image](/docs/image-type) ([See the configuration guide](/docs/agent-actions/agent-actions-image-generation))
- [Reference](/docs/reference-type) ([See the configuration guide](/docs/agent-actions/generate-add-references). Requires the [Embeddings API](/docs/compute-and-ai/embeddings-index-api-overview))
- [Date](/docs/date-type) and [Datetime](/docs/datetime-type): To enable these fields, use the `localeSettings` in the request. [Learn more in the configuration guide](/docs/agent-actions/agent-actions-date-support).

Agent Actions won't write to explicitly hidden or readOnly fields. These are fields with `readOnly: true` and `hidden: true` defined in the schema. They do support conditionally readOnly or hidden fields. See the [common patterns guide](https://www.sanity.io/docs/agent-actions/agent-action-cheatsheet#e11a6752f9f7) for more details.



# Operations

Agent Actions use the `targetDocument` property to establish how they write to your dataset. 

> [!TIP]
> Generate and Patch support initial values
> Generate and Patch operations also allow for the initialValues property. With it, you can set initial values similar to how you would with initial values templates. See the Generate examples below.

## Default behavior

By default, **Agent Actions will never mutate a published document**. Whenever you supply a published ID, the action creates a draft first before applying any changes. If a draft already exists, it will use the existing draft as the source. 

To change this behavior, you can supply `forcePublishedWrite: true` to the action request. For example:

```
await client.agent.action.transform({
  schemaId: 'sanity.workspace.schema.default',
  documentId: 'publishedId',
  targetDocument: {
    operation: 'edit',
    _id: 'publishedId'
  },
  forcePublishedWrite: true,
  instruction: 'Replace "Create" with "Canvas"',
})
```

Documents that use `liveEdit: true` in their schema are treated as `forcePublishedWrite: true` by default.

For content release version documents, the operations will only create version documents when a version ID is paired with an operation that creates a new document.

> [!TIP]
> Check the returned _id for clarity
> You can check the returned _id of the response as needed to confirm the document's published state. This pairs well with the helper functions in @sanity/id-utils to confirm various document types.

## Operation types

### `edit`

Requires an `_id` for an existing document. This is the verbose version of omitting  `targetDocument` and only relying on `documentId`.

```
await client.agent.action.transform({
  schemaId: 'sanity.workspace.schema.default',
  documentId: 'drafts.id',
  targetDocument: {
    operation: 'edit',
    _id: 'drafts.id'
  },
  instruction: 'Replace "Create" with "Canvas"',
})
``````
await client.agent.action.generate({
  schemaId: 'sanity.workspace.schema.default',

  // In generate, `edit` is equivalent to using `documentId`
  // without a targetDocument.
  targetDocument: {
    operation: 'edit',
    _id: 'drafts.id',
    
  },
  instruction: 'Create a blog post about Sanity, the Content Operating System',
})
``````
await client.agent.action.translate({
  schemaId: 'sanity.workspace.schema.default',
  documentId: 'fromLanguageDoc.id',
  
  targetDocument: {
    operation: 'edit',
    _id: 'draft.id'
  },
  
  fromLanguage: { id: 'en-GB',title: 'English' },
  toLanguage: { id: 'no-NB', title: 'Norwegian Bokmål' },
}
``````
await client.agent.action.patch({
  schemaId: 'sanity.workspace.schema.production',
  targetDocument: {
    operation: 'edit',
    _id: 'documentId'
  },
  target: {path: 'title', operation: 'set', value: 'New title'}
})
```

### `create`

The `_id` is optional. If omitted, a draft document is created. You can provide a valid version ID as the `_id` to create a release version of a document.

```
await client.agent.action.transform({
  schemaId: 'sanity.workspace.schema.default',
  documentId: 'document-id',
  targetDocument: {
    operation: 'create',
    _id: 'new-document-id' // optional
  },
  instruction: 'Replace "Create" with "Canvas"',
})
``````
await client.agent.action.generate({
  schemaId: 'sanity.workspace.schema.default',
  
  targetDocument: {
    operation: 'create',
    _type: 'post',
    _id: '<document-id>', // optional
    // Use initialValues to set fields when the document is created.
    initialValues: {
      author: {
        _type: 'reference',
        _ref: '<author-reference-id>'
      }
    }
    
  },
  instruction: 'Create a blog post about Sanity, the Content Operating System',
})
``````
await client.agent.action.translate({
  schemaId: 'sanity.workspace.schema.default',
  documentId: 'fromLanguageDoc.id',
  
  targetDocument: {
    operation: 'create',
    _id: 'toLanguage.id' // optional
  },
  
  fromLanguage: { id: 'en-GB',title: 'English' },
  toLanguage: { id: 'no-NB', title: 'Norwegian Bokmål' },
}
``````
await client.agent.action.patch({
  schemaId: 'sanity.workspace.schema.production',
  targetDocument: {
    operation: 'create',
    type: '_documentType',
    _id: 'documentId'
  },
  target: {path: 'title', operation: 'set', value: 'New title'}
})
```

### `createOrReplace`

If you provide an existing `_id`, the new document will override it. If the provided `_id` doesn't exist, the Agent Action will create a new document with that ID.

```
await client.agent.action.transform({
  schemaId: 'sanity.workspace.schema.default',
  documentId: 'document-id',
  targetDocument: {
    operation: 'createOrReplace',
    _id: 'new-document-id'
  },
  instruction: 'Replace "Create" with "Canvas"',
})
``````
await client.agent.action.generate({
  schemaId: 'sanity.workspace.schema.default',
  
  targetDocument: {
    operation: 'createOrReplace',
    _type: 'post',
    _id: '<document-id>', // if the ID doesn't exist, a new document is created with the ID. If it does, the new document will replace the old.
    // Optional: Use initialValues to set fields when the document is created.
    initialValues: {
      author: {
        _type: 'reference',
        _ref: '<author-reference-id>'
      }
    }
    
  },
  instruction: 'Create a blog post about Sanity, the Content Operating System',
})
``````
await client.agent.action.translate({
  schemaId: 'sanity.workspace.schema.default',
  documentId: 'fromLanguageDoc.id',
  
  targetDocument: {
    operation: 'createOrReplace',
    _id: 'toLanguage.id'
  },
  
  fromLanguage: { id: 'en-GB',title: 'English' },
  toLanguage: { id: 'no-NB', title: 'Norwegian Bokmål' },
}
``````
await client.agent.action.patch({
  schemaId: 'sanity.workspace.schema.production',
  targetDocument: {
    operation: 'createOrReplace',
    type: '_documentType',
    _id: 'documentId'
  },
  target: {path: 'title', operation: 'set', value: 'New title'}
})
```

### `createIfNotExists`

If the provided `_id` does not exist, a document is created using the `documentId` as the source. If it does exist, it used the provided `_id` document as the source.

```
await client.agent.action.transform({
  schemaId: 'sanity.workspace.schema.default',
  documentId: 'document-id',
  targetDocument: {
    operation: 'createIfNotExists',
    _id: 'new-document-id'
  },
  instruction: 'Replace "Create" with "Canvas"',
})
``````
await client.agent.action.generate({
  schemaId: 'sanity.workspace.schema.default',
  
  targetDocument: {
    operation: 'createIfNotExists',
    _type: 'post',
    _id: '<document-id>', // if the ID doesn't exist, a new document is created with the ID.
    // Optional: Use initialValues to set fields when the document is created.
    initialValues: {
      author: {
        _type: 'reference',
        _ref: '<author-reference-id>'
      }
    }
    
  },
  instruction: 'Create a blog post about Sanity, the Content Operating System',
})
``````
await client.agent.action.translate({
  schemaId: 'sanity.workspace.schema.default',
  documentId: 'fromLanguageDoc.id',
  
  targetDocument: {
    operation: 'createIfNotExists',
    _id: 'toLanguage.id'
  },
  
  fromLanguage: { id: 'en-GB',title: 'English' },
  toLanguage: { id: 'no-NB', title: 'Norwegian Bokmål' },
}
``````
await client.agent.action.patch({
  schemaId: 'sanity.workspace.schema.production',
  targetDocument: {
    operation: 'createIfNotExists',
    type: '_documentType',
    _id: 'documentId'
  },
  target: {path: 'title', operation: 'set', value: 'New title'}
})
```



# Targets and paths

In addition to [document operations](/docs/agent-actions/operations), many Agent Actions use `target` to identify specific parts of a document to interact with.

If you haven't already, complete one of the quick start guides. The examples in this document use a mix of Generate, Patch, Transform, and Translate code.

#### Quick starts

[Generate quick start](/docs/agent-actions/generate-quickstart)

[Transform quick start](/docs/agent-actions/transform-quickstart)

[Translate quick start](/docs/agent-actions/translate-quickstart)

[Patch quick start](/docs/agent-actions/patch-quickstart)



## Target

Agent Actions that create or modify documents include a `target` property that determines which sections of the document will be affected by the instruction.

In actions where the target is optional, omitting it will set the document root as the target.

Actions accept a single `target` object or an array of `target` objects to support editing multiple parts of the document.

### Path

A target can be as simple as providing an individual [JSONMatch-style](/docs/content-lake/json-match) path to a field. For example:

```
await client.agent.action.generate({
  schemaId: "sanity.workspace.schema.default",
  documentId: "<document-id>",
  instruction: `Rewrite the title to something more catchy`,
  target: {
    path: "title",
  }
});
```

Or even multiple paths, for example:

```
await client.agent.action.generate({
  schemaId: "sanity.workspace.schema.default",
  documentId: "<document-id>",
  instruction: `Write an article about cats`,
  target: [
    { path: "title" }, // target title
    { path: ["body", "description"] } // target body.description
  ]
});
```

### `maxPathDepth`

By default, Agent Actions will traverse 4 levels deep, starting at the root of the document or the target you define, to mutate your data. Depth is calculated based on the path depth. For example:

- `title` has a depth of 1.
- `array[_key="no"].title` has depth of 3.

```
await client.agent.action.generate({
  schemaId: "sanity.workspace.schema.default",
  documentId: "<document-id>",
  instruction: `Write an article about cats`,
  target: [
    { path: "title" },
    { path: ["body", "description"], maxPathDepth: 2 }
  ]
});
```

### `include` and `exclude`

Targets also accept `include` and `exclude` arrays of paths or targets. By default, all children up to the maxPathDepth are included. Setting either `include` or `exclude` will invalidate the other. For example, if you set include to a field, all sibling fields will be excluded.

```
await client.agent.action.generate({
  schemaId: 'default-schema',
  targetDocument: { operation: 'create', _type: 'article'},
  instruction: 'Stuff about dogs',
  target: {include: ['title', 'description']},
  // target: {exclude: ['title', 'description']} // or exclude
});
```

You can also combine these with path to refine a target.

```
await client.agent.action.generate({
  schemaId: 'default-schema',
  targetDocument: { operation: 'create', _type: 'article'},
  instruction: 'Stuff about dogs',
  target: {path: ['objectField'], include: ['title', 'description']}
});
```

Include also accepts the same shape as `target`, so you can recursively nest target configurations if needed.

```
await client.agent.action.generate({
  schemaId: 'default-schema',
  targetDocument: { operation: 'create', _type: 'article'},
  instruction: 'Stuff about dogs',
  target: {
    path: 'objectField', 
    include: [
      {path: ['nestedObject', 'title']}, 
      {path: ['otherObject', 'deeplyNested']} 
    ]
  }
});
```

### `types`

The `types` property is an object that accepts either `include` or `exclude`. These are mutually exclusive, and let you define an array of types to include or exclude from the target.

```
await client.agent.action.generate({
  schemaId: 'default-schema',
  targetDocument: { operation: 'create', _type: 'article'},
  instruction: 'Stuff about dogs',
  target: {
    path: 'objectField', 
    types: {
      include: ['string', 'text']
    }
  }
});
```

### `operation`

Generate and Patch support the `operation` property to influence how they should affect the target field or fields.

- `set`: Overwrites and replaces the value of the field. For Patch, `set` will merge objects when targeting an object. Otherwise it will overwrite as expected.
- `append`:- Array fields: Appends new items to the end of the array.
- String fields: Adds the new content to the end of the existing content.
- Text fields: Adds the new content to a new line at the end of the existing content.
- Number fields: Adds (+) the new number to the existing number, resulting in the sum as the final value.
- All other fields will use `set` instead of append.


- `mixed`: (default) Applies `set` to non-array fields, and `append` to array fields.
- `unset`: (Patch only) Removes the value of the target field.
- `image-description`: (Transform only) Lets the transform action select an image asset, select a field, and describe the image in the field. You can also set an `imageUrl` to describe remote images. See usage below.  

Nested fields inherit the operation from their parent. Use `include` to perform per-path overrides.

```
await client.agent.action.generate({
  schemaId: 'default-schema',
  targetDocument: { operation: 'create', _type: 'article'},
  instruction: 'Add more stuff about dogs',
  target: {path: ['title'], operation: 'append'}
});
```

#### Patch values

When using `operation` with patch, a `value` or array of values is required for each operation type except `unset`.

```
await client.agent.action.patch({
  schemaId: 'default-schema',
  documentId: 'docId',
  target: {
    path: ['object', 'title'],
    operation: 'set',
    value: 'New title'
  }
});
```

#### `image-description` usage

Unlike the other operations, `image-description` accepts an object instead of the operation type as a string. This feature works for any `text`, `string`, or Portable Text (`array` with `block`) field.

For adjacent sources, where the field you want Transform to write to sits alongside the asset—like in a parent wrapper—you only need to set the type. Transform will infer which image you want to describe.

For sources that aren't colocated with the field, use `sourcePath` as shown in the example below to set the path to the image asset.

```
await client.agent.action.transform({
  schemaId: '_.schemas.default',
  documentId: 'document-id',
  instruction: 'Describe the image in one to two sentences.',
  target: [{
    path: ['image', 'alt'],
    operation: {
      type: 'image-description'
    }
  }]
});
``````
await client.agent.action.transform({
  schemaId: '_.schemas.default',
  documentId: 'document-id',
  instruction: 'Describe the image in one to two sentences.',
  target: [{
    path: ['content','description'],
    operation: {
      type: 'image-description',
      sourcePath: ['image', 'asset']
    }
  }]
});
``````
await client.agent.action.transform({
  schemaId: '_.schemas.default',
  documentId: 'document-id',
  instruction: 'Describe the image in one to two sentences.',
  target: [{
    path: ['content','description'],
    operation: {
      type: 'image-description',
      imageUrl: "https://www.sanity.io/static/images/favicons/android-icon-192x192.png?v=2"
    }
  }]
});
```

As with other Transform operations, you can apply target-level instructions to the  operation if you need to change the instruction on a per-path basis.

#### Additional target resources

[Agent Actions patterns](/docs/agent-actions/agent-action-cheatsheet)

[Generate cheat sheet](/docs/agent-actions/generate-cheatsheet)



### `instruction` / `styleGuide`

Some actions also allow target-level instructions. Transform uses `instruction` and Translate uses `styleGuide`. These operate the same way as their top-level counterparts, and have access to any `instructionParams` or `styleGuideParams` from the request.

```
await client.agent.action.translate({
  schemaId: 'default-schema',
  documentId: 'drafts.id', 
  
  fromLanguage: { id: 'en-GB',title: 'English' },
  toLanguage: { id: 'no-NB', title: 'Norwegian Bokmål' },
  
  languageFieldPath: ['language'],
  
  styleGuide: 'Follow the vibe when translating: $vibe',
  styleGuideParams: {
    vibe: { type: 'field', path: ['vibe']}
  },
  target: [
    {path: 'title'}, // Uses the default style guide
    {path: 'description', styleGuide: 'Only lowercase.' }, // Uses its own style guide
  ]
})
``````
await client.agent.action.transform({
  schemaId: 'default-schema',
  documentId: 'drafts.id', 
  instruction: 'Make everything all-caps.',
  target: [
    {path: 'title'}, // Uses the default instruction
    {path: 'description', instruction: 'Use only lowercase' }, // Uses its own instruction
  ]
})
```



# Creating instructions

Instructions tell Agent Actions how to manipulate your data. Some actions, like Generate, rely almost entirely on your instructions and your schema. Others, like Translate use instructions to further refine their default behavior. If you've used other AI tools, instructions are like prompts.

There are two types of instructions:

- `instruction`: Used by Generate and Transform. This pairs with the `instructionParams` option to pass data into the instruction.
- `styleGuide`: Used by Translate. This pairs with the `styleGuideParams` option to pass data into the instruction`.`

Aside from the difference in syntax, the concepts are the same for both `instruction` and `styleGuide`.

> [!TIP]
> Sanity Client
> The code examples on this page use client to refer to a configured @sanity/client. For details on configuring the client, refer to the client documentation or one of the Agent Action guides, like the Generate quick start.

## Basic instructions

At their most basic, instructions are a string telling the Agent Action what you want it to do.

```
await client.agent.action.generate({
  // Replace with your schema ID
  schemaId: "sanity.workspace.schema.default",

  // Tell the client to create a new 'movie' document type.
  targetDocument: { operation: "create", _type: "movie" },

  // Provide an instruction, or prompt.
  instruction: "Create a movie about cats.",
});
``````
await client.agent.action.transform({
  // Replace with your schema ID
  schemaId: "sanity.workspace.schema.default",

  // Tell the client the ID of the document to transform.
  documentId: "<document-id>",

  // Provide an instruction, or prompt.
  instruction: "Change all instances of 'Alien' to 'Lifeform from another planet'. Match the case of the existing text.",
});
``````
await client.agent.action.translate({
  // Replace with your schema ID
  schemaId: "sanity.workspace.schema.default",

  // Tell the client the ID of the document to use as the source.
  documentId: "<document-id>",
  
  // Set the operation mode
  targetDocument: { operation: "create" },

  // Set the 'from' and 'to' language
  fromLanguage: {id: "en-US", title: "English"},
  toLanguage: {id: "el-GR", title: "Greek"},

  // Use `styleGuide` instead of instruction for Translate
  styleGuide: "Use a formal tone when translating.",
});
```

## Adding `instructionParams`

You can provide additional information to the instructions by defining and passing `instructionParams` (and `styleGuideParams` for Translate).

Here's an example that uses a basic constant value.

```
await client.agent.action.generate({
  // Replace with your schema ID
  schemaId: "sanity.workspace.schema.default",

  // Tell the client to create a new 'movie' document type.
  targetDocument: { operation: "create", _type: "movie" },

  // Provide an instruction, or prompt.
  instruction: "Create a movie about $topic.",

  instructionParams: {
    topic: 'cats'
  }
});
``````
await client.agent.action.transform({
  // Replace with your schema ID
  schemaId: "sanity.workspace.schema.default",

  // Tell the client the ID of the document to transform.
  documentId: "<document-id>",

  // Provide an instruction, or prompt.
  instruction: "Change all instances of '$old' to '$new'. Match the case of the existing text.",

  instructionParams: {
    old: 'Alien',
    new: 'Lifeform from another planet'
  }
});
``````
await client.agent.action.translate({
  // Replace with your schema ID
  schemaId: "sanity.workspace.schema.default",

  // Tell the client the ID of the document to use as the source.
  documentId: "<document-id>",
  
  // Set the operation mode
  targetDocument: { operation: "create" },

  // Set the 'from' and 'to' language
  fromLanguage: {id: "en-US", title: "English"},
  toLanguage: {id: "el-GR", title: "Greek"},

  // Use `styleGuide` instead of instruction for Translate
  styleGuide: "Use a $tone tone when translating.",
  styleGuideParams: {
    tone: 'formal'
  }
});
```

There are two essential things to note about this example:

- The parameter names (*topic*, *old*, *new*, and *tone*) can be any variable name.
- The parameter name is passed into the instruction by prepending `$`.

This example was a constant value, but there multiple types of parameters.

### Constant

You've already seen the shorthand version of the `constant` parameter type in the previous example. Here's the full version:

```
await client.agent.action.generate({
  // ...
  instruction: "Write the details for a movie about $topic",
  instructionParams: {
    topic: {
      type: "constant",
      value: "cats"
    }
  }
})
``````
await client.agent.action.translate({
  // ...
  styleGuide: "Use a $tone tone when translating.",
  styleGuideParams: {
    tone: {
      type: "constant",
      value: "formal"
    }
  }
});
```

### Field

The field type parameter picks the value from a field in the source document (if one exists), then passes that to the instruction with the $-prefixed parameter syntax. 

You can also provide an optional `documentId` along with the field path to pick a value from any document in your dataset.

```
await client.agent.action.generate({
  // ...
  instruction: "Write the details for a movie about $topic",
  instructionParams: {
    topic: {
      type: "field",
      path: "movie_idea",
      documentId: '123456' // optional. Otherwise, the source document set by the top level documentId or the ID in an edit operation is used.
    }
  }
})
``````
await client.agent.action.translate({
  // ...
  styleGuide: "Use a $tone tone when translating.",
  styleGuideParams: {
    tone: {
      type: "field",
      path: "formality",
      documentId: '123456' // optional
    }
  }
});
```

The path should be a full path to the field in the document. For examples of paths, see the [common patterns guide](/docs/agent-actions/agent-action-cheatsheet).

### Document

The document sets the parameter to the full contents of a document. For larger documents, this might become too large and cause issues with the accuracy of the instruction.

```
await client.agent.action.generate({
  // ...
  instruction: "Develop a new movie. Use these details to generate the concept: $background ",
  instructionParams: {
    background: {
      type: "document",
      documentId: '123456'
    }
  }
})
``````
await client.agent.action.translate({
  // ...
  styleGuide: "Use the following company guidelines when translating: $guidelines",
  styleGuideParams: {
    guidelines: {
      type: "document",
      documentId: '123456'
    }
  }
});
```

This can be useful for passing in singleton-style documents, or using existing documents as background context.

### GROQ

If field and document aren't powerful enough, you can also write GROQ queries to populate the contents of your instruction parameters.

```
await client.agent.action.generate({
  // ...
  instruction: "Develop a new movie. Use these details to generate the concept: $background ",
  instructionParams: {
    background: {
      type: "groq",
      query: "*[_id=$id]",
      params: {
        id: "123456"
      }
    }
  }
})
``````
await client.agent.action.translate({
  // ...
  styleGuide: "Use the following company guidelines when translating: $guidelines",
  styleGuideParams: {
    guidelines: {
      type: "groq",
      query: "*[_id=$id]",
      params: {
        id: "123456"
      }
    }
  }
});
```

GROQ queries accept filters and projections, and can receive their own parameters as seen in the example.



## Multiple parameters

The examples so far have only been single parameter instructions, but you can mix any combination of parameter types to build your instruction.

```
await client.agent.action.generate({
  schemaId: "sanity.workspace.schema.default",
  targetDocument: { operation: "create", _type: "movie" },

  instruction: "Create a movie with the title: $title. Use the following details to come up with a synopsis and characters: $backgroundDetails",

  instructionParams: {
    title: {
      type: "constant",
      value: "Sanity: The Content Operating System"
    },
    backgroundDetails: {
      type: "document",
      documentId: "32156481"
    }
  },
});
```

## Per-path instructions

Some Agent Actions support per-patch instructions. For example, you can provide a top-level instruction for all fields, and then specific instructions for individual fields. Learn more about this approach in the [targets and paths documentation](/docs/agent-actions/targets-paths).

## Instruction size considerations

These instruction settings are only part of what Agent Actions use when creating content. They also know about your schema and any document used as the source. 

As instructions get larger, they can exceed the max size accepted by the large language models. If you're experiencing inconsistent or unexpected results, try reducing the information you pass in to the instructions.



# Common patterns

Agent Actions offer an interface to enhance Sanity documents with the use of large language models (LLMs). This document showcases a collection of common patterns and concepts that apply to all Agent Actions.

Prerequisites:

- Complete the quick start for one or more of the Agent Actions.
- `@sanity/client` v7.1.0 or later and an environment to run client requests. 
- API version vX or later for any requests using Agent Actions.

#### Quick starts

[Generate quick start](/docs/agent-actions/generate-quickstart)

[Transform quick start](/docs/agent-actions/transform-quickstart)

[Translate quick start](/docs/agent-actions/translate-quickstart)



Many examples in this document use `@sanity/client` and expect that you've installed and configured it for your project. If your client is named something other than `client`, update the code examples accordingly. 

Here's an example of the client implementation:

```typescript
// client.ts
import { createClient } from "@sanity/client";
export const client = createClient({
  projectId: '<project-id>',
  dataset: '<dataset-name>',
  useCdn: 'true',
  apiVersion: 'vX',
  token: '<read-write-token>'
})
```

Then, import `client` in your code before using the examples below.

> [!NOTE]
> Multiple action examples
> Some examples in this guide are for specific Agent Actions, so the code may differ slightly. For instance, Translate doesn't have an instruction concept in the same way Generate does, but the techniques in each example are the same regardless of action.

## Use `noWrite` to avoid mutations

The `noWrite` property prevents the instruction from writing changes to your dataset. This is useful when creating in-memory documents, previewing changes, or working with multiple requests before making a final write. You could even use it to combine multiple Agent Actions.

All Agent Actions support the `noWrite` option.

```typescript
const response = await client.agent.action.generate({
  schemaId: "sanity.workspace.schema.default",
  noWrite: true,
  targetDocument: {operation: 'create', _type: 'movie'},
  instruction: "Write the details for a movie titled $title.",
  instructionParams: {
    title: { type: "constant", value: "Sanity: The Content Operating System" },
  },
});
console.log(response);

``````
const response = await client.agent.action.transform({
  schemaId: "sanity.workspace.schema.default",
  noWrite: true,
  documentId: '<source-document-id>',
  instruction: "Replace every instance of 'Create' with 'Canvas'",
});
console.log(response);

``````
const response = await client.agent.action.translate({
  schemaId: 'default-schema',
  documentId: '<source-document-id>', 
  noWrite: true,
  fromLanguage: { id: 'en-GB',title: 'English' },
  toLanguage: { id: 'no-NB', title: 'Norwegian Bokmål' },
})
```

Instead of creating or modifying a document in your dataset, this code returns the document to the `response` constant and logs it to the console.

Keep in mind that these requests still count against your usage limits. 

## Enable read/write on conditional fields

By default, Agent Actions ignore [conditional](/docs/studio/conditional-fields) `readOnly` and `hidden` fields. You can allow actions to interact with these fields by setting the `conditionalPaths` parameter. The examples below use the `generate` syntax, but the `conditionalPaths` configuration is the same across all actions.

To enable access to all hidden and readOnly fields across your schema, use the following to change the default behavior:

```typescript
await client.agent.action.generate({
  schemaId: 'sanity.workspace.schema.default',
  targetDocument: {operation: 'create', _type: '<document-type>'},
  instruction: `<insert instruction here>`,
  instructionParams: { ... },
  conditionalPaths: {
    defaultReadOnly: false,
    defaultHidden: false
  }
})
```

You can also limit read/write to specific `paths`. Add additional paths to the array as needed.

```typescript
await client.agent.action.generate({
  schemaId: 'sanity.workspace.schema.default',
  targetDocument: {operation: 'create', _type: '<document-type>'},
  instruction: `<insert instruction here>`,
  instructionParams: { ... },
  conditionalPaths: {
    paths: [
      {
        path: 'secretPathName',
        readOnly: false,
        hidden: false,
      }
    ]
  }
})
```

## Asynchronously modify multiple documents

The `async` parameter helps initiate instructions and move on. Asynchronous calls to actions return a document ID rather than the complete document shape. In this example:

- We loop through the IDs of documents from a GROQ request.
- `async` is set to true to enable asynchronous requests.
- The instruction rewrites the title of each document.

```typescript
const ids = await client.fetch(`*[_type == 'movie' ][0...5] { _id }`);

for (const id of ids) {
  await client.agent.action.generate({
    schemaId: "sanity.workspace.schema.default",
    documentId: id._id,
    instruction: `Re-imagine the title, $title, so that it is more engaging and interesting.`,
    async: true,
    path: "title",
    instructionParams: {
      title: {
        type: "field",
        path: "title",
      },
    },
  });
}
```

> [!TIP]
> Protip
> We're calling this an asynchronous call, but we're also using await. That's because the asynchronous call happens behind the scenes. We aren't waiting on the AI to finish and Content Lake to update. Instead, we're waiting on the underlying request to Sanity to respond that it initiated those actions.

Note that you can't combine `async` and `noWrite`, as `noWrite` would require the request to wait for a response from the AI.

## Target specific fields 

Restrict the fields that actions can write to by setting a single `path` or multiple `include` fields in the `target` parameter.

To write to a single path, or all child paths of a single parent, use the `target` property with `path`.

```typescript
await client.agent.action.generate({
  schemaId: "sanity.workspace.schema.default",
  targetDocument: {operation: 'create', _type: 'movie'},
  instruction: `Your instruction here.`,
  // ... other properties
  target: {
    path: "body", // set to whichever field or fieldset you like. Ex. 'title', 'name', etc.
  }
});
```

To define specific fields, use `include`. They will be relative to a path, if set, or the document if not set. In the example below, they are relative to the document.

```typescript
await client.agent.action.generate({
  schemaId: "sanity.workspace.schema.default",
  targetDocument: {operation: 'create', _type: 'movie'},
  instruction: `Your instruction here.`,
  // ... other properties
  target: {
    include: ["title", "overview", "poster"],
  },
});
```

You can also do the same to exclude any paths you want to block the instruction from mutating.

```typescript
await client.agent.action.generate({
  schemaId: 'sanity.workspace.schema.default',
  targetDocument: {operation: 'create', _type: 'movie'},
  instruction: `Your instruction here.`,
  // ... other properties
  target: {
    exclude: ['humanOnlyField']
  }
})
```

### Target patterns

The following are an assortment of examples using `target` and its options. For Transform and Translate, omit the Generate instruction and add the source `documentId` property.

```typescript
/*
using path
this sets 'title' field
*/
{
 targetDocument: {operation: 'create', _type: 'article'},
 schemaId: 'default-schema',
 instruction: 'A title for an article about dogs',
 target: {path: ['title']}
}

/*
using include
 this sets:
 - title
 - description 
 */
{
 targetDocument: {operation: 'create', _type: 'article'},
 schemaId: 'default-schema',
 instruction: 'Stuff about dogs',
 target: {include: ['title', 'description']},
}

/*
 this sets:
 - objectField.title
 - objectField.description 
*/
{
 targetDocument: {operation: 'create', _type: 'article'},
 schemaId: 'default-schema',
 instruction: 'Stuff about dogs',
 target: {path: ['objectField'], include: ['title', 'description']}
}


/*
multiple target paths
 this sets:
 - objectField.title
 - objectField.description
 - people[_key=="someKey"].name //ie, the name of a single item in the people array 
*/
{
 targetDocument: {operation: 'create', _type: 'article'},
 schemaId: 'default-schema',
 instruction: 'Stuff about dogs',
 target: [
    {path: ['objectField'], include: ['title', 'description']},
    {path: ['people', {_key: 'someKey'}], include: ['name']}
 ]
}

/* 
Deeply nested fields from a common target path.
This sets:
 - objectField.nestedObject.title
 - objectField.otherObject.deeplyNested 
   - all its children(assuming deeplyNested is an object)
*/
{
 targetDocument: {operation: 'create', _type: 'article'},
 schemaId: 'default-schema',
 instruction: 'Stuff about dogs',
 target: {
	 path: 'objectField', 
	 include: [
		 {path: ['nestedObject', 'title']}, 
		 {path: ['otherObject', 'deeplyNested']} 
	 ]
 }
}
```





# Enable references

> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

Agent Actions can populate reference fields with the help of the AI Assist studio plugin and the [Embeddings Index API](/docs/compute-and-ai/embeddings-index-api-overview). This guide will help you enable related content references for instructions.

Prerequisites:

- Complete the [Generate quick start](/docs/agent-actions/generate-quickstart).
- An embeddings index connected to your project and dataset. [Follow the setup process](/docs/compute-and-ai/embeddings-index-api-overview) if you haven't done so already. You can also use the [Studio plugin](https://www.npmjs.com/package/@sanity/embeddings-index-ui).
- Access to your Studio codebase.

If you've previously set up the AI Assist plugin and have been using it to generate images inside Sanity Studio, you can skip the setup and configuration steps.

## Install the AI Assist plugin

While Agent Actions don't require the Assist plugin, the plugin provides type completion and enables presence in your studio when Actions are actively mutating a document or field. 

```sh
npm install sanity@latest @sanity/assist@latest
```

Next, import and add the plugin to your studio config's `plugins` array.

```tsx
import { defineConfig } from 'sanity'
import { assist } from '@sanity/assist'
/* other imports */

export default defineConfig({
  /* other config */
  plugins: [
    /* other plugins */
    assist(),
  ]
})
```

## Enable indexing of reference fields

Generate needs to know which index to use when making connections. Add the `aiAssist.embeddingsIndex` option to any references that use the index. In our movie schema example, we've created an index called "people" that targets all documents of `_type == person`.

```typescript
 defineField({
  name: 'person',
  title: 'Person',
  type: 'reference',
  to: [{type: 'person'}],
  options: {
    aiAssist:{
      embeddingsIndex: 'people',
    }
  }
}),
```

The code above tells AI Assist and Generate to use the `people` index for this `person` reference. 

With those changes, you're now set to use Assist with references. 

## Create an instruction

Agent Actions can often intuit the needs of your instruction based on your schema, but it's also helpful to be explicit. These examples use Generate, but other actions support the same concept. First, set up your client if you haven't already.

```typescript
// instruction.ts

import { createClient } from "@sanity/client";

export const client = createClient({
  projectId: "<project-id>",
  dataset: "<datset-name>", // such as 'production'
  apiVersion: "vX",
  token: "<editor-token>",
});

```

Next, create an instruction.

```typescript
// instruction.ts
// ...client setup

await client.agent.action.generate({
  schemaId: "sanity.workspace.schema.default",
  targetDocument: {operation: 'create', _type: 'movie'},
  instruction: `
    Come up with an idea for a movie. 
    Give it a title and overview.
    Generate a poster image based on the overview and title.
    Select cast members to be involved in the movie as the cast. Give their characters names that fit the theme.
    Assign crew members to work on the movie.
  `,
});
```

The code above creates a detailed instruction that explicitly lists the steps we want it to take. Notice that it doesn't use a GROQ query to inject information about the `person` type. Because Generate knows about your schema and has access to the index, it understands there are references for cast and crew. 

If you wanted to be more explicit, perhaps to limit which people it chooses, you could combine this with a GROQ or other instruction parameters to provide more context.

Give the code, or your modified version, a try. Generate will do its best to match related references based on your instructions. Sometimes, you may need to be more explicit to help it make the best decisions.

## Related resources

[Generate images](/docs/agent-actions/agent-actions-image-generation)

[Common use cases and patterns](/docs/agent-actions/generate-cheatsheet)







# Enable image generation

> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

This guide takes you through the steps required to enable image generation with  Generate or Transform.

Prerequisites**:**

- Complete the [Generate quick start](/docs/agent-actions/generate-quickstart) or [Transform quick start](/docs/agent-actions/transform-quickstart).
- If using the AI assist plugin approach, you'll need access to your Studio codebase.

There are two ways to generate images with Agent Actions. You can either explicitly target the image's asset with the `target` property, or you can enable the AI assist plugin along with image prompts in your schema.

This guide assumes you have a configured Sanity client. The examples for both approaches use the following configuration and reference `client`:

```
import { createClient } from "@sanity/client";

export const client = createClient({
  projectId: '<project-id>',
  dataset: '<datset-name>',
  apiVersion: 'vX',
  token: '<editor-token>'
})
```

## Explicit targets

Generating images with the explicit targets approach requires instructions that directly target an image asset, but doesn't require a schema change and limits generation to agent actions.

> [!TIP]
> Image generation is asynchronous
> The API returns a success status before images are fully generated. Studios will show an in-progress status as if a user were uploading an image, but it won't be available until it completes. This results in asset references that aren't updated until after the image generation completes. Keep this in mind if you rely on the returned asset data at the time of generation.

Both Generate and Transform use the `target` key to narrow instructions down to a specific field or fields. 

To allow the actions to create or update an image, your request needs to target the image's `asset` field directly. In this example, the target provides a direct path to the asset.

```
await client.agent.action.generate({
  documentId: 'someDocumentId',
  schemaId: 'default-schema',
  instruction: 'Create an image about cats wrangling project managers.',
  target: {path: ['image', 'asset']}
})
``````
await client.agent.action.transform({
  documentId: 'someDocumentId',
  schemaId: 'default-schema',
  instruction: 'Change the image to cats wrangling project managers.',
  target: {path: ['image', 'asset']}
})
```

You can also target related fields at the same time, such as the image alt text.

```
await client.agent.action.generate({
  documentId: 'someDocumentId',
  schemaId: 'default-schema',
  instruction: 'Create an image about cats wrangling project managers.',
  target: [
    {path: ['image', 'alt']},
    {path: ['image', 'asset']}
  ]
})
``````
await client.agent.action.transform({
  documentId: 'someDocumentId',
  schemaId: 'default-schema',
  instruction: 'Change the image to an image about cats wrangling project managers.',
  target: [
    {path: ['image', 'alt']},
    {path: ['image', 'asset']}
  ]
})
```

This approach doesn't require you to write image-only instructions. You can provide instructions that apply to multiple field types. In this instance, the instruction is more generic and `include` is used alongside the asset path in `target`.

```
await client.agent.action.generate({
  documentId: 'someDocumentId',
  schemaId: 'default-schema',
  instruction: 'Create content about cats wrangling project managers.',
  target: [
    {include: ['title', 'description', 'body', 'image']},
    {path: ['image', 'asset']},
  ]
})
``````
await client.agent.action.transform({
  documentId: 'someDocumentId',
  schemaId: 'default-schema',
  instruction: 'Create content about cats wrangling project managers.',
  target: [
    {include: ['title', 'description', 'body', 'image']},
    {path: ['image', 'asset']},
  ]
})
```

Transform can perform path-level instructions. This allows you to apply specific image updates when transforming a document.

```
await client.agent.action.transform({
  documentId: 'someDocumentId',
  schemaId: 'default-schema',
  instruction: 'Create content about cats wrangling project managers.',
  target: {
    path: ['image'], 
    include: [
      {path: 'asset', instruction: 'Make it a blue dog.'},
      'alt',
    ]
  }
})
```

See additional target examples in the [common patterns guide](/docs/agent-actions/agent-action-cheatsheet).

## AI Assist

The AI assist method allows for less-specific instructions, but requires modifying your Studio's schema and installing the [AI Assist plugin](/docs/studio/install-and-configure-sanity-ai-assist).

If you have previously set up the AI Assist plugin and used it to generate images within Sanity Studio, you can skip the setup and configuration steps.

### Install the AI Assist plugin

While Generate doesn't require the Assist plugin to operate, the plugin provides type completion and enables presence in your studio when Assist is actively updating a document or field. 

```sh
npm install sanity@latest @sanity/assist@latest
```

Next, import and add the plugin to your studio config's `plugins` array.

```tsx
import { defineConfig } from 'sanity'
import { assist } from '@sanity/assist'
/* other imports */

export default defineConfig({
  /* other config */
  plugins: [
    /* other plugins */
    assist(),
  ]
})
```

### Enable instructions for image fields

Image generation in schemas works by having Instruct write an image prompt to a text field, then using the field's contents to generate the image. Having an explicit field for the prompt allows content editors to view it and make changes. One way to set this up is to create a new field as part of your images. For example

```typescript
defineField({
  name: 'image',
  title: 'Image',
  type: 'image',
  fields: [
    defineField({
      type: 'text',
      name: 'instruction',
      title: 'Image Prompt',
    })
  ],
  options: {
    hotspot: true,
    aiAssist: {
      imageInstructionField: 'instruction',
    }
  },
}),
```

This code creates a new `instruction` text field that Generate will use to write an image prompt. It also configures the AI Assist plugin and Generate to recognize that field and associate it with the parent `image`.

You must implement this pattern for any images you'd like AI Assist to interact with. 

### Deploy the updated schema

Deploy the updated schema. Do this by deploying the Studio to sanity with `sanity deploy` or with `sanity schema deploy` command.

```sh
sanity schema deploy
```

Note the resulting `schemaId` if you haven't previously used this workspace/dataset combination with Generate.

### Write an image generation instruction

Once you've stored your schema, create a script with your function.

> [!TIP]
> Protip
> For this guide, we'll follow the same pattern from the Quickstart of using Node.js to invoke a TypeScript file. We'll also rely on the starter movie schema and dataset available through sanity init. Modify document types and fields as you follow along to fit your schema.

First, set up your client.

```typescript
// instruction.ts

import { createClient } from "@sanity/client";

export const client = createClient({
  projectId: '<project-id>',
  dataset: '<datset-name>', // such as 'production'
  apiVersion: 'vX',
  token: '<editor-token>'
})
```

Next, create a new instruction.

```typescript
// instruction.ts
// ...client setup

await client.agent.action.generate({
  schemaId: 'sanity.workspace.schema.default',
  targetDocument: {operation: 'create', _type: 'movie'},
  instruction: `
    Come up with an idea for a movie. 
    Give it a title and overview.
    Generate a poster image based on the overview and title.
  `,
})
```

This instruction doesn't explicitly call out the image fields, but that's okay. Generate is good at finding fields and intuiting what you mean. If you want to be more explicit, you can set a target path. Let's look at an example that reads an existing movie document and then targets the poster image field to generate the image.

```typescript
// instruction.ts

// ..client setup

const docId = 'your-movie-id'
await client.agent.action.generate({
  schemaId: 'sanity.workspace.schema.default',
  documentId: docId,
  instruction: `
    Add a poster image for this movie.
    Use the information in $background to come up with the image.
  `,
  instructionParams: {
    background: {
      type: 'document'
    },
  },
  target: {
    path: 'poster'
  }
})
```

The code in this example does the following:

- It uses `documentId` instead of `targetDocument` to update an existing document. 
- It sets the `path` to `poster`, which is the image in the movie schema. Setting the path tells Assist to apply the instruction to that field. 
- It uses a document-type instruction parameter to query the details of the existing document.

Earlier, we mentioned that Assist writes an image prompt to the specific text field in the image, but in this example, we're targeting `poster`. This code works because the Agent Actions can navigate to children of the supplied path and use the fields it needs to generate the image.

## Additional resources

[Generate overview](/docs/agent-actions/generate-quickstart)

[Transform quick start](/docs/agent-actions/transform-quickstart)

[Agent Actions patterns](/docs/agent-actions/agent-action-cheatsheet)

[Common patterns and use cases](/docs/agent-actions/generate-cheatsheet)





# Enable date and datetime support

Agent Actions can interact with `date` and `datetime` field types by adding time and location details to each request.

**Prerequisites:**

- Complete any of the Agent Actions quick start guides, or be familiar with making requests through the actions.
- API version `vX` and `@sanity/client` version `7.1.0` or higher.

## Include `localeSettings` in the request

To support natural language and relative time, the instruction needs to know the localized language you're using and the timezone it should use as a baseline for phrases like "tomorrow" or "three hours from now."

The following code:

7. Sets up a client.
7. Creates an instruction to change a `datetime` field at the path `publishedAt` and configures the locale settings. 

This example uses Generate, but the same `localSettings` apply to all Agent Actions.

```typescript
import { createClient } from "@sanity/client";

const client = createClient({
  projectId: "<project-id>",
  dataset: "<dataset-name>", // such as 'production'
  apiVersion: "vX",
  token: "<editor-token>",
});

await client.agent.action.generate({
  schemaId: "<schema-id>",
  documentId: "<document-id>",
  instruction: `
    Set the publishedAt date to the thirty first of October, 2025 at midnight.
  `,
  target: { path: "publishedAt" },
  localeSettings: {
    locale: "en-US",
    timeZone: "America/Los_Angeles",
  },
});
```

The `localeSettings` requires:

- `locale`: A BCP 47 locale identifier, such as `en-US` or `no-NO`. [Learn more about the specification](https://en.wikipedia.org/wiki/IETF_language_tag).
- `timeZone`: A IANA time zone identifier, such as `America/New_York` or `Europe/Berlin`. [Learn more about the supported values](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).

These settings enable the Agent Actions to understand the intended meaning behind natural language times and dates, and then apply them to `date` and `datetime` fields in a predictable way.



# Quick start



> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

Generate lets you programmatically run schema-aware AI instructions on Sanity documents. You can run instructions from anywhere you can execute code, such as cloud functions, webhook listeners, CI/CD pipelines, migration scripts, and more.

In this guide, you'll use Generate to create a document and write content based on your instructions. You'll use `@sanity/client` to create the instructions (you can also make requests using the [HTTP API](/docs/http-reference/agent-actions) directly).



**Prerequisites**:

- `@sanity/client` v7.1.0 or higher and an environment to run client requests.
- In Node.js v23.6 and above, you can run the TypeScript examples below without additional servers or build processes. Alternatively, you can use [earlier versions with an experimental flag](https://nodejs.org/en/learn/typescript/run-natively).
- `sanity` CLI v3.88.0 or higher.
- A Sanity project for testing. The examples below use details from the sample "Movies" studio schema that you can select when initializing a new project.- A read/write API token to authenticate requests.
- A valid `projectId` and `dataset` name.



## Step 1: Obtain a schema ID

Generate requires an uploaded schema. If you've deployed recently, you can check for a list of uploaded schemas by running the `schema list` command. If you don't see a schema or want to deploy the latest version, redeploy your studio to Sanity or deploy the schema.

```sh
npx sanity schema list
``````sh
npx sanity deploy
``````sh
npx sanity schema deploy
```

Copy the schema ID, which you'll need for making Agent Action requests. 

[You can learn more about schema deployment here](/docs/apis-and-sdks/schema-deployment).

## Step 2: Configure the client

Import and configure `@sanity/client` with the `projectId`, `dataset`, API `token`, and an `apiVersion` of vX.

```typescript
// instruction.ts

import { createClient } from "@sanity/client";

export const client = createClient({
    projectId: '<project-id>',
    dataset: '<datset-name>', // such as 'production'
    apiVersion: 'vX',
    token: '<editor-token>'
})
```

If you're already using the client elsewhere in an application, you can reuse its base configuration. If you need to adjust the token and/or API version, use the `withConfig` method to create a new client based on your existing one. For example:

```typescript
// ...
const generateClient = client.withConfig({
  token: '<your-token>',
})
```

## Step 3: Create an instruction

[Instructions](/docs/agent-actions/instructions) describe the content to target and the actions to take upon that content. They can create new documents or update existing ones. In the simplest form, `generate` takes the following settings:

- `targetDocument` or `documentId`: The `targetDocument` setting defines an `operation`. Setting `documentId` is shorthand for using `targetDocument` with the `edit` operation.
- `instruction`: The instruction you want to send to Generate.
- `schemaId`: The ID of your schema.

In this example, we'll create an instruction that adds a new movie to our movie database. 

Update the code to include the following instruction:

```typescript
await client.agent.action.generate({
  // Replace with your schema ID
  schemaId: "sanity.workspace.schema.default",

  // Tell the client to create a new 'movie' document type.
  targetDocument: { operation: "create", _type: "movie" },

  // Provide an instruction, or prompt.
  instruction: "Write the details for a movie titled $title.",

  // Optionally, provide any params for the instruction.
  // You can access them with the $key syntax.
  instructionParams: {
    title: { type: "constant", value: "Sanity: The Content Operating System" },
  },
});
```

This code creates a new draft document of the `movie` type, then tells Generate to write details about the movie. In this case, rather than directly telling the AI in the instructions that the title should be "Sanity: The Content Operating System", `instructionParams` is used to pass it in as the `$title` parameter.

> [!WARNING]
> Gotcha
> Depending on your schema, you may find that the instruction doesn't generate images or connect references. To enable these features, you'll need additional schema changes. See the guides below to configure each feature.
> 
> Enable image generation
> 
> Add support for references

Run the code to see Generate add a new movie titled "Sanity: The Content Operating System" to your dataset.

> [!TIP]
> Protip
> With the latest version of Node.js, you can run TypeScript files directly from your terminal. Run node instruction.ts, replacing instruction with the path to your file.

By default, the `create` operation creates a draft. If you want the instruction to create a published document, provide an `_id` to `targetDocument` in addition to the `_type` and `operation`.

> [!TIP]
> Protip
> You may wonder why we're using instructionParams to pass variables when we could use string interpolation or other methods to build the instruction string. By using instructionParams and then passing them to the instruction with the $key syntax, Generate has more control over how it shapes and sends your requests to the LLMs.

## Step 4: Modify an existing document

To update an existing document, set the `documentId` or use the edit operation with `targetDocument: { operation: "edit", _id: "<document-id>" }`. 

This example uses the existing document details to rewrite the title. Obtain the document ID from your studio by selecting **Inspect** from the **"..."** menu in the document title bar, querying the document in [Vision](/docs/content-lake/the-vision-plugin), or querying it with client.fetch(). 

```typescript
const docId = "<existing-document-id>";
await client.agent.action.generate({
  schemaId: "sanity.workspace.schema.default",
  // documentId is equivalent to targetDocument: {operation: 'edit', _id: docId }
  documentId: docId,
  instruction: `
    Update the title based on the details about the movie.
    Use the information in $details to come up with the new title.
  `,
  instructionParams: {
    details: {
      type: "field",
      path: "overview",
    },
  },
  target: {
    path: "title",
  }
});
```

In addition to swapping the `targetDocument` property for `documentId`, this example also has a new `instructionParams`.

As with the previous title example, you can name these keys whatever you like. The `details` key in this instance is a `field` type, and just like `title` in the earlier example, you can reference it in the instructions with a $ prefix (`$details`).

Field-type instruction parameters expect a path leading to fields in the document. In this case, it uses the overview field to read a summary of the movie that the instruction can use as context.

Another approach is to use a GROQ-type query and capture the whole or parts of other documents as context.

```typescript
await client.agent.action.generate({
  schemaId: "sanity.workspace.schema.default",
  documentId: "<existing-document-id>",
  instruction: `
    Update the title so that it aligns closer to the other movie titles.
    Use the information in $background to come up with the new title.
  `,
  instructionParams: {
    background: {
      type: "groq",
      query: `*[_type == "movie"].title`,
    },
  },
  target: {
    path: "title",
  }
});
```

GROQ-type instruction parameters take a GROQ query and pass the result to the parameter. In this case, it passes the titles of other movie documents in our dataset.

> [!TIP]
> Protip
> If you want to pass an individual document, you can use the document type param. For example: thisDocument: { type: 'document', documentId: '<document-id>'}. If you omit the Id, the parameter will be set to the current document.

This example also introduces the `target` parameter, and its child `path`. Target lets you explicitly tell the instruction which fields to write to. Check out more examples of `target` in [the cheat sheet](/docs/agent-actions/generate-cheatsheet).

When you run either of the examples above, they will respond with an updated document, including a new title based on other titles in the movie project.

## Next steps

These examples run once, but you can loop over multiple documents, build multi-step workflows, and much more. The resources below provide additional examples and details.

[Create images with Generate](/docs/agent-actions/agent-actions-image-generation)

[Enable references in Generate](/docs/agent-actions/generate-add-references)

[Generate cheat sheet](/docs/agent-actions/generate-cheatsheet)

[Agent Actions](/docs/http-reference/agent-actions)





# Common patterns

Generate offers an interface to enhance Sanity documents with the use of large language models (LLMs). This document showcases a collection of common patterns and concepts.

Prerequisites:

- Complete the [Generate quick start](/docs/agent-actions/generate-quickstart).
- `@sanity/client` v7.1.0 or higher and an environment to run client requests.
- API version vX or later for any requests using Generate.

Many examples in this document use `@sanity/client` and expect that you've installed and configured it for your project. If your client is named something other than `client`, update the code examples accordingly. 

Here's an example of the client implementation:

```typescript
// client.ts
import { createClient } from "@sanity/client";
export const client = createClient({
  projectId: '<project-id>',
  dataset: '<dataset-name>',
  useCdn: 'true',
  apiVersion: 'vX',
  token: '<read-write-token>'
})
```

Then, import `client` before using the examples below.

## Common patterns

The patterns in this guide are unique to Generate, but there are more patterns shared across all Agent Actions.

[Agent Actions patterns](/docs/agent-actions/agent-action-cheatsheet)



## Create multi-stage instructions

A single instruction is often fine for smaller tasks like updating an individual field. However, splitting instructions into multiple steps or stages for more significant tasks like writing complete documents with complex schemas returns better results. 

One approach to improve the LLM success rate is to approach instructions more like a human would. For example:

14. Make a skeleton or outline by populating simple, foundational fields like title, description, and categories or topics.
14. Run instructions for more complex areas, like an article's main content field, individually by passing in the results of step 1 as field parameters.
14. Run any summarization tasks at the end for content like SEO fields, social copy, or connecting related content.

This example creates a document with an instruction and then uses the generated content to influence future instructions.

```typescript
const customTopic =
  "A multi-generational epic, but all the characters are cats.";

const { _id } = await client.agent.action.generate({
  schemaId: "sanity.workspace.schema.default",
  targetDocument: {operation: 'create', _type: 'movie'},
  instruction: `
    Come up with a movie idea.
    Use the information in $topic as the basis for the movie.`,
  instructionParams: {
    topic: { type: "constant", value: customTopic },
  },
  target: {
    include: ["title", "overview"],
  },
});

await client.agent.action.generate({
  schemaId: "sanity.workspace.schema.default",
  documentId: _id,
  instruction: `Create a poster for the movie based on the $document.`,
  path: "poster",
  instructionParams: {
    document: { type: "document" },
  },
});

await client.agent.action.generate({
  schemaId: "sanity.workspace.schema.default",
  documentId: _id,
  instruction: `Translate the $overview into Japanese.`,
  path: "overviewJPN",
  instructionParams: {
    overview: { type: "field", path: "overview" },
  },
});
```

## Create release versions for AI changes

You can combine Generate with Content Releases to power a safer, supervised content pipeline. This example will:

- Read details about a published document and rewrite the title based on the instructions.
- Take that document and use the Actions API to create a new version document attached to an existing release, leaving the original published version unchanged. Note: you need v7.2.0 or later of the client and API v2025-02-19 or later to use version actions.

```typescript
const releaseId = "<release-id>";
const documentId = "<published-document-id>";

// Build the version path Id by combining versions with the release name and document Id.
const versionId = `versions.${releaseId}.${documentId}`;

// Create an instruction to rewrite the title
const result = await client.agent.action.generate({
  schemaId: "sanity.workspace.schema.default", // replace with your schema Id
  documentId: documentId,
  noWrite: true, // only write the changed document to the `result` variable
  instruction: `
    Re-imagine the title so that it is more engaging and interesting.
    Use the information in $document to help you come up with a new title.
  `,
  instructionParams: {
    document: {
      type: "document",
    },
  },
  target: {
    path: "title",
  }
});

// Call the Actions API with the client to create a new version.
await client.action(
  {
    actionType: 'sanity.action.document.version.create',
    publishedId: documentId,
    document: {
      ...result,
      _id: versionId,
    }
  }
)
```

> [!TIP]
> Protip
> You can use this same approach to create a draft document. The sanity.action.document.version.create action works the same for drafts, with one minor modification. 
> 
> Instead of versions.releaseId.documentId, set a draft Id with drafts.documentId. For example, drafts.movie12345.



# Quick start



> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

Translate is a Sanity Agent Action that lets you programmatically run schema-aware AI translations on Sanity documents. You can run translations from anywhere you can execute code, such as [Sanity Functions](/docs/compute-and-ai/functions-introduction), custom components, webhook listeners, CI/CD pipelines, migration scripts, and more.

In this guide, you'll first use Translate to convert a document into a new language. You'll use `@sanity/client` to create the translation requests (you can also make requests using the [HTTP API](/docs/http-reference/agent-actions) directly).



**Prerequisites**:

- `@sanity/client` v7.1.0 or higher and an environment to run client requests.
- API Version `vX` is required for any requests to the Agent Actions APIs.
- Optional: In Node.js v23.6 and above, you can run the TypeScript examples below without additional servers or build processes. Alternatively, you can use [earlier versions with an experimental flag](https://nodejs.org/en/learn/typescript/run-natively). Converting the examples to JavaScript is okay too.
- `sanity` CLI v3.88.0 or later.
- A Sanity project for testing. The examples below use details from the sample "Movies" studio schema that you can select when initializing a new project.- A read/write API token to authenticate requests.
- A valid `projectId` and `dataset` name.



## Obtain a schema ID

Translate requires an uploaded schema. If you've deployed recently, you can check for a list of uploaded schemas by running the `schema list` command. If you don't see a schema or want to deploy the latest version, redeploy your studio to Sanity or deploy the schema.

```sh
npx sanity schema list
``````sh
npx sanity deploy
``````sh
npx sanity schema deploy
```

Copy the schema ID, which you'll need for making Agent Action requests. 

[You can learn more about schema deployment here](/docs/apis-and-sdks/schema-deployment).

## Configure the client

Import and configure `@sanity/client` with the `projectId`, `dataset`, API `token`, and an `apiVersion` of vX.

```typescript
// instruction.ts

import { createClient } from "@sanity/client";

export const client = createClient({
    projectId: '<project-id>',
    dataset: '<datset-name>', // such as 'production'
    apiVersion: 'vX',
    token: '<editor-token>'
})
```

If you're already using the client elsewhere in an application, you can reuse its base configuration. If you need to adjust the token and/or API version, use the `withConfig` method to create a new client based on your existing one. For example:

```typescript
// ...
const generateClient = client.withConfig({
  token: '<your-token>',
})
```

## Translate a document

It's common to want a complete, translated version of a document. To achieve this with Translate:

- Provide a source `documentId` of the original document.
- Define the `fromLanguage`. This is optional, but it avoids the AI interpreting the document as a language other than the one you expect.
- Define the `toLanguage` that you want the document translated into.
- Set the operation. Operations tell Agent Actions what to do. In this example, we'll use `create`. [Learn more about operations](/docs/agent-actions/operations).

Here's a minimal example that uses an existing English language document to create a new translation in Greek.

```
await client.agent.action.translate({
  // Replace with your schema ID
  schemaId: "sanity.workspace.schema.default",

  // Tell the client the ID of the document to use as the source.
  documentId: "<document-id>",
  
  // Set the operation mode
  targetDocument: { operation: "create" },

  // Set the 'from' and 'to' language
  fromLanguage: {id: "en-US", title: "English"},
  toLanguage: {id: "el-GR", title: "Greek"},
});
```

This creates a new draft document based on the source (`documentId`).

> [!NOTE]
> Create makes an unlinked draft
> Using the create operation with an ID in Translate creates a new, unlinked draft. This means it's not directly associated with the original document the way a draft of a published document is.

## Customize the output with style guides

If you're familiar with the other Agent Actions, `styleGuide` is Translate's version of `instruction`. It lets you add additional context and guidance beyond setting a target language.

In this example, we tell Translate to use a formal tone:

```
await client.agent.action.translate({
  // Replace with your schema ID
  schemaId: "sanity.workspace.schema.default",

  // Tell the client the ID of the document to use as the source.
  documentId: "<document-id>",
  
  // Set the operation mode
  targetDocument: { operation: "create" },

  // Set the 'from' and 'to' language
  fromLanguage: {id: "en-US", title: "English"},
  toLanguage: {id: "el-GR", title: "Greek"},

  styleGuide: "Use a formal tone when translating.",
});
```

You can also pass information into the style guide with `styleGuideParams`. 

```
await client.agent.action.translate({
  // Replace with your schema ID
  schemaId: "sanity.workspace.schema.default",

  // Tell the client the ID of the document to use as the source.
  documentId: "<document-id>",
  
  // Set the operation mode
  targetDocument: { operation: "create" },

  // Set the 'from' and 'to' language
  fromLanguage: {id: "en-US", title: "English"},
  toLanguage: {id: "el-GR", title: "Greek"},

  // Use `styleGuide` instead of instruction for Translate
  styleGuide: "Use a $tone tone when translating.",
  styleGuideParams: {
    tone: 'formal'
  }
});
```

In this example, we use a `constant` type parameter to assign the string "formal" to the `tone` key. Then, we pass it into the style guide as `$tone`. This is just one type of parameter. You can do everything from including full documents to making GROQ queries. Learn more about [passing parameters into style guides](/docs/agent-actions/instructions).

## Next steps

To learn more about what you can do with Translate, explore the other guides and resources available for [Agent Actions](/docs/agent-actions).

#### Explore more

[Translate cheat sheet](/docs/agent-actions/translate-cheatsheet)

[Agent Actions patterns](/docs/agent-actions/agent-action-cheatsheet)

[Agent Actions](/docs/http-reference/agent-actions)





# Common patterns

Translate offers an interface to translate Sanity documents using large language models (LLMs). This document showcases a collection of common patterns and concepts.

Prerequisites:

- Complete the [Translate quick start](/docs/agent-actions/translate-quickstart).
- `@sanity/client` v7.1.0 or higher and an environment to run client requests.
- API version vX or later for any requests using Generate.

Many examples in this document use `@sanity/client` and expect that you've installed and configured it for your project. If your client is named something other than `client`, update the code examples accordingly. 

Here's an example of the client implementation:

```typescript
// client.ts
import { createClient } from "@sanity/client";
export const client = createClient({
  projectId: '<project-id>',
  dataset: '<dataset-name>',
  useCdn: 'true',
  apiVersion: 'vX',
  token: '<read-write-token>'
})
```

Then, import `client` before using the examples below.

## Define protected phrases

Translate allows you to specify certain words or phrases that are protected and will be ignored by the translation. Supply an array of strings to the `protectedPhrases` property.

```
await client.agent.action.translate({
  // Replace with your schema ID
  schemaId: "sanity.workspace.schema.default",

  // Tell the client the ID of the document to use as the source.
  documentId: "<document-id>",
  
  // Set the operation mode
  targetDocument: { operation: "create" },

  // Set the 'from' and 'to' language
  fromLanguage: {id: "en-US", title: "English"},
  toLanguage: {id: "el-GR", title: "Greek"},
  protectedPhrases: [
    "Sanity",
    "Media Library",
    "Agent Actions"
  ]
});
```

## Set a document's language

You may need to set a field in the new document so that it knows what language it is in. This is particularly important for routing and automation, where you may check against a field to determine its language. While we could *hope* the AI fills this in correctly, you can explicitly target the field with `languageFieldPath`. This will tell Translate to set the field to the same value as the `toLanguage` ID.

For example, if our document has a field, `language`, where editors select the document language, we would set the `languageFieldPath` to `language`.

```
await client.agent.action.translate({
  // Replace with your schema ID
  schemaId: "sanity.workspace.schema.default",

  // Tell the client the ID of the document to use as the source.
  documentId: "<document-id>",
  
  // Set the operation mode
  targetDocument: { operation: "create" },

  // Tell Translate to set this field to the target language,
  // in this case, 'el-GR'.
  languageFieldPath: "language",

  // Set the 'from' and 'to' language
  fromLanguage: {id: "en-US", title: "English"},
  toLanguage: {id: "el-GR", title: "Greek"},
  protectedPhrases: [
    "Sanity",
    "Media Library",
    "Agent Actions"
  ]
});
```

## Common patterns

In addition to the patterns on this page, there are many common patterns that apply to all Agent Actions.

#### Explore more patterns

[Agent Actions patterns](/docs/agent-actions/agent-action-cheatsheet)







# Quick start



> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

Transform is a Sanity Agent Action that lets you programmatically run schema-aware AI transformations on Sanity documents. You can run instructions from anywhere you can execute code, such as Sanity Functions, custom components, webhook listeners, CI/CD pipelines, migration scripts, and more.

In this guide, you'll use Transform to run a find/replace style instruction on content across multiple fields in a document. You'll use `@sanity/client` to create the instructions (you can also make requests using the [HTTP API](/docs/http-reference/agent-actions) directly).



**Prerequisites**:

- `@sanity/client` v7.1.0 or higher and an environment to run client requests.
- API Version `vX` is required for any requests to the Agent Actions APIs.
- Optional: In Node.js v23.6 and above, you can run the TypeScript examples below without additional servers or build processes. Alternatively, you can use [earlier versions with an experimental flag](https://nodejs.org/en/learn/typescript/run-natively). Converting the examples to JavaScript is okay too.
- `sanity` CLI v3.88.0 or later.
- A Sanity project for testing. The examples below use details from the sample "Movies" studio schema that you can select when initializing a new project.- A read/write API token to authenticate requests.
- A valid `projectId` and `dataset` name.



## Obtain a schema ID

Transform requires an uploaded schema. If you've deployed recently, you can check for a list of uploaded schemas by running the `schema list` command. If you don't see a schema or want to deploy the latest version, redeploy your studio to Sanity or deploy the schema.

```sh
npx sanity schema list
``````sh
npx sanity deploy
``````sh
npx sanity schema deploy
```

Copy the schema ID, which you'll need for making Agent Action requests. 

[You can learn more about schema deployment here](/docs/apis-and-sdks/schema-deployment).

## Step 2: Configure the client

Import and configure `@sanity/client` with the `projectId`, `dataset`, API `token`, and an `apiVersion` of vX.

```typescript
// instruction.ts

import { createClient } from "@sanity/client";

export const client = createClient({
    projectId: '<project-id>',
    dataset: '<datset-name>', // such as 'production'
    apiVersion: 'vX',
    token: '<editor-token>'
})
```

If you're already using the client elsewhere in an application, you can reuse its base configuration. If you need to adjust the token and/or API version, use the `withConfig` method to create a new client based on your existing one. For example:

```typescript
// ...
const generateClient = client.withConfig({
  token: '<your-token>',
})
```

## Step 3: Transform a document

Transform uses the [concept of an instruction](/docs/agent-actions/instructions)*. *This is where you tell Transform what to do with the content in a document. In the simplest form, `transform` takes the following settings:

- `schemaId`: The ID of your schema.
- `documentId`: The `documentId` is defines both the source and the target document. This lets you run the transformation in-place. You can provide a published ID, draft ID, or a version ID.
- `instruction`: The instruction is where you tell the system how to act.

In this example, we create an instruction that changes the term "Alien" or "Aliens" in a movie document to "lifeform from outer space" and "lifeforms from outer space". 

> [!TIP]
> Get a document ID
> This example uses the existing document details to rewrite the title. Obtain the document ID from your studio by selecting Inspect from the "..." menu in the document title bar, querying the document in Vision, or querying it with client.fetch(). 

Update the code to include the following instruction, and change the document ID to a valid ID in your project, and the instruction to one that matches your content:

```typescript
await client.agent.action.transform({
  // Replace with your schema ID
  schemaId: "sanity.workspace.schema.default",

  // Tell the client the ID of the document to transform.
  documentId: "<document-id>",

  // Provide an instruction, or prompt.
  instruction: "Change all instances of 'Alien' to 'Lifeform from another planet'. Match the case of the existing text.",
});
```

This code reads each field in the document and runs the instruction against them.

> [!WARNING]
> Gotcha
> Depending on your schema, you may find that Transform doesn't interact with images or connect references. To enable these features, you'll need additional schema changes. See the guides below to configure each feature.
> 
> Enable image generation
> 
> Add support for references

Run the code to see Transform edit the document and update the content. In the case of the example, it updated the movie's title and updated parts of the description. 

> [!TIP]
> Protip
> With the latest version of Node.js, you can run TypeScript files directly from your terminal. Run node instruction.ts, replacing instruction with the path to your file.

## Step 4: Create a new document with advanced parameters.

Transform can also create new documents based on the content in the source. If you want to create new documents from scratch, give [Generate a try instead](/docs/agent-actions/generate-quickstart).

In this step we'll modify the code from the previous example so that it will create a new document that uses the original document as the source, and we'll pass in parameters.

- Add `targetDocument`: This takes an operation type, `create`, and optionally an ID. 
- Add `instructionParams`: The parameters can have any `$key` name you like. This example uses the field and const parameter types. Field targets a specific field in the source document, which, in this example is the title field. The const type uses a shorthand syntax to set it like a string.
- Update the instruction: Include the newly defined parameters, and prefix them with `$`. In the example, these are `$title` and `$new`.
- Add a `target` paths (optional): This example also adds explicit targets. Instead of affecting the whole document, only the paths set in `target` will see changes.

```typescript
const docId = "<existing-document-id>";
await client.agent.action.transform({
  schemaId: "sanity.workspace.schema.default",
  // documentId is equivalent to targetDocument: {operation: 'edit', _id: docId }
  documentId: docId,
  targetDocument: {
    operation: 'create'
  },
  instruction: "Replace every instance of $title with $new. Match the case of the existing text.",
  instructionParams: {
    title: {
      type: "field",
      path: "title",
    },
    new: "lifeforms from outer space"
  },
  target: [
    { path: ['title'] },
    { path: ['overview'] },
  ]
});
```

Field-type instruction parameters expect a path leading to fields in the document. In this case, it uses the title field to read the title.

> [!TIP]
> Protip
> You may wonder why we're using instructionParams to pass variables when we could use string interpolation or other methods to build the instruction string. By using instructionParams and then passing them to the instruction with the $key syntax, Generate has more control over how it shapes and sends your requests to the LLMs.

Another approach is to use a GROQ-type query and capture the whole or parts of other documents as context.

```typescript
await client.agent.action.transform({
  // ...
  instructionParams: {
    title: {
      type: "groq",
      query: `*[_id == "some-other-document-id"].title`,
    },
  },
  // ...
});
```

GROQ-type instruction parameters take a GROQ query and pass the result to the parameter. In this case, it passes the titles of other movie documents in our dataset.

> [!TIP]
> Protip
> If you want to pass an individual document, you can use the document type param. For example: thisDocument: { type: 'document', documentId: '<document-id>'}. If you omit the Id, the parameter will be set to the current document.

This example also introduces the `target` parameter, and its child `path`. Target lets you explicitly tell the instruction which fields to write to. This is a really powerful field, and can even take field-level `instruction` requests. Check out more examples of `target` in [the cheat sheet](/docs/agent-actions/generate-cheatsheet).

Run the example to see a new draft populate for your document.

## Next steps

These examples run once, but you can loop over multiple documents, build multi-step workflows, custom components, and much more. The resources below provide additional examples and details.

[Create images with Generate](/docs/agent-actions/agent-actions-image-generation)

[Enable references in Generate](/docs/agent-actions/generate-add-references)

[Generate cheat sheet](/docs/agent-actions/generate-cheatsheet)







# Common patterns

Transform offers an interface to enhance Sanity documents using large language models (LLMs). This document showcases a collection of common patterns and concepts.

Prerequisites:

- Complete the [Transform quick start](/docs/agent-actions/transform-quickstart).
- `@sanity/client` v7.1.0 or higher and an environment to run client requests.
- API version vX or later for any requests using Generate.

Many examples in this document use `@sanity/client` and expect that you've installed and configured it for your project. If your client is named something other than `client`, update the code examples accordingly. 

Here's an example of the client implementation:

```typescript
// client.ts
import { createClient } from "@sanity/client";
export const client = createClient({
  projectId: '<project-id>',
  dataset: '<dataset-name>',
  useCdn: 'true',
  apiVersion: 'vX',
  token: '<read-write-token>'
})
```

Then, import `client` before using the examples below.

## Edit a full document

Perform in-place edits on a document. This will not create a new document. Providing a published `documentId` will update the published document.

```
await client.agent.action.transform({
  schemaId: 'sanity.workspace.schema.default',
  documentId: 'drafts.id',
  instruction: 'Replace "Create" with "Canvas"',
})
```

## Edit part of a document

Perform an in-place edit on only part of a document.

```
await client.agent.action.transform({
  schemaId: 'sanity.workspace.schema.default',
  documentId: 'drafts.id',   
  instruction: 'Replace "Create" with "Canvas"',
  target: {path: ['body']} // only transforms body (and any sub-fields/items)
})
```

## Multiple instruction parameters

This instruction pulls one paramater from a GROQ query, and another from a field, `year`, in the source document and creates a new draft with the changes.

```
await client.agent.action.transform({
  schemaId: 'sanity.workspace.schema.default',
  documentId: 'document-id',
  targetDocument: {
    operation: 'create',
  },
  instruction: 'Add $fieldValue to every instance of $groqTitle',
  instructionParams: {
    groqTitle: {
      type: 'groq',
      query: '*[_id==$id].title',
      params: {
        id: 'abc123'
      }
    },
    fieldValue: {
      type: 'field',
      path: 'year'
    }
  }
})
```

## Apply instructions to individual fields

This transformation has a top-level instruction, but sets an individual instruction for the title field using `target`.

```
await client.agent.action.transform({
  schemaId: 'sanity.workspace.schema.default',
  documentId: 'document-id',
  targetDocument: {
    operation: 'create',
  },
  instruction: 'Replace "$old" with "$new"',
  instructionParams: {
    new: 'lifeform from another planet',
    old: 'alien'
  },
  target: [
    {
      path: ['title'],
      instruction: 'Replace "$old" with "$new". Use title-case.'
    },
    { path: 'body' } // anything in or below 'body' uses the default instruction.
  ]
})
```

Learn more about targeting individual fields in the [Target and paths](/docs/agent-actions/targets-paths) documentation.

## Create captions and alt text with Transform

Transform has a special operation type that you can apply to individual fields that helps describe the contents of an image asset.

This example creates a draft of the document defined in `documentId`, then describes the image asset adjacent to the `['image', 'alt']` field.

```
await client.agent.action.transform({
  schemaId: '_.schemas.default',
  documentId: 'document-id',
  instruction: 'Describe the image in one to two sentences.',
  target: [{
    path: ['image', 'alt'],
    operation: {
      type: 'image-description'
    }
  }]
});
```

Learn more about Transform's `image-description` operation in the [Target and paths](/docs/agent-actions/targets-paths) documentation.



Learn more about `target` in the [common Agent Action patterns guide](/docs/agent-actions/agent-action-cheatsheet).



# Quick start

> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

Prompt is a Sanity Agent Action that lets you make large language model (LLM) requests without bringing in external AI tooling. You can run prompts from anywhere you can execute code, such as [Sanity Functions](/docs/compute-and-ai/functions-introduction), custom components, webhook listeners, CI/CD pipelines, migration scripts, and more.

In this guide, you'll first use Prompt to make a request to the LLM. You'll use `@sanity/client` to run the Prompt (you can also make requests using the [HTTP API](/docs/http-reference/agent-actions) directly).



**Prerequisites**:

- `@sanity/client` v7.4.0 or higher and an environment to run client requests.
- API Version `vX` is required for any requests to the Agent Actions APIs.
- Optional: In Node.js v23.6 and above, you can run the TypeScript examples below without additional servers or build processes. Alternatively, you can use [earlier versions with an experimental flag](https://nodejs.org/en/learn/typescript/run-natively). Converting the examples to JavaScript is okay too.
- An API or personal token to make authenticated requests.

## Configure the client

Import and configure `@sanity/client` with the `projectId`, `dataset`, API `token`, and an `apiVersion` of vX.

```typescript
import { createClient } from "@sanity/client";

export const client = createClient({
    projectId: '<project-id>',
    dataset: '<datset-name>', // such as 'production'
    apiVersion: 'vX',
    token: '<editor-token>'
})
```

If you're already using the client elsewhere in an application, you can reuse its base configuration. If you need to adjust the token and/or API version, use the `withConfig` method to create a new client based on your existing one. For example:

```typescript
// ...
const promptClient = client.withConfig({
  token: '<your-token>',
})
```

## Prompt for ideas

Prompts don't edit or create documents. They return text, or json, so you can use it however you please. 

```
const response = await client.agent.action.prompt({
  // write an instruction
  instruction: `Give me some ideas for a blog post 
    about using AI with structured content.`
});

console.log(response)
```

This prompt returns a text response that you can use. The examples on this page log the response to the console.

## Use parameters

You can further customize the instruction by passing in parameters. For example, if you want use a document as background information for the prompt, you use the `document` parameter type.

```
const response = await client.agent.action.prompt({
  // write an instruction
  instruction: `Give me some ideas for a blog post 
    about using AI with structured content. Use the following as context for the ideas: $background`,
  instructionParams: {
    background: {
      type: 'document',
      documentId: '<target-document-id>'
    }
  }
});
console.log(response)
```

You can learn more about parameters and the available types in the [creating instructions guide](/docs/agent-actions/instructions).

## Change the output

Prompt will return a text response by default, but you can also tell it to return JSON. To do so, you must set the `format` to "json" and explicitly include the world "JSON" or "json" in the instruction. It also helps to provide an example shape in the instruction.

```
const response = await client.agent.action.prompt({
  // write an instruction
  instruction: `Give me some ideas for a blog post 
    about using AI with structured content. Respond in JSON with the following format: { "ideas": ['idea one', 'idea two', 'etc'] }`,
  format: 'json'
});
console.log(response)
```

## Add variety

You can tune the variance of responses by adjusting the `temperature` of the request.

```
const response = await client.agent.action.prompt({
  // write an instruction
  instruction: `Give me some ideas for a blog post 
    about using AI with structured content.`,
  temperature: '0.8' // Set between 0 and 1, inclusively. Default: 0.3
});
console.log(response)
```

Higher values result in more variety of responses, while lower values result in more predictable results when given the same instruction.

## Next steps

To learn more about what you can do with Translate, explore the other guides and resources available for [Agent Actions](/docs/agent-actions).

#### Explore more

[Agent Actions patterns](/docs/agent-actions/agent-action-cheatsheet)

[Agent Actions](/docs/http-reference/agent-actions)







# Quick start

> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

Patch is a Sanity Agent Action that helps you make schema-aware patches to documents. You can run Patch from anywhere you can execute code, such as [Sanity Functions](/docs/compute-and-ai/functions-introduction), custom components, webhook listeners, CI/CD pipelines, migration scripts, and more.

In this guide, you'll use Patch to modify documents in a safe, schema-aware way. You'll use `@sanity/client` to run Patch (you can also make requests using the [HTTP API](/docs/http-reference/agent-actions) directly).

> [!TIP]
> Patch doesn't use an LLM
> Unlike many Agent Actions, Patch doesn't use an LLM and instead relies on your schema.
> 
> This means it uses standard Sanity API billing for API requests.

**Prerequisites**:

- `@sanity/client` v7.4.0 or higher and an environment to run client requests.
- API Version `vX` is required for any requests to the Agent Actions APIs.
- Optional: In Node.js v23.6 and above, you can run the TypeScript examples below without additional servers or build processes. Alternatively, you can use [earlier versions with an experimental flag](https://nodejs.org/en/learn/typescript/run-natively). Converting the examples to JavaScript is okay too.
- An API or personal token to make authenticated requests.

## Obtain your schema ID

Patch requires an uploaded schema. If you've deployed recently, you can check for a list of uploaded schemas by running the `schema list` command. If you don't see a schema or want to deploy the latest version, redeploy your studio to Sanity or deploy the schema.

```sh
npx sanity schema list
``````sh
npx sanity deploy
``````sh
npx sanity schema deploy
```

Copy the schema ID, which you'll need for making Agent Action requests. 

[You can learn more about schema deployment here](/docs/apis-and-sdks/schema-deployment).

## Configure the client

Import and configure `@sanity/client` with the `projectId`, `dataset`, API `token`, and an `apiVersion` of vX.

```typescript
import { createClient } from "@sanity/client";

export const client = createClient({
    projectId: '<project-id>',
    dataset: '<datset-name>', // such as 'production'
    apiVersion: 'vX',
    token: '<editor-token>'
})
```

If you're already using the client elsewhere in an application, you can reuse its base configuration. If you need to adjust the token and/or API version, use the `withConfig` method to create a new client based on your existing one. For example:

```typescript
// ...
const patchClient = client.withConfig({
  token: '<your-token>',
})
```

## Patch basics

At it's core, Patch works much like the patch format used by many Content Lake APIs. The big difference is that Agent Action Patch is aware of your schema. It validates paths and ensures that the provided values are compatible with the target schema.

Patch relies heavily targets, paths, and operations. 

- **Targets** and **paths **tell Patch which parts of a document to affect.
- **Operations** tell Patch how to reconcile new and old data.

To learn more about these concepts, see the [Targets and paths documentation](/docs/agent-actions/targets-paths).

Here's an example of a patch request that updates a nested title field and changes the title to "New title".

```
await client.agent.action.patch({
  schemaId: 'sanity.workspace.schema.production',
  documentId: 'documentId',
  target: {
    path: ['metadata', 'title'], // path to metadata.title
    operation: 'set',
    value: 'New title'
  }
});
```

## Multi-target patches

Patch excels at targeted, multi-target edits. This example uses multiple targets with different operations to mutate the document.

```
await client.agent.action.patch({
  schemaId: 'sanity.workspace.schema.production',
  documentId: 'documentId',
  target: [
    { path: ['title'], operation: 'set', value: 'New title' },
    {
      path: ['array'], 
      operation: 'append', 
      value: [
        { _type: 'item', title: 'New Array item' }, // key will be generated
        { _type: 'item', title: 'Another new array item', _key: 'explicitKey' }
      ]
    },
    { path: ['customFieldName', {_key: 'abc'}, 'title'], operation: 'unset'},
    
    // 'mixed' will set non-array fields, and append to array fields.
    // Objects are merged, not overwritten.
    {
      path: ['customObject'],
      operation: 'mixed', 
      value: {
        // mixed mode implies set for string fields
        description: 'Hello',
        // mixed mode implies append for arrays
        otherArray: [{_type: 'item', title: 'a'}]
      }
    }
  ]
});
```



# Setting up your studio

## Create a new Studio with Sanity CLI

![Video](https://stream.mux.com/wIMs3CS7T4pP7hRArpQZsBZ01Be02vCjbK)

Run the command in your Terminal to initialize your project on your local computer.

See the documentation if you are [having issues with the CLI](/docs/help/cli-errors).

```sh
npm create sanity@latest -- --dataset production --template clean --typescript --output-path studio-hello-world
cd studio-hello-world
```

## Run Sanity Studio locally

Inside the directory of the Studio, start the development server by running the following command.

```sh
npm run dev
```

## Log in to the Studio

**Open** the Studio running locally in your browser from [http://localhost:3333](http://localhost:3333).

You should now see a screen prompting you to log in to the Studio. Use the same service (Google, GitHub, or email) that you used when you logged in to the CLI.



# Defining a schema

## Create a new document type

![Video](https://stream.mux.com/IfVfAwxfwOKN2khdGCQ3cs5IuF1rYte1)

Create a new file in your Studio’s `schemaTypes` folder called `postType.ts` with the code below which contains a set of fields for a new `post` document type.

```
import {defineField, defineType} from 'sanity'

export const postType = defineType({
  name: 'post',
  title: 'Post',
  type: 'document',
  fields: [
    defineField({
      name: 'title',
      type: 'string',
      validation: (rule) => rule.required(),
    }),
    defineField({
      name: 'slug',
      type: 'slug',
      options: {source: 'title'},
      validation: (rule) => rule.required(),
    }),
    defineField({
      name: 'publishedAt',
      type: 'datetime',
      initialValue: () => new Date().toISOString(),
      validation: (rule) => rule.required(),
    }),
    defineField({
      name: 'image',
      type: 'image',
    }),
    defineField({
      name: 'body',
      type: 'array',
      of: [{type: 'block'}],
    }),
  ],
})
```

## Register the `post` schema type to the Studio schema

Now you can import this document type into the `schemaTypes` array in the `index.ts` file in the same folder.

```
import {postType} from './postType'

export const schemaTypes = [postType]
```

## Publish your first document

When you save these two files, your Studio should automatically reload and show your first document type. Click the `+` symbol at the top left to create and publish a new `post` document.



# Query content with GROQ

## Write your first GROQ query

![Video](https://stream.mux.com/Mc12Sdeu00ugrGuQyz00Du1G4AQZmT36UV)

Open **Vision** in your Studio's top nav bar and paste this query into the **Query** code block field.

```groq
*[_type == "post"]{
  _id,
  title,
  slug,
  publishedAt
}
```

- `*` represents all documents in a dataset as an array
- `[_type == "post"]` represents a **filter** to only return matching documents
- `{ _id, title, slug, publishedAt }` represents a **projection** which defines the attributes from those documents that you wish to include in the response.

## Run the query

Click **Fetch** to see the JSON output in **Results**. You should see the document you previously published in the results.

Queries run in Vision use your authenticated session, so you will see private documents – which have a `.` in the `_id` key, like `drafts.`. You will not see when queried from your front end in the next step.



# Deploying the Studio

## Deploy your Studio with Sanity

![Video](https://stream.mux.com/CvYhCQr8e1oZt98NW202BZLLNv376VVKc)

In your Studio directory (`studio-hello-world`) run the following command to deploy your Sanity Studio.

```sh
npm run deploy
```

## Invite a collaborator

Now that you’ve deployed your Studio, you can optionally invite a collaborator to your project. Navigate to: `https://www.sanity.io/manage/project/{{PROJECT_ID}}/members`.

They will be able to access the deployed Studio, where you can collaborate together on creating content.





# Dashboard

#### Explore Dashboard

[Meet the Dashboard](/docs/dashboard/dashboard-introduction)

[Set up and configure Dashboard](/docs/dashboard/dashboard-configure)



#### Custom apps for Dashboard

[App SDK](/docs/app-sdk)

[App SDK Quickstart Guide](/docs/app-sdk/sdk-quickstart)





# Meet the Dashboard

> [!TIP]
> Find your dashboard
> To find your organization dashboard, visit www.sanity.io/welcome!

## Dashboard at a glance

The Sanity Dashboard is the central hub for your organization's content operations. Here you'll find your deployed [studios](/docs/studio), [custom apps](/docs/app-sdk), and official Sanity apps like [Canvas](undefined) and [Media Library](undefined).

![Shows the dashboard with the home screen active](https://cdn.sanity.io/images/3do82whm/next/acb30da7d5120de7fa8c38ad8fd71235098d76b2-1256x965.png)

Your dashboard is centered around your organization, and gives access to all deployed studios and apps within the organization, across projects and datasets.

> [!NOTE]
> What about the dashboard plugin?
> As the keen reader may have observed, there is already a "dashboard" in the Sanity ecosystem, namely the official dashboard plugin for Sanity Studio. This plugin will continue to be available for your intra-studio dashboard needs.

## Touring the dashboard interface

Your dashboard consists of the main area, which will adapt to the app you are currently working in, with sidebars on either side. We'll further separate each sidebar into their top and bottom groups, as shown in the image below.

![Shows the dashboard with annotations from 1 to 5, where 1 is the main content area, and 2-5 are the sidebar sections listed clockwise from top left](https://cdn.sanity.io/images/3do82whm/next/23295ac6edc7a394b6ec9520b2b93eea6b9cf43f-2560x1984.png)

### 1. Main content area

Unsurprisingly, the main content area takes up the lion's share of the screen real estate. This is where you'll be spending most of your time, after all. The main content area adapts to whatever app you're currently working in. Shown below is a studio active in the dashboard. 

![Shows the dashboard with an example studio active in the main area](https://cdn.sanity.io/images/3do82whm/next/d98acefd1bca5e4dd53d7db7ac1cabd93e8845e8-1253x965.png)

If there is no active app, this area defaults to show you links to your most likely destinations, as well as some nifty insights about your content.

![Shows information about the content from different studios connected to the organization](https://cdn.sanity.io/images/3do82whm/next/5546fe009621d2f7673814f706a00698033b18c3-931x504.png)

### 2. Left sidebar – top

The top section in the left sidebar is where you'll switch between the apps and studios connected to your organization, as well as a menu to switch between different organizations if you belong to more than one.

![a screenshot of the organization switcher dashboard home canvas media library custom application studio and another studioShows icons with labels: Organization Switcher, Dashboard Home, Canvas, Media Library, Custom App, Studio, Another Studio](https://cdn.sanity.io/images/3do82whm/next/3e42e6f4b802b94bd123831ba6bb33b879027c26-638x463.png)

#### Organization Switcher

Click to bring up a list of the organizations available to you.

#### Home

Loads the dashboard home page

#### Canvas & Media Library

Grouped right below the Home button you'll find the official Sanity apps available to your organization, such as [Canvas](/docs/canvas) and [Media Library](/docs/media-library).

#### Studios and custom apps

Below the official Sanity apps, you'll find any [custom apps](/docs/app-sdk) and [studios](/docs/studio) deployed by your organization. If you can't find an app or studio you expected to see here, your studio maintainer might have to do a little [configuration](/docs/dashboard/dashboard-configure) first.

### 3. Left sidebar - bottom

In the bottom left sidebar section you'll find a list of all your deployed studios and apps, as well as options to manage your organization and its members. 

![a screenshot of the application and studios organization settings page](https://cdn.sanity.io/images/3do82whm/next/a9bca236f75172963a257f829d82d17baabcc021-424x158.png)

#### Applications and studios

Selecting this option will load a list of, well, applications and studios, into the main content area. You'll also find some convenients links that'll help you add any missing destinations, or kickstart your own custom application.

![a screenshot of the studios and applications page](https://cdn.sanity.io/images/3do82whm/next/764ae5f9c9e3862cced2bfef1dca651771a1f23b-1102x833.png)

If you want to remove an app or a studio from the sidebar, you can do so by togggling the pin icon to the left of each listing's title, as shown above.



#### Organization settings

![a button that says invite members and organization settings](https://cdn.sanity.io/images/3do82whm/next/eb4d25c6dcb39736708467164dcb95e0ca05bb6a-412x171.png)

This menu option will reveal outbound links to manage your organization and its members.



### 4. Right sidebar – bottom

This is where you'll find some helpful links, as well as an option to submit feedback about your experience.

![a help and feedback button with a speech bubble](https://cdn.sanity.io/images/3do82whm/next/7cb5a5dc546de9a1ba8e2097f411558fc4fc2d59-247x140.png)

#### Help

Clicking the question mark icon will bring up a contextual menu with some fairly straight-forward options. Hope to see you in our community!

![a screenshot of a website that says join the community](https://cdn.sanity.io/images/3do82whm/next/47fbef225e231fc20cde3af0780b7121559a427e-216x264.png)

#### Feedback

Have something to say? We'd love to hear it! 

![Shows a popover feedback form with a send button](https://cdn.sanity.io/images/3do82whm/next/0f6cd3ca05c30d780bfec487a6353d423805516d-616x211.png)

### 5. Right sidebar – top

If the bottom left section was all about the organization, top right is all about you! Here you'll find a link to your account settings, as well as an option to create favorite bookmarks for destinations you visit frequently.

![Shows the account settings popover with a publically displayed email that no one in their right mind would ever abuse](https://cdn.sanity.io/images/3do82whm/next/917b88a4df826c9af84349721bdc9d5e25657df1-453x252.png)









# Configuring the Dashboard

## Set up your content operations dashboard

> [!TIP]
> Find your dashboard
> To find your organization dashboard, visit www.sanity.io/welcome!

The Sanity Dashboard is the hub for your organization's content operations. Here you'll find your deployed studios, custom apps, and official Sanity apps like [Canvas](/docs/canvas) or [Media Library](/docs/media-library).

![The Sanity Dashboard home page](https://cdn.sanity.io/images/3do82whm/next/acb30da7d5120de7fa8c38ad8fd71235098d76b2-1256x965.png)

Your dashboard is centered around your organization, and gives access to all deployed studios and apps within the organization, across projects and datasets. 

> [!NOTE]
> Dashboard plugin
> As the keen reader may have observed, there is already a "dashboard" in the Sanity ecosystem, namely the official dashboard plugin for Sanity Studio. This plugin will continue to be available for your intra-studio dashboard needs.

## Configure your studios

![a computer screen shows a list of movies including galaxy quest](https://cdn.sanity.io/images/3do82whm/next/2c70c211650d4f8368bafef8c24eabbbdebb959e-1250x920.png)

For almost everyone: Your pre-dashboard studios will automatically work as before, with all your customization intact. To fully enjoy the benefits of the integrated dashboard, a studio deployment is required. Depending on your setup, this process will differ slightly.

### Requirements

Dashboard should *mostly* work with studios going all the way back to v2.28.0 (shoutout to OGs still running v2), but for the best experience we heartily recommend [upgrading](/docs/studio/upgrade) to `@latest`. 

- Studio version must be: - At least >= `v2.28.0` 
- Preferably >= `v3.88.1` 
- Ideally `@latest`


- Schema and manifest files must be extracted and made available. For a detailed look at how schema deployment works, visit [this article](/docs/apis-and-sdks/schema-deployment).
- Self-hosted and embedded studios must also define the canonical studio URL in the [project management settings](https://sanity.io/manage). 
- For self-hosted and embedded studios that are not compiled using Sanity build tools (`sanity build` or `sanity deploy`), you'll also need to add a small bridge script to connect with the dashboard. 



## Sanity-hosted studio

If you are using Sanity's hosting service, you get the most straightforward route. To set up your project to automatically generate the necessary schema and manifest files on every deployment, follow these steps:

- Make sure your project is [upgraded](/docs/studio/upgrade) to `v3.88.1` or later of Sanity Studio. `@latest` is always recommended! 
- Deploy your studio by running the commmand  `npx sanity deploy`.

The Sanity CLI will automatically build your studio and manifest files and deploy them to the configured host.  The manifest file should be available at `<studioHost>.sanity.studio/static/create-manifest.json`

> [!TIP]
> Even auto-updating studios?
> Yes! Even if you are opted into auto-updating studios, you still need to make a one-time manual deployment in order to fully integrate with the dashboard. 
> 
> Update your local studio to sanity@latest, then run npx sanity deploy.

## Self-hosted studio

If you are not using Sanity's hosting service, you will need to manually deploy your studio schema and make sure the resulting files are available at the expected location.

- Make sure your project is updated to `v3.88.1` or later of Sanity Studio. `@latest` is always recommended!
- Generate and deploy the schema and manifest files by running `npx sanity schema deploy`.
- Serve the manifest files over HTTP GET from `<custom-studio-url>/static/<manifest-file>` (see filenames above).
- You can control where the manifest will be stored in your project by using the `--manifest-dir` parameter. For example, in Next.js it would be common to use `npx sanity schema deploy --manifest-dir /public/static`.
- Ensure the manifest files are publicly accessible on the internet without authentication.
- Add the studio URL in your [project management settings](https://sanity.io/manage).



### Studio embedded in Next.js

For Next.js projects with embedded studios, you should follow the same steps as in the [previous section](undefined), with a small change to how you generate the manifest files. 

- Make sure your project is [upgraded](/docs/studio/upgrade) to `v3.88.1` or later of Sanity Studio. `@latest` is always recommended!
- Generate the manifest files running `npx sanity manifest extract`. You'll need to specify a `--path` for the generated files that corresponds to the path of your studio relative to the root of your Next.js project. E.g., `npx sanity manifest extract --path public/studio/static`
- Generate and deploy your schema by running `npx sanity schema deploy`.
- Next.js will handle serving your manifest over HTTP GET for Dashboard when you deploy your application. 
- Add the studio URL in your [project management settings](https://sanity.io/manage). Make sure you include the full path to your studio. E.g., `https://cool-domain.com/admin`.
- Finally, add the dashboard bridge script to your studio route as shown in the next section, and deploy your project.

### Adding the bridge component

For self-hosted and embedded studios that are not compiled using `sanity build` or `sanity deploy`, OR using [next-sanity](https://github.com/sanity-io/next-sanity) you will also need to add a small script to enable the dashboard to properly interact with your studios. 

```html
<script src="https://core.sanity-cdn.com/bridge.js" async type="module" />
```

Exactly where you should put the script will vary depending on your exact setup, but a generalized example might look as follows:

```tsx
import {preloadModule} from 'react-dom'

const bridgeScript = 'https://core.sanity-cdn.com/bridge.js'

export default function StudioLayout({
  children,
}: {
  children: React.ReactNode
}) {
  preloadModule(bridgeScript, {as: 'script'})
  return (
    <>
      <script src={bridgeScript} async type="module" />
      {children}
    </>
  )
}
```

## Add a token for CI/CD pipelines

If you deploy your studio as part of an automated workflow, you will need to add a deploy token to your project in the Sanity project management settings and include a schema deployment step with the following command: 

```sh
SANITY_AUTH_TOKEN=<deploy_token> npx sanity schema deploy
```

A deploy token can be obtained by navigating to the API section of your [project management dashboard](https://sanity.io/manage).

#### Next steps

[Canvas](/docs/canvas)

[Media Library](/docs/media-library)

[Studio](/docs/studio)

[App SDK](/docs/app-sdk)







# Setting up your studio

## Create a new Studio with Sanity CLI

![Video](https://stream.mux.com/wIMs3CS7T4pP7hRArpQZsBZ01Be02vCjbK)

Run the command in your Terminal to initialize your project on your local computer.

See the documentation if you are [having issues with the CLI](/docs/help/cli-errors).

```sh
npm create sanity@latest -- --dataset production --template clean --typescript --output-path studio-hello-world
cd studio-hello-world
```

## Run Sanity Studio locally

Inside the directory of the Studio, start the development server by running the following command.

```sh
npm run dev
```

## Log in to the Studio

**Open** the Studio running locally in your browser from [http://localhost:3333](http://localhost:3333).

You should now see a screen prompting you to log in to the Studio. Use the same service (Google, GitHub, or email) that you used when you logged in to the CLI.



# Defining a schema

## Create a new document type

![Video](https://stream.mux.com/IfVfAwxfwOKN2khdGCQ3cs5IuF1rYte1)

Create a new file in your Studio’s `schemaTypes` folder called `postType.ts` with the code below which contains a set of fields for a new `post` document type.

```
import {defineField, defineType} from 'sanity'

export const postType = defineType({
  name: 'post',
  title: 'Post',
  type: 'document',
  fields: [
    defineField({
      name: 'title',
      type: 'string',
      validation: (rule) => rule.required(),
    }),
    defineField({
      name: 'slug',
      type: 'slug',
      options: {source: 'title'},
      validation: (rule) => rule.required(),
    }),
    defineField({
      name: 'publishedAt',
      type: 'datetime',
      initialValue: () => new Date().toISOString(),
      validation: (rule) => rule.required(),
    }),
    defineField({
      name: 'image',
      type: 'image',
    }),
    defineField({
      name: 'body',
      type: 'array',
      of: [{type: 'block'}],
    }),
  ],
})
```

## Register the `post` schema type to the Studio schema

Now you can import this document type into the `schemaTypes` array in the `index.ts` file in the same folder.

```
import {postType} from './postType'

export const schemaTypes = [postType]
```

## Publish your first document

When you save these two files, your Studio should automatically reload and show your first document type. Click the `+` symbol at the top left to create and publish a new `post` document.



# Querying content with GROQ

## Write your first GROQ query

![Video](https://stream.mux.com/Mc12Sdeu00ugrGuQyz00Du1G4AQZmT36UV)

Open **Vision** in your Studio's top nav bar and paste this query into the **Query** code block field.

```groq
*[_type == "post"]{
  _id,
  title,
  slug,
  publishedAt
}
```

- `*` represents all documents in a dataset as an array
- `[_type == "post"]` represents a **filter** to only return matching documents
- `{ _id, title, slug, publishedAt }` represents a **projection** which defines the attributes from those documents that you wish to include in the response.

## Run the query

Click **Fetch** to see the JSON output in **Results**. You should see the document you previously published in the results.

Queries run in Vision use your authenticated session, so you will see private documents – which have a `.` in the `_id` key, like `drafts.`. You will not see when queried from your front end in the next step.



# Displaying content in Nuxt.jxt

## Install a new Nuxt application

![Video](https://stream.mux.com/L02yip5K7fwXyG100zIGTpGi02ZOiktCSpV)

If you have an *existing* application, skip this first step and adapt the rest of the lesson to install Sanity dependencies to fetch and render content.

**Run** the following in a new tab or window in your Terminal (keep the Studio running) to create a new [Nuxt](https://nuxt.com/) application using the [Nuxt UI](https://ui.nuxt.com/) template for Tailwind CSS.

```sh
# outside your studio directory
npx nuxi@latest init -t ui nuxt-hello-world
cd nuxt-hello-world
```

You should now have your Studio and Nuxt application in two separate, adjacent folders:

```sh
├─ /nuxt-hello-world
└─ /studio-hello-world
```

## Install Sanity dependencies

**Run** the following inside the `nuxt-{{PROJECT_NAME_SLUGIFIED}}` directory to:

- Install and configure the [Nuxt Sanity integration](https://nuxt.com/modules/sanity)
- Install [@sanity/image-url](https://github.com/sanity-io/image-url) for generating images from Sanity content

```sh
npx nuxi@latest module add sanity
npm install @sanity/image-url
```

## Configure the Sanity client

**Update** the integration configuration to configure a Sanity Client to fetch content.

```
export default defineNuxtConfig({
  compatibilityDate: "2024-04-03",
  devtools: { enabled: true },
  modules: ["@nuxt/ui", "@nuxtjs/sanity"],
  // 👇 Add these lines
  sanity: {
    projectId: PROJECT_ID,
    dataset: PROJECT_DATASET",
  },
});
```

## Start the development server

**Run** the following command and open [http://localhost:3000](http://localhost:3333) in your browser.

```sh
npm run dev
```

## Update root layout

**Remove** the default entry point `app.vue` to use the pages directory strategy for routing.

```sh
rm app.vue
```

## Display content on the home page

Nuxt performs data fetching inside `script` tags at the top of `.vue` files

**Create** a route for a page with a list of posts fetched from your Sanity dataset, and visit [http://localhost:3000](http://localhost:3333)

```tsx
<script setup lang="ts">
import type { SanityDocument } from "@sanity/client";

const POSTS_QUERY = groq`*[
  _type == "post"
  && defined(slug.current)
]|order(publishedAt desc)[0...12]{_id, title, slug, publishedAt}`;

const { data: posts } = await useSanityQuery<SanityDocument[]>(POSTS_QUERY);
</script>

<template>
  <main class="container mx-auto min-h-screen max-w-3xl p-8">
    <h1 class="text-4xl font-bold mb-8">Posts</h1>
    <ul class="flex flex-col gap-y-4">
      <li v-for="post in posts" :key="post._id" class="hover:underline">
        <nuxt-link :to="`/${post.slug.current}`">
          <h2 class="text-xl font-semibold">{{ post.title }}</h2>
          <p>{{ new Date(post.publishedAt).toLocaleDateString() }}</p>
        </nuxt-link>
      </li>
    </ul>
  </main>
</template>
```

## Display individual posts

**Create** a new route for individual post pages.

The dynamic value of a slug when visiting `/[slug]` in the URL is used as a parameter in the GROQ query used by Sanity Client.

Notice that we’re using [Tailwind CSS Typography](https://github.com/tailwindlabs/tailwindcss-typography)’s `prose` class name to style the post’s `body` block content.

```tsx
<script setup lang="ts">
import type { SanityDocument } from "@sanity/client";
import imageUrlBuilder from "@sanity/image-url";
import type { SanityImageSource } from "@sanity/image-url/lib/types/types";

const POST_QUERY = groq`*[_type == "post" && slug.current == $slug][0]`;
const { params } = useRoute();

const { data: post } = await useSanityQuery<SanityDocument>(POST_QUERY, params);
const { projectId, dataset } = useSanity().client.config();
const urlFor = (source: SanityImageSource) =>
  projectId && dataset
    ? imageUrlBuilder({ projectId, dataset }).image(source)
    : null;
</script>

<template>
  <main
    v-if="post"
    class="container mx-auto min-h-screen max-w-3xl p-8 flex flex-col gap-4"
  >
    <a href="/" class="hover:underline">&larr; Back to posts</a>
    <img
      v-if="post.image"
      :src="urlFor(post.image).width(550).height(310).url()"
      :alt="post?.title"
      class="aspect-video rounded-xl"
      width="550"
      height="310"
    />
    <h1 v-if="post.title" class="text-4xl font-bold mb-8">{{ post.title }}</h1>
    <div class="prose">
      <p v-if="post.publishedAt">
        Published: {{ new Date(post.publishedAt).toLocaleDateString() }}
      </p>
      <SanityContent v-if="post.body" :blocks="post.body" />
    </div>
  </main>
</template>
```





# Deploying Studio and inviting editors

## Deploy your Studio with Sanity

![Video](https://stream.mux.com/CvYhCQr8e1oZt98NW202BZLLNv376VVKc)

In your Studio directory (`studio-hello-world`) run the following command to deploy your Sanity Studio.

```sh
npm run deploy
```

## Invite a collaborator

Now that you’ve deployed your Studio, you can optionally invite a collaborator to your project. Navigate to: `https://www.sanity.io/manage/project/{{PROJECT_ID}}/members`.

They will be able to access the deployed Studio, where you can collaborate together on creating content.





# APIs and SDKs

#### App SDK

[App SDK Quickstart](/docs/app-sdk/sdk-quickstart)

[App SDK Reference](https://reference.sanity.io/_sanity/sdk-react/)

[App SDK Explorer](https://sdk-explorer.sanity.io)



#### Schemas

[Introduction to schemas](/docs/apis-and-sdks/introduction-to-schemas)

[Studio schema reference](/docs/studio/schema-types)

[Schema Deployment](/docs/apis-and-sdks/schema-deployment)



#### Command Line Interface

[Introduction to the CLI](/docs/apis-and-sdks/cli)

[Importing Data](/docs/content-lake/importing-data)

[CLI Configuration](/docs/cli-reference/cli-config)



#### Asset API

[Presenting Images](/docs/apis-and-sdks/presenting-images)

[Image Metadata](/docs/apis-and-sdks/image-metadata)

[Asset CDN](/docs/apis-and-sdks/asset-cdn)





# Introduction to schemas

Schemas are the foundation of how content is structured, stored, and presented in the Sanity ecosystem. This guide will help you understand what schemas are, how they work, and how to use them effectively in your Sanity projects.

## What is a schema?

The schema consists of simple JavaScript (or TypeScript) objects that define your content model. They describe the structure, relationships, and constraints of your content, allowing you to create a tailored content management experience.

While the Content Lake itself is schema-less (providing flexibility for content storage), the schema control how content is organized and can be used to:

- [Generate TypeScript types](/docs/apis-and-sdks/sanity-typegen) for your projects
- Generate user-friendly content forms for [Sanity Studio](/docs/studio/schemas-and-forms)
- Define metadata fields (Aspects) for assets in [the Media Library](/docs/media-library)
- Enable apps like [Canvas](/docs/canvas/configure-content-mapping) to transform free-form documents to structured content
- Power agent actions through well-defined content structures

## Where schemas live

Traditionally, schemas have been defined within Sanity Studio projects, and this remains the primary location for schema definitions. However, the Sanity ecosystem has evolved to use schemas in other contexts:

- **Sanity Studio**: The main place for defining document types and field structures.
- **Media Library**: Uses schemas (called Aspects) to define sets metadata fields for assets.
- **Content Mapping**: Uses a schema to map free-form content from Canvas to a structured document in Content Lake, so it can be edited in the Studio and other apps.
- **Agent Actions**: Schemas provide the structure needed for AI agents to work with your content.

As the Sanity Content Operating System evolves, schemas will remain a central component that bridges the gap between the schema-less Content Lake and the structured interfaces used to create and manage content.

## How to design schemas

Schemas in Sanity represent your content model(s), which should be a reflection of your organization's unique business reality. When designing your schema, it's crucial to align it with how your teams actually work and think about content, rather than forcing teams to adapt to rigid technical structures. A well-designed schema considers the mental models of content creators, the workflows they follow, and the relationships between different content types in your business domain. 

By mapping your schema to these real-world considerations, you create a more intuitive content management experience that reduces friction, improves adoption, and ultimately leads to better content outcomes. 

Remember that schemas can evolve over time as your business needs change—start with the core concepts that matter most to your teams, then iterate and expand as you learn more about how your content model performs in practice.

#### Learn more about content modeling

[Content modeling in Sanity Studio](/docs/archive/content-modelling)

[Content modeling Course](https://www.sanity.io/learn/course/hello-structured-content/intro-to-structured-content)



## Your schemas will change

Schemas naturally evolve as your business requirements change, content strategies mature, and new channels emerge. Sanity embraces this reality by providing robust tooling to manage schema migrations and content transformations. The Content Lake's schema-less architecture gives you the flexibility to modify your content model without rebuilding your entire database. 

The schema migration tooling allow you to programmatically transform existing content to match new schema structures and run your validation rules against your whole dataset. Rather than treating schema changes as exceptional events, Sanity's approach acknowledges them as a normal part of the content lifecycle.

[Migrating your schema and content](/docs/content-lake/schema-and-content-migrations)



## Anatomy of schemas

### Content model or schema

Your overall content model (or schema) is the complete collection of document types and field definitions that make up your content structure. Think of it as the blueprint for your content.

### Document types

Document types are collections of documents used to build standalone pieces of content. You can think of them as similar to tables in SQL databases. A document type:

- Consists of multiple fields
- Has revision history
- Can have copies to represent published, draft, and version states
- Can have indexed and queryable references between them

**Note**: Document types can be whatever you need them to be and don't have to map directly to "a page" or "a post". It can also be "project," "person," "product," and "place."

### Field types

Field types are the building blocks of your schema that define what kind of data can be stored in each field. Sanity comes with a variety of built-in field types that you can use to model your content:

- **String**: For text content like titles, names, and descriptions
- **Number**: For numerical values
- **Boolean**: For true/false values
- **Date** and **DateTime**: For temporal data
- **Image** and **File**: For media assets
- **Reference**: For creating relationships between documents
- **Array**: For repeatable lists of other field types
- **Object**: For grouping related fields together
- **Block**: When used in an array, it gives you block content with Portable Text
- **Slug**: For URL-friendly strings
- **Geopoint**: For geographical coordinates

You can also create custom field types by combining existing types or extending them with custom validation and input components.

#### Field hoisting

While you can define custom field types "inline" in the document type definition's `fields` array, we recommend isolating them in their own files, importing them to the `schemas` array in the configuration, and then use them in your document types by referencing their `name` as the `type`.

> [!TIP]
> Standardize and reuse custom fields
> It's good practice to reuse and standardize custom fields. It makes it easier and more predictable to work on projects as they evolve, and what to expect in the data that comes out of your queries. You can also use GROQ projections to reshape data as you need it for specific contexts without having to change the content model.

## Example: Your first schema type

The easiest place to start with a schema is declaring it in the root configuration for your Sanity Studio project, typically in a file named `sanity.config.ts`.

While you can declare schemas inline in the configuration, the common practice is to organize them in external files and import them into the `schema.types` array.

Here's a simple example of a schema for a `person` document type:

```
import { defineType, defineField } from 'sanity'

export const personType = defineType({
  name: 'person',
  title: 'Person',
  type: 'document',
  fields: [
    defineField({
      name: 'name',
      title: 'Full name',
      type: 'string'
    }),
    defineField({
      name: 'portrait',
      title: 'Portrait',
      type: 'image'
    })
  ]
})
``````
import { person } from './person'

export const schemaTypes = [person]
``````
import { defineConfig } from 'sanity'
import { structureTool } from 'sanity/structure'
import { schemaTypes } from './schemaTypes'

export default defineConfig({
  name: 'default',
  title: 'My Sanity Project',

  projectId: 'your-project-id',
  dataset: 'production',

  plugins: [structureTool()],

  schema: {
    types: schemaTypes,
  },
})
```

This schema type definition creates a "Person" document type with fields for a name (string) and portrait (image), which Sanity Studio will automatically transform into appropriate form inputs. In this example, we use the helper function `defineType` and `defineField` which are there to help you validate that the schema is correctly configured and will give you autocomplete for available options.

## Advanced schema features

### Schema deployment 

To make your schemas available across the Sanity ecosystem (for content mapping, agent actions, etc.), you need to deploy them:

42. Ensure your Studio is updated to the latest version
42. Create a deploy token with the appropriate permissions
42. Run the command: `npx sanity@latest schema deploy`

This stores your schemas as system documents (of type `_system.schema`) in your dataset at the workspace level. If you have multiple workspaces, each will have its own schema. These deployed schemas are essential for:

- Content mapping between Sanity Canvas and Content Lake so it can be edited in Sanity Studio
- Enabling agent actions to work with your content
- Other integrations that need to understand your content structure

### Schemas for Media Library (Aspects)

The Media Library uses schemas (called Aspects) to define metadata fields for assets. These aspects help organize and categorize assets across your organization.

[Create an aspect](/docs/media-library/create-aspect)



## Conclusion

Schemas are a powerful tool in the Sanity ecosystem, providing structure and organization to your content while maintaining flexibility. Whether you're building a simple blog or a complex content platform, understanding how to effectively use schemas will help you create a tailored content management experience for your team.

By designing schemas that reflect your organization's business reality and leveraging features like schema deployment, you can create a cohesive content experience across all Sanity tools and integrations.



# Naming things

Naming things can be hard. When you set up the Sanity Studio you will need to name two kinds of things – your **documents/types** and the **fields** they contain.

## Naming documents

There are few formal constraints for what characters the names of documents and types may contain, but for simplicity, you might want to stick with the convention for field names and only use:

- Letters (a-z / A-Z)
- Numbers
- Underscore

Naming types in a singular form will improve the readability of your queries and code. Let’s see what happens if we use plural type in plural "movies":

```javascript
{
  name: 'movies', // DON'T do this. It's better to name it "movie"
  type: 'document',
  fields: [
    {name: 'title', type: 'string'}
  ]
}
```

With this schema, your query for a list of movies will now look something like: `*[_type == 'movies']`. if you were to spell this query out, you could say *"give me all the documents of type 'movies'" *when it might make more sense to say *"give me all the documents of type 'movie'"*.

## Naming fields

The names of fields contained within documents and objects have some real formal requirements and **must not**:

- Start with underscores (`_`), which are reserved for system fields
- Contain characters other than:- Letters (a-z / A-Z)
- Numbers
- Underscores



So keep in mind that field names can't contain hyphens or unicorn emoji for that matter.

Apart from that you may do as you like, but we recommend using the plural form for arrays, like in this example from a minimal schema for a movie where the array name of `castMembers` is plural:

```javascript
export default {
  name: 'movie',
  title: 'Movie',
  type: 'document',
  fields: [
    {
      name: 'title',
      title: 'Title',
      type: 'string',
      required: true
    },
    {
      name: 'castMembers',
      title: 'Cast Members',
      type: 'array',
      of: [{type: 'castMember'}]
    }
  ]
}

```

You might also want to consistently follow capitalization and naming conventions that you like and that fit with the languages you'll be using to consume the data. For example, if you want to use [dot-notation](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Property_Accessors#dot_notation) in JavaScript, it is required that the key be a [valid identifier](https://developer.mozilla.org/en-US/docs/Glossary/Identifier). 

You should also consider that programming languages have reserved keywords (e.g. [class, import, or return in JavaScript](https://262.ecma-international.org/#prod-ReservedWord)) or common variables names in their environments (such as `global`, `window`, or `process`). If you discover such namespace collisions, you can use the [renameField migration](https://github.com/sanity-io/sanity-recipes/blob/master/snippets/renameField.js) script to rename those fields in your dataset.



# Attribute limit

## What is the attribute limit?

The attribute limit determines how many unique combinations of attribute and datatype you can have in your dataset. Depending on what plan your project is on, your limit is one of the following:

- Free: 2k attributes
- Growth: 10k attributes
- Enterprise: Custom # attributes

> [!WARNING]
> Gotcha
> The attribute limit is a hard technical limit right now. For this reason, we do not currently offer a pay-as-you-go option for extra attributes.

## What counts as an attribute?

As shown above, an attribute is officially defined as *a unique combination of attribute and datatype*. An alternative way to think about them is as the different paths through your content.

Let's take a basic data structure:

```json
{
  "foo": [
    {
      "bar":…,
      "baz":…
    },
    {
      "bar":…,
      "baz":…
    },
    {
      "bat": {
        "bar":…
      }
    }
  ]
}
```

This structure contains six unique paths or attributes:

10. foo -> an array
10. foo[] -> an object
10. foo[].bar -> a string
10. foo[].baz -> a string
10. foo[].bat -> an object
10. foo[].bat.bar -> a string

Paths only count towards your attribute limit when they hold actual content. Solely changing your schema definitions will not affect the attribute count. Schema definitions define the structure of your content, a bit like a blueprint defines the structure of a building. Until you add or remove content using the Sanity Studio or the HTTP API, your attribute count will remain unchanged.

Each unique path is counted once, no matter how often it is used. Removing a path from your attribute count requires deleting every piece of content on that path across all documents.

In short, your attribute count:

- goes up when you first add content on a path
- goes down when a path no longer holds any content
- stays the same regardless of whether a path is used once or many times

## Best practices

When structuring your content, there are a few pitfalls to keep in mind to avoid hitting the attribute limit. Although this is not an exhaustive list, following the best practices below should go a long way in keeping your attribute count in check.

### Use arrays for page building

A common use case for Sanity is using structured content for [page building](https://www.sanity.io/guides/how-to-use-structured-content-for-page-building). In setting up a page builder, it may be tempting to use the block content type as the editor gives a lot of flexibility and allows adding any number of custom objects that can then be used inline.

However, a block content field has quite an extensive data structure by default:
• a `blockContent` array, with inside of it:
• `blocks` objects, with inside of them:
• `markDefs` and `children` arrays, with inside of them:
• `span` types, with inside of them:
• a `marks` array and a `text` field

This nested structure is further extended by any custom types you add to it, all with their own unique paths. A block content field with many custom objects may therefore lead to a hefty amount of attributes.

Another issue with this approach is that people sometimes want to use block content fields *inside* of custom objects. This is likely to lead to even more attributes as a result of now having the above structure embedded in the same structure. Moreover, when the exact same block content component is used, allowing this type of nesting basically gives editors the freedom to nest to an arbitrarily deep level, which can then drag a project over the attribute limit.

To avoid any of these challenges and keep the attribute count as low as possible, we recommend using arrays for page building. In addition to fewer attributes, greater control over the exact content structure, and reduced risk of getting into nesting situations, this approach has the added advantage of not having to deal with serializers for complex custom objects. 

### Avoid excessive nesting and recursive data structures

Things get worse when subsequently the same block content configuration is used for any block content fields inside the custom objects, so editors can endlessly nest the entire page builder inside itself.

### Focus on meaning, not presentation

Before responsive web made its entrance and people started optimizing for different devices, it was customary to mix content with presentation. A headline could be blue, have font size 24px, line-height 30px, and a bottom padding of 10px. Although it may still be tempting today to offer that same level of control to editors, there are several downsides to this approach. For one, whenever you want to change your front-end's design, editors will have to review all relevant content.

Most importantly for this guide, adding all these presentational attributes is likely to boost your attribute count significantly as they would exist for nearly every piece of content.

Instead of mimicking CSS properties in your schema definitions, we recommend a separation of concerns. Leave the presentational aspects to wherever you implement your content and instead stick to semantics in your content structure - in other words, focus on the *meaning* of your content.

### Beware of multipliers in translation/localization

There is a variety of i18n/l10n approaches out there, some of which have a greater impact on your attribute count than others. For example, one approach suggests wrapping all your fields inside a language object, so you get the following structure:

```json
{
 "de": {
  ...
 }
 "en": {
  ...
 }
}
```

This basically multiplies the number of attributes by the number of languages added, as all fields get duplicated on a language path. Adding more than a few languages this way means trouble.

Instead of duplicating the fields inside a document, thereby creating all these extra paths, a more frugal approach is to duplicate the *document* instead. To differentiate between the different languages and more easily query for them, you can consider adding a (hidden) internationalization field to your document type and/or add the language to the document ID. As you will be reusing the same fields across different documents, adding an extra language no longer affects your attribute count at all.

## What to do if you hit the limit?

If you inadvertently hit the attribute limit on one of your datasets, you will see the following error when opening your Sanity Studio: `Total attribute count exceeds limit`.



### Export your data

Before deleting any content or changing your data structure, we highly recommend running a full export of your dataset to prevent any unintended data loss. To do so, you can run the [dataset export](https://www.sanity.io/docs/migrating-data#c4665bde1f66) command in your terminal. For example:

`sanity dataset export production production.tar.gz`

### Get unblocked

The first step after exporting your data is to get unblocked so you and other users on your project can work in the studio again. In other words, the challenge is to get back below the attribute limit.

Perhaps there is a heavily nested structure with block content *and* translations that could be optimised. Or maybe you have singletons for different pages that could be folded into a single page type instead to further reduce the number of unique paths.

A final note is that it also helps to remove any unused content from schema revisions. For example, if you used to have a particular document type with a bunch of documents, but later removed that type, or even some fields within a type, make sure to clean up the content so there are no leftovers in the datastore that will count towards the attribute limit.

### Restructure your content

How to restructure your content depends on your content model and is therefore different per project. However, there are a bunch of examples to get you started. Please note that in all cases, it is highly recommended to run a full dataset export *before *proceeding. 

### Track your progress

To keep an eye on your attribute limit while restructuring your content, you can use this URL: `https://<projectId>.api.sanity.io/v1/data/stats/<datasetName>`

The attribute count is the value of `fields.count.value` and the limit is inside `fields.count.limit`.

## Closing remarks

Although this guide was specifically about the attribute limit, the principles outlined above are best practices that are likely to lead to a more solid, flexible, and future-proof content model in any situation.



# Studio schema reference

The top level `schema` configuration accepts an object with two properties: `templates` and `types:`

- The `templates` property accepts an array of Initial Value Template configuration objects or a callback function returning the same.
- The `types` property accepts an array of schema definition objects or a callback function returning the same. 

In both cases, the callback function is called with the current value as the first argument and a context object as the second. Thus, you can access schema definitions and Initial Value Templates implemented by plugins.

#### Properties

| Property | Description |
|----------|-------------|
| templates | An array of initial value templates, or a callback function that resolves to the same. |
| types | An array of schema definitions or a callback function that resolves to the same. |


The `templates` property is discussed in greater detail [in this article](/docs/studio/initial-value-templates), and a reference article can be found [here](/docs/studio/initial-value-templates-api). The rest of this article will deal with the default set of schema types supported in the Sanity Studio.

All schema types are listed below or in the documentation menu.

[Array](/docs/studio/array-type)

[Block](/docs/studio/block-type)

[Boolean](/docs/studio/boolean-type)

[Cross Dataset References](/docs/studio/cross-dataset-references)

[Date](/docs/studio/date-type)

[Datetime](/docs/studio/datetime-type)

[Document](/docs/studio/document-type)

[File](/docs/studio/file-type)

[Geopoint](/docs/studio/geopoint-type)

[Image](/docs/studio/image-type)

[Number](/docs/studio/number-type)

[Object](/docs/studio/object-type)

[Reference](/docs/studio/reference-type)

[Slug](/docs/studio/slug-type)

[String](/docs/studio/string-type)

[Span](/docs/studio/span-type)

[Text](/docs/studio/text-type)

[URL](/docs/studio/url-type)

[Global Document Reference](/docs/studio/global-document-reference-type)



## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Name of any valid schema type. This will be the type of the value in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | Required. The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | Takes a static or a callback function that resolves to a boolean value (truthy or falsy) and hides the given field based on it. You can use this property for conditional fields. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a document type or a field as deprecated. This will render the field(s) as read-only with a visual deprecation message defined by the reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

## Validation

**Note**: The properties listed above are common for all data types. For a more thorough description of how to use them, have a look at the [Object Type](/docs/object-type).

The studio loads all schemas defined under `schema.types` in `studio.config.js`.

```javascript
//sanity.config.js
import {defineConfig} from 'sanity'

export default defineConfig({
  /* ... */
  schema: {
    types: [
      {
        title: "My Example Document Type",
        name: "exampleDocumentType",
        type: "document",
        fields: [
          {
            title: "Greeting",
            name: "greeting",
            type: "string"
          }
        ]
      }  
    ]
  }
})

```

To keep things organized, consider keeping the types array in a separate file and import it into `studio.config.js`. 

```javascript
//schemaTypes.js
export const schemaTypes = [
  {
    title: "My Example Document Type",
    name: "exampleDocumentType",
    type: "document",
    fields: [
      {
        title: "Greeting",
        name: "greeting",
        type: "string"
      }
    ]
  }  
]

//sanity.config.js
import {defineConfig} from 'sanity'
import {schemaTypes} from './schemaTypes'

export default defineConfig({
  /* ... */
  schema: {
    types: schemaTypes
  }
})

```

You should also consider using the `defineType`, `defineField` and `defineArrayMember` helper functions when working with schemas. These will give you better IDE auto-suggestions and provide type-safety when used in TypeScript files. Using these functions is *completely optional.*

```javascript
import {defineType, defineField, defineArrayMember} from 'sanity'

export const someDocumentType = defineType({
  title: "Some Document Type",
  name: "exampleDocumentType",
  type: "document",
  fields: [
    defineField({
      title: "String array",
      name: "strings",
      type: "array",
      of: [
        defineArrayMember({ type: "string" })  
      ]
    })
  ]
})  

```

### Plugins

Plugins may also provide types. They will be available in the studio exactly like studio configured types. 

Using plugins to organize your code can be helpful as the studio codebase grows.

```javascript
// pluginWithSchema.js
import {definePlugin, defineType, defineField} from 'sanity'

export const pluginWithSchema = definePlugin({
  name: 'plugin-with-schema',
  schema: {
    types: [
      defineType({
        title: "Plugin object",
        name: "exampleObject",
        type: "document",
        fields: [
          defineField({
            title: "Title",
            name: "title",
            type: "string"
          })
        ]
      })    
    ]
  }
})

//sanity.config.js
import {defineConfig} from 'sanity'
import {pluginWithSchema} from './pluginWithSchema'

export default defineConfig({
  /* ... */
  plugins: [pluginWithSchema()]
})

```



# Schema Deployment

With the launch of version [3.88.0](https://www.sanity.io/changelog/525de82a-dd7b-40c7-bf38-12248136c339), Sanity Studio gains support for deploying a representation of your content model, in the form of a schema, to your dataset. This allows for robust integration between your finely tuned studios and apps like [Dashboard](/docs/dashboard) and [Canvas](/docs/canvas).

> [!TIP]
> You only need to do this once per studio environment. If you have run sanity schema deploy as part of setting up Canvas integration, you don't need to do it again for Dashboard, and vice versa.

## Overview

The schema commands belong to the `sanity schema` group. They allow you to deploy your schemas at the workspace level to your corresponding combination of `dataset` and `projectId`, making them accessible to various Sanity apps and APIs.

For commands requiring a deploy token (when not logged in with proper privileges), use:

```sh
SANITY_AUTH_TOKEN=<your-deploy-token> sanity schema <command>
```

## Available Commands

### `sanity schema deploy`

Deploys schema documents to workspace datasets. If you've already run `sanity login`, you typically have deploy permission by default. In CI environments where `sanity login` hasn't been executed, you'll need to provide a deploy token.

**Options:**

- `--workspace <workspace_name>` - Deploy for a specific workspace. Essential for studios with multiple projectIds.
- `--tag <tag>` - Add a tag suffix to the schema id for testing without overwriting existing schemas.
- `--manifest-dir <directory>` - Directory with manifest file (default: `./dist/static`). Crucial for embedded studios.
- `--no-extract-manifest` - Skip manifest generation when nothing has changed.
- `--verbose` - Show detailed deployment information, including the schemaId.

**Examples:**

```sh
# Deploy all workspace schemas
sanity schema deploy

# Deploy schema for specific workspace
sanity schema deploy --workspace default

# Use existing manifest file
sanity schema deploy --no-extract-manifest
```

### `sanity schema list`

Lists all schemas in the current dataset. This is important for identifying the schema `id` needed for [agent actions](/docs/agent-actions).

**Options:**

- `--json` - Get schema as JSON
- `--id <schema_id>` - Fetch a single schema by id
- `--manifest-dir <directory>` - Directory containing manifest file (default: `./dist/static`)
- `--no-extract-manifest` - Skip manifest generation

**Examples:**

```sh
# List all schemas
sanity schema list

# Get specific schema
sanity schema list --id _.schemas.workspaceName

# Get schemas as JSON
sanity schema list --json
```

### `sanity schema delete`

Removes schema documents by id. Useful when you need to remove schemas from Canvas or Agent Actions.

**Options:**

- `--ids <schema_id_1,schema_id_2,...>` - Comma-separated list of schema ids to delete
- `--dataset <dataset_name>` - Delete schemas from a specific dataset
- `--manifest-dir <directory>` - Directory containing manifest file (default: `./dist/static`)
- `--no-extract-manifest` - Skip manifest generation

**Examples:**

```sh
# Delete single schema
sanity schema delete --ids _.schemas.workspaceName

# Delete multiple schemas
sanity schema delete --ids _.schemas.workspaceName,_.schemas.otherWorkspace.tag.taggedSchema
```

### `sanity manifest extract`

Extracts studio configuration as JSON manifest files, which are used by other Sanity tools to understand your studio setup.

**Options:**

- `--path <directory>` - Custom destination directory for manifest files (default: `/dist/static`)

**Examples:**

```sh
# Extract manifests to default location
sanity manifest extract

# Extract to custom directory
sanity manifest extract --path /public/static
```

## Manifest File Structure

The extracted manifest follows this structure:

```
interface CreateManifest {
  version: number        // Current version: 2
  createdAt: string      // ISO timestamp
  workspaces: ManifestWorkspaceFile[]
}

interface ManifestWorkspaceFile {
  name: string
  title?: string
  subtitle?: string
  basePath: string
  dataset: string
  projectId: string
  schema: string        // filename with serialized schema
  tools: string         // filename
  icon: string | null
}
```

## Integration with Other Commands

The manifest system works with several Sanity CLI commands:

37. **Schema Commands** - `deploy`, `list`, and `delete` use the manifest to identify schemas
37. **Build Process** - The manifest is generated during builds and used by Sanity tools

## Additional Commands

The following commands are only relevant for [Typegen](/docs/apis-and-sdks/sanity-typegen) and don't impact server-side schema functionality or compatibility with Dashboard, Canvas, or Agent Actions:

### `sanity schema validate`

Validates schema types in a workspace.

**Options:**

- `--workspace <name>` - Workspace to validate
- `--format <pretty|ndjson|json>` - Output format
- `--level <error|warning>` - Minimum reporting level (default: warning)

### `sanity schema extract`

Extracts a JSON representation of a Sanity schema.

**Options:**

- `--workspace <name>` - Workspace for schema generation
- `--path` - Custom destination for the schema file
- `--enforce-required-fields` - Makes required fields non-optional (default: false)
- `--format=[groq-type-nodes]` - Format schema as GROQ type nodes

## Related Commands

- `sanity deploy` - Includes schema deployment in the process
- `sanity typegen generate` - Creates TypeScript types from schema types and GROQ queries

## Things to Remember

51. You don't need to keep your manifest in version control since it's derived from your codebase.
51. For embedded studios, ensure your manifest is located at `<studio-url>/static/create-manifest.json`. You can specify this using the `--path` option.
51. Most commands regenerate manifests by default. Use `--no-extract-manifest` to use existing files.
51. All schema commands require appropriate project permissions.

## Common Errors

- **Manifest Not Found** - Workspace schema file doesn't exist at specified path
- **Invalid Manifest Format** - The manifest file has formatting issues
- **Permission Errors** - Insufficient permissions to read schema from project/dataset



# Aspects schema for Media Library

Aspects are sets of properties that describe an asset and are defined just like schemas. Asset managers can apply aspects to assets in the library, with mutations, or programmatically during upload. This information stored in aspects is specific to the Media Library. For local metadata, you should use fields in your Studio projects.

In this guide, you'll create a new aspect and deploy it to your Media Library.

Prerequisites:

- `sanity` v3.85.1 or higher

## Configure your aspect directory

In a project with a `sanity.cli.ts` file, edit the configuration to include a `mediaLibrary.aspectsPath`.

```
import {defineCliConfig} from 'sanity/cli'

export default defineCliConfig({
  api: {
    projectId: '3do82whm',
    dataset: 'production'
  },
  mediaLibrary: {
    aspectsPath: 'aspects',
  },
  autoUpdates: true,
})
```

In the example above, the `aspects` directory is relative to the location of the `sanity.cli.ts` file.

> [!NOTE]
> Where should you place aspect definitions?
> At this time, in order to deploy aspects you need a sanity.cli.ts configuration connected to a project and dataset. We're working to improve this experience in the future. For the time being, we recommend manually setting up a configuration file, or working directly in an existing Sanity Studio project.

## Define a new aspect

In the same directory, generate a new aspect with the Sanity CLI.

```sh
sanity media create-aspect
```

This command prompts you for a name and creates a new aspect in your project. Aspect names must be unique and contain only letters, numbers, and hyphens.

Aspects can be a single field or an object containing multiple fields. They can contain strings, objects, arrays, or nearly any [Studio schema type](/docs/studio/schema-types). Validation rules, image types, file types, and custom components are not allowed. 

The CLI creates an object-type aspect with a single string field similar to the example below. In this case, we set the name as `copyright`.

```
import {defineAssetAspect, defineField} from 'sanity'

export default defineAssetAspect({
  name: 'copyright',
  title: 'copyright',
  type: 'object',
  fields: [
    defineField({
      name: 'string',
      title: 'Plain String',
      type: 'string',
    }),
  ],
})

```

Modify the aspect with more fields. In this example, we'll update the existing string field and add a `date` type field.

```
import {defineAssetAspect, defineField} from 'sanity'

export default defineAssetAspect({
  name: 'copyright',
  title: 'copyright',
  type: 'object',
  fields: [
    defineField({
      name: 'copyrightHolder',
      title: 'Copyright Holder',
      type: 'string',
    }),
    defineField({
      name: 'copyrightDate',
      title: 'Date',
      type: 'date',
    }),
  ],
})

```

Once deployed, this will look like the following in your Media Library:

![a screenshot of a web page that says aspects copyright copyright holder and date .](https://cdn.sanity.io/images/3do82whm/next/49ac62823d932a41274fce626ea3b92e1e2e02eb-882x806.png)

You can see more aspect examples in the [aspect patterns cheat sheet](/docs/media-library/aspect-patterns).

## Deploy the aspect

With your aspect defined, it's time to deploy it to your Media Library.

Run the following to deploy a single aspect. Replace `copyright` with your aspect name.

```sh
sanity media deploy-aspect copyright
```

If you make additional changes to the aspect, you can update it by running the `deploy-aspect` command again.

### Delete an aspect

To delete an aspect from your library, run the following command, replacing `copyright` with the name of your aspect.

```sh
sanity media delete-aspect copyright
```

This deletes the aspect from your Library, but will not remove the local definition file.



# Introduction

```sh
# Installing the CLI globally
npm install --global sanity@latest

# Alternatively
yarn global add sanity@latest
pnpm install --global sanity@latest

# Running the CLI without global installation
npx -y sanity@latest [command]

```

The `sanity` Command Line Interface (CLI) is a collection of tools for managing, developing, debugging, and deploying your Sanity Studio projects as well as running scripts to migrate or manipulate your data.

The CLI can be installed as a global dependency in your development environment, locally on a per-project basis, or in many cases, accessed entirely using `npx sanity@latest [command]`, which bypasses the need to install the CLI globally.

[Reference: Command Line Interface](/docs/cli-reference/cli-config)



> [!WARNING]
> Gotcha
> Whether you choose to install the sanity CLI or use it only with npx, you will need node and npm installed on your system.
> 
> How to install node and npm?

## The CLI configuration file

You can add project-specific CLI configuration by adding a file named `sanity.cli.js` (`.ts`) in your project‘s root folder. 

### Minimal example

```javascript
// sanity.cli.js
import { defineCliConfig } from "sanity/cli";

export default defineCliConfig({
  api: {
    projectId: "<your-project-id>",
    dataset: "production",
  }
});

```

### Advanced example

You can embed further settings in your CLI configuration file, including specifying the local server port for `sanity dev`, GraphQL deployments, and extending the Vite configuration.

```javascript
// sanity.cli.js
import { defineCliConfig } from "sanity/cli";

export default defineCliConfig({
  api: {
    projectId: "<your-project-id>",
    dataset: "production",
  },
  server: {
    hostname: "localhost",
    port: 3333,
  },
  graphql: [{
    tag: "default",
    playground: true,
    generation: "gen3",
    nonNullDocumentFields: false,
  }],
  vite: (config) => config,
});

```

## Uninstall Sanity CLI

If you prefer to invoke the CLI with `npx` or similar, or you no longer want Sanity CLI installed globally, you can uninstall it using the preferred method for your package manager.

```sh
# Remove the CLI globally
npm uninstall --global sanity

# Alternatively
yarn global remove sanity
pnpm remove --global sanity
```





# Importing content

> [!NOTE]
> Media Library available
> This guide outlines details for importing documents, including images and files, into a dataset. The Media Library allows images and files to be used in any dataset in your organization.
> 
> For details on importing assets to a centralized library, review our guide on importing assets.

There are two ways to import data into your Sanity project. 

The recommended way of importing data is to use the [Command Line Interface](/docs/apis-and-sdks/cli). You can run  `sanity dataset import --help` for a quick summary of syntax and options. Your other option is to use one of our client libraries and handle it yourself. 

> [!WARNING]
> Gotcha
> Consider disabling any webhooks you might have that could cause high volumes of traffic to the receiving endpoint on importing data.

## Import using the CLI

The Sanity import tool operates on [newline-delimited JSON](https://github.com/ndjson/ndjson-spec) (NDJSON) files. Basically, each line in a file is a valid JSON-object containing a document you want to import.

Documents should follow the structure of your [data model](/docs/archive/content-modelling) – most importantly, the requirement of a `_type` attribute. The `_id` field is optional – but helpful – in case you want to make references or be able to re-import your data replacing data from an old import. `_id`s in Sanity are usually a [GUID](http://guid.one/guid), but any string containing only letters, numbers, hyphens, and underscores are valid.

During import, all references are automatically set to *weak*, then flipped to *strong* after all documents are in place. This ensures that you can import documents that reference other documents in any order you like.

Assets (images and files) are stored using references in Sanity. To make it easy to import these and refer to them within your documents, you can use a special `_sanityAsset` property where you would normally put a `_ref`. For instance, let's say you want your document to end up like this:

```javascript
{
  "_id": "movie_123",
  "_type": "movie",
  "title": "Rogue One",
  "poster": {
    "_type": "image",
    "asset": {
      "_ref": "image_234",
      "_type": "reference"
    }
  }
}
```

This is what your ready-to-import document should look like:

```javascript
{
  "_id": "movie_123",
  "_type": "movie",
  "title": "Rogue One",
  "poster": {
    "_type": "image",
    "_sanityAsset": "image@file:///local/path/to/rogue-one-poster.jpg",
  }
}
```

However, ndjson uses the newline character as delimiter (NDJSON == Newline Delimited JSON), therefore your ndjson file must be structured with one document on each line, like this:

```json
{"_id": "movie_123", "_type": "movie", "title": "Rogue One", "poster": {"_type": "image", "_sanityAsset": "image@file:///local/path/to/rogue-one-poster.jpg"}}
{"_id": "another_movie", "_type": "movie"}
{"_id": "yet_another_movie", "_type": "movie"}

```

Note that you need to prefix the asset URL with a type declaration – either `image@` or `file@`.

If your asset is on the Internet use `image@https://example.com/path/to/rogue-one-poster.jpg` instead of `image@file:///local/path/to/rogue-one-poster.jpg`.

> [!WARNING]
> Gotcha
> File URIs are absolute so include the entire path.

Once you have prepared your ndjson file, you can run the import using the Sanity CLI.

> [!NOTE]
> What should I import?
> In some cases you will want to import your ndjson file, such as when you've exported your dataset, made changes to the ndjson file, and are importing it back into the same dataset.
> 
> In other cases you will want to compress your dataset back into a tarball / tar file (.tar, .tar.gz, or .tgz), which includes the ndjson file and your assets. You might take this approach when migrating data to a new dataset, as you'll want to maintain references to assets.
> 
> If you're getting an import error like Error: Error while fetching asset from "file://./images/<image-name>.<ext>": File does not exist at the specified endpoint, you can either (1) make the filenames absolute or (2) import a tarball (including assets) rather than an ndjson file.

```bash
sanity dataset import <file> <targetDataset>
```

E.g.:

```bash
sanity dataset import my-data-dump.ndjson production

// or

sanity dataset import staging.tar.gz production
```

> [!TIP]
> Protip
> The import will fail if an incoming document already exists in the dataset. A couple of options allow you to amend this:
> 
> --replace Overwrite existing documents. If you specify _id in the imported data, this flag can be very useful. It will let you reimport stuff that you got wrong in an earlier pass.
> --missing Only create documents which don't exist, leave the rest alone.
> 
> The import will also fail if an asset is unavailable. This typically happens if the file isn't at the given path on your local system or the asset URL returns 404. You can tell the import not to fail on a missing asset by passing the --allow-failing-assets option.

> [!TIP]
> Protip
> Check out our reference-type docs page for more ways on how to reference different documents.

## Import using a client library

If you prefer not to use our CLI import tool, you may of course do the import yourself with help from one of our client libraries.

There are some common pitfalls to keep in mind:

- *Concurrency*. While you may have thousands of documents to import, you shouldn't trigger thousands of requests in parallel. This is going to exceed API rate limits and might fail. We advise you to use a queue with a reasonably low concurrency.
Use a library to keep your import below our [API rate limit](/docs/content-lake/technical-limits):

```javascript
const {default: PQueue} = require('p-queue')
const queue = new PQueue({
  concurrency: 1,
  interval: 1000 / 25
})

queue.add(() => client.create(...))
queue.add(() => client.patch('id').inc('visits').commit())
```

- *API usage limits*. Importing large data sets can quickly cause a lot of requests, especially if you import a single document per request. It is usually a good idea to send [multiple mutations within a single transaction](https://www.sanity.io/docs/js-client#multiple-mutations-in-a-transaction).
- *Mutation size limits*. While it's a good idea to do multiple mutations per transaction, you need to make sure that the size of the request is [within our limits](https://www.sanity.io/docs/technical-limits#c854fda72658), in terms of byte size.
- *Mutation visibility*. A Sanity client will use the [visibility mode](https://www.sanity.io/docs/http-mutations#visibility-937bc4250c79) of `sync` by default, which means that it will wait for the documents to be searchable before returning. This should not be necessary when importing large datasets, so we recommend you use `deferred`. If you have a lot of documents, it can take a little while for them to be searchable, but the import job will move along much faster.
- *References*. If you are referring to one document from another, they either need to be imported in the right order, or the reference needs to be flagged as *weak* by setting the `_weak` property to `true`. After importing, you probably want to remove the weak property in order to prevent referenced documents from being deleted.

> [!WARNING]
> Gotcha
> When a weak reference is desired, you should use the weak property when defined in the schema but _weak when set up using a client. Using the weak property with the client will likely return the error: key "weak" not allowed in ref.
> 
> weak in the schema, _weak in the JSON.

- *Assets*. Since assets (e.g., files and images) in Sanity are stored using references, you'll need to upload the assets first and put the returned document ID in your reference.

With this in mind, do check out our [client libraries](/docs/client-libraries) documentation to see how to perform mutations.



# Reference

The `sanity` Command Line Interface (CLI) is a handy tool for managing your Sanity projects in your terminal. Note that there are some commands that can only be run in a project folder and global ones.

[Learn more about the Sanity CLI](/docs/apis-and-sdks/cli)



## Configuration file

Sanity CLI can read configuration from a `sanity.cli.js` (`.ts`) file in the same folder that the command is run in. It will fall back on the configuration in the `sanity.config.ts` file.

#### Properties

| Property | Description |
|----------|-------------|
| api | Defines the projectId, dataset that the CLI should connect and run its commands on |
| server | Defines the hostname and port that the development server should run on. hostname defaults to localhost, and port to 3333. |
| graphql | Defines the GraphQL APIs that the CLI can deploy and interact with. |
| reactStrictMode | Wraps the Studio in <React.StrictMode> root to aid in flagging potential problems related to concurrent features (startTransition, useTransition, useDeferredValue, Suspense). Can also be enabled by setting SANITY_STUDIO_REACT_STRICT_MODE="true"\|"false".  It only applies to sanity dev in development mode and is ignored in sanity build and in production. Defaults to false. |
| vite | Exposes the default Vite configuration for the Studio so it can be changed and extended. |
| project | Contains the property basePath which lets you change the top-level slug for the Studio. You typically need to set this if you embed the Studio in another application where it is one of many routes. Defaults to an empty string. |


> [!WARNING]
> Gotcha
> If you run sanity --help outside a folder with a project configuration file and without a specified projectId flag, you will only see the subset of commands that's non project specific.

## GraphQLAPIConfig

#### Properties

| Property | Description |
|----------|-------------|
| id | ID of GraphQL API. Only (currently) required when using the --api flag for sanity graphql deploy, in order to only deploy a specific API. |
| workspace | Name of workspace containing the schema to deploy

Optional, defaults to default (eg the one used if no name is defined) |
| source | Name of source containing the schema to deploy, within the configured workspace

Optional, defaults to default (eg the one used if no name is defined) |
| tag | API tag for this API - allows deploying multiple different APIs to a single dataset 

Optional, defaults to default |
| playground | Whether or not to deploy a "GraphQL Playground" to the API url - an HTML interface that allows running queries and introspecting the schema from the browser. Note that this interface is notsecured in any way, but as the schema definition and API route is generally open, this does notexpose any more information than is otherwise available - it only makes it more discoverable.
Optional, defaults to true |
| generation | Generation of API to auto-generate from schema. New APIs should use the latest (gen3).

Optional, defaults to gen3 |
| nonNullDocumentFields | Define document interface fields (_id, _type etc) as non-nullable. If you never use a document type as an object (within other documents) in your schemas,  you can (and probably should) set this to true. Because a document type could be used inside other documents, it is by default set to false, as in these cases these fields can be null.

Optional, defaults to false |
| filterSuffix | Suffix to use for generated filter types.

Optional, Defaults to Filter. |


## Commands

```text
usage: sanity [--default] [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   build      Builds the Sanity Studio configuration into a static bundle
   codemod    Runs a code modification script
   cors       Configures CORS settings for Sanity projects
   dataset    Manages datasets, like create or delete, within projects
   debug      Gathers information on Sanity environment
   deploy     Builds and deploys Sanity Studio to Sanity hosting
   dev        Starts a local dev server for Sanity Studio with live reloading
   docs       Opens the Sanity documentation
   documents  Manages documents in your Sanity Content Lake datasets
   exec       Executes a script within the Sanity Studio context
   graphql    Deploys changes to your project's GraphQL API(s)
   help       Displays help information about Sanity
   hook       Sets up and manages webhooks within your Sanity project
   init       Initialize a new Sanity Studio project
   install    Installs dependencies of the current project
   login      Authenticates against the Sanity.io API
   logout     Logs out of the Sanity.io session
   manage     Opens the Sanity project management UI
   migration  Manages content migrations for Content Lake datasets
   preview    Starts a server to preview a production build of Sanity Studio
   projects   Interact with projects connected to your logged in user
   schema     Interacts with Sanity Studio schema configurations
   start      Alias for `sanity preview`
   telemetry  Interact with telemetry settings for your logged in user
   undeploy   Removes the deployed Sanity Studio from Sanity hosting
   users      Manages users of your Sanity project
   versions   Shows the installed versions of Sanity CLI and core components

See 'sanity help <command>' for specific information on a subcommand.
```

> [!NOTE]
> CLI option flag order
> For commands with option flags, add the option flag to the end after any arguments. When adding option flags to both commands and subcommands, make sure the command flags are before the subcommand. For example: 
> 
> sanity COMMAND [args] [--command-flags] SUBCOMMAND [args] --[subcommand-flags]
> 
> You can always run sanity COMMAND --help for usage tips and examples.

## Changing <hostname>.sanity.studio

To change the host name of your Sanity-hosted Studio (e.g., `https://<1oldHostName>.sanity.studio` to `https://<2newHostName>.sanity.studio`), please see [Undeploying the Studio](https://www.sanity.io/docs/deployment#702097e3bdad).

## Debugging `sanity` commands

Not to be confused with [sanity debug](https://www.sanity.io/docs/debug), which returns information about your Sanity environment, you can use the `DEBUG` environment variable with your `sanity` commands to get more verbose results and troubleshoot potential issues.

For full debugger results, use a wildcard on its own (`DEBUG=* sanity <command>`). For more targeted results, you can specify a namespace followed by a wildcard (`DEBUG=sanity* sanity <command>` or `DEBUG=sanity:cli* sanity <command>`).

> [!NOTE]
> Example
> Least verbose
> sanity dataset import production.tar.gz dev
> 
> More verbose, returning all debuggers in the sanity namespace
> DEBUG=sanity* sanity dataset import production.tar.gz dev
> 
> Most verbose, returning all debuggers
> DEBUG=* sanity dataset import production.tar.gz dev

Results can also be excluded by using a `-` prefix. `DEBUG=sanity*,-sanity:export* sanity dataset export production production.tar.gz` would return all debuggers in the `sanity` namespace except for `sanity:export` debuggers (e.g., `sanity:cli` and `sanity:client`) during export of the `production` dataset.

## Authorizing the CLI

In most cases, you'll use `sanity login` to authenticate with the Sanity API. When you need to run the CLI unattended, like in a CI/CD environment, set the `SANITY_AUTH_TOKEN` environment variable to a token. You can generate tokens in the [project management dashboard](https://sanity.io/manage).



# Managing backups

Sanity offers a backup feature that provides a robust solution for disaster recovery and content history auditing, ensuring your data's safety and integrity. With the ability to restore your production environment seamlessly and to inspect historical data states, this feature is a powerful tool for maintaining data continuity and compliance.

_This is a paid feature, available on the Enterprise plan._

[Backup dataset with CLI](/docs/cli-reference/backup)

[Export dataset with CLI](/docs/cli-reference/dataset)

[Dataset Cloud Clone](/docs/content-lake/how-to-use-cloud-clone-for-datasets)

[Import dataset with CLI](/docs/content-lake/importing-data)



## What is a backup?

A “backup” in the context of this article is an archived snapshot of the state of your dataset (documents and assets) at a specific time. You can use it to audit the history of your content or roll it back to a known safe state in case of data loss or unintended changes.

Each backup contains all documents and assets from your dataset in their state from when the backup job ran. This includes hidden documents used for settings and configuration by the studio and installed plugins. [Comments](/docs/studio/comments) and [document history](/docs/http-reference/history) (the timeline shown when you select **Review changes** in the studio) are not included in the backup.

Once you enable the backup service (as described in the next section of this article) Sanity will perform a backup of your dataset daily.

> [!WARNING]
> Gotcha
> The backup service runs at set regular intervals, so your initial backup may take up to 24 hours to become available.

Your backups are managed by Sanity in an offsite third-party storage location for data redundancy and security. Daily backups are stored for 365 days. Weekly backups are stored for an additional 2 years on top of the 1 year of daily backups.

When you download a backup, the resulting file contains an archive with all your documents exported into a single [NDJSON](https://github.com/ndjson/ndjson-spec) file alongside your files and images in separate folders, neatly collected into a single gzip-compressed archive file with a `.tar.gz` file type - colloquially known as a “tarball.”

```bash
production-backup-2024-02-23-a9bfa2d7-9ba1-42cc-beb2-f9f448bec656/
├── data.ndjson
├── files
│   └── file.txt
└── images
    └── image.png
```

## Enabling and disabling backups

Enabling and disabling the backup service is mainly done with the Sanity CLI.

### Prerequisites

- The relevant project is on a supported plan
- The backup feature is enabled for the project
- The Sanity CLI is up to date (v3.31.0 or later is required)
- The user has administrator permissions for the project

### Enable backups

To enable backups for a dataset, use the `sanity backup enable` CLI command in your project folder:

```sh
sanity backup enable [DATASET_NAME]
```

You should see a confirmation message in your CLI, and your first backup should be available within 24 hours.

### Disable backups

To disable backups for a dataset, use the `sanity backup disable` CLI command in your project folder:

```sh
sanity backup disable [DATASET_NAME]
```

No further backups will be scheduled. Your existing backups will continue to exist and be available for downloading.

## Listing available backups

To list all available backups for a dataset, use the following CLI command in your project folder:

```sh
sanity backup list [DATASET_NAME]
```

Running this command will list the available backups for the dataset in question.

```bash
┌──────────┬─────────────────────┬─────────────────────────────────────────────────┐
│ RESOURCE │ CREATED AT          │ BACKUP ID                                       │
├──────────┼─────────────────────┼─────────────────────────────────────────────────┤
│ Dataset  │ 2024-02-21 16:57:34 │ 2024-02-21-cf51334d-4caa-4487-a746-75a49b078e82 │
│ Dataset  │ 2024-02-22 02:40:30 │ 2024-02-22-c66adb69-cbed-4e4f-88a2-f97b5feeb464 │
│ Dataset  │ 2024-02-23 01:43:28 │ 2024-02-23-e1b1dcd3-fa9b-45a2-ab58-9f1c1eae45c7 │
└──────────┴─────────────────────┴─────────────────────────────────────────────────┘
```

By default, this command will list the 30 most recent backups. You can use the `limit` parameter to increase the listing threshold to a maximum of 100, or you can use the `after` and `before` parameters to target a specific time period from which to list backups.

```sh
sanity backup list production --after 2024-01-31 --before 2024-01-10 --limit 10
```



## Downloading backups

To download a specific backup for a dataset, use the following CLI command in your project folder:

```sh
sanity backup download [DATASETNAME] --backup-id [BACKUPID] --out [FILE_NAME]
```

`[BACKUP_ID]` needs to match the ID of an existing backup, and `[FILE_NAME]` should be a valid file name for the resulting downloaded file. A more realistic example is shown below.

```sh
sanity backup download production --backup-id 2024-02-23-a9bfa2d7-9ba1-42cc-beb2-f9f448bec656 --out backup_2024.tar.gz
```

If you don’t specify the file name in the `--out` flag, the backup file will follow the `[dataset name]-backup-[backup ID].tar.gz` convention.

To learn about all the options for this command, refer to the CLI reference article, or run `sanity backup download --help` in the CLI.

## Restoring from a backup

You can use the `sanity dataset import` CLI command to restore from a downloaded backup.

```sh
sanity dataset import ~/Downloads/backup_2024.tar.gz production
```



> [!WARNING]
> Gotcha
> If you are importing a backup into a different dataset than the one the backup originated from, you will have to use the --allow-assets-in-different-dataset option on import. Read about this and other parameters and options available for this command in the relevant CLI reference article, or by running sanity dataset import --help in the CLI.

Downloaded backups are structured to be ready for importing, both in their original compressed file state and in their decompressed file structure.

## Deleted datasets

If you delete a dataset then no new backups will be created. Any existing backups will continue to be accessible. If you later create a new dataset with the same name then:

47. You will need to actively enable backups for this new dataset to start the backup service up again.
47. Backups for the older, deleted, dataset will no longer be accessible directly through the CLI. Contact support if you require them.

### Conclusion

The backup feature from Sanity offers a straight-forward and easy to use solution for securing your content, providing data redundancy, means of compliance, and peace of mind. Contact your account manager to have backups enabled for your enterprise project, or visit our [pricing page](https://www.sanity.io/pricing), to learn more about Sanity's enterprise plan offerings.



# Generating types

> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

If you use TypeScript for your front end or web application, you will want to type the content from the Sanity Content Lake API. With the Sanity TypeGen tooling, you can generate type definitions from the schema types in your Studio and the results of your GROQ queries in your front ends and applications.

Typing your content is useful for:

- Catching bugs and errors caused by wrongly handled data types, like forgetting to check for `null` or a `undefined` property
- Autocomplete of fields available in the result of your GROQ query
- Making it easier to refactor integration code when you make changes to the schema

This article will present the different aspects of type generation and walk you through workflows depending on your project structure (separate repositories, monorepos, and embedded Studio).

[Course: Typed content with Sanity TypeGen](https://www.sanity.io/learn/module/typescripted-content/introduction)



## Requirements

- Sanity CLI (Command Line Interface), v3.35.0 or later
- A Sanity Studio project with a schema
- GROQ queries assigned to variables and using the [groq template string helper](https://github.com/sanity-io/sanity/tree/next/packages/groq) 

## Overview

You can use Sanity TypeGen to generate types for your Sanity Studio schema and for the return value of a GROQ query run against documents made from that schema.

Types from your schema can be useful for cases where GROQ isn't used, such as [Studio customization](/docs/customization) and [schema change management](/docs/content-lake/schema-and-content-migrations).

The most common use case is generating types for GROQ queries. TypeGen works by "overlaying the schema types" over the GROQ query to determine what types the returned data will have.

### Using GraphQL?

If you primarily use [the Sanity GraphQL API](/docs/content-lake/graphql), we recommend using established GraphQL TypeScript tooling, like [GraphQL Code Generator](https://the-guild.dev). You can use your GraphQL API URL as the configuration setting for `schema`.

## Minimal example

Sanity TypeGen needs to access a static representation of your Studio schema to generate types. You can use the `sanity schema extract` command to create the `schema.json` file the TypeGen command requires:

```sh
$ cd ~/my-studio-folder
$ sanity schema extract # outputs a `schema.json` file
✔ Extracted schema

$ sanity typegen generate
✔ Generated TypeScript types for 2 schema types and 2 GROQ queries in 1 files into: ./sanity.types.ts
```

## Types from schemas

Take this simple schema type for “event” documents:

Input

```typescript
// ./src/schema/event.ts

export const event = defineType({
  name: 'event',
  type: 'document',
  title: 'Event',
  fields: [
	  defineField({
		  name: 'name',
		  type: 'string',
		  title: 'Event name',
		  validation: rule => rule.required()
	  }),
	  defineField({
	    name: 'description',
	    type: 'text',
		  title: 'Event description'	    
	  })
  ]
})
```

Generated types

```typescript
// sanity.types.ts
export type Event = {
  _id: string;
  _type: 'event';
  _createdAt: string;
  _updatedAt: string;
  _rev: string;
  name?: string;
  description?: string
}
```

### Supported schema types

Nearly all schema types and all permutations of schema types are supported:

- Document types
- Literal fields (boolean, string, text, number, geopoint, date, dateTime)
- Object types
- Array of types
- Portable Text (Block content)
- References
- Image and file assets

Unsupported schema that will be typed as `unknown`:

- Cross-dataset references

> [!NOTE]
> ☝ Is missing support for certain schema types blocking you? Let us know!

### Supported schema features

Since Studio schemas are defined in JavaScript, it can get gnarly to represent and generate statically TypeScript definitions from specific configuration options. However, the following schema configuration options are supported.

#### Required field validation and non-optional fields

If you add `validation: rule => rule.required()`  to a field, you might want to translate required rules into non-optional types depending on your use case. You do this by adding the `--enforce-required-fields` flag when extracting the schema:

```sh
$ npx sanity schema extract --enforce-required-fields
✔ Extracted schema, with enforced required fields
$ npx sanity typegen generate
✔ ...
```

> [!WARNING]
> Gotcha
> If you have enabled previews of unpublished content, then remember that values might be undefined or null even though the field is set as required. Validation is only checked for published documents, and draft documents are allowed to be in an "invalid" state.

“Built-in” fields required for documents in the Sanity Content Lake will also be set to required in the TypeScript definition: `_id`, `_type`, `_createdAt`, `_updatedAt`, `_rev`.

**Literals and options.list**

The string schema type supports adding a list of predefined values in `options.list`. This gets generated into a literal type in the TypeScript definition:

Input

```typescript
// ./src/schema/event.ts
export const event = defineType({
  name: 'event',
  type: 'document',
  title: 'Event',
  fields: [
	  defineField({
		  name: 'name',
		  type: 'string',
		  title: 'Event name',
		  validation: rule => rule.required()
	  }),
	  defineField({
	    name: 'description',
	    type: 'text',
		  title: 'Event description'	    
	  }),
	  defineField({
		  name: 'format',
		  type: 'string',
		  title: 'Event format',
		  options: {
		    list: ['in-person', 'virtual'],
		    layout: 'radio',
		  },
		}),
  ]
})

```

Generated types

```typescript
// sanity.types.ts
export type Event = {
  _id: string;
  _type: 'event';
  _createdAt: string;
  _updatedAt: string;
  _rev: string;
  name: string;
  description?: 'string';
  format?: 'in-person' | 'virtual';
}
```

## Types from GROQ queries

Sanity TypeGen can also generate TypeScript definitions for GROQ query *results*. This is useful since GROQ is a query language that lets you specify which fields to return in projections and re-shape that data to fit your needs.

The CLI command requires that a GROQ query is:

- Assigned to a variable (it does *not* need to be exported)
- Uses the `groq` [template literal](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals), or `defineQuery`, from the [groq](https://www.npmjs.com/package/groq) package
- Validates as a GROQ expression

The `typegen` requires all queries to have a unique name. This also means that no inline queries are included in the generated types.

```tsx
// ✅ Will be included
async function getStuff() {
	const myUniquelyNamedQuery = groq`*[_type == 'post']{ slug, title }`
	const result = await client.fetch(myUniquelyNamedQuery)
	return result
}

async function getMoreStuff() {
	const myUniquelyNamedQuery = defineQuery(`*[_type == 'post']{ slug, title }`)
	const result = await client.fetch(myUniquelyNamedQuery)
	return result
}

// ❌ Will not be included
async function getInlineStuff() {
	const result = await client.fetch(groq`*[_type == 'post']{ slug, title }`)
	return result
}
```

### Supported GROQ features

Since GROQ is so versatile, we are still working on identifying edge cases, and some functions are not yet supported.

Unsupported GROQ expressions will be typed as `unknown` by the TypeGen. The exception to this is [GROQ custom functions](/docs/content-lake/custom-groq-functions), which are not currently supported and will cause an error. Use the [ignoring individual features technique](https://www.sanity.io/docs/apis-and-sdks/sanity-typegen#c4ec4db7a627) in the section below to ignore custom functions.

Supported features:

- Data types: Null, Boolean, Number, String, Array, Object
- Selectors: Everything (`*`), this (`@`), attribute filters (`[name == "string"]`), parent (`^`)
- Functions: `pt:text()`, `coalesce()`, `select()` , `dateTime::now()`, `global::now()`, `round()`, `upper()`, `lower()`, `select()` (and `=>`), and array functions
- Compounds: parenthesis, traversals (`[]`), pipe function calls (`|`)
- Operators: and (`&&`), or (`||`), not (`!=`), equality (`==`), comparison (`<, <=, >, >=`), plus (`+`), minus (`--`), unaries (`++` `--`), star (`*`), slash (`/`), percent (`%`), star star (`**`)

> [!NOTE]
> ☝ Is missing support for certain GROQ features blocking you? Let us know in the community!

### Automatic Sanity Client type inference

By using `defineQuery`  when writing your GROQ queries the Sanity Client will automatically return types when the query is used with `fetch`, after running `sanity typegen generate`.

#### Example

```typescript
// sanity.queries.ts
import { defineQuery } from 'groq'

export const postsQuery = defineQuery(`*[_type == "event"]{title}`)

// data.ts
import { createClient } from '@sanity/client'
import { postsQuery } from './sanity.queries.ts'

const client = createClient({...})

export function getPosts() {
  return client.fetch(postsQuery) // <- the returned type here is automatically inferred 
}

```

> [!WARNING]
> Gotcha
> For TypeScript to return the query types generated by TypeGen the generated sanity.types.ts needs to be included in the pattern configured in the includes array in tsconfig.json.

> [!NOTE]
> Opt out of automatic type inference
> You can opt out by setting overloadClientMethods to false in your sanity-typegen.json.

### Ignoring individual queries

You can instruct the type generator to skip generating query types for individual queries by having **@sanity-typegen-ignore** in a leading comment before the query, similar to how ESLint and TypeScript can be instructed.

### Minimal example

Input

```typescript
// sanity.queries.ts
import { groq } from 'groq'
// import { groq } from 'next-sanity'

const postQuery = groq`*[_type == "event"]{title}`

const authorQuery = groq`*[_type == "author" && name == $name][0]{_type, name, description}`

// this query wont get generated types because of the instruction below
// @sanity-typegen-ignore
const anotherQuery = groq`*[_type == "another"][0]`
```

Generated types

```typescript
// sanity.types.ts

// Variable: postQuery
// Query: *[_type == "event"]{title}
export type PostQueryResult = Array<{
  title: string | null
}> 

// Variable: authorQuery
// Query: *[_type == "author" && name == $name][0]{name, description}
export type AuthorQuery = {
  _type: 'author'
  name: string | null
  description: string | null
} | null
```

## Type generation workflows

Generating types from your schemas and GROQ queries using the Sanity CLI in the root Sanity Studio folder for the relevant project. First, you’ll run a command to extract the structure of your schemas into a format suited for further processing and then convert it into a handy JSON file. Then you’ll run another command to generate and output type definitions based on that same JSON file.

### General workflow

71. Extract current schema with `sanity schema extract`
71. Generate types from schemas and queries with `sanity typegen generate`

### Extracting studio schema to `schema.json`

The first step towards generating type definitions based on your schemas is to extract the entire schema structure into a single JSON file for the `typegen` command to ingest.

👉 In your Sanity project root (wherever your `sanity.config.ts` lives), run the following CLI command:

```sh
$ npx sanity schema extract
✔ Extracted schema

# If you have multiple workspaces defined, specify which one to use:
$ npx sanity schema extract --workspace=commerce
✔ Extracted schema
```

The CLI tool will pick up the schema definition from your project configuration, and generate a representation of your complete schema structure in a new file named `schema.json` unless otherwise specified. You are now ready to proceed to the next step.

🔗 Learn how to override the default output file and more in the CLI reference docs.

### Generate `sanity.types.ts` from `schema.json`

Once you have extracted your schema as described in the previous section, you are all set to generate some types.

👉 Still in your Sanity project root, run the following command in your CLI:

```sh
$ npx sanity typegen generate
✔ ...
```

The CLI tool will look for the `schema.json` file you created in the previous step and will create a new file by default named `sanity.types.ts` containing all the type declarations for your schema and for any GROQ query found in the default source file path, which is `./src`.

The `generate` command can be configured by adding a file named `sanity-typegen.json` containing a configuration object with the following shape:

```typescript
{
  "path": "./src/**/*.{ts,tsx,js,jsx}", // glob pattern to your typescript files. Can also be an array of paths
  "schema": "schema.json", // path to your schema file, generated with 'sanity schema extract' command
  "generates": "./sanity.types.ts", // path to the output file for generated type definitions
  "overloadClientMethods": true, // set to false to disable automatic overloading the sanity client
}
```

The example above is shown with the default values. Note the the `path` can be defined as either an array of strings or a single string.



### Example: Embedded Studio

A common pattern is to embed Sanity Studio in another application, keeping everything in a single repository. You can find several example repositories that follow this convention in the templates section of the Sanity Exchange. This is the happiest of paths since your studio and application are sharing a single project root. Likely, the only adjustment you might need to make is to specify the path to your queries (unless it’s in a sub-directory of `./src` in which case the default settings have you fully covered!)

### Example: Monorepo

Another common way of structuring a Sanity-powered project is to create a “monorepo” within which your Studio and your consuming application live separately side by side, possibly in sub-repositories of their own. Depending on the needs and preferences of your project you could use the configuration options of each CLI command to output the generated files into your consuming application, or you could keep the generated files in the studio folder and put it on the application to find them by traversing the monorepo.

```bash
// Use the --path flag to output schema.json elsewhere

npx sanity schema extract --path ../../my-cool-app/sanity-schemas.json
```

```json
// Use the config options in sanity-typegen.json to change output directory
{
  "path": "'../../my-cool-app/src/**/*.{ts,tsx,js,jsx}'", // glob pattern to your typescript files
  "schema": "../../my-cool-app/sanity-schemas.json", // path to your schema file, generated with 'sanity schema extract' command
  "generates": "../../my-cool-app/sanity.types.ts" // path to the output file for generated type definitions
}
```

### Example: Separate repos

You might also find yourself working on a project that keeps separate repos for the studio and consuming applications. Since there is currently no way of accessing generated type definitions through the Sanity Client, you must rely on more rudimentary methods of making types available in your frontends – such as copy and paste, or adapting the monorepo example if your folders have stable paths.





# Programmatic control

Content Releases allow teams to organize and schedule updates across multiple documents. Teams can plan, preview, and validate significant changes in advance, ensuring seamless and conflict-free content deployment.

This document explores interacting with Content Releases using Sanity's APIs. For details on using Content Releases in Sanity Studio, or customizing the experience follow these links:

[User guide](/docs/user-guides/content-releases)

[Configure content releases](/docs/studio/content-releases-configuration)



_This is a paid feature, available on the Growth plan._

APIs that interact with Content Releases require API version `2025-02-19` or later.

## Releases and document versions

Releases are Sanity documents with a type of `system.release`. They contain information about the release such as the title, release type, and state.

> [!TIP]
> Protip
> Users of content resources and custom roles can restrict access for:
> 
> 
> 1. Editing documents ​in​ releases by using a filter like id in path("versions.**") for any release or id in path("versions.rA29bfjqa.**") for documents in a specific release.
> 
> 2. Performing release actions such as creating, publishing and archiving releases by using a filter like id in path("_.releases.**") for any release or id == "_.releases.rA29bfjqa" for a specific release.

Releases and documents are connected by a document ID system similar to the `drafts.` syntax. For releases, document IDs start with the `versions.` prefix. For example, if a document with an ID of `movie_70981` is published, has a draft, and exists in a future release, it could end up with the following versions:

- The published version: `movie_70981`
- A draft version: `drafts.movie_70981`
- A release version: `versions.<release-name>.movie_70981`

Releases have an automatically generated name, not to be confused with the user-supplied title. This name matches the end of the ID. For example, a release name of `rSC2jjcUJ` results in an `_id` of `_.releases.rSC2jjcUJ`.

## Release states

The current status of a release is known as the release `state`. Releases begin in the `active` state. This information is available on the `state` property in documents with a `_type` of `system.release`.

![Release state flowchart](https://cdn.sanity.io/images/3do82whm/next/a838b3784ead16282b719b7ff4d124e3e9eb3cb1-1523x2248.png)

A release may have the following states:

- `active`: The general state of a release that is not within one of the other states. *This is the default state of a new release*.
- `scheduled`: A state resulting from calling the `sanity.action.release.schedule` action on the release or scheduling the release in Studio.
- `published`: A state resulting from either calling the `sanity.action.release.publish` action, publishing the release in Studio, or when a scheduled release is published due to reaching its `publishAt` time.
- `archived`: A state resulting from calling the `sanity.action.release.archive` action or archiving the release in Studio.
- `deleted`: A state resulting from calling the `sanity.action.release.delete` action on an archived release or deleting the release in Studio.

Additional transient states exist to indicate the asynchronous points when releases move between states:

- `scheduling`/`unscheduling`: Intermediate states that will exist when moving to/from the `scheduled` state.
- `archiving`/`unarchiving`: Intermediate states that will exist when moving to/from the `archived` state.
- `publishing`: Intermediate state that will exist before reaching the `published` state. Note that a scheduled release will also transition through `publishing`.

> [!WARNING]
> Gotcha
> Even releases set for immediate publishing will run through the scheduling and scheduled states. Instead of staying there, they immediately move through to publishing. Keep this in mind if you listen for state changes on release documents.

## Query releases and versions

Releases are Sanity documents and respect the existing query and mutation APIs. The guide below provides examples of querying and interacting with releases and their documents.

[Content Release patterns](/docs/apis-and-sdks/content-releases-cheat-sheet)



## Additional resources

[Actions](/docs/http-reference/actions)

[GROQ functions](/docs/specifications/groq-functions)

[@sanity/id-utils](https://github.com/sanity-io/id-utils)







# Cheat sheet

Interfacing with [Content Releases](/docs/apis-and-sdks/content-releases-api) is similar to interfacing with other documents and relationships in the Content Lake. 

**Prerequisites:**

- The examples below use a variety of APIs to cover common patterns. As releases aren't public, make sure your requests are [authenticated](/docs/content-lake/http-auth) and match the correct [URL format](/docs/content-lake/http-urls) for each API. For example, using Sanity clients or other integrations, you'll need to configure them with an appropriate token and permissions to view unpublished content.
- Release APIs and features are available in API version `2025-02-19` and later unless otherwise noted.
- Many query interactions on this page assume a perspective that can view release information. You should set the perspective to `raw` or a unique release stack when viewing release information.
- For examples that use the `@sanity/client` actions and shorthand methods, version 7.2.0 or higher is required.

## Create a new release

As releases are Sanity documents, you can interact with them the same way you would any other document. The preferred method is with the [Actions API](/docs/http-reference/actions) with the `@sanity/client`.

```
import { createClient } from "@sanity/client";

const client = createClient({
    projectId: '<project-id>',
    dataset: '<dataset>',
    useCdn: true,
    apiVersion: '2025-02-19',
    token: '<token>',
})

client
  .action({
    actionType: 'sanity.action.release.create',
    releaseId: 'new-bikes-release',
    metadata: {
      title: 'New bikes',
      releaseType: 'undecided',
    },
  })
  .then(() => {
    console.log('`new-bikes-release` created')
  })
  .catch((err) => {
    console.error('Create release failed: ', err.message)
  })
```

For a full list of available metadata properties, see the `sanity.action.release.create` [reference documentation](https://www.sanity.io/docs/http-actions#b6800cddd015).

## Modify release information

As releases are Sanity documents, you can interact with them the same way you would any other document. The preferred method is with the [Actions API](/docs/http-reference/actions) and the `@sanity/client`.

```
import { createClient } from "@sanity/client";

const client = createClient({
    projectId: '<project-id>',
    dataset: '<dataset>',
    useCdn: true,
    apiVersion: '2025-02-19',
    token: '<token>',
})

client
  .action({
    actionType: 'sanity.action.release.edit',
    releaseId: 'new-bikes-release',
    patch: {
      set: {
        metadata: {
          releaseType: 'asap',
        },
      },
    },
  })
  .then(() => {
    console.log('`new-bikes-release` changed to `asap` release type')
  })
  .catch((err) => {
    console.error('Edit release failed: ', err.message)
  })
```

You can also edit releases using the [Mutate API](/docs/http-reference/mutation).

## Get all releases for a project and dataset

Access releases by querying the documents with the `releases::all()` GROQ function. This is the preferred method for retrieving a list of releases.

### JS Client

Use the client's fetch method to query for releases.

Input

```typescript
import { createClient } from "@sanity/client";

const client = createClient({
    projectId: '<project-id>',
    dataset: '<dataset>',
    useCdn: true,
    apiVersion: '2025-02-19',
    token: '<token>',
    perspective: 'raw'
})

const query = "releases::all()"
const params = {}
client.fetch(query, params).then((data)=>{
  console.log(data)
})
```

Response

```json
[
    {
      "_createdAt": "2024-11-26T21:30:57Z",
      "finalDocumentStates": null,
      "_updatedAt": "2024-12-17T16:33:26Z",
      "_type": "system.release",
      "name": "rHw6FBu82",
      "_id": "_.releases.rHw6FBu82",
      "state": "active",
      "metadata": {
        "releaseType": "scheduled",
        "title": "End of year release",
        "intendedPublishAt": "Mon Dec 30 2024"
      },
      "publishAt": "2024-12-30T08:00:00Z",
      "_rev": "JmI5JuFTDPq3paS6p09Jmu",
      "userId": "paATypsg4"
    },
    {
      "publishAt": null,
      "_rev": "1kbjGQwz5Z0FmijO2l7Lwl",
      "finalDocumentStates": [
        {
          "id": "versions.rglJO3Sfg.movie_70981",
          "_key": "1kbjGQwz5Z0FmijO2l7M0E"
        }
      ],
      "_id": "_.releases.rglJO3Sfg",
      "state": "published",
      "metadata": {
        "title": "Quick fixes",
        "releaseType": "asap"
      },
      "_createdAt": "2024-11-26T22:01:56Z",
      "_type": "system.release",
      "name": "rglJO3Sfg",
      "_updatedAt": "2024-12-02T17:32:59Z",
      "userId": ""
    },
    {
      "_createdAt": "2024-11-26T18:34:14Z",
      "name": "rqZSzJ1uS",
      "finalDocumentStates": [
        {
          "id": "versions.rqZSzJ1uS.movie_10681"
        }
      ],
      "userId": "paATypsg4",
      "_id": "_.releases.rqZSzJ1uS",
      "state": "published",
      "_updatedAt": "2024-12-05T17:22:00Z",
      "metadata": {
        "releaseType": "scheduled",
        "description": "Experimental updates for testing",
        "title": "Experimental updates",
        "intendedPublishAt": "2024-12-05T17:22:00.000Z"
      },
      "publishAt": "2024-12-05T17:22:00Z",
      "_rev": "5dKCVUpSDccCmU1E23aUAc",
      "_type": "system.release"
    },
  // ...
  ],
```

### Query API

You access releases by querying for documents with the `releases::all()` GROQ function using the [Query API](/docs/http-reference/query). This is the preferred method for retrieving a list of releases.

Use the following GROQ query in the API request:

```groq
releases::all()
```

Input

```sh
curl "https://<project-id>.api.sanity.io/2025-02-19/data/query/<dataset-name>?query=<GROQ-QUERY>" \
    --H "Authorization: Bearer <token>" \
```

Example response

```json
{
  "query": "*[releases::all()]",
  "result": [
    {
      "_createdAt": "2024-11-26T21:30:57Z",
      "finalDocumentStates": null,
      "_updatedAt": "2024-12-17T16:33:26Z",
      "_type": "system.release",
      "name": "rHw6FBu82",
      "_id": "_.releases.rHw6FBu82",
      "state": "active",
      "metadata": {
        "releaseType": "scheduled",
        "title": "End of year release",
        "intendedPublishAt": "Mon Dec 30 2024"
      },
      "publishAt": "2024-12-30T08:00:00Z",
      "_rev": "JmI5JuFTDPq3paS6p09Jmu",
      "userId": "paATypsg4"
    },
    {
      "publishAt": null,
      "_rev": "1kbjGQwz5Z0FmijO2l7Lwl",
      "finalDocumentStates": [
        {
          "id": "versions.rglJO3Sfg.movie_70981",
          "_key": "1kbjGQwz5Z0FmijO2l7M0E"
        }
      ],
      "_id": "_.releases.rglJO3Sfg",
      "state": "published",
      "metadata": {
        "title": "Quick fixes",
        "releaseType": "asap"
      },
      "_createdAt": "2024-11-26T22:01:56Z",
      "_type": "system.release",
      "name": "rglJO3Sfg",
      "_updatedAt": "2024-12-02T17:32:59Z",
      "userId": ""
    },
    {
      "_createdAt": "2024-11-26T18:34:14Z",
      "name": "rqZSzJ1uS",
      "finalDocumentStates": [
        {
          "id": "versions.rqZSzJ1uS.movie_10681"
        }
      ],
      "userId": "paATypsg4",
      "_id": "_.releases.rqZSzJ1uS",
      "state": "published",
      "_updatedAt": "2024-12-05T17:22:00Z",
      "metadata": {
        "releaseType": "scheduled",
        "description": "Experimental updates for testing",
        "title": "Experimental updates",
        "intendedPublishAt": "2024-12-05T17:22:00.000Z"
      },
      "publishAt": "2024-12-05T17:22:00Z",
      "_rev": "5dKCVUpSDccCmU1E23aUAc",
      "_type": "system.release"
    },
  ],
  "syncTags": [
    "s1:r6H+EQ"
  ],
  "ms": 3
}
```

To view only active releases, and exclude archived releases, adjust your GROQ query to compare the `state` property.

```text
releases::all()[state == 'active']
```

## Get all documents from a release

Query all documents associated with a release.

### `sanity::partOfRelease` GROQ function

The sanity::partOfRelease GROQ function accepts a release name and returns all documents associated with the release.

> [!TIP]
> Release ID vs. release name
> Release names are the final piece of a release ID. For example, a release with an id of _.releases.rEGM2JqQ3 has a name of rEGM2JqQ3. Use just the name final portion of the ID when referencing releases by name.

Use the function in a GROQ query and pass the release name string.

```typescript
import { createClient } from "@sanity/client";

const client = createClient({
    projectId: '<project-id>',
    dataset: '<dataset>',
    useCdn: false,
    apiVersion: '2025-02-19',
    token: '<token>',
    perspective: 'raw'
})

const query = "*[sanity::partOfRelease(<release-name>)] { _id }"
const params = {}
client.fetch(query, params).then((data)=>{
  console.log(data)
})
```

## Get all versions of a document

Query all versions (published, drafts, and release versions) of a document.

### `sanity::versionOf` GROQ function

The `sanity::versionOf` GROQ function accepts a document ID and returns all versions of a document.

Use the function in a GROQ query and pass the document ID string.

```typescript
import { createClient } from "@sanity/client";
import { getPublishedId } from "sanity"

const client = createClient({
    projectId: '<project-id>',
    dataset: '<dataset>',
    useCdn: false,
    apiVersion: '2025-02-19',
    token: '<token>',
    perspective: 'raw'
})

const query = "*[sanity::versionOf($publishedId)] { _id }"
const params = {
  publishedId: getPublishedId(documentId)
}
client.fetch(query, params).then((data)=>{
  console.log(data)
})
```

> [!WARNING]
> Gotcha
> The function expects a published document ID. For example: abc123 is acceptable, but drafts.abc123 and versions.r1324.abc123 are not. You can use the getPublishedId helper imported from sanity to derive it from any Id.
> 
> 

The function also works with the Query API and anywhere that supports GROQ functions.

### Doc API

Use the document ID, along with the [Doc API endpoint](/docs/http-reference/doc) to retrieve all versions of a specific document. The `includeAllVersions` boolean query parameter returns all versions for the document.

Input

```sh
GET /vX/data/doc/production/movie_70981?includeAllVersions=true
```

Example response

```json
{
  "documents": [
    {
      "_createdAt": "2018-06-13T08:57:45Z",
      "_id": "movie_70981",
      "_rev": "1kbjGQwz5Z0FmijO2l7Lwl",
      "_type": "movie",
      "_updatedAt": "2024-12-02T17:32:59Z",
      // ...
    },
    {
      "_createdAt": "2018-06-13T08:57:45Z",
      "_id": "drafts.movie_70981",
      "_rev": "3276f9d1-0343-4b78-a79e-8c6561942f1b",
      "_type": "movie",
      "_updatedAt": "2024-12-02T17:14:27Z",
      // ...
    },
    {
      "_createdAt": "2018-06-13T08:57:45Z",
      "_id": "versions.rHw6FBu82.movie_70981",
      "_rev": "a000fa99-072c-434c-8669-825103a111b7",
      "_type": "movie",
      "_updatedAt": "2024-11-27T18:36:19Z",
      // ...
    }
  ],
  "omitted": []
}

```

## Use releases in perspective queries

Content Releases use a layering system that layers document versions atop one another, allowing you to create a custom perspective stack. You can learn more about layering in the [Content Releases User Guide](/docs/user-guides/content-releases). 

Perspective in addition to accepting the `raw`, `published`, and `drafts` states, also accepts a comma-separated list of release names. Releases take priority from left to right.

For example, in the perspective `a,b,c` you would see changes in `a` take priority over `b` and `c`, and changes in `b` take priority over `c`.

The `published` perspective is automatically added to the end, so even if a release only contains changes to one document, the response will include all matching published documents in addition to the release changes.

### Query API

To query against a list of releases, use the `perspective` query parameter and order the release names by priority from left to right. For example: `?perspective=a,b,c`.

Using a GROQ query such as `*[_type == 'movie'] { _id }` will return all document IDs that match the releases layer.

Input

```sh
GET https://<projectId>.api.sanity.io/vX/data/query/<dataset>?query‌‌‌‌‌‌=<GROQ-QUERY>&perspective=<release-name1>,<release-name2>
```

Example response

```json
{
  "query": "*[_type == 'movie']{ _id }",
  "result": [
    { "_id": "a306e7cf-ea18-4a43-8ce2-0586073c41c8" },
    { "_id": "movie_10681" },
    { "_id": "movie_118340" },
    { "_id": "movie_126889" },
    { "_id": "movie_157336" },
    { "_id": "movie_17654" },

  ],
  "syncTags": ["s1:+jIWIw"],
  "ms": 5
}

```

### JavaScript Client

In addition to the `raw`, `published`, and `drafts` values, the client also accepts an array of release name strings. You can add `drafts` to the end of the array to include drafts that aren't part of any release.

Edit the perspective value to insert your release names.

```typescript
import { createClient } from "@sanity/client";

const client = createClient({
    projectId: '<project-id>',
    dataset: '<dataset>',
    useCdn: false, // Don't use the CDN for draft/release previewing
    apiVersion: '2025-02-19',
    token: '<token>',
    perspective: ['<release-name-1>', '<release-name-2>']
})

const query = "*[_type == 'movie']"
const params = {}
client.fetch(query, params).then((data)=>{
  console.log(data)
})
```

Another common pattern is to extend your client configuration for releases and draft preview by creating a new client from the existing one.

```tsx
import { createClient } from "@sanity/client";

const client = createClient({
    projectId: '<project-id>',
    dataset: '<dataset>',
    useCdn: true,
    apiVersion: '2024-08-01',
    token: '<token>',
    perspective: 'published' //default
})

const previewClient = client.withConfig({
  useCdn: false,
  apiVersion: '2025-02-19',
  perspective: ['<release name>', 'drafts']
})
```

## Trigger webhook by release state

The Release documents can be queried and trigger assigned webhooks. 

The most useful way of triggering webhooks might be off the release `state`. 

A release may have the following states:

- `active`: The general state of a release that is not within one of the other states.
- `scheduled`: A state resulting from calling the `sanity.action.release.schedule` action on the release.
- `published`: A state resulting from either calling the `sanity.action.release.publish` action, or when a scheduled release is published due to reaching its `publishAt` time.
- `archived`: A state resulting from calling the `sanity.action.release.archive` action.
- `deleted`: A state resulting from calling the `sanity.action.release.delete` action on an archived release.

Additional transient states exist to indicate the asynchronous points when releases move between states:

- `scheduling`/`unscheduling`: Intermediate states which will exist when moving to/from the `scheduled` state.
- `archiving`/`unarchiving`: Intermediate states which will exist when moving to/from the `archived` state.
- `publishing`: Intermediate state which will exist before reaching the `published` state. Note that a scheduled release will also transition through `publishing`.

To create a webhook that listens to all new releases, define a webhook rule as follows:

```json
"rule":{
  "on":["create"],
  "filter":"_type == 'system.release'"
}
```

Additionally filters can enable triggering only on releases of a particular state. In this example only releases that have transitioned into a `published` state will trigger the webhook:

```json
"rule": {
  "on": ["update"],
  "filter": "_type == 'system.release' && delta::changedAny(state) && state == 'published'"
}
```

## Find documents scheduled for deletion

Sometimes you have a release that will delete documents. You can query for a list of scheduled-for-deletion docs with GROQ. Replace the `$releaseName` parameter in the following example with your own release name.

```
releases::all()[name == $releaseName] {
  _id,
  "docs": *[sanity::partOfRelease(^.name) && _system.delete == true]._id
}
```



# Presenting Images

## The Sanity Image Pipeline

When you use Sanity you have a globally-distributed asset CDN at your fingertips. At any time you can request your image in a new size, with a new crop or whatever you need and the asset is created for you and automatically cached close to your users. One thing to keep in the back of your mind as you implement your front-end is that to achieve maximum performance, **you should take care to reuse crops and sizes across your front-end to make sure your cached assets are re-used**.

## How images are represented

Before we go into the practical details, let's recap how images are handled in Sanity: The `image` type represents an image used as a field in a document, or embedded in a block text. It may contain additional fields, like caption, crediting, etc., and may contain the specific *crop* and *hotspot* for this image. The `image` references an `asset` which represents the actual image data to be displayed for this image. (One `asset` may actually be shared by several `image`s allowing editors to reuse an asset with different crops, captions etc.)

## The basic way to show images

The absolutely most basic way to present images from Sanity is to get the base url of the image from the asset. Let's say we have a document type person that has a field image, we can get the base url with this simple query:

```text
*[_type == 'person']{
  name,
  "imageUrl": image.asset->url
}
```

Here we follow the reference `asset` from the image and extract only the url.

```javascript
{
  name: "Sean Gunn",
  imageUrl: "https://cdn.sanity.io/images/zp7mbokg/production/G3i4emG6B8JnTmGoN0UjgAp8-300x450.jpg"
}
```

> [!WARNING]
> Gotcha
> Be aware, that requesting un-optimised images directly via the image URL, as detailed here, can lead to overages in your usage! 

We can now append url-options to this base-url in order to constrain size, crop the image, blur it or perform other operations on it. Some examples:

```javascript
// The base image
https://cdn.sanity.io/images/zp7mbokg/production/G3i4emG6B8JnTmGoN0UjgAp8-300x450.jpg

// Resized to have the height 200
https://cdn.sanity.io/images/zp7mbokg/production/G3i4emG6B8JnTmGoN0UjgAp8-300x450.jpg?h=200

// Extract a rectangle from the image (x, y, width, height)
https://cdn.sanity.io/images/zp7mbokg/production/G3i4emG6B8JnTmGoN0UjgAp8-300x450.jpg?rect=70,20,120,150

// Extract a rectangle from the image (x, y, width, height) and constrain height to 64
https://cdn.sanity.io/images/zp7mbokg/production/G3i4emG6B8JnTmGoN0UjgAp8-300x450.jpg?rect=70,20,120,150&h=64

// Blur the image
https://cdn.sanity.io/images/zp7mbokg/production/G3i4emG6B8JnTmGoN0UjgAp8-300x450.jpg?blur=50

```



> [!WARNING]
> Gotcha
> Small images get scaled up to the width or height you specify. To avoid this use &fit=max.

See the [image url reference documentation](/docs/apis-and-sdks/image-urls) for the full list of parameters.

## The crop and hot-spot

Some images may have a crop and a hot-spot. This may be [enabled in the schema](/docs/image-type). If there is a crop/hotspot, that means the editor has used an interface like this:

![](https://cdn.sanity.io/images/3do82whm/next/3ad04e1303b079d952cc710601b1d5117a950733-1652x1114.png)

The crop describes which part of the image the editor wants to allow to be used, while the hot-spot specifies what she wants preserved when the image needs to be cropped additionally in a front-end.

An image record with a crop and hot-spot might look like this from the api:

```javascript
{
  _type: "image",
  asset: {
    _ref: "image-G3i4emG6B8JnTmGoN0UjgAp8-300x450-jpg",
    _type: "reference"
  },
  // The crop is specified in fractions of the image dimensions
  // and measured from the edge of the image. This image is cropped
  // from the bottom at 44% of the image height. The other dimensions
  // are left un-touched.
  crop: {
    bottom: 0.44,
    left: 0,
    right: 0,
    top: 0
  },
  // The hot-spot position x, y is in fractions of the image dimensions.
  // This hot-spot is centered at 43% of the image width from the left,
  // 26% of the image height from the top. The width and height is 
  // in the same unit system. This hot spot is 44% of the image width wide,
  // 65% of the image height tall, this rectangle is centered on the x,y
  // coordinate given.
  hotspot: {
    height: 0.44,
    width: 0.65,
    x: 0.43,
    y: 0.26
  }
}
```

It is the responsibility of the front-end to respect the crop/hot-spot according to the wishes of the editor, but conveniently we provide a JavaScript library that takes care of this for you.

## Let the url-builder do it all

For JavaScript projects, we provide the `@sanity/image-url` npm package that will generate image-urls for you given an image record like the one above. You may specify additional constraints like width and height and trust that the crop/hot-spot is respected where applicable.

Let's see how this package might be used with React to render images. (There is nothing React-specific about it, you can use it with any framework you desire)

You install it to your project in the usual manner:

```sh
npm install --save @sanity/image-url

```

Then you need to configure it with your projectId and dataset. The simplest way to do this, is to initialize it with the Sanity client you already have in your project. To do this, import it using the right path like below. In the example we have a `sanityClient.js` file at the root of our project, which is assumed to return a configured client (complete with `projectId`, `dataset`, and `apiVersion`). Read more about the client in our [JavaScript client documentation](/docs/js-client).

```javascript
import React from 'react'
import client from './sanityClient'
import imageUrlBuilder from '@sanity/image-url'

// Get a pre-configured url-builder from your sanity client
const builder = imageUrlBuilder(client)

// Then we like to make a simple function like this that gives the
// builder an image and returns the builder for you to specify additional
// parameters:
function urlFor(source) {
  return builder.image(source)
}

```

Now you can use this handy builder-syntax to create your urls:

```javascript
<img src={urlFor(person.image).width(200).url()} />
```

You can add more options like this:

```javascript
<img src={urlFor(person.image).width(200).invert().flipHorizontal().url()} />
```

See the full list of builder options in the [reference documentation](/docs/image-url).

### Example image query for crop & hotspot support

> [!WARNING]
> Gotcha
> To ensure that crop and hotspot settings are automatically applied, make sure to pass the entire image field (image record) to the image builder.
> 
> You can still append additional values from the asset, like metadata.lqip for generating blurHash URLs or alt text from the asset itself. However, it's important not to modify or remove any existing field values if you want the editor’s adjustments to be reflected.

While the image-builder library handles the heavy lifting by generating the optimized image URL from the image record (with the asset reference intact), you can still extract useful details from your image in your query. Just be sure to leave the `asset` reference, `hotspot`, `crop`, and other related fields unchanged.

```javascript
// in your query 
image {
  ..., // this will ensure you keep the existing data
  ...asset-> {
    altText,
    caption,
    ...metadata {
      lqip, // the lqip can be used for blurHashUrl or other low-quality placeholders
      ...dimensions {
        width,
        height
      }
    }
  }
}
```

## Filename for downloads

To support downloading an image with a specific filename in a front end, you need to append the [dl= query parameter](/docs/image-urls#dl-9836df902327) to the URL, e.g. `https://<some-image-url>&dl=`. By default this will use the filename the image had when it was uploaded, if  [storeOriginalFilename](/docs/image-type#storeOriginalFilename-90bd9dce3f53) was not disabled.

You can also specify a filename by including it in the URL like so `https://<some-image-url>&dl=<filename-of-your-choice>`. The asset id will be used as the filename if there is no original filename or a filename was not provided in the URL. 

## Additional resources for Asset Pipeline

- [Image transformation and metadata](/docs/apis-and-sdks/image-urls)
- [Image schema type in Studio](/docs/image-type)
- [Full asset documentation](/docs/content-lake/assets)
- [Information about the Asset CDN](/docs/apis-and-sdks/asset-cdn)



# Image transformations

## The anatomy of the image URL

This article provides a detailed rundown of all the options for transforming images with Sanity. A general introduction to our image pipeline and tools can be found [here](/docs/apis-and-sdks/presenting-images).


Let's start by dissecting this Sanity image URL:

```
https://cdn.sanity.io/images/zp7mbokg/production/G3i4emG6B8JnTmGoN0UjgAp8-300x450.jpg
```

- `https://cdn.sanity.io/images/` is the common base for all Sanity image URLs. 
- `zp7mbokg` is the project ID 
- `production` is the dataset name
- `G3i4emG6B8JnTmGoN0UjgAp8` is the asset ID and the asset metadata document `_id`
- `300x450` is the width and height of the original image
- `jpg` is the file format of the *original* asset file

The image URLs can always be found in the asset metadata document referred to in an asset reference. Still, you don't have to fetch this document as the asset document ID contains all the information and represents a stable, documented interface you can trust.

The asset ID corresponding to the URLs above looks like this: `"image-G3i4emG6B8JnTmGoN0UjgAp8-300x450-jpg"`.  It provides the name, dimensions, and format. Given the project ID and dataset name, you have every piece you need to assemble the URLs without fetching the asset document:

`https://cdn.sanity.io/images/<project id>/<dataset name>/<asset name>-<original width>x<original height>.<original file format>`

This represents the base URL. If you fetch this, you will be served the original asset. This potentially uses a lot of bandwidth as content managers are advised to upload full-resolution assets. With the Sanity image pipeline, you can scale, crop, and process images on the fly based on URL parameters. E.g. by appending `?h=200` to the base URL, you instruct Sanity to scale the image to be 200 pixels tall:

`https://cdn.sanity.io/images/zp7mbokg/production/G3i4emG6B8JnTmGoN0UjgAp8-300x450.jpg?h=200`

You can specify any number of parameters. This will extract a rectangle from the image starting at 70 pixels from the left and 20 pixels from the top at a width of 120 pixels and a height of 150 pixels, scale it to 200 pixels tall, and blur it:

`https://cdn.sanity.io/images/zp7mbokg/production/G3i4emG6B8JnTmGoN0UjgAp8-300x450.jpg?rect=70,20,120,150&h=200&blur=10`

Even though the Sanity image backend is fast, you get a tremendous performance boost if your front end limits the number of sizes and crops you ask for. Sanity will cache the result in the global CDN, and if we see the same URLs again, we serve the same data directly from the edge cache closest to the user.

> [!WARNING]
> Gotcha
> Non-integer values for parameters expecting integers may cause performance issues or timeouts. It is recommended that you always use integer values when the parameter calls for it (e.g., w and h), including when returning calculated values.
> 
> &h=200 - Correct
> 
> &h=200.0 - May be problematic

## Supported image types

While the [Image schema type](/docs/image-type) supports a wide range of [image formats](/docs/content-lake/assets), transformations are limited to JPEG, PNG, WebP, PJPG, TIFF, AVIF, and GIF. For all other formats, you should convert the image to one of the supported file types before performing additional transformations.

> [!TIP]
> Protip
> The image pipeline now supports transforming animated GIFs.

## The URL parameters

> [!WARNING]
> Gotcha
> Small images get scaled up to the width or height you specify. To avoid this use &fit=max.

#### Properties

| Property | Description |
|----------|-------------|
| auto | Set auto=format to automatically return an image in in the most optimized format supported by the browser as determined by its Accept header. To achieve the same result in a non-browser context, use the fm parameter instead to specify the desired format, for example fm=webp. |
| bg | Fill in any transparent areas in the image with a color. The string must be resolve to a valid hexadecimal color (RGB, ARGB, RRGGBB, or AARRGGBB). E.g. bg=ff00 for red background with no transparency. |
| blur | Blur 1-2000. |
| crop | Use with fit=crop to specify how cropping is performed:

top, bottom, left and right: The crop starts from the edge specified. crop=top,left will crop the image starting in the top left corner.

center: Will crop around the center of the image

focalpoint: Will crop around the focal point specified using the fp-x and fp-y parameters.

entropy: Attempts to preserve the "most important" part of the image by selecting the crop that preserves the most complex part of the image. |
| dl | Configures the headers so that opening this link causes the browser to download the image rather than showing it. The browser will suggest to use the file name you provided. |
| dlRaw | As dl but requests the original file/image asset. Requires authentication. |
| dpr | Specifies device pixel ratio scaling factor. From 1 to 3. |
| fit | Affects how the image is handled when you specify target dimensions.

clip: The image is resized to fit within the bounds you specified without cropping or distorting the image.

crop: Crops the image to fill the size you specified when you specify both w and h

fill: Like clip, but any free area not covered by your image is filled with the color specified in the bg parameter.

fillmax: Places the image within box you specify, never scaling the image up. If there is excess room in the image, it is filled with the color specified in the bg parameter.

max: Fit the image within the box you specify, but never scaling the image up.

scale: Scales the image to fit the constraining dimensions exactly. The resulting image will fill the dimensions, and will not maintain the aspect ratio of the input image.

min: Resizes and crops the image to match the aspect ratio of the requested width and height. Will not exceed the original width and height of the image. |
| flip | Flipping. Flip image horizontally, vertically or both. Possible values: h, v, hv |
| fm | Convert image to jpg, pjpg, png, or webp.

Note that avif is not a valid option for this parameter as AVIF transformations are generated asynchronously. See the AVIF format details below.

This property also accepts a value of json, which does not convert the image but returns information about the image including width, height, frame count, content length, and content type. |
| fp-x | Focal Point X. Specify a center point to focus on when cropping the image. Values from 0.0 to 1.0 in fractions of the image dimensions. (See crop) |
| fp-y | Focal Point Y. Specify a center point to focus on when cropping the image. Values from 0.0 to 1.0 in fractions of the image dimensions. (See crop) |
| frame | The frame of an animated image. The only valid value is 1, which is the first frame. |
| h | Height of the image in pixels. Scales the image to be that tall. |
| invert | Invert the image. |
| max-h | Maximum height. Specifies size limits giving the backend some freedom in picking a size according to the source image aspect ratio. This parameter only works when also specifying fit=crop. |
| max-w | Maximum width in the context of image cropping. Specifies size limits giving the backend some freedom in picking a size according to the source image aspect ratio. This parameter only works when also specifying fit=crop. |
| min-h | Minimum height. Specifies size limits giving the backend some freedom in picking a size according to the source image aspect ratio. This parameter only works when also specifying fit=crop. |
| min-w | Minimum width. Specifies size limits giving the backend some freedom in picking a size according to the source image aspect ratio. This parameter only works when also specifying fit=crop. |
| or | Orientation. Possible values: 0, 90, 180 or 270.Rotate the image in 90 degree increments. |
| pad | The number of pixels to pad the image.  Applies to both width and height. |
| q | Quality 0-100. Specify the compression quality (where applicable). Defaults are 75 for JPG and WebP. |
| rect | Crop the image according to the provided coordinate values (left, top, width, height). 

left: Number of pixels from the left of the image

top: Number of pixels from the top of the image

width: Width, in pixels, of the crop from the left value

height: Height, in pixels, of the crop from the top value |
| sat | Saturation. Currently the asset pipeline only supports sat=-100, which renders the image with grayscale colors. Support for more levels of saturation is planned for later. |
| sharp | Sharpen 0-100. |
| w | Width of the image in pixels. Scales the image to be that wide. |


## AVIF transformations

Images that have the query parameter `auto` set to `format` and are requested from a browser that supports the AVIF format will potentially get an AVIF returned. 

There are a few exceptions/quirks:

The first few requests for an AVIF may get the "second best option" (WebP if supported, otherwise PNG/JPG depending on the source image). Subsequent requests will eventually get an AVIF back. This is done to ensure a speedy response, since encoding AVIFs is a slow process. 

Image requests made prior to the AVIF rollout may already be cached in our CDN and will not return an AVIF response until they expire/fall out of the cache. In other words: if you are not seeing AVIF images being returned, don't worry — they should eventually return AVIF. 

You can use `curl` to verify the behavior:

```sh
# Replace the URL with an actual URL from your project.
# Remember to include `?auto=format`!
curl -sS -I \
  -H 'accept: image/avif,image/webp,image/*' \
  'https://cdn.sanity.io/images/:projectId/:dataset/:filename?auto=format' \
  | grep 'content-type:'
```

On the first request, you will likely see `image/webp` returned. After waiting 30 seconds, run the same command again, and you should see `image/avif`. If you don't, wait a little longer and retry. If you still do not see AVIF, ensure that the accept header includes `image/avif` (before other formats) and that the query parameters includes `auto=format`.

> [!WARNING]
> Gotcha
> Because AVIF transformations are generated asynchronously, you cannot explicitly request AVIF transformations using the fm query parameter. Instead, use the accept header as described above.

## Read more

[Client library for generating urls](https://github.com/sanity-io/image-url)







# Image Metadata

The `metadata` option for image fields warrants a closer look. It takes an array of strings describing which types of metadata Sanity should attempt to extract or generate from uploaded images and save alongside the asset. An example of an image field with every metadata option specified looks as follows:

```javascript

  {
      name: 'metaImage',
      title: 'Image with metadata',
      type: 'image',
      options: {
        metadata: [
          'blurhash',   // Default: included
          'lqip',       // Default: included
          'palette',    // Default: included
          'image',      // Default: not included
          'exif',       // Default: not included
          'location',   // Default: not included
        ],
      },
    },

```

There are three additional metadata options that are always included and cannot be disabled: `dimensions`, `hasAlpha`, and `isOpaque`. Specifying an invalid option in the metadata array—including any of those three terms—will throw an error.

The metadata fields fall into one of three "default behaviors": **Always included**, **included by default**, and **not included by default**. We'll look at each default setting and the metadata fields that adhere to it.

> [!WARNING]
> Gotcha
> Metadata is added to your assets asynchronously after uploading! If your query for image metadata returns unexpectedly empty, wait a moment and try again!

> [!WARNING]
> Gotcha
> Metadata is applied to an image asset when the image is uploaded and based on the schema settings at that time. If a metadata array is set to include exif or location data, changing the schema later will not remove those details. If removing those details is desired, you can do so with a script or using the Media browser plugin, among other options. Likewise, adding options to the metadata array will not add those details to images previously uploaded.

## Dimensions, Alpha Channel, and Opaqueness

> [!NOTE]
> Always included
> These values are always available and you do not need to ask for them. In fact, they are not valid options in the options.metadata array so including them will throw an error.

### `hasAlpha`

`hasAlpha` will return `true` if the image has an alpha channel, even if unused.

### `isOpaque`

`isOpaque` returns `true` if the image is fully opaque (i.e., has no transparency).

### `dimensions`

The `dimensions` object contains the numeric values: **aspectRatio**, **height,** and **width,** which together describe the physical features of the image. A photo taken in portrait mode might yield the following payload:

```json
{
  "dimensions" : {
    "_type": "sanity.imageDimensions",
    "aspectRatio": 0.75,
    "height": 4032,
    "width": 3024
  }
}
```

## Placeholders and Colors

> [!NOTE]
> Included by default
> These values are available by default. If you don't ask for any metadata at all (that is, if you don't specify a metadata array), you will get these values. Beware though: If you do specify a metadata array and explicitly leave these out, they will not be returned.

### `lqip` and `blurHash`

Sanity will generate low-fidelity representations of your images automatically. These are useful for creating placeholders for loading images in your front end. These downsampled previews come in two different flavors: LQIP and blurhash.

**LQIP** (Low-Quality Image Preview) is a 20-pixel wide version of your image (height is set according to aspect ratio) in the form of a base64-encoded string and can be used as-is in your front end, as shown below. A typical value for lqip might look like this:

```json

"lqip": "data:image/png;base64,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"

```

And can be used like this:

```html

		<!-- 
			The LQIP value is actual image data
			encoded into a base64-string which can be 
			used directly as the src property of an img tag!
			Remember to set the height and width 
			properties, though, or it'll be very small
		-->
		<img
      height="100"
      width="100"
      src="data:image/png;base64,iVBORw0KGgo[...50 lines of this stuff omitted for brevity...]Jggg=="
    />

```

**BlurHash** is a more [advanced method](https://blurha.sh/) of creating a lightweight image preview that can give a superior result and comes in a more concise format. The trade-off is that you'll need to decode the value using a [helper library](https://github.com/woltapp/blurhash) before use. A blurHash value might look something like this:

```json

"blurHash" : "d79Z$I-o4:IoxaofR*WC00Io?GxtM{Rkt7s:~VxaNGRk"

```

Example of use in a JavaScript project: 

```javascript
import { decode } from "blurhash";

const pixels = decode("LEHV6nWB2yk8pyo0adR*.7kCMdnj", 32, 32);

const canvas = document.createElement("canvas");
const ctx = canvas.getContext("2d");
const imageData = ctx.createImageData(width, height);
imageData.data.set(pixels);
ctx.putImageData(imageData, 0, 0);
document.body.append(canvas);
```

### `palette`

Sanity will generate a color palette by analyzing your image. Along with the dominant swatches, a collection of suggestions for colors that contrast nicely with them is returned, as well as a numeral indication of how prominently each color is represented in the image. A palette object might look like:

```json
{
  "_type": "sanity.imagePalette",
  "darkMuted": {
    "_type": "sanity.imagePaletteSwatch",
    "background": "#653a2d",
    "foreground": "#fff",
    "population": 3.8,
    "title": "#fff"
  },
  "darkVibrant": {
    "_type": "sanity.imagePaletteSwatch",
    "background": "#c4850b",
    "foreground": "#fff",
    "population": 0.08,
    "title": "#fff"
  },
  "dominant": {
    "_type": "sanity.imagePaletteSwatch",
    "background": "#d5c3ba",
    "foreground": "#000",
    "population": 7.17,
    "title": "#fff"
  },
  "lightMuted": {
		// [...] truncated for brevity
  },
  "lightVibrant": {
		// [...] truncated for brevity
  },
  "muted": {
		// [...] truncated for brevity
  },
  "vibrant": {
		// [...] truncated for brevity
  }
}
```

> [!TIP]
> Protip
> If lqip, blurHash, or palette values are absent from your image asset, it's likely that at the time the image was uploaded, a metadata array was specified and the value in question was not included in the array.

## Camera and Location

> [!NOTE]
> Excluded by default
> These values are not included in your image metadata unless a metadata array is specified and these values are specifically requested. This is because camera and location data generally contain private or identifying information.

### `image`

This field contains basic information about the image such as camera make and model, resolution, and orientation. For more detailed information, use the `exif` field. The following is an example readout:

```json
{
  "_type": "sanity.imageExifTags",
  "Make": "Apple",
  "Model": "iPhone 6",
  "Orientation": 1,
  "XResolution": 72,
  "YResolution": 72,
  "ResolutionUnit": 2,
  "Software": "Photos 1.0",
  "ModifyDate": Sat Feb 28 2015 17:13:57 GMT-0800 (PST),
  "ExifOffset": 198,
  "GPSInfo": 1008
}
```

### `exif`

Short for [Exchangeable Image File](https://en.wikipedia.org/wiki/Exif) format, this field contains information about the image file itself and the conditions under which it was produced – typically camera settings. Exactly what data is contained here depends on the origins of the file. Below is an example readout of the Exif object for a photo taken with an iPhone camera:

```json
{
  "_type": "sanity.imageExifMetadata",
  "ApertureValue": 1.6959938128383605,
  "BrightnessValue": 1.7619172145845785,
  "DateTimeDigitized": "2020-03-19T12:25:17.000Z",
  "DateTimeOriginal": "2020-03-19T12:25:17.000Z",
  "ExposureBiasValue": 0,
  "ExposureMode": 0,
  "ExposureProgram": 2,
  "ExposureTime": 0.020833333333333332,
  "FNumber": 1.8,
  "Flash": 16,
  "FocalLength": 4.25,
  "FocalLengthIn35mmFormat": 26,
  "ISO": 250,
  "LensMake": "Apple",
  "LensModel": "iPhone 11 Pro back triple camera 4.25mm f/1.8",
  "LensSpecification": [
    1.5399999618512084,
    6,
    1.8,
    2.4
  ],
  "MeteringMode": 5,
  "PixelXDimension": 4032,
  "PixelYDimension": 3024,
  "SceneCaptureType": 0,
  "SensingMethod": 2,
  "ShutterSpeedValue": 5.586024712398807,
  "SubSecTimeDigitized": "900",
  "SubSecTimeOriginal": "900",
  "SubjectArea": [
    2323,
    710,
    1410,
    1412
  ],
  "WhiteBalance": 0
}
```

### `location`

This field, as you might expect, returns geographical data, usually representing the coordinates where the photo was taken. It conforms to the specification of the [geopoint](https://www.sanity.io/docs/geopoint-type) schema type, and might look like this:

```json
{
  "_type": "geopoint",
  "alt": 168.32554596241746,
  "lat": 59.948811111111105,
  "lng": 10.867780555555557
}
```



## In conclusion

Image assets in your Sanity Content Lake may include a range of helpful metadata. 

- **Always included:** Essential facts about your image: height, width, aspect ratio, and information about transparency.
- **Included by default:** Useful information generated from the image on upload: minified placeholders and palette values.
- **Excluded by default: **Potentially private information about the place and circumstances under which the image was created: exif and location values.



# Asset CDN

Sanity offers a global content delivery network (CDN) for serving assets, at cdn.sanity.io. This is based on [Google's global CDN](https://cloud.google.com/cdn/). Note that this is a different system from our [API CDN](/docs/content-lake/api-cdn).

Assets are uploaded content such as images, videos, and other files - see [separate article](/docs/content-lake/assets) for details. These assets can only be accessed by clients via our asset CDN, optionally with processing by our [image pipeline](/docs/apis-and-sdks/image-urls). When an asset is first requested, it is processed by our backend systems and then cached by the CDN on servers located near end-users. Subsequent requests are then served from the cache, ensuring fast response times and a better user experience.

Assets are cached indefinitely. The asset URL includes a SHA-1 hash of the asset contents, so any content changes will generate a new URL, thus avoiding the need to invalidate the cached entries. We only invalidate caches when a dataset/project is deleted.

Image responses larger than 10 MB currently cannot be cached in the CDN, and are instead returned from the backend servers. However, for all other file types (including videos) we support caching of responses up to 5 TB.

Clients can use standard cache headers such as `Cache-Control`, `If-Modified-Since`, `If-None-Match`, and `Accept-Encoding` to control cache behavior - for details, see the [Google Cloud CDN documentation](https://cloud.google.com/cdn/docs/caching).





# Introduction

The [Sanity Connect application for Shopify](https://apps.shopify.com/sanity-connect) is used to synchronize content between a Sanity dataset and your Shopify store. This gives you flexibility to use the tools that are right for your needs. You can take a headless approach using Shopify's Hydrogen framework and Next.js, or you can sync data into Shopify's platform and use Liquid or the Storefront API.

## Requirements

To take advantage of Sanity Connect you will need:

4. A Shopify store
4. A Sanity project and dataset

If you are starting with a new Sanity dataset, you can create the dataset and a pre-configured Studio instance using:

```sh
npm create sanity@latest -- --template shopify --create-project "Shopify Store" --dataset production --typescript --output-path shopify-store
```

## Installation

To install Sanity Connect in your Shopify store and connect it to a project:

- Find [Sanity Connect on the Shopify app store](https://apps.shopify.com/sanity-connect) and push the Add App button
- If you have multiple Shopify accounts, you need to choose the one that contains the store you want to add the app to
- After choosing the store, Shopify will show you the permissions Sanity Connect needs to work and its data policies. You can push the Install app button to continue.
- The app will ask you to connect to your Sanity account. If you don't have one, you can choose to **Create new account**.
- When you're logged in, you will need to connect your shop with a project on Sanity. You can choose between existing projects or create a new one (for free).
- Select organization to list out projects under it, and then the project and dataset you want to sync to.
- You are now ready to configure the app.

> [!WARNING]
> Gotcha
> Once you chose Start synchronizing now, the app will add product documents to your content lake. It can be wise to test it against a non-production dataset if you haven't tried it before.

You might also want to consider using our [Shopify asset plugin](https://github.com/sanity-io/sanity-plugin-shopify-assets), which allows you to select assets from your Shopify store in the context of your Sanity Studio, allowing you to serve assets from the Shopify CDN in your front ends.

## **Settings**

You can configure how and when Sanity Connect should synchronize products to your content lake, and whether content should be synchronized back to your Shopify store. You can change these options at any time.

![Settings panel with synchronization settings](https://cdn.sanity.io/images/3do82whm/next/8c633519b6b003dd7a95026d8e8c13df9df5b809-1274x1346.png)

![Settings panel with synchronization settings](https://cdn.sanity.io/images/3do82whm/next/f8a87a923c694905c6c8293814534ea436df29fc-1282x1438.png)

### Sync content from Sanity to Shopify

This setting allows you to sync any custom fields and document types you've created in Sanity back into Shopify. Your custom content will sync as Shopify metafields and metaobjects.

For a deeper dive, review our documentation on [displaying Sanity content within Shopify](/docs/developer-guides/displaying-sanity-content-in-shopify).

### How to synchronize

Sanity Connect offers two ways to synchronize content from Shopify into your content lake - direct sync and custom sync.

**Direct Sync**

This will synchronize all products, product variants and collections as documents to your content lake. You can check the [reference](/docs/apis-and-sdks/sanity-connect-for-shopify-reference) to preview the data model for these documents.

> [!TIP]
> Protip
> Synced documents created by Sanity Connect will count towards your Sanity document usage limit. One document will be created for every product, product variant and collection in your storefront.

**Custom Sync**

This option will let you enter an endpoint that receives updates from Shopify and syncs data to your content lake. Typically that will be a serverless function handler where you can reshape the data and do other business logic as part of the sync.

You may, for example, want to reduce document usage by syncing products but not variants, or sync variants as objects on a product document rather than individual variant documents.

We have further documentation on [custom sync handlers](/docs/developer-guides/custom-sync-handlers-for-sanity-connect) including an example serverless function.

### When to synchronize

**Sync data automatically:** Automatically sync whenever you save products. Note: The sync will update the Shopify information for both published and draft documents. An update is typically available in your content lake after a couple of seconds.

**Sync manually:** There will no automatic sync, and you'll have to go into the Sanity Connect settings to trigger a synchronization manually.

> [!WARNING]
> Gotcha
> Sanity Connect will do an initial synchronization once you choose one of these options.

### Sync collections

The Sanity Connect app can optionally sync collections data. This will sync data and properties about your collection, but it will not sync the product membership of your collections.

## **Set up your Studio**

You can install a production-ready reference studio that's set up with a great editor experience by running this command in your local shell. Replace the `PROJECT_ID` and `DATASET_NAME` placeholders with the actual values from the project your Shopify store is connected to:

```sh
npx @sanity/cli init --template shopify --project PROJECT_ID --dataset DATASET_NAME
```

You'll find comprehensive documentation for this studio in its `README.md`.

![Screenshot of Shopify reference studio](https://cdn.sanity.io/images/3do82whm/next/58ebc2e9801b90061c4184d22ff0d267f534a25e-720x427.png)

### Integrate with an existing studio

If you've already set up a studio instance, you can follow the patterns exposed in [sanity-shopify-studio](https://github.com/sanity-io/sanity-shopify-studio). This repository showcases the same studio customizations that are implemented when creating a new studio with the `shopify` template.

## Further reading

[Sanity Studio for Shopify](https://github.com/sanity-io/sanity-shopify-studio)

[Shopify asset selection for Sanity Studio](https://github.com/sanity-io/sanity-plugin-shopify-assets)





# Custom sync handlers

A custom sync handler allows you to provide an endpoint which receives updates from Shopify and passes data into your content lake. Typically, this will be a serverless function where you can reshape the data from Shopify and apply business logic before it is passed to your content lake.

## When to use a custom sync handler

There are a number of scenarios where you may choose to implement a custom sync handler - common examples include:

- Where you need to apply additional logic to the data - for example, querying additional APIs to retrieve more data (e.g. the Shopify API to get additional metafields)
- You may want to reduce your document usage on Sanity by only syncing selected products, or syncing variants as an object on product documents rather than variant documents.
- Where you want to amend the default manner in which Sanity Connect handles a product being deleted on Shopify - by setting `isDeleted` to `true` - to fully delete the document from your content lake.

## How custom sync handlers work

When enabled, the custom sync handler will send a payload on every update from Shopify as a POST request. You can write your custom business logic in your endpoint and [update](https://www.sanity.io/docs/transactions) your content lake accordingly in the function, or respond with a set of documents which Sanity Connect will update for you.

Sanity Connect expects a response header with `content-type: application/json` and will regard a `200` status code as a success. Any other status code will be considered a failure.

You can find the [shape of the payload your handler](/docs/apis-and-sdks/sanity-connect-for-shopify-reference) will receive in our Sanity Connect reference.

> [!WARNING]
> Gotcha
> The request has a 10s timeout and your handler needs to reply before that. Any failed requests will be retried up to 10 times.
> 
> If your handler needs more time to complete updates (for example if it calls a third party API), a common pattern would be to store the payload in a queue for background processing, and respond 200 OK immediately to acknowledge receipt of the payload.

> [!WARNING]
> Gotcha
> This operation will in batched when manually syncing, especially when dealing with larger catalogs.

> [!WARNING]
> Gotcha
> Changes in product inventory (through sales) will also trigger updates to your custom handler.
> 
> Make sure to tailor your custom handler to account for how our API CDN invalidates cache on writes to non-draft documents, especially if operating on a high traffic stores with fast moving content.
> 
> 

## Example custom sync handler function

Below is an example of a barebones custom function that will:

- Create/update/delete products (including drafts) in the Content Lake on Shopify product operations
- Only deal with products (variants are included as objects within products)
- Manual sync will create and update products on your dataset, but will not delete products that have since been removed.

For a more complete example, refer to [this gist](https://gist.github.com/snorrees/1ca7c3191d62ede6b9b5d0a1822d7103#file-requirements-md).

```javascript
import {createClient} from "@sanity/client";

// Document type for all incoming synced Shopify products
const SHOPIFY_PRODUCT_DOCUMENT_TYPE = "shopify.product";

// Prefix added to all Sanity product document ids
const SHOPIFY_PRODUCT_DOCUMENT_ID_PREFIX = "product-";

// Enter your Sanity studio details here.
// You will also need to provide an API token with write access in order for this
// handler to be able to create documents on your behalf.
// Read more on auth, tokens and securing them: https://www.sanity.io/docs/http-auth
const sanityClient = createClient({
  apiVersion: "2021-10-21",
  dataset: process.env.SANITY_DATASET,
  projectId: process.env.SANITY_PROJECT_ID,
  token: process.env.SANITY_ADMIN_AUTH_TOKEN,
  useCdn: false,
});

/**
 * Sanity Connect sends POST requests and expects both:
 * - a 200 status code
 * - a response header with `content-type: application/json`
 * 
 * Remember that this may be run in batches when manually syncing.
 */
export default async function handler(req, res) {
  // Next.js will automatically parse `req.body` with requests of `content-type: application/json`,
  // so manually parsing with `JSON.parse` is unnecessary.
  const { body, method } = req;

  // Ignore non-POST requests
  if (method !== "POST") {
    return res.status(405).json({ error: "Method not allowed" });
  }

  try {
    const transaction = sanityClient.transaction();
    switch (body.action) {
      case "create":
      case "update":
      case "sync":
        await createOrUpdateProducts(transaction, body.products);
        break;
      case "delete":
        const documentIds = body.productIds.map((id) =>
          getDocumentProductId(id)
        );
        await deleteProducts(transaction, documentIds);
        break;
    }
    await transaction.commit();
  } catch (err) {
    console.error("Transaction failed: ", err.message);
  }

  res.status(200).json({ message: "OK" });
}

/**
 * Creates (or updates if already existing) Sanity documents of type `shopify.product`.
 * Patches existing drafts too, if present.
 *
 * All products will be created with a deterministic _id in the format `product-${SHOPIFY_ID}`
 */
async function createOrUpdateProducts(transaction, products) {
  // Extract draft document IDs from current update
  const draftDocumentIds = products.map((product) => {
    const productId = extractIdFromGid(product.id);
    return `drafts.${getDocumentProductId(productId)}`;
  });

  // Determine if drafts exist for any updated products
  const existingDrafts = await sanityClient.fetch(`*[_id in $ids]._id`, {
    ids: draftDocumentIds,
  });

  products.forEach((product) => {
    // Build Sanity product document
    const document = buildProductDocument(product);
    const draftId = `drafts.${document._id}`;

    // Create (or update) existing published document
    transaction
      .createIfNotExists(document)
      .patch(document._id, (patch) => patch.set(document));

    // Check if this product has a corresponding draft and if so, update that too.
    if (existingDrafts.includes(draftId)) {
      transaction.patch(draftId, (patch) =>
        patch.set({
          ...document,
          _id: draftId,
        })
      );
    }
  });
}

/**
 * Delete corresponding Sanity documents of type `shopify.product`.
 * Published and draft documents will be deleted.
 */
async function deleteProducts(transaction, documentIds) {
  documentIds.forEach((id) => {
    transaction.delete(id).delete(`drafts.${id}`);
  });
}

/**
 * Build Sanity document from product payload
 */
function buildProductDocument(product) {
  const {
    featuredImage,
    id,
    options,
    productType,
    priceRange,
    status,
    title,
    variants,
  } = product;
  const productId = extractIdFromGid(id);
  return {
    _id: getDocumentProductId(productId),
    _type: SHOPIFY_PRODUCT_DOCUMENT_TYPE,
    image: featuredImage?.src,
    options: options?.map((option, index) => ({
      _key: String(index),
      name: option.name,
      position: option.position,
      values: option.values,
    })),
    priceRange,
    productType,
    status,
    title,
    variants: variants?.map((variant, index) => {
      const variantId = extractIdFromGid(variant.id);
      return {
        _key: String(index),
        compareAtPrice: Number(variant.compareAtPrice || 0),
        id: variantId,
        inStock: !!variant.inventoryManagement
          ? variant.inventoryPolicy === "continue" ||
            variant.inventoryQuantity > 0
          : true,
        inventoryManagement: variant.inventoryManagement,
        inventoryPolicy: variant.inventoryPolicy,
        option1: variant?.selectedOptions?.[0]?.value,
        option2: variant?.selectedOptions?.[1]?.value,
        option3: variant?.selectedOptions?.[2]?.value,
        price: Number(variant.price || 0),
        sku: variant.sku,
        title: variant.title,
      };
    }),
  };
}

/**
 * Extract ID from Shopify GID string (all values after the last slash)
 * e.g. gid://shopify/Product/12345 => 12345
 */
function extractIdFromGid(gid) {
  return gid?.match(/[^\/]+$/i)[0];
}

/**
 * Map Shopify product ID number to a corresponding Sanity document ID string
 * e.g. 12345 => product-12345
 */
function getDocumentProductId(productId) {
  return `${SHOPIFY_PRODUCT_DOCUMENT_ID_PREFIX}${productId}`;
}
```



# Reference

You will find all data synced from Shopify under the `store` property of each document. Typically, you want to set these fields as `readOnly` or `hidden` in your Sanity Studio schemas.

## Sanity Publish State

All products sync from Shopify into Sanity, and we attempt to keep the `Status` in Shopify linked to the publishing state in Sanity.

- If a product is 'Draft' in Shopify- The document is created as unpublished in Sanity.
- Any changes to the product cascade to the unpublished draft in Sanity.


- If a product is 'Active' in Shopify- The document is created as published in Sanity.
- Any changes to the product cascade to the published document as well as any unpublished draft.



If a product is Archived or switched to the 'Draft' status, then we attempt to unpublish the matched document in Sanity. This operation will fail if the published Sanity document is referenced by another document in your dataset. We allow the operation to fail, and we will attempt to unpublish the document again on the next sync.

Because 'Draft' products are not published, we do not support syncing custom fields as [Shopify metafields](/docs/developer-guides/displaying-sanity-content-in-shopify) on draft products. These custom fields will sync once the product is switched to 'Active'.

## **Product document**

This is an example of a product document. Note the array of references to variant documents.

```json
{
  "_createdAt": "2022-05-18T07:45:26Z",
  "_id": "shopifyProduct-7696133062907",
  "_rev": "sERZ3ZJ9MtNiP4BmT5zftt",
  "_type": "product",
  "_updatedAt": "2022-08-31T21:41:10Z",
  "body": [],
  "store": {
    "createdAt": "2022-05-12T17:39:51+01:00",
    "descriptionHtml": "",
    "gid": "gid://shopify/Product/7696133062907",
    "id": 7696133062907,
    "isDeleted": false,
    "options": [
      {
        "_key": "Color",
        "_type": "option",
        "name": "Color",
        "values": [
          "Blue",
          "Ecru",
          "Pink"
        ]
      }
    ],
    "previewImageUrl": "https://cdn.shopify.com/s/files/1/0639/3285/8619/products/Green_1.jpg?v=1655598944",
    "priceRange": {
      "maxVariantPrice": 25.5,
      "minVariantPrice": 25
    },
    "productType": "",
    "slug": {
      "_type": "slug",
      "current": "soap-dish"
    },
    "status": "active",
    "tags": "",
    "title": "AUTOGRAF Soap Dish",
    "variants": [
      {
        "_key": "c8b492e1-3c24-527d-bffd-accc634177c7",
        "_ref": "shopifyProductVariant-43068621422843",
        "_type": "reference",
        "_weak": true
      },
      {
        "_key": "9128c62c-f887-594c-b9b8-ddaaf850ce84",
        "_ref": "shopifyProductVariant-43068621455611",
        "_type": "reference",
        "_weak": true
      },
      {
        "_key": "5d861cdf-bcfe-5781-81dd-d62db159442b",
        "_ref": "shopifyProductVariant-43068621488379",
        "_type": "reference",
        "_weak": true
      }
    ],
    "vendor": "Lucy Holdberg"
  }
}
```

## **Variant document**

This is an example of a variant document.

```json
{
  "_createdAt": "2022-05-27T08:49:54Z",
  "_id": "shopifyProductVariant-43068621422843",
  "_rev": "sERZ3ZJ9MtNiP4BmT5zftt",
  "_type": "productVariant",
  "_updatedAt": "2022-08-31T21:32:01Z",
  "store": {
    "compareAtPrice": 35,
    "createdAt": "2022-05-27T09:49:52+01:00",
    "gid": "gid://shopify/ProductVariant/43068621422843",
    "id": 43068621422843,
    "inventory": {
      "isAvailable": true,
      "management": "SHOPIFY",
      "policy": "CONTINUE"
    },
    "isDeleted": false,
    "option1": "Blue",
    "option2": "",
    "option3": "",
    "previewImageUrl": "https://cdn.shopify.com/s/files/1/0639/3285/8619/products/Blue_1.jpg?v=1655598950",
    "price": 25.5,
    "productGid": "gid://shopify/Product/7696133062907",
    "productId": 7696133062907,
    "sku": "AGSD_BLUE",
    "status": "active",
    "title": "Blue"
  }
}
```

## **Collection document**

This is an example of a collection document:

```json
{
  "_createdAt": "2022-06-07T10:00:11Z",
  "_id": "shopifyCollection-396461834491",
  "_rev": "0penztPZlC32Cv2tesREk7",
  "_type": "collection",
  "_updatedAt": "2022-08-26T15:07:57Z",
  "store": {
    "createdAt": "2022-08-26T15:07:56.895Z",
    "descriptionHtml": "",
    "disjunctive": false,
    "gid": "gid://shopify/Collection/396461834491",
    "id": 396461834491,
    "imageUrl": "https://cdn.shopify.com/s/files/1/0639/3285/8619/collections/BLOMST_print.jpg?v=1655599663",
    "isDeleted": false,
    "rules": [
      {
        "_key": "7803ad21-682e-56b6-ae2a-4d380d0d120c",
        "_type": "object",
        "column": "TYPE",
        "condition": "Poster",
        "relation": "CONTAINS"
      }
    ],
    "slug": {
      "_type": "slug",
      "current": "prints"
    },
    "sortOrder": "BEST_SELLING",
    "title": "Prints"
  }
}
```

Below are the data types for the properties of a collection document:

```javascript
export type ShopifyDocumentCollection = {
  _id: `shopifyCollection-${string}` // Shopify product ID
  _type: 'collection'
  store: {
    id: number
    gid: `gid://shopify/Collection/${string}`
    createdAt: string
    isDeleted: boolean
    descriptionHtml: string
    imageUrl?: string
    rules?: {
      _key: string
      _type: 'object'
      column: Uppercase<string>
      condition: string
      relation: Uppercase<string>
    }[]
    disjunctive?: boolean
    slug: {
      _type: 'slug'
      current: string
    }
    sortOrder: string
    title: string
    updatedAt: string
  }
}
```

## **Custom webhook sync payload**

If you use the custom webhook sync, your handler will receive the shape described `Product` (and `Collection` if enabled) below. You can still use JavaScript or any other programming language in your custom handler even though we describe the payload using [TypeScript](https://www.typescriptlang.org/) syntax.

```typescript
export type Product = {
  id: `gid://shopify/ProductVariant/${string}`
  title: string
  description: string
  descriptionHtml: string
  featuredImage?: ProductImage
  handle: string
  images: ProductImage[]
  options: ProductOption[]
  priceRange: ProductPriceRange
  productType: string
  tags: string[]
  variants: ProductVariant[]
  vendor: string
  status: 'active' | 'archived' | 'draft' | 'unknown'
  publishedAt: string
  createdAt: string
  updatedAt: string
}
export type ProductImage = {
  id: `gid://shopify/ProductImage/${string}`
  altText?: string
  height?: number
  width?: number
  src: string
}
export type ProductOption = {
  id: `gid://shopify/ProductOption/${string}`
  name: string
  position: number
  values: string[]
}
export type ProductPriceRange = {
  minVariantPrice?: number
  maxVariantPrice?: number
}
export type ProductVariant = {
  id: `gid://shopify/ProductVariant/${string}`
  title: string
  compareAtPrice?: number
  barcode?: string
  inventoryPolicy: string
  inventoryQuantity: number
  inventoryManagement: string
  position: number
  requiresShipping: boolean
  sku: string
  taxable: boolean
  weight: number
  weightUnit: string
  price: string
  createdAt: string
  updatedAt: string
  image?: ProductImage
  product: {
    id: `gid://shopify/Product/${string}`
    status: 'active' | 'archived' | 'draft' | 'unknown'
  }
  selectedOptions: {
    name: string
    values: string[]
  }[]  
}
export type Collection = {
  id: `gid://shopify/Collection/${string}`
  createdAt: string
  handle: string
  descriptionHtml: string
  image?: CollectionImage
  rules?: {
    column: string
    condition: string
    relation: string
  }[]
  disjunctive?: boolean
  sortOrder: string
  title: string
  updatedAt: string
}
export type CollectionImage = {
  altText: string
  height?: number
  width?: number
  src: string
}

// When products are created, updated or manually synced
export type payloadProductsSync = {
  action: 'create' | 'update' | 'sync'
  products: Product[]
}

// When products are deleted
export type payloadProductsDelete = {
  action: 'delete'
  productIds: number[]
}

// When collections are created, updated or manually synced
export type payloadCollectionsSync = {
  action: 'create' | 'update' | 'sync'
  collections: Collection[]
}

// When collections are deleted
export type payloadCollectionsDelete = {
  action: 'delete'
  collectionIds: number[]
}

export type requestPayload = payloadProductsDelete | payloadProductsSync | payloadCollectionsDelete | payloadCollectionsSync
```



# Build custom applications on Sanity

#### Get started

[App SDK Quickstart Guide](/docs/app-sdk/sdk-quickstart)

[Conceptual Walkthrough](/docs/app-sdk/sdk-introduction)



#### Reference and examples

[App SDK – Reference](https://reference.sanity.io/_sanity/sdk-react/)

[App SDK Explorer](https://sdk-explorer.sanity.io)



#### Concepts

[Document Handles](/docs/app-sdk/document-handles)

[React Hooks](/docs/app-sdk/sdk-react-hooks)

[React Suspense](/docs/app-sdk/react-suspense-sdk)





# Quickstart

## Create a new App SDK app

Initialize a new project by running `npx sanity@latest`: 

```sh
npx sanity@latest init --template app-quickstart
```

When prompted:

- Select **yes** when asked to install the sanity package
- Choose your organization, or create a new one
- Specify a location to save your project locally
- Choose whether you want to work with TypeScript or JavaScript

Once you've worked through these options, the CLI should proceed to install all the necessary dependencies, and report back with a confirmation.

```sh
Success! Now, use this command to continue:

pnpm dev - to run your Sanity application

Other helpful commands
npx sanity docs - to open the documentation in a browser
npx sanity manage - to open the project settings in a browser
npx sanity help - to explore the CLI manual 
```

## Navigate to the project directory

If you chose to install your project in a folder different to the current directory, such as a sub-folder, navigate into the project root.

```sh
cd my-cool-project
```

## Inspect the project folder

In your favorite editor, open the project root and have a look around. Note the `sanity.cli.ts`, `App.tsx`, and `ExampleComponent.tsx` files in particular.

> [!TIP]
> JS|TS|JSX|TSX
> For readability we won't note every time a file could be either a js/jsx-file or a ts/tsx-file. We'll default to showing the examples inTypeScript going forth. If you are working in JavaScript, replace those T's with J's!

### sanity.cli.ts

This is the main configuration for your project . By default, it contains the unique ID for your organization, and the entrypoint for your app.

```
import {defineCliConfig} from 'sanity/cli'

export default defineCliConfig({
  app: {
    organizationId: 'your-org-id',
    entry: './src/App.tsx',
  },
})
```

### src/App.tsx

This is the main entrypoint for your application. It contains the `<SanityApp />` context provider, and demonstrates how to connect your application to an existing Sanity project. The `<SanityApp />` component provides child components with the necessary context to use the SDK React hooks to interact with the content in your project.

**Before moving on,** modify the `config` variable to include the `projectId` and `dataset` for the Sanity project you’d like to work with in your custom app.

```tsx
import {type SanityConfig} from '@sanity/sdk'
import {SanityApp} from '@sanity/sdk-react'
import {ExampleComponent} from './ExampleComponent'
import './App.css'  
  
export default function App() {
  // apps can access one or many different projects or other sources of data
  const config: SanityConfig[] = [
    {
      projectId: 'project-id',
      dataset: 'dataset-name',
    }
  ]
  
  return (
    <div className="app-container">
      <SanityApp config={config} fallback={<div>Loading...</div>}>
        {/* add your own components here! */}
        <ExampleComponent />
      </SanityApp>
    </div>
  )
}
```

### src/ExampleComponent.tsx

This component just displays some static content to welcome you to your project. Feel free to get rid of it, or use it as a springboard to write something cooler.

```tsx
import './ExampleComponent.css'

export function ExampleComponent() {
  return (
    <div className="example-container">
      <h1 className="example-heading">Welcome to your Sanity App!</h1>
      <p className="example-text">
        This is an example component. You can replace this with your own content
        by creating a new component and importing it in App.tsx.
      </p>
      <div className="code-hint">
        <p>Quick tip: Create new components in separate files and import them like this in App.tsx / App.jsx:</p>
        <pre>{`import {YourComponent} from './YourComponent'

// Then use it in your JSX 
<SanityApp config={config}>
  <YourComponent />
</SanityApp>`}</pre>
      </div>
    </div>
  )
}

```

## Start the development server

It's time to actually run the app! Enter the following command in your terminal:

```sh
npm run dev
```

You should see the CLI reporting on its progress.

```sh
✓ Checking configuration files...
✓ Starting dev server
Dev server started on port 3333
View your app in the Sanity dashboard here:
https://sanity.io/@your-org-id?dev=http://localhost:3333
```

Once having successfully launched your app, the CLI will provide you with a URL where you can see it running locally in the Sanity Dashboard. Open this link in your browser to see the Dashboard frontpage, then locate your application in the sidebar.

![a welcome to your sanity app page](https://cdn.sanity.io/images/3do82whm/next/dcd155e20ae9696632da4c0114811d3aaeb282d5-1229x935.png)

## Deploy your app

Finally, when you are happy with your custom app, it's time to deploy it. Run the following command:

```sh
npx sanity@latest deploy
```

Your custom app will be deployed and made available in your organization dashboard.

## Troubleshooting

If you see an error about the port being in use:

- Kill any existing process using port 3333, or
- Start the dev server on a different port:

```sh
npm run dev -- --port 3334
```

If you see an error about missing authorization:

- Make sure your user account has the appropriate privileges
- Log out and back in to Sanity

```
npx sanity@latest logout

npx sanity@latest login
```

## Next steps

- Explore the [React App SDK reference docs](https://reference.sanity.io/_sanity/sdk-react/)
- See examples of the App SDK in action in the [SDK Explorer](https://sdk-explorer.sanity.io)
- Read the [introduction to the Sanity App SDK](/docs/app-sdk/sdk-introduction)



# Installation

The Sanity App Software Development Kit (App SDK) is distributed as two separate npm packages – the core TypeScript SDK, and a ready-to-go React implementation.

- [@sanity/sdk](https://reference.sanity.io/_sanity/sdk/)
- [@sanity/sdk-react](https://reference.sanity.io/_sanity/sdk-react/)

While the core SDK can be used on its own, its primary purpose is to enable the React SDK’s functionality, as well as to leave the door open for other framework-specific implementations in the future. For now, our React SDK is our primary focus, and it’s what we’ll be installing via the bootstrapping process described below.

## Prerequisites

- Some familiarity with JavaScript and/or TypeScript development
- A terminal
- Node.js v20

## Bootstrapping a new app using the `sanity` CLI

Use the [sanity Command Line Interface (CLI)](/docs/cli-reference/cli-config) to initialize a new application. A new React app with all the necessary dependencies and boilerplate will be created. 

```sh
npx sanity@latest init --template app-quickstart
```

Your app will be bootstrapped and preconfigured with your organization ID. If you have ever worked on a Sanity Studio project locally, this should feel familiar. 

Before running your app locally, you’ll need to add a small bit of configuration, which we’ll walk through in the next article. 

#### Further reading

- Read the [App SDK Quickstart Guide](/docs/app-sdk/sdk-quickstart) to get up and running quickly
- Read about the [Sanity CLI](/docs/apis-and-sdks/cli)





# Configuration

## Configuration

The term "configuration" in the context of SDK apps will generally refer to one of two things:

- Your root app configuration file: `sanity.cli.ts` or `.js`, found at the root of your project. This file includes your organization ID, and the entry point for your app. Once your project has been deployed, an `id` for your application will be added to facilitate future deployment.

```
import {defineCliConfig} from 'sanity/cli'

export default defineCliConfig({
  app: {
    organizationId: 'your-org-id',
    entry: './src/App.tsx',
    id: 'your-app-id',
  },
})
```

- One or many `SanityConfig` objects that contain a project ID and dataset belonging to the Sanity dataset(s) you wish to interact with in your app. This configuration is passed to the `SanityApp` provider which typically wraps your application and provides all the necessary context needed to interact with your content.

```
import {type SanityConfig} from '@sanity/sdk'
import {SanityApp} from '@sanity/sdk-react'
import {MyCoolComponent} from './components/MyCoolComponent'
import './App.css'

export function App() {
  // apps can access one or many different projects or other sources of data
  const config: SanityConfig[] = [
    {
      projectId: 'my-project-id',
      dataset: 'my-dataset',
    }
  ]

  return (
    <div className="app-container">
      <SanityApp 
        config={config} 
        fallback={<div>Loading...</div>}
      >
        {/* add your own components here! */}
        <MyCoolComponent />
      </SanityApp>
    </div>
  )
}

export default App

```

Before you can run your application locally for the first time, you’ll need to update the `config` variable in `src/App.tsx` to include the `projectId` and `dataset` for the Sanity project(s) you want your app to work with. You can enter one or more of these configuration objects in the `config` array if you’d like your app to work with multiple projects and datasets.

- [Read more about SanityConfig in the reference docs](https://reference.sanity.io/_sanity/sdk-react/exports/SanityApp/)

## Local development

To run your app in development mode, run the following command in your terminal from your app’s directory:

```sh
npm run dev
```

Your app will open in your organization’s Dashboard, even while running locally.



# Deployment

## Deploy your app

To deploy your custom application, you use the same command as when deploying a studio: [sanity deploy](/docs/cli-reference/deploy)

```sh
npx sanity deploy
```

Note that to deploy SDK apps you need to have the `sanity.sdk.applications.deploy` permission enabled for your user account. This permission is enabled for Organization Admins and Developers by default. Read more about roles and permissions [here](/docs/content-lake/roles-concepts).

## Undeploy your app

To undeploy your custom application, you can use [sanity undeploy](/docs/cli-reference/undeploy) from within your custom app’s directory.

```sh
npx sanity undeploy
```

Note that you’ll need to have your `app.id` saved in your `sanity.cli.ts` file (as prompted during the deploy process) in order for your app’s deployment to be removed.



# Document Handles

## Introducing Document Handles

In this article, we’ll describe what Document Handles ( `DocumentHandle`, see their type definition [here](https://reference.sanity.io/_sanity/sdk/index/DocumentHandle/)) are, why they’re useful, and how to work with them.

## What is a Document Handle?

In short, a `DocumentHandle` is a 'stub' of a document — a small piece of metadata, encoded in a JavaScript object, that acts as a reference to a complete document in your dataset(s).

It looks like this:

```typescript
const myDocumentHandle = {
  documentId: 'my-document-id',
  documentType: 'article'
}
```

A Document Handle may also contain optional information about the project and dataset it originates from; in that case, it would look like this:

```typescript
const myDocumentHandle = {
  documentId: 'my-document-id',
  documentType: 'author',
  dataset: 'dataset-name',
  projectId: 'my-project-id'
}
```

Therefore, for a document in a given dataset that looks (in part) like this:

```json
{
  "_id": "123456-abcdef",
  "_type": "book",
  "title": "Into the Cool",
  "publisher": "The University of Chicago Press",
  "pages": 378,
  "…": "…"
}
```

…the corresponding Document Handle would look like this:

```json
{
  documentId: "123456-abcdef",
  documentType: "book"
}
```

## Why are Document Handles used?

Hooks like [useDocuments](https://reference.sanity.io/_sanity/sdk-react/exports/useDocuments/) and [usePaginatedDocuments](https://reference.sanity.io/_sanity/sdk-react/exports/usePaginatedDocuments/) can return potentially large numbers of documents matching your specified parameters. If these hooks were to return every matching document in its entirety, this could end up being a potentially performance heavy operation, which could thus slow down your application and result in a poor user experience. Additionally, you may not need each returned document in its entirety to begin with — perhaps, for example, you just want to render a document preview, or one or two arbitrary fields of a document, or to simply get a count of documents matching your parameters.

This is where the concept of Document Handles comes in. By returning a small amount of metadata for each document instead of unfurling every returned document, hooks like `useDocuments` can respond as fast as possible, allowing your application to remain snappy.

Of course, unless you’re just looking to get a count of documents matching the parameters you pass to these hooks, Document Handles aren't incredibly useful on their own. This is by design — they’re only meant to serve as references to documents which can then be consumed by more specialized hooks, such as [useDocumentProjection](https://reference.sanity.io/_sanity/sdk-react/exports/useDocumentProjection/), [useDocument](https://reference.sanity.io/_sanity/sdk-react/exports/useDocument/), and many more hooks provided by the SDK. These specialized hooks are designed to consume document handles and emit only the document content you request, which also delivers huge performance benefits. Other hooks, such as [useDocumentEvent](https://reference.sanity.io/_sanity/sdk-react/exports/useDocumentEvent/) and [useDocumentPermissions](https://reference.sanity.io/_sanity/sdk-react/exports/useDocumentPermissions/) have no need to know the contents of a document — instead, they use the provided Document Handle to reference a document and retrieve information pertaining to that document.

In short, Document Handles promote deferring the retrieval of document contents until such time as those contents are actually needed by your application.

## Using your own Document Handles

You’re not limited to using Document Handles returned by hooks like `useDocuments` — if it suits your use case (for example: if you know the document ID and type of the document you want to reference), you can certainly write and use your own Document Handles.

### Creating Handles Manually

You can create a handle simply by defining an object that matches the DocumentHandle interface:

```tsx
import {useDocumentSyncStatus, type DocumentHandle} from '@sanity/sdk-react'

const myDocumentHandle: DocumentHandle = {
  documentId: 'my-document-id',
  documentType: 'book',
}

const documentSynced = useDocumentSyncStatus(myDocumentHandle)
```

### Using the `createDocumentHandle` Helper (Recommended with Typegen)

The SDK also provides helper functions like `createDocumentHandle` for creating handles.

```tsx
import {createDocumentHandle} from '@sanity/sdk' // Or specific package import

const myDocumentHandle = createDocumentHandle({
  documentId: 'my-document-id',
  documentType: 'book',
})
```

While creating handles as plain objects works fine, using the `createDocumentHandle` helper (or similar helpers like `createDatasetHandle`) is recommended, **especially if you are using** [sanity typegen](/docs/apis-and-sdks/sanity-typegen).

Why? When [using the SDK hooks with Typegen](https://reference.sanity.dev/_sanity/sdk-react/Typescript_with_TypeGen_(experimental)/), the hooks can provide much richer type information if they know the *specific* literal type of the `documentType` (e.g., knowing it's exactly `'book'`, not just any `string`). The `createDocumentHandle` function helps TypeScript capture this literal type automatically.

If you prefer not to use the helper function when working with Typegen, you can achieve the same result by using `as const` when defining the handle object:

```
import {type DocumentHandle} from '@sanity/sdk' // Or specific package import

const myDocumentHandle = {
  documentId: 'my-document-id',
  documentType: 'book',
} as const // <-- Using 'as const' captures the literal type 'book'

// Now, myDocumentHandle.documentType is typed as 'book', not string
```

Using either `createDocumentHandle` or `as const` ensures that subsequent hooks like `useDocument` or `useDocumentProjection` can correctly infer types based on the specific `documentType` provided in the handle when Typegen is enabled.

## A quick example

Let’s say you’d like to get all of the documents of type `'author'` from a dataset. In your Sanity application, you could use the `useDocuments` hook to do that:

```tsx
import {useDocuments} from '@sanity/sdk-react'

export function AuthorList() {
  const {data: authors} = useDocuments({documentType: 'author'})
}
```

At this point, the `authors` variable contains an array of Document Handles, which, because we’re filtering for only the `author` content type, will look like this:

```json
{ documentId: 'the-document-id', documentType: 'author' }
```

With this information, we could render the number of authors in the dataset — for example:

```tsx
import {useDocuments} from '@sanity/sdk-react'

export function AuthorList() {
  const {data: authors} = useDocuments({documentType: 'author'})

  return <p>There are currently {authors.length} authors in our dataset.</p>
}
```

If we wanted to instead render content from each of these documents — for example, the author’s name — we’d then need to provide each Document Handle to a different hook — for example, `useDocumentProjection`. Note how the document handle is [spread](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Spread_syntax) in the arguments to the `useDocumentProjection` hook below:

```tsx
import {useDocumentProjection, type DocumentHandle, type useDocumentProjectionResults} from '@sanity/sdk-react'

interface NameProjection {
  name: string
}

// The AuthorDetails component will accept a Document Handle for its `document` prop
export function AuthorDetails({document}: {document: DocumentHandle}) {
  const {data}: useDocumentProjectionResults<NameProjection> = useDocumentProjection({
    ...document,
    projection: '{ name }',
  })

  return <p>The author's name is {data.name}</p>
}
```

With this in place, we can then use our `AuthorDetails` component with our previously created `AuthorList` component, and pass along the Document Handles to each instance of the `AuthorDetails` component:

```tsx
import {useDocuments} from '@sanity/sdk-react'

import AuthorDetails from './AuthorDetails.tsx'

export function AuthorList() {
  const { data: authors } = useDocuments({documentType: 'author'})

  return (
    <>
      <p>There are {authors.length} authors in our dataset! Here they are:</p>
      <ul>
        {authors.map(author => (
          <li key={author.documentId}>
            <AuthorDetails document={author} />
          </li>
        )}
      </ul>
    </>
  )
}
```

We’ve now both retrieved a list of Document Handles, and used each of them in a dedicated component with a hook that consumes Document Handles. No matter how many authors there are in our dataset, nor how many fields might exist on our author type, this will keep our application performing as fast as possible by separating the concerns of retrieving author type documents (or rather, Document Handles) and retrieving data from those documents.



# React Hooks

The Sanity App SDK comes with a range of hooks available for interacting with your content. A full reference is available for your perusal here:

- [Sanity React App SDK Reference Docs](https://reference.sanity.io/_sanity/sdk-react/)

While visiting every hook, type and component is beyond the scope of this article, a few of the most important hooks are briefly introduced below to give you a sense of how you'll be interacting with your Sanity content using the App SDK. 

For the sake of legibility, assume that each example is invoked with a proper [DocumentHandle](https://reference.sanity.io/_sanity/sdk-react/Introducing_Document_Handles/), which is a valid combination of a document ID and document type, and an optional project ID and dataset name indicating the source of the document.

```
import {type DocumentHandle} from '@sanity/sdk-react'

const documentHandle: DocumentHandle = {
  documentId: 'document-id',
  documentType: 'book',
  projectId: 'project-id',
  dataset: 'production',
}

<OrderLink documentHandle={documentHandle} />
```

- [useDocument](https://reference.sanity.io/_sanity/sdk-react/exports/useDocument/) - Read and subscribe to a document in real time with local-first updates. When provided a path, it returns the corresponding field value.

```tsx
import {type DocumentHandle, useDocument} from '@sanity/sdk-react'

export function OrderLink({documentHandle}: {documentHandle: DocumentHandle}) {
  const {data: title} = useDocument({
    ...documentHandle,
    path: 'title',
  })
  const {data: id} = useDocument({
    ...documentHandle,
    path: '_id',
  })

  return (
    <a href=`/order/${id}`>Order {title} today!</a>
  )
} 
```

- [useEditDocument](https://reference.sanity.io/_sanity/sdk-react/exports/useEditDocument/) - Mutate a document's values with optimistic, local-first updates. When provided a path, it returns a function for updating the corresponding field value.

```tsx
import {type DocumentHandle, useDocument, useEditDocument} from '@sanity/sdk-react'

export function EditableTitle({documentHandle}: {documentHandle: DocumentHandle}) {
  const {data: title} = useDocument<string>({
    ...documentHandle, 
    path: 'title',
  })
  
  const editTitle = useEditDocument<string>({
    ...documentHandle,
    path: 'title'
  })
  
  function handleTitleChange(event: React.ChangeEvent<HTMLInputElement>) {
    editTitle(event.currentTarget.value)
  }
  
  return (
    <input type='text' value={title} onChange={handleTitleChange} />
  )
}
```

- [useDocumentProjection](https://reference.sanity.io/_sanity/sdk-react/exports/useDocumentProjection/) - Used for efficient live data fetching. When provided a GROQ projection, it returns the projected values. These values are live and will update in sync with the document values in Content Lake!

```tsx
import {type DocumentHandle, useDocumentProjection} from '@sanity/sdk-react'

export function BookDisplay({documentHandle}: {documentHandle: DocumentHandle}){
  const {data: {title, coverImage, authors}, isPending} = useDocumentProjection({
      ...documentHandle,
      projection: `{
        title,
        'coverImage': cover.asset->url,
        'authors': array::join(authors[]->{'name': firstName + ' ' + lastName + ' '}.name, ', ')
      }`,
    })
  
    return (
      <article style={{ opacity: isPending ? 0.5 : 1}}>
        <h2>{title}</h2>
        <img src={coverImage} alt={title} />
        <p>{authors}</p>
      </article>
    )
}
```

- [useDocuments](https://reference.sanity.io/_sanity/sdk-react/exports/useDocuments/) and [usePaginatedDocuments](https://reference.sanity.io/_sanity/sdk-react/exports/usePaginatedDocuments/) - Retrieves batches of documents (in the form of Document Handles), narrowed by optional filters, text matching and custom ordering.

```tsx
import {useDocuments} from '@sanity/sdk-react'

export function DocumentList() {
  const {count, data, hasMore,loadMore} = useDocuments({
    documentType: 'post',
    search: searchTerm,
    batchSize: 10,
    orderings: [{field: '_createdAt', direction: 'desc'}]
  })
  
  return (
    <div>
      Total documents: {count}
      <ol>
        {data.map((doc) => (
          <li key={doc.documentId}>
            <MyDocumentComponent doc={doc} />
          </li>
        ))}
      </ol>
      {hasMore && <button onClick={loadMore}>Load More</button>}
    </div>
  )
}
```

These are only a select handful of the most useful hooks provided by the SDK. To explore the entire set of hooks, components and types visit the [React SDK reference docs](https://reference.sanity.io/_sanity/sdk-react/). To see some example code in action, check out the [SDK Explorer](https://sdk-explorer.sanity.io).



# Suspense

The App SDK [hooks](/docs/app-sdk/sdk-react-hooks) are optimized for use with [React Suspense](https://react.dev/reference/react/Suspense). This allows you to write code in a synchronous fashion, as if the data you’re requesting from our hooks is available immediately.

For example, note in the example below how we use the value returned by the `useProjects` hook without checking if the request for it is in flight or resolved:

```tsx
// ProjectsList.tsx
import {useProjects} from '@sanity/sdk-react'

import ProjectListItem from './ProjectListItem'

export function ProjectsList() {
  const projects = useProjects()

  return (
    <ul>
      {projects.map((project) => (
        <li key={project.id}>
          <ProjectListItem projectId={project.id} />
        </li>
      ))}
    </ul>
  )
}
```

## Rendering fallback content

Because our hooks suspend during data fetching, you can render fallback content until data fetching is resolved using Suspense boundaries.

For example, given the above Projects List component (which uses the `useProjects` hook), we can wrap instances of this component with a Suspense boundary as follows:

```tsx
// ProjectsPanel.tsx
import {Suspense} from 'react'

import ProjectsList from './ProjectsList'
import LoadingSkeleton from './LoadingSkeleton'

export function ProjectsPanel() {
  return (
    <Suspense fallback={<LoadingSkeleton />}>
      <ProjectsList />
    </Suspense>
  )
}
```

Additionally, if the `ProjectListItem` component made use of the `useProject` hook, we could also wrap its component instances in Suspense boundaries within the `ProjectsList` component:

```tsx
// ProjectsList.tsx
import Suspense from 'react'
import {useProjects} from '@sanity/sdk-react'

import ProjectListItem from './ProjectListItem'

export function ProjectsList() {
  const projects = useProjects()

  return (
    <ul>
      {projects.map((project) => (
        <li key={project.id}>
          <Suspense fallback={'Loading project…'}>
            <ProjectListItem projectId={project.id} />
          </Suspense>
        </li>
      ))}
    </ul>
  )
}
```

## Notes

- The [SanityApp](https://reference.sanity.io/_sanity/sdk-react/exports/SanityApp/) component rendered by all Sanity custom apps comes with a root level Suspense boundary baked in. You can (and should!) pass a fallback component to its `fallback` prop to use as fallback content at the root level of your app. You may wish to wrap other components which use our hooks in a Suspense boundary. To learn more about how Suspense works, [refer to the React Suspense docs](https://react.dev/reference/react/Suspense).
- Our hooks also make use of [useTransition](https://react.dev/reference/react/useTransition) internally in order to keep UI that has already been rendered responsive and visible during data fetching.



# App SDK best practices

If you’ve worked with Sanity before, your experience querying the Content Lake is likely grounded in building Server-Side Rendered (SSR) or statically generated front-end applications designed for page load time performance.

Now, with the Sanity App SDK, you can build feature-rich content applications for authoring. However, this requires a different approach: swapping SSR thinking for Single-Page Application (SPA) best practices.

On top of this, if you’re used to writing React applications, some common patterns for building form-based user interfaces are best avoided when working with App SDK.

## What makes a great content application?

Content applications are defined as distinct, new experiences that give authors a focused environment to perform content operations. Instead of digging through a general-purpose CMS interface, authors work in a fit-for-purpose user interface to get the job done.

Content applications developed with the Sanity App SDK should be:

### Real-time

Any number of documents fetched and rendered into the user interface should continue to update as mutations happen to the source documents. Content applications should avoid concepts that handle stale data like "submit," “save” or "lock" buttons.

### Multiplayer

Two authors looking at the same document should be able to continually make and see edits without fear of overwriting one another’s work.

### Fast

Content rendered in the application should be locally cached, updated optimistically, and kept eventually consistent with the Content Lake.

### Accurate

There should never be stale data in an author's browser as they write content, nor after page load when fetched content is rendered. Updates should be written to and received directly from the Content Lake.

### This is all built-in to Sanity App SDK

These are the baseline expectations that Sanity’s engineers have had while developing Sanity Studio since 2017, and they’re now democratized for everyone to take advantage of via React Hooks in the Sanity App SDK.

## Get comfortable with more fetches

If you’ve built an SSR front end with Sanity before (such as in Next.js), you’ve likely created a Sanity Client and fetched all on-page content in a single query like this.

```tsx
// The SSR way: query and render "event" type documents

import { client } from "../sanity/client";

export async function Page() {
  const events = await client.fetch(
    `*[_type == "event"]`
  );

  return (
    <ul>
      {events.map((event) => (
        <li key={event._id}>{event.title}</li>
      ))}
    </ul>
  );
}
```

This can work great for SSR apps—where only the initial page load is important—since the grunt work of optimization is done behind the scenes, cached and delivered fast in a static format to your end users. But it falls short of a great SPA experience which may involve querying and editing an evolving number and type of documents, while keeping the user interface up to date in real-time.

### Prefer useDocuments over useQuery to fetch documents

Your natural inclination may be to use the App SDK hooks to recreate the "fetch everything in one query" pattern.

```tsx
// ❌ Do not simply swap client.fetch for useQuery
// It's too easy to over-fetch!

import { useQuery } from "@sanity/sdk-react";

export function Page() {
  const { data: events } = useQuery(
    `*[_type == "event"]`
  );

  if (!events) return null;

  return (
    <ul>
      {events.map((event) => (
        <li key={event._id}>{event.title}</li>
      ))}
    </ul>
  );
}
```

This list of documents will receive real-time updates—an upgrade from `client.fetch`—but may unknowingly fetch 1000’s of documents, each with 100’s of attributes.

> [!WARNING]
> Keeping raw GROQ queries performant
> The query in this particular example is problematic for performance. There’s no “array slicing” such as [0..10] to reduce the total number of documents returned, and no projection such as { title } to reduce the number of attributes returned. 
> 
> High performance is built-in when you use hooks like useDocuments and usePaginatedDocuments to return a filtered list of document handles, but your implementation will need to be more carefully considered when fetching by GROQ queries with useQuery.

`useQuery` exists to fetch content with a GROQ query should you need to—but makes it your responsibility to maintain your application’s performance. One particular example of where this may be useful is when a parent component needs all the details of child documents. 

In most cases, you should prefer `useDocuments` to fetch a list of [document handles](undefined), and render components that do their own data fetching for more content.

> [!TIP]
> Document handles provide stable `key` values
> Among the benefits of fetching for and using document handles is that they provide a stable documentId attribute which can be used as the key value when mapping over the response to render a list. 
> 
> Stable unique identifiers are preferable to using the index when rendering lists in a real-time React application.

Here's an example of fetching and rendering the same documents using the App SDK’s more purpose-built hooks.

```tsx
// ✅ Fetch and render event type documents the App SDK way

import { Suspense } from "react";
import {
  useDocuments,
  useDocumentProjection,
  type DocumentHandle,
} from "@sanity/sdk-react";

// Parent component that queries and renders event documents
export function EventsList() {
  const { data: events } = useDocuments({
    documentType: 'event',
  });

  if (!events) return null;

  return (
    <ul>
      {events.map((event) => (
        <Suspense key={event.documentId} fallback={<li>Loading...</li>}>
          <Event {...event} />
        </Suspense>
      ))}
    </ul>
  );
}

// Event component now renders the <li> itself
function Event(props: DocumentHandle) {
  const { data } = useDocumentProjection({ ...props, projection: `{ title }` });

  if (!data) return null;

  return <li>{data.title}</li>;
}
```

This may feel like an anti-pattern if you've been regularly building SSR front-ends—so many fetches! 

Rest assured that in a custom app, this is acceptable and in fact the intended usage pattern. The App SDK will handle concerns around caching and query batch sizing to avoid over-fetching.

**Summary:** Don’t fetch everything at once. First fetch for document handles, then fetch individual documents’ content within dedicated components.

## Apply Suspense boundaries liberally

[Suspense](https://react.dev/reference/react/Suspense) may be an unfamiliar part of the React library to many developers, but it won’t be once you’re familiar with the App SDK. The hooks in the App SDK use Suspense for data fetching—this means that when fetches are in flight, React will navigate up the component “tree” to the nearest Suspense boundary and trigger its `fallback` prop.

```tsx
// A very simple example of using Suspense

import { useDocuments } from "@sanity/sdk-react"
import { Stack, Text } from "@sanity/ui"
import { Suspense } from "react"

// 👇 The `useDocuments` hook in this component returns a promise
function FeedbackListDocuments() {
  const { data } = useDocuments({
    documentType: "feedback",
  })

  return (
    <Stack>
      {data?.map((feedback) => (
        <Text key={feedback.documentId}>{feedback.documentId}</Text>
      ))}
    </Stack>
  )
}

// 👇 So the component must be wrapped in Suspense
// which will render the `fallback` prop until data is loaded
function FeedbackList() {
  return (
    <Suspense fallback={<Text>Loading...</Text>}>
      <FeedbackListDocuments />
    </Suspense>
  )
}
```

The `SanityApp` component which wraps Sanity custom applications includes a Suspense boundary itself. So, if you have not put any Suspense boundaries throughout your app, you may constantly see the entire app re-render.

Keep in mind that for a small, simple enough app, you may rarely see a Suspense boundary invoked. For larger applications, or those that use more complex rendering libraries like TanStack Table or a Google Map, you may occasionally see runaway re-renders and wonder why. Suspense is likely why.

See the [App SDK Suspense documentation page](/docs/app-sdk/react-suspense-sdk) for more information.

### Expect re-renders from real-time updates

Because fetches for Sanity content with the App SDK are real-time and kept up to date, you may see re-renders happening when nothing seems to change. 

It may be that a document has been edited—just not in a way that would be rendered in your application. For example: A document rendered by your application may receive edits in Sanity Studio by another editor to fields that your application is not rendering, thus updating the latest edited date on the document and potentially causing a re-render.

Thus, you need to account for changes to content not performed by your application which will impact your application re-rendering.

### Wrap Suspense around the parent, not the child

A component which uses a data fetching hook such as `useDocuments` may trigger the outer Suspense boundary. In the example below, this means React will look **above** this component in the tree.

```tsx
import { Suspense } from "react"
import { type DocumentHandle, useDocuments } from "@sanity/sdk-react"
import { Stack, Button } from "@sanity/ui"

// 👇 This component needs to be wrapped in Suspense
// because it contains a fetching hook
export function FeedbackList() {
  const { data, hasMore, loadMore } = useDocuments({
    documentType: "feedback",
  })

  return (
    <Stack space={2} padding={5}>
      {data?.map((feedback) => (
        // 👇 Just like FeedbackItem needs to be wrapped
        // because it does its own data fetching too
        <Suspense key={feedback.documentId}>
          <FeedbackItem {...feedback} />
        </Suspense>
      ))}
    </Stack>
  )
}
```

The child elements are wrapped in Suspense (because they also fetch for data), but if the result of `useDocuments` is being updated, it is **this** parent component which needs to be wrapped in Suspense.

**Summary:** Wrap **every** data-fetching component in Suspense.

### Anticipate and prevent layout shift

"Layout shift" is when an element in an application changes dimensions or location, potentially moving other elements as a result.

A common example is when an image loads, changes size and pushes elements below it further down the web page. This can be a little problematic in web applications, and the effect is exacerbated in real-time applications.

In a real-time application an element which renders content could change dimension without user interaction. Another user may publish a change which adds a value that previously didn't exist, or a string may go from a few words to an entire paragraph. Your application should account for changes—unexpected or otherwise—to any content being fetched and rendered.

### How to prevent layout shift with Suspense  

A Suspense boundary’s `fallback` prop can take a component, not just text. Consider creating a “skeleton” version of the component which has the exact same dimensions as the final component that renders when content has been fetched.

In the example below, the `useNavigateToStudioDocument` hook requires a Suspense boundary. 

```tsx
import { Suspense } from "react"
import {
  type DocumentHandle,
  useNavigateToStudioDocument,
} from "@sanity/sdk-react"
import { Button } from "@sanity/ui"

const BUTTON_TEXT = "Open in Studio"

type OpenInStudioProps = {
  handle: DocumentHandle
}

// The exported component, pre-wrapped in Suspense
export function OpenInStudio({ handle }: OpenInStudioProps) {
  return (
    <Suspense fallback={<OpenInStudioFallback />}>
      <OpenInStudioButton handle={handle} />
    </Suspense>
  )
}

// The fallback component, rendered while the final component is loading
function OpenInStudioFallback() {
  return <Button text={BUTTON_TEXT} disabled />
}

// The final component, rendered after the fallback
function OpenInStudioButton({ handle }: OpenInStudioProps) {
  const { navigateToStudioDocument } = useNavigateToStudioDocument(handle)

  return <Button onClick={navigateToStudioDocument} text={BUTTON_TEXT} />
}
```

So within the one component, we have: 

- The exported component containing the Suspense boundary, which will render either the fallback or the child component depending on the loading state of the `useNavigateToStudioDocument` hook.
- A fallback button, disabled, with the same text to fill the same space.
- The final, active button to be clicked.

**Summary: **Components should not change size or location based on the availability of content or their loading state.

### Components should only have one Suspenseful hook

A good rule to keep in mind is that each component should only have one instance of a hook that fetches content (such as `useDocuments` or `useDocumentProjection`)

```tsx
// ❌ Don't put multiple fetchers in a single component!
// Updates to either list of documents will rerender both lists.

import { useDocuments } from "@sanity/sdk-react";
import { List } from "./List";

export function EventsAndVenues() {
  const { data: events } = useDocuments({
    documentType: 'event'
  });

  const { data: venues } = useDocuments({
    documentType: 'venue'
  });

  if (!events || !venues) return null;

  return (
    <>
      <List title="Events" items={events} />
      <List title="Venues" items={venues} />
    </>
  );
}
```

It’s possible to use multiple fetching hooks in a single component, but any one of these that receive an update will cause React to look up the component tree for a Suspense boundary, putting the entire component (and maybe others) into a loading state.

```tsx
// ✅ Separate fetchers into their own list components

import { Suspense } from 'react'
import { useDocuments } from '@sanity/react-sdk'
import { List } from './List'

// Component that renders two independent document lists
export function EventsAndVenues() {
  return (
    <>
      <Suspense fallback="Loading events...">
        <DocumentListSection documentType="event" />
      </Suspense>

      <Suspense fallback="Loading venues...">
        <DocumentListSection documentType="venue" />
      </Suspense>
    </>
  )
}

// Reusable component to fetch and render a list of documents by type
function DocumentListSection({ documentType }: { documentType: string }) {
  const { data: items } = useDocuments({ documentType })

  if (!items) return null

  return <List items={items} />
}
```

**Summary:** Separate individual fetchers into individual components.

## Read and write state from Content Lake 

Real-time applications require values to be up to date in **all** browsers. Therefore you should always read from and write to Content Lake instead of your local state at all times.

### Antipattern: Local state with controlled inputs with `useEditDocument`

For as long as you’ve been building React applications with hooks, you’ve likely implemented forms with controlled inputs where the `useState` hook stores, writes and renders the value of a field and content is submitted upon completion.

```tsx
// ❌ Do not copy this code example! 
// It only writes values to the browser, not the Content Lake

import { useState, FormEvent } from "react";
import { useEditDocument, type DocumentHandle } from "@sanity/sdk-react";

export function TitleForm(props: DocumentHandle) {
  const [value, setValue] = useState("");
  const editTitle = useEditDocument({ ...props, path: "title" });

  // 😱 This edit will only happen on submission
  function handleSubmit(event: FormEvent) {
    event.preventDefault();
    editTitle(value);
  }

  return (
    <form onSubmit={handleSubmit}>
      <input
        type="text"
        // 😱 This value only exists in your browser!
        value={value}
        onChange={(e) => setValue(e.target.value)}
        placeholder="Enter title"
      />
      <button type="submit">Save</button>
    </form>
  );
}
```

In a real-time application this leads to stale data in an author's browser, and creates scenarios where one author can overwrite another's work unknowingly.

### Correct controlled inputs the App SDK way

Hooks in the App SDK have been written to handle the local-first optimistic edits seen in `useState`, while sending and receiving mutations to the Content Lake behind the scenes. This is why you’ll see `useDocument` and `useEditDocument` combined in a pattern like the one below, to achieve the same effect as demonstrated in the incorrect example above.

```tsx
// ✅ Read from and write values directly to the document

import { useDocument, useEditDocument, type DocumentHandle } from '@sanity/react-sdk'

export function TitleInput(props: DocumentHandle) {
  const { data: title } = useDocument({ ...props, path: 'title' })
  const editTitle = useEditDocument({ ...props, path: 'title' })

  return (
    <input
      type="text"
      value={title ?? ''}
      onChange={e => editTitle(e.currentTarget.value)}
      placeholder="Enter title"
    />
  )
}
```

The behavior of this component works the same as the first one, but in a way that will continue to render from and write changes to the Content Lake.

Most magical of all, if your document is in a published state, the first edit made to any value in the document will invoke a new draft version of the document—something Sanity Studio has always done and is now made easy by App SDK.

Instead of requiring the author to “save” changes when they are done, edits are written directly to a “draft” version document. You can “publish” the draft version of the document with the `useApplyActions` hook—[see the documentation](https://reference.sanity.io/_sanity/sdk-react/exports/useApplyDocumentActions/) for more details.

**Summary:** Avoid creating forms that rely on a user’s local session, and always read from and write to the Content Lake.

## What more would you like to know?

Custom applications and the Sanity App SDK are relatively new parts of the Sanity Content Operating System. As such, these best practices are in their early days too. If you feel there is something architecturally difficult to understand, let us know in the [#app-sdk channel of our community](https://snty.link/community).



# Meet the App SDK

The Sanity Application Software Development Kit, or **App SDK** for short, is a robust set of tooling that will let you create fully custom apps that interface and interact with your Sanity content. It brings the powerful real-time capabilities and content management features you know from Sanity Studio to your own custom React applications. With a comprehensive set of React hooks and data stores, you can easily build applications that work seamlessly with your Sanity content across multiple projects and datasets.

In this introduction to the App SDK we will explore how to build applications that interact with your Sanity content in real-time. We'll cover the core concepts and patterns that make the App SDK powerful, demonstrate how to efficiently retrieve and manipulate documents, and show you how to create responsive user interfaces that stay in sync with your content.

By the end of this guide, you'll understand:

- How Document Handles enable efficient document operations
- When to use different hooks for retrieving and updating content
- Best practices for building performant real-time applications
-  How to work with content across multiple projects and datasets

## What is the App SDK?

The App SDK is a toolkit for building custom React applications that interact with your Sanity content. It provides a set of React hooks and data stores that enable real-time content operations, seamlessly handling content from multiple projects and datasets, and with complete freedom to create your own interfaces and experiences. 

### Purpose and Key Features

- Build fully custom applications that work with Sanity content
- Enable real-time content operations and live updates
- Work across multiple projects and datasets
- Create tailored user experiences beyond what Studio offers

### SDK apps and Sanity Studio

![An SDK app and a studio showing different ways to interact with the same content](https://cdn.sanity.io/images/3do82whm/next/34ec1da769de3803cffabb9eb01b0fe5c6dd70a0-600x306.png)



It's worthwhile to pause briefly to take a comparative look at Sanity Studio, and how it relates to the App SDK. Sanity Studio is a content management powerhouse, and for many Sanity users, Studio *is* Sanity. Or, in other words, their studio is the main interface through which they interact with the Sanity platform. The ambition of the App SDK is to enable you to build apps that work beyond the scope of a single studio, project, and dataset, unlocking opportunities for new content workflows and operations — all while allowing you complete freedom over your application’s UI and UX.

### Similarities

- Real-time content operations
- Live updates and collaboration features
- Access to Sanity's content platform
- Authentication and permissions handling

### Key Differences

- **Multiple Projects and Datasets: **While Studios can work with a single project and dataset at a time, SDK apps can be configured to work with as many of your organization’s projects and datasets as you like
- **Complete UI Freedom**: Unlike Studio's structured interface, you control every aspect of the UI
- **Custom Workflows**: Build exactly the workflow your users need
- **Focused Feature Set**: No built-in validation or form building - bring your favourite UI components with you, and shape the functionality just as you want it

## Technical Implementation

### Technology Stack

- [TypeScript](https://www.typescriptlang.org/) for type safety and developer experience
- [React](https://react.dev/) for application framework and hooks
- Built on modern React patterns including [Suspense](https://react.dev/reference/react/Suspense)

### Requirements

- React v19 or higher
- Node.js environment v20 or higher

## What's Included

- React hook based interface, taking advantage of modern React patterns like [Suspense](https://react.dev/reference/react/Suspense) and [Transitions](https://react.dev/reference/react/useTransition)
- Document retrieval and content rendering, all live by default
- Optimistic, local-first document editing, ready for collaborative interfaces
- Batchable document actions
- Permissions checking with detailed outputs
- Support for [Sanity Typegen](/docs/apis-and-sdks/sanity-typegen)

### Not Included

- UI components or design system- However, the App SDK pairs nicely with [Sanity UI](https://sanity.io/ui) for building applications that are visually consistent with other Sanity apps


- Router
- Form validation
- Schema validation

These aspects are left to your implementation, giving you complete control over the user experience while the SDK handles the complex data operations underneath.



# Setup and development

> [!TIP]
> Looking for the Quickstart Guide?
> For a no-frills, straight to the point, quickstart guide, go here.

In this article we will go through the process of setting up a complete App SDK development environment. We'll create a new SDK app, configure it to work with our example movie dataset, and get familiar with the basic project structure. By the end of this article, you'll have a working development environment ready for building custom Sanity applications. We'll cover:

- Creating a new SDK app using the quickstart template
- Configuring the app to connect to your Sanity project
- Understanding the basic application structure
- Running the development server
- Making your first changes to the app

Whether you want to follow along and set up a new Sanity project with some example data, or want to jump straight to the section on bootstrapping a new SDK app, your next step will be to open your terminal and interact with the [sanity Command Line Interface (CLI)](/docs/archive/getting-started-with-sanity-cli).



## Follow along (optional)

The power of the App SDK is more easily demonstrated when working with an actual real-life Sanity project that has some content for you to interact with.

Throughout the following introductory articles, we'll use a sci-fi movies dataset as our working example. You may already be familiar with this example project from initializing a Sanity Studio, but no prior knowledge is assumed.

While we encourage you to follow along with the examples, you can still learn the core App SDK concepts without setting up the example project. Feel free to read through the guide and adapt the concepts to your own use case.

### Setting up the example project

If you'd like to follow along with the examples, you'll need to set up a Sanity project with our sci-fi movies dataset:

- Create a new Sanity project

```sh
npm create sanity@latest
```

- When prompted:- Choose "Create new project"
- Give your project a name (e.g., "cool-tapes")
- Select or create an organization for this project
- Use the default dataset configuration
- Choose your output path (e.g., "cool-tapes-studio") 
- Select project template- Movie Project (schema + sample data)


- Choose whether to use TypeScript or plain JavaScript
- Add a sampling of sci-fi movies to your dataset on the hosted backend- Yes




- Make note of your:- Project ID
- Dataset name



You'll need these values to configure the SDK in the upcoming steps.

- Start the studio!

```sh
cd cool-tapes-studio
npm run dev
```

The Studio will now be running with a schema designed for movie content. The dataset includes some sample sci-fi movies to get you started. As we progress through the guide, we'll build features that interact with this content in various ways.

![Shows a Sanity Studio filled with entries about classic Sci-fi movies](https://cdn.sanity.io/images/3do82whm/next/fb10d93f5487f0d6e09c0288302c387d4390459a-600x446.png)

If you prefer not to set up the example project, that's perfectly fine! The concepts and patterns we'll cover apply to any content structure. You can still follow along with the explanations and adapt the examples to your own schema and content model.

## Bootstrapping your first SDK app

> [!TIP]
> Keeping stuff neat
> Before proceeding, make sure that you cd out of the folder where you installed the studio earlier if you are following along. You want to have two separate folders for the studio and custom app.
> 
> /cool-tapes-studio
> /cool-tapes-app

Now that we have our content ready, let's create a new SDK app. The quickest way to get started is to use the App SDK quickstart template:

```sh
npx sanity@latest init --template app-quickstart
```

When prompted:

- Choose or create your organization
- Choose the output destination for your local development
- Choose whether to use TypeScript or JavaScript

You should see a confirmation message like the one below. 

```sh
Success! Now, use this command to continue:

npm run dev - to run your Sanity application

Other helpful commands
npx sanity docs - to open the documentation in a browser
npx sanity manage - to open the project settings in a browser
npx sanity help - to explore the CLI manual
```

Congratulations! You just boostrapped your first custom application with the Sanity App SDK!

However, before running your app locally for the first time, you’ll need to configure the app to use the data from the example project you created in the previous step. That’s exactly what we’ll do in the next article.



# Connecting your app

## Configuring your app

The term "configuration" in the context of developing a custom app with the App SDK can refer to two different things:

- The main configuration file for the project, found in `sanity.cli.ts` at the root of your project. It contains your organization ID, and your project's entry point, as well as an unique ID for the application itself once deployed.

```typescript
import {defineCliConfig} from 'sanity/cli'

export default defineCliConfig({
  app: {
    organizationId: 'your-org-id',
    entry: './src/App.tsx',
    id: 'your-app-id', // added when deployed
  },
})
```

- More interesting to us at the moment: One or more `SanityConfig` objects, which each contain a project ID and dataset name belonging to the Sanity project(s) you wish to interact with in your app. This configuration is passed to the `SanityApp` provider which typically wraps your application and provides all the necessary context needed to interact with your content.

```tsx
import {SanityApp, type SanityConfig} from '@sanity/sdk-react'

export function App() {
  const config: SanityConfig[] = [
    {
      projectId: 'your-project-id',
      dataset: 'your-dataset-name',
    }
  ]
  return (
    <div className="app-container">
      <SanityApp config={config} fallback={<div>Loading...</div>}>
        {/* add your own components here! */}
      </SanityApp>
    </div>
  )
}

export default App
```

This last meaning is what we'll be concentrating on for this article.

### Configuring your app

To begin, locate the entry point for your app in `src/App.tsx`. You should see something very much like the code example below, containing a configuration with some placeholder values on lines 7-12, which you will want to replace with your actual details. 

```tsx
import {SanityApp, type SanityConfig} from '@sanity/sdk-react'
import {ExampleComponent} from './ExampleComponent'
import './App.css'

export function App() {
  // apps can access one or many different projects or other sources of data
  const config: SanityConfig[] = [
    {
      projectId: 'your-project-id',
      dataset: 'your-dataset-name',
    }
  ]

  return (
    <div className="app-container">
      <SanityApp config={config} fallback={<div>Loading...</div>}>
        {/* add your own components here! */}
        <ExampleComponent />
      </SanityApp>
    </div>
  )
}

export default App

```

Note that `config` is an *array* of `SanityConfig` values, which means you can hook your app up to several Sanity projects if you so wish. The [SanityConfig](https://reference.sanity.io/_sanity/sdk/index/SanityConfig/) type is simply a valid set of a `projectId` and a `dataset`.

The `config` array is then passed to the [SanityApp](https://reference.sanity.io/_sanity/sdk-react/exports/SanityApp/) context provider component on lines 16-19. The `SanityApp` component provides all child components with the necessary context to work with the SDK hooks and methods. (You may also notice that this component accepts a `fallback` prop, which is your clue that the SDK supports the [React Suspense](https://react.dev/reference/react/Suspense) pattern, which should make our lives a bit easier while fetching and interacting with content.)

Use the project ID and dataset name from the example project you created in the previous article to modify the `projectId` and `dataset` values in the `config` variable, save the file, and then return to your terminal and run:

```sh
npm run dev
```

You should get a confirmation like the one displayed below.

```sh
Dev server started on port 3333
View your app in the Sanity dashboard here:
https://www.sanity.io/@[ORGANIZATION-ID]?dev=http%3A%2F%2Flocalhost%3A3333
```

CMD or CTRL-click the URL to visit your app in the Sanity Dashboard. You should be welcomed by your app running in local development mode, and displaying some boilerplate content.

![a welcome to your sanity app martin jacobsen page](https://cdn.sanity.io/images/3do82whm/next/60c9aba86e8b9bfbee14c028ef95210017efb67b-1229x935.png)

Excellent! Your shiny new app is running locally. Trouble is, it doesn’t really *do* much as of yet. Let’s fix that by fetching some documents!

- In the `src/ExampleComponent.tsx` file, replace the content in its entirety with the following code:

```tsx
import './ExampleComponent.css'
import {useDocuments} from '@sanity/sdk-react'

export function ExampleComponent() {
  const {data, hasMore, isPending, loadMore, count} = useDocuments({
    documentType: 'movie',
    batchSize: 10,
  })

  return (
    <div>
      Total documents: {count}
      <ol>
        {data.map((doc) => (
          <li key={doc._id}>
            <code>{JSON.stringify(doc, null, 2)}</code>
          </li>
        ))}
      </ol>
      {hasMore && (
        <button onClick={loadMore} disabled={isPending}>
          {isPending ? 'Loading...' : 'Load More'}
        </button>
      )}
    </div>
  )
} 
```

In the example above we've imported and used the `UseDocuments` hook to query for documents of a certain type (`movie)`, and then we print out the result. Note that we can treat the resulting data synchronously, i.e., we don't have to wait for a promise to resolve before we print out the result.

You should see something like the following:

![Shows a list of documents within an SDK starter app](https://cdn.sanity.io/images/3do82whm/next/3a2fc5dc204267a7379ca7902eadef97aed22b0c-600x424.png)

Note how `useDocuments()` returns an array of minimalist objects containing a `documentId`, `documentType`, `projectId` and `dataset` for each document. This construct is known as a [DocumentHandle](/docs/app-sdk/document-handles), and is a very useful abstraction to work with when fetching a list of documents. We'll look more closely at how they work in the next article.



# Fetching and handling content

## Document handles

- What are Document Handles?
- Why use Document Handles?
- Document Handle structure and components
- Best practices for working with Document Handles

A [DocumentHandle](/docs/app-sdk/document-handles) is a core concept in the SDK that enables efficient document operations. A `DocumentHandle` is essentially a lightweight reference to a document, containing just enough information to uniquely identify and access it - at minimum, this will include the document’s ID and type, but details about the project and dataset the document belongs to may also be included. This minimalist approach allows the SDK to work with large sets of documents without having to load their full content immediately.

For example, when you use `useDocuments()` to fetch a list of movies, instead of loading complete movie documents with all their fields, cast information, and poster images, you get back an array of Document Handles that look like this:

```javascript
{
  documentId: 'movie-123',
  documentType: 'movie',
  projectId: 'project-123',
  dataset: 'production'
}
```

This lightweight representation serves several important purposes:

- **Performance**: Loading just the handles instead of full documents reduces initial data transfer and improves application responsiveness
- **Flexibility**: Handles can be passed to other hooks that load only the specific document data needed for a particular view or operation
- **Real-time Updates**: The SDK can efficiently track changes to documents by monitoring their handles

### Working with Document Handles

Once you have a Document Handle, you can use it with other SDK hooks to access or modify the actual document content. Here are some common patterns:

```tsx
const movieHandle: DocumentHandle = {
  documentId: 'movie_348',
  documentType: 'movie'
}

// Get specific fields using useDocumentProjection
const {data} = useDocumentProjection({
  ...movieHandle,
  projection: '{title, releaseYear}'
})

// Get the full document
const {data: movieDoc} = useDocument({...movieHandle})

// Edit a specific field
const editTitle = useEditDocument({
  ...movieHandle, 
  path: 'title',
})
```

This separation between handles and content allows you to build efficient interfaces that load data progressively as needed, while maintaining live synchronization with your Sanity dataset.

## Data Retrieval Hooks

### [useDocuments](https://reference.sanity.io/_sanity/sdk-react/exports/useDocuments/) - Getting collections of documents

The `useDocuments` hook is your primary tool for retrieving collections of documents from your Sanity dataset. It returns Document Handles for documents matching your specified document type (and optional filters and parameters), making it ideal for building document lists and overviews. 

```tsx
const {data, hasMore, isPending, loadMore} = useDocuments({
 documentType: 'movie',
 batchSize: 10,
 orderings: [{ field: '_createdAt', direction: 'desc' }]
})
```

### [usePaginatedDocuments](https://reference.sanity.io/_sanity/sdk-react/exports/usePaginatedDocuments/) - Paginated document lists

The `usePaginatedDocuments` hook provides a more traditional pagination interface compared to the infinite scroll pattern of `useDocuments`. This makes it ideal for building interfaces with discrete pages of content and explicit navigation controls:

```tsx
const { 
  data, 
  isPending,
  currentPage, 
  totalPages,
  nextPage, 
  previousPage,
  hasNextPage,
  hasPreviousPage
} = usePaginatedDocuments({ 
  documentType: 'movie',
  pageSize: 10,
  orderings: [{ field: '_createdAt', direction: 'desc' }]
})
```

### [useDocument](https://reference.sanity.io/_sanity/sdk-react/exports/useDocument/) - Reading individual documents

The `useDocument` hook provides real-time access to individual document content. It's designed for reading and subscribing to a document's state, incorporating both local and remote changes:

```tsx
// Get the full document
const {data: movie} = useDocument({...movieHandle})

// Get a specific field
const {data: title} = useDocument({
  ...movieHandle,
  path: 'title',
})
```

The hook automatically handles displaying local-first, optimistic updates made via the `useEditDocument` hook, making it ideal for building collaborative editing interfaces that need to stay synchronized with remote changes. However, for static displays where local-first, optimistic updates aren't needed, consider using `useDocumentProjection` (which still return content that's live by default).

### [useDocumentProjection](https://reference.sanity.io/_sanity/sdk-react/exports/useDocumentProjection/) - Accessing specific document fields

The `useDocumentProjection` hook allows you to efficiently retrieve specific fields from a document using GROQ projections:

```
const {data: { title, authorName }} = useDocumentProjection({
  ...documentHandle,
  projection: `{
    title,
    'authorName': author->name
  }`
})
```

## Document Manipulation Hooks

### [useEditDocument](https://reference.sanity.io/_sanity/sdk-react/exports/useEditDocument/) - Modifying documents

This hook is particularly useful for building forms and collaborative editing interfaces. It provides a simple way to update document fields in real-time:

```tsx
const editTitle = useEditDocument({
  ...movieHandle, 
  path: 'title',
})

function handleTitleChange(e: React.ChangeEvent<HTMLInputElement>) {
  editTitle(e.currentTarget.value)
}
return (
 <input 
   type="text"
   value={title || ''}
   onChange={handleTitleChange}
 />
)
```

### [useApplyDocumentActions](https://reference.sanity.io/_sanity/sdk-react/exports/useApplyDocumentActions/) - Document operations

The `useApplyDocumentActions` hook provides a way to perform document operations like publishing, unpublishing, creating, and deleting documents:

```tsx
import {
  useApplyDocumentActions,
  publishDocument,
  unpublishDocument,
} from '@sanity/sdk-react'

const apply = useApplyDocumentActions()

function MovieActions({ movieHandle }) {
  return (
    <div>
      <button onClick={() => apply(publishDocument(movieHandle))}>
        Publish
      </button>
      <button onClick={() => apply(unpublishDocument(movieHandle))}>
        Unpublish
      </button>
    </div>
  )
}
```

### [useDocumentEvent](https://reference.sanity.io/_sanity/sdk-react/exports/useDocumentEvent/) - Handling document events

The `useDocumentEvent` hook allows you to subscribe to document events like creation, deletion, and updates. This is useful for building features that need to react to changes in your content:

```tsx
import {useDocumentEvent, type DocumentEvent} from '@sanity/sdk-react'

const eventCallback = (event) => {
  if (event.type === DocumentEvent.DocumentDeletedEvent) {
    console.log(`Document ${event.documentId} was deleted`)
  } else if (event.type === DocumentEvent.DocumentEditedEvent) {
    console.log(`Document ${event.documentId} was edited`)
  }
})

useDocumentEvent({
  ...documentHandle,
  onEvent: eventCallback,
})
```

This hook is particularly valuable when building interfaces that need to maintain consistency with document state changes, such as notification systems or live collaboration features.

Here's an example of using `useDocumentEvent` to build a simple notification system that alerts users when documents are modified:

```tsx
function DocumentChangeNotifier({ documentHandle }) {
  const [notifications, setNotifications] = useState<string[]>([])

  const eventCallback = (event) => {
    switch (event.type) {
      case DocumentEvent.DocumentEditedEvent:
        setNotifications(prev => [
          `Document ${event.documentId} was just edited`,
          ...prev
        ])
        break
      case DocumentEvent.DocumentPublishedEvent:
        setNotifications(prev => [
          `Document ${event.documentId} was published`,
          ...prev
        ])
        break
    }
  }

  useDocumentEvent({
    ...documentHandle,
    onEvent: eventCallback,
  })

  return (
    <div className="notifications">
      {notifications.map((msg, i) => (
        <div key={i} className="notification">{msg}</div>
      ))}
    </div>
  )
}
```





# Sanity UI

## Integrating UI Components

The Sanity App SDK gives you complete freedom to craft your application’s design. Whether your preferred styling solution is [Sanity UI](https://www.sanity.io/ui), [Tailwind](/docs/app-sdk/tailwind-sdk), vanilla CSS, or something else entirely, the SDK‘s headless approach allows you to style your app with the tools your team knows best, while benefiting from powerful React hooks that unlock Sanity platform capabilities.

## Example: Sanity UI

First, begin by installing Sanity UI:

```sh
npm install @sanity/ui styled-components
```

Then, in your custom application’s `src/App.tsx`, instantiate Sanity UI’s ThemeProvider as usual:

```tsx
// App.tsx
import {SanityApp, type SanityConfig} from '@sanity/sdk-react'

// Sanity UI
import {ThemeProvider} from '@sanity/ui'
import {buildTheme} from '@sanity/ui/theme'

import {ExampleComponent} from './ExampleComponent'

// Build the Sanity UI theme
const theme = buildTheme()

export function App() {
  // apps can access many different projects or other sources of data
  const config: SanityConfig[] = [
    {
      projectId: 'project-id',
      dataset: 'dataset-name',
    },
  ]

  return (
    <ThemeProvider theme={theme}>
      <SanityApp config={config} fallback={<div>Loading...</div>}>
        {/* add your own components here! */}
        <ExampleComponent />
      </SanityApp>
    </ThemeProvider>
  )
}

export default App
```

You can now use Sanity UI as expected within your custom application.

This approach can be used for other component libraries and styling solutions, as well — just be sure to set them up with `src/App.tsx`.

## Using the Sanity UI App Template

If you know you’d like to use Sanity UI as your component library of choice when creating your custom app, you can choose to initialize your app with our Sanity UI template, which implements the work shown above for you.

Just initialize your app using the `app-sanity-ui` template instead of the usual `app-quickstart` template:

```sh
npx sanity@latest init --template app-sanity-ui
```



# Tailwind CSS

[Tailwind](https://tailwindcss.com/) is a popular styling library beloved by many developers — including some who build custom apps with the Sanity App SDK. This guide demonstrates how to get up and running with Tailwind and the App SDK.

> [!NOTE]
> This guide was written and tested with Tailwind 4.1. We endeavor to keep up, but if you spot any outdated information, please let us know!

## Step 1: Prerequisites

In order to get started, we’ll presume you’ve already got a custom app initialized with the App SDK. If you don’t, [follow along with our quickstart guide](/docs/app-sdk/sdk-quickstart)!

Once your app is initialized, you’ll need to install two Tailwind dependencies — the main [Tailwind library](https://www.npmjs.com/package/tailwindcss), and [Tailwind’s Vite plugin](https://www.npmjs.com/package/@tailwindcss/vite):

```sh
npm i tailwindcss @tailwindcss/vite
```

## Step 2: Configure Tailwind’s Vite plugin

As you may know, custom apps built with the App SDK are React applications that run on [Vite](https://vite.dev/).

Given that fact, you might be tempted to follow [Tailwind’s installation guide for Vite](https://tailwindcss.com/docs/installation/using-vite), but we require a slightly different setup process to accomodate for the [auto updating](/docs/studio/auto-updating-studios) nature of apps built with the SDK.

Thus, rather than setting up your own Vite config that might interfere with the core functionality of custom apps, you’ll need to extend the built in Vite config in order to register Tailwind’s Vite plugin. You can do this within the `sanity.cli.ts` file at the root of your custom app.

Use the example below to update the contents of your `sanity.cli.ts` file and register the Tailwind plugin:

```
import {defineCliConfig} from 'sanity/cli'

export default defineCliConfig({
  app: {
    organizationId: 'your organization ID goes here',
    entry: './src/App.tsx',
  },
  vite: async (viteConfig) => {
    const {default: tailwindcss} = await import('@tailwindcss/vite')
    return {
      ...viteConfig,
      plugins: [
        ...viteConfig.plugins,
        tailwindcss()
      ],
    }
  }
})
```

The Tailwind plugin is now configured for your app! All that’s left to do is to import Tailwind and put it to work.

## Step 3: Import Tailwind’s CSS

Within your app’s `src/App.css` (or any other CSS file you choose to make use of and import within the scope of your application), add an `@import` statement to import Tailwind’s CSS:

```css
@import 'tailwindcss';
/* Other styles here if you like… */
```

Be sure that the CSS file you add this `@import` statement to is itself imported within your application — likely within your `src/App.tsx` file:

```
import './App.css'
// Rest of your App.tsx here…
```

## Step 4: Start the development server

Start your custom app’s development server as usual:

```sh
npm run dev
```

## Step 5: Start using Tailwind in your components

You can now use Tailwind’s CSS classes in any of your app’s React components. For further guidance on using Tailwind, [refer to Tailwind’s docs](undefined). Have fun!



# TypeGen (Experimental)

> [!NOTE]
> This page is for using TypeGen with the App SDK. See the TypeGen article to generate types for your Studio and front end applications.

[Sanity TypeGen](/docs/apis-and-sdks/sanity-typegen) is a tool that generates TypeScript types directly from your Sanity schemas and GROQ queries. When used with the Sanity App SDK, it provides strong type safety and autocompletion suggestions for your documents, query results, and projections.

In this guide, we’ll walk through setting up and using TypeGen within your SDK app.

> [!WARNING]
> Experimental feature
> TypeGen support in the App SDK is currently in its early stages. We’re actively working on improving this integration and the developer experience around it. For now, some parts of this process may be suboptimal, but we invite the adventurous among you to follow along!

## Setup

Using Typegen involves two main steps: extracting your schema(s) and then generating the types. Both commands are available via the CLI.

### Extract schemas

First, you need to extract your Sanity schema(s) into a JSON format that Typegen can understand. **Currently, this step relies on the full** **sanity** **package**, typically used within your Sanity Studio project, as Typegen needs access to the complete schema definition to generate accurate types.

Schema extraction is performed within your Studio setup to generate the `schema.json` file. Once created, this file can be used independently by other tools or parts of your workflow.

> [!NOTE]
> We recognize that requiring the Studio environment solely for this generation step isn't ideal, and we're actively working on improving this workflow in future App SDK updates to make the process more self-contained.

Use the `sanity schema extract` command within your Studio project or a project that has the `sanity` package installed:

```sh
npx sanity schema extract --workspace <workspace-name> --output-path <path/to/schema.json>
```

This `schema.json` file can be copied to (or the `--output-path` can be set directly to) your Sanity app's repository. Your application itself does *not* need the full `sanity` package as a dependency to use the generated types; it only needs the `schema.json` file for the `typegen generate` step.

If your Studio project defines multiple workspaces or you need types for different schemas (e.g., for different datasets), run the `extract` command for each one, outputting to separate JSON files. For example, you could configure you Studio’s `package.json` as follows:

```json
{
  "scripts": {
    "schema:extract:test": "sanity schema extract --workspace test --output-path ../my-frontend-app/schema-test.json",
    "schema:extract:prod": "sanity schema extract --workspace production --output-path ../my-frontend-app/schema-prod.json",
    "schema:extract": "npm run schema:extract:test && npm run schema:extract:prod"
  }
}
```

We plan to improve this schema extraction process as the SDK matures to potentially reduce the dependencies and improve overall developer experience.

### Install (experimental) packages

To use the Typegen features described in this guide, your SDK app needs specific experimental versions of `@sanity/cli` and `groq` installed. Install these packages from within your SDK app directory:

```sh
npm install groq@typegen-experimental-2025-04-23
npm install @sanity/cli@typegen-experimental-2025-04-23 --save-dev
```

> [!WARNING]
> Package names and installation
> These are experimental pre-release versions. The package names and installation process may change as these features stabilize.

### Configure TypeGen (optional)

For the most common use case – a single Sanity schema for your project – **no configuration file is needed**. However, you'll need to create a TypeGen configuration file for more complex use cases, such as:

- Using multiple schemas (e.g., from different workspaces or for different datasets).
- Needing to explicitly map a single schema to a specific `schemaId` for accurate schema scoping (instead of using the default  `'default'`).
- Using a different name or location for your schema file(s).
- Specifying a custom output path for the generated types file.

If you need this level of configuration, create a TypeGen configuration file (`sanity-typegen.json` ) at the root of your SDK app and use the `unstable_schemas` array:

```json
// sanity-typegen.json
{
  "unstable_schemas": [
    {
      // Path to the schema
      "schemaPath": "./schemas/products-schema.json",
      // The schema ID, formatted as `projectId.datasetName`
      "schemaId": "your-project-id.products"
    },
    {
      "schemaPath": "./schemas/authors-schema.json",
      "schemaId": "your-project-id.authors"
    }
    // Add more schema objects if needed
  ],
  "overloadClientMethods": false // client methods are not needed for the App SDK
  // Optional: Specify output path for generated types
  // "outputPath": "./src/generated/sanity-types.ts"
}
```

Objects in the `unstable_schemas` array each consist of the following properties:

- **schemaPath:** The path (relative to the project root) to the corresponding extracted schema JSON file.
- **schemaId:** A string combining your `projectId` and `dataset` (e.g., `"your-project-id.your-dataset-name"`). This is used to map the schema to the correct project and dataset context for type generation, as the extracted `schema.json` doesn't contain this information itself.

The optional **outputPath** property specifies where to write the generated `sanity.types.ts` file. It defaults to the project root.

By default, TypeGen works seamlessly for the common single-schema setup without extra configuration. Use `sanity-typegen.json` only when your needs require more explicit control.

### Generate types

With the necessary packages installed and your schema(s) extracted (and optionally configured in `sanity-typegen.json`), you can run the `sanity typegen generate` command from within your SDK app directory:

```sh
# use `@sanity/cli` package directly for now
./node_modules/@sanity/cli/bin/sanity typegen generate
```

This command reads your configuration (either `sanity-typegen.json` or the default `schema.json`), processes the specified schemas, and generates a `sanity.types.ts` file, which contains your types. It's recommended to add this command to your SDK app’s `package.json` scripts. For example:

```json
{
  "scripts": {
    "typegen": "./node_modules/@sanity/cli/bin/sanity typegen generate"
  }
}
```

Congratulations! You’ve now generated types for your schema documents, projections, and query results. With your `sanity.types.ts` file in place, the App SDK hooks will automatically pick up these types.

Next, we’ll cover how to make use of these generated types in your SDK app.

## Use the generated types

TypeGen generates interfaces for each document type defined in your schemas. For projects using multiple schemas/datasets defined in `sanity-typegen.json`, it utilizes a helper type `SchemaOrigin` (imported from `groq`) to brand the types.

This allows TypeScript to narrow down the possible document types based on the dataset context provided via a `DocumentHandle`. See the code below for an example of this:

```
import {useDocument, createDocumentHandle} from '@sanity/sdk-react'

// Assuming 'book' is only in 'test' dataset, 'dog' only in 'production'
const testHandle = createDocumentHandle({
  projectId: 'your-project-id',
  dataset: 'test',
  documentId: 'some-id',
  documentType: 'book', // Type narrowed to 'book'
})

const prodHandle = createDocumentHandle({
  projectId: 'your-project-id',
  dataset: 'production',
  documentId: 'another-id',
  documentType: 'dog', // Type narrowed to 'dog'
})

function MyComponent() {
  const {data: bookData} = useDocument(testHandle)
  // bookData is correctly typed as Book

  const {data: dogData} = useDocument(prodHandle)
  // dogData is correctly typed as Dog

  // ...
}
```

### Handles and literal types

For TypeGen to correctly infer types in hooks like `useDocument`, it needs to know the *specific* literal type of the `documentType` (e.g., `'book'` instead of just `string`).

The App SDK provides helper functions (like `createDocumentHandle `and `createDatasetHandle`) that help capture these literal types:

```
import {createDocumentHandle} from '@sanity/sdk'

// Using the helper ensures handle.documentType is typed as 'book'
const handle = createDocumentHandle({
  documentId: '123',
  documentType: 'book',
  dataset: 'production',
  projectId: 'abc',
})
```

Alternatively, if you prefer defining handles as plain objects, use `as const` to ensure the `documentType` has the literal type of `'book'`:

```
const handle = {
  documentId: '123',
  documentType: 'book',
  dataset: 'production',
  projectId: 'abc',
} as const // 'as const' ensures documentType is 'book', not string
```

> [!TIP]
> We recommend that you use createDocumentHandle (or other create*Handle helpers) when using Typegen for cleaner code.

### Document projections

To get types for GROQ projections used with `useDocumentProjection` and TypeGen, you **must** define them using the `defineProjection` helper from `groq`.

```
import {defineProjection} from 'groq'
import {useDocumentProjection, type DocumentHandle} from '@sanity/sdk-react'

// Typegen derives the type name (AuthorSummaryProjectionResult) from the variable name
export const authorSummary = defineProjection(`{
  "name": name,
  "favoriteBookTitles": favoriteBooks[]->title,
}`)

function AuthorDetails({doc}: {doc: DocumentHandle<'author'>}) {
  // The type of `data` is inferred from `authorProjection`
  const {data} = useDocumentProjection({
    ...doc, // Spread the handle containing documentId, type, etc.
    projection: authorProjection,
  })

  // data is typed as AuthorSummaryProjectionResult
  // Autocompletion works for data.name and data.favoriteBookTitles
  return <div>{data.name}</div>
}
```

There are few important things to note here:

- In the example above, the generated type (e.g., `AuthorSummaryProjectionResult`) includes a `ProjectionBase` [brand](https://www.learningtypescript.com/articles/branded-types), allowing unions of projection results if a projection applies to multiple document types.
- TypeGen intelligently removes types from the projection result if all fields in the projection evaluate to `null` for a given document type.
- When using Typegen, you **cannot** pass raw projection strings to `useDocumentProjection` and get type inference; you must use `defineProjection`.

### GROQ queries

Similarly to `useProjection`, when using the `useQuery` hook, you **must** define your GROQ queries using `defineQuery` from the `groq` package to get type inference:

```
import {defineQuery} from 'groq'
import {useQuery} from '@sanity/sdk-react'

// Typegen derives the type name (AllBooksQuery) from the variable name
export const allBooksQuery = defineQuery('*[_type == "book"]{ _id, title }')

function BookList() {
  // Type of `data` is inferred from `allBooksQuery`
  const {data} = useQuery({query: allBooksQuery})

  // data is typed as Array<{_id: string, title: string}> (or similar)
  return (
    <ul>
      {data.map((book) => (
        <li key={book._id}>{book.title}</li>
      ))}
    </ul>
  )
}
```

Note that `useQuery` accepts options as a single object, allowing you to spread handles easily. For example:

```
const handle = createDatasetHandle({dataset: 'test', projectId: 'abc'})
const {data} = useQuery({...handle, query: allBooksQuery})
```

### Document lists

The App SDK’s document list hooks, `useDocuments` and `usePaginatedDocuments`, benefit from TypeGen through dataset scoping (as shown earlier). You can use the `documentType` option to specify the document type(s) you are querying:

```
import {usePaginatedDocuments, createDatasetHandle} from '@sanity/sdk-react'
import {DocumentPreview} from './your-document-preview'

const testDataset = createDatasetHandle({dataset: 'test', projectId: 'abc'})

function MixedList() {
  // Specify the types being queried
  const {data} = usePaginatedDocuments({
    ...testDataset,
    documentType: ['author', 'book'], // Pass string or array of strings
  })

  // `data` is an array of DocumentHandles, correctly scoped.
  // If used with `useDocument` (and other hooks) later, types will be scoped
  // appropriately (e.g. Author | Book).
  return (
    <ul>
      {data.map((doc) => (
        <Suspense key={doc.documentId} fallback={<li>Loading...</li>}>
          <DocumentPreview doc={doc} />
        </Suspense>
      ))}
    </ul>
  )
}
```

### Specific document types

When you know the specific document type you're dealing with, you can make your TypeScript code even more precise using the methods described below.

#### Parameterizing `DocumentHandles`

`DocumentHandle` is a generic type that accept type parameters. You can provide a specific document type literal (like `'book'`) as a type argument. This is useful for typing props or variables that should only reference a handle for a specific document type:

```
import {type DocumentHandle} from '@sanity/sdk-react'

// This function expects a handle that *must* reference a 'book' document
function BookComponent({doc}: {doc: DocumentHandle<'book'>}) {
  // Thanks to DocumentHandle<'book'>, TypeScript knows the context
  const {data} = useDocument(doc)
  // `data` will be typed as the generated `Book` interface
  // ...
}
```

This works because the full definition of `DocumentHandle` includes generic type parameters (`TDocumentType`, `TDataset`, `TProjectId`) that default to `string` but can be made more specific.

#### Using `SanityDocument` for document data

If you need the type for the actual document *data* itself (not just the handle), the `groq` package exports the `SanityDocument<TDocumentType>` helper type. Pass the document type literal to get the corresponding generated interface for the document content:

```
import {type SanityDocument} from 'groq'

type BookData = SanityDocument<'book'>
// BookData is now equivalent to the generated Book interface (e.g., { _id: string; title: string; ... })

// This function expects the fully typed book data
function processBook(book: BookData) {
  console.log(book.title) // Autocomplete works!
}
```

In summary:

- Use `DocumentHandle<'yourType'>` to constrain a document handle to documents of a specific type.
- Use `SanityDocument<'yourType'>` to type the actual data structure of a document of a specific type.

## Workflow considerations

### Regeneration

You'll need to re-run `npm run typegen` whenever you:

- Change your Sanity schemas.
- Add or modify queries/projections defined with `defineQuery` or `defineProjection`.
- Consider integrating this into your `dev` script or a file watcher.

### TypeGen is additive

TypeGen is designed to enhance the App SDK experience. If you don't use it, the App SDK hooks will still work, but data types will often default to `any` or `unknown`, losing the benefits of TypeScript. Adopting TypeGen later should be a non-breaking change that simply adds type safety.

### JavaScript projects

Even if your project doesn't use TypeScript, you can still leverage TypeGen to enhance your JavaScript development experience.

By following the steps in this guide – extracting your schema, installing the necessary packages, using helpers like `createDocumentHandle`, `defineProjection`, and `defineQuery`, and running `npm run typegen` – you create a `sanity.types.ts` file.

While your JavaScript code won't undergo compile-time type checking, modern code editors (like VS Code) that use the TypeScript language service can read this generated file.

This often results in significantly better autocompletion within your JavaScript files when interacting with App SDK hooks and data. Remember, however, that using `defineProjection` and `defineQuery` is still required for TypeGen to generate types for those specific artifacts.



# Studio

#### The basics

[Configuration](/docs/studio/configuration)

[Schema types](/docs/studio/schemas-and-forms)

[Block Content & Portable Text](/docs/block-content)

[Customization](/docs/customization)



#### Get ready for production

[Visual Editing](/docs/visual-editing)

[Hosting and deployment](/docs/studio/deployment)



#### Customize the Studio

[Custom components](/docs/studio/intro-to-custom-studio-components)

[Structure builder](/docs/studio/structure-builder-introduction)

[Localize the Studio](/solution/localization)



#### Bells and whistles

[Comments for Sanity Studio](/docs/studio/comments)

[Tasks for Sanity Studio](/docs/studio/tasks)

[Dashboard](/docs/dashboard)

[Content releases](/docs/studio/content-releases-configuration)

[Localizing Sanity Studio](/docs/studio/localizing-studio-ui)

[Install and configure Sanity AI Assist](/docs/studio/install-and-configure-sanity-ai-assist)





# Installation

If you’re new to Sanity, we recommend exploring [the different ways of getting started](undefined) as this article won't go into any detail beyond the installation instructions.

## Initiating a new Studio from the CLI

Launching a studio from the CLI is typically useful when you:

- are starting a new project
- prefer to figure tools out on your own
- need to set up a content backend quickly 

To install and run the Sanity Studio development server locally, [you will need to have Node and npm installed](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) (or [an npm-compatible JavaScript runtime](https://developer.mozilla.org/en-US/docs/Learn/Tools_and_testing/Understanding_client-side_tools/Package_management#what_exactly_is_a_package_manager)). 

To initiate a new Studio, you can run the following command using a package manager:

```sh
npm create sanity@latest

# Alternatives
yarn create sanity@latest
pnpm create sanity@latest
```

The CLI will take you through creating or signing into an account and choosing a studio template, TypeScript, preferred package manager, etc. It will make a new folder on the desired path and bootstrap a studio with the necessary configuration.

Once the CLI has installed the studio, you can `cd` go into the studio folder and run `npm run dev` to start the local development server.



# Project Structure

The common way of customizing a CMS is to just replace parts of the code with your own. As most people who have modified a WordPress installation can attest, this makes upgrades difficult and quickly leads to tangled code.

Sanity Studio’s file structure is a slim single-page React application where logic and code are contained in [npm modules](https://docs.npmjs.com/about-packages-and-modules). The Studio comes with a framework that lets you customize and add your own components to different parts of it.

This makes it possible to confidently upgrade to new versions of Sanity Studio, and also to install and ship plugins in self-contained packages.

## Studio File Layout

The file structure of Studio projects can look different depending on how you installed it. The example below is from the blog template example that you can initiate from the CLI:

```text
.
├── README.md
├── dist
│   ├── index.html
│   └── static
│       ...
│       └── ...
├── package-lock.json
├── package.json
├── sanity.cli.ts
├── sanity.config.ts
├── schemas
│   ├── author.ts
│   ├── blockContent.ts
│   ├── category.ts
│   ├── index.ts
│   └── post.ts
├── static
└── tsconfig.json
```

Sanity Studio contains the following files and folders out of the box:

- `package.json`: Contains the necessary dependencies for the studio project. There will also be a dependency lock file that might look different depending on which package manager you use.
- `sanity.cli.ts`: The configuration file for the Sanity CLI. Contains information on what project ID and dataset the CLI should connect to for project-specific commands.
- `sanity.config.ts`: The configuration file for the Studio contains information about what project(s) that the Studio should connect to, as well as schemas, plugins, and other customizations.
- `schemas`: It's a convention to organize schema files in a dedicated folder. It's not required to have a schemas folder, as schemas are imported as JavaScript into the Studio config object.
- `static`: If you want to bundle static files in the studio, you can place these files in the static folder. The built-in developer tooling using Vite will pick these up automatically. If you use a different bundler, then you might need to configure this to bundle files from this folder as well.
- `tsconfig.json` (only if TypeScript is used) Contains the settings for [the transpilation of TypeScript into JavaScript](https://vitejs.dev/guide/features.html#typescript).
- `dist`: This folder is auto-generated and contains the production build of Sanity Studio as a result of running `npm run build.`





# Development

Sanity Studio is distributed as [a single package on npm](https://www.npmjs.com/package/sanity). It also comes with built-in tooling for local development based on [Vite](https://vitejs.dev). The package also exports the Studio as a React component, including a render function for mounting it on a DOM node in an HTML document.

> [!TIP]
> Protip
> Sanity Studio always connects to a dataset in the hosted Content Lake, also when run locally. Your content is never stored locally. This means that you can have differences in the schema between a local and a hosted Studio. If the Studio finds content in a document that doesn't match its schema, it will display a warning. 
> 
> You can safely change schemas with the confidence that no existing content will be changed. To change content to comply with schema changes, you will need to run a migration script.

## Local development

You can start a local development server with the following command within the Studio project folder:

```sh
# For Studios initated with the CLI
npm run dev

# Alternative method
npx sanity dev
```

### To start the development server on a different port

The local development server will make the Studio available on `http://localhost:3333` by default. You can specify the port with the following command:

```sh
# For Studios initated with the CLI
npm run dev -- --port 3000

# Alternative method
npx sanity dev --port 3000
```

> [!WARNING]
> Gotcha
> To run the Studio on a different port locally, you will have to enable CORS origins for that domain with authenticated requests enabled. 
> 
> You can add this with the Sanity CLI by running the following command in your Studio project folder:
> 
>  npx sanity cors add http://localhost:3000 --credentials 

## Local production build

To build your Studio for production locally, run the following command in the Studio project folder:

```sh
# For Studios initated with the Sanity CLI
npm run build

# Alternative method
npx sanity build
```

The build command will bundle the Studio files into a dist folder by default. You can specify the production folder name (for example `public`) by passing it as a parameter:

```sh
# For Studios initated with the Sanity CLI
npm run build -- public

# To build the Studio to a folder named "public"
npx sanity build public
```

This can be useful if your hosting provider requires a specific filename when you are [self-hosting the Studio](/docs/studio/deployment).

### Preview a production build locally

To preview the local *production *build, you can run the following command:

```sh
npx sanity preview

# To specify the folder ("./public") for the production build
npx sanity preview public
```

This will run a local server for the production build of the Studio on `http://localhost:3333` .

> [!WARNING]
> Gotcha
> It's easy to forget that any changes you make to the Studio files won't get reflected when running the preview of the production build. To enable hot-module reloading, you have to run npm run dev or npx sanity dev. 

## Customizing the built-in Vite configuration

To extend or change the built-in Vite configuration, you need [a configuration file for the Sanity CLI](/docs/apis-and-sdks/cli). Let‘s say you want to alias your root folder to enable relative imports like `import CustomComponent from '@/components/CustomComponent'`. The following code examples show you how you can overwrite certain properties in the Studio's Vite configuration to do so:

```javascript
// sanity.cli.js
import {defineCliConfig} from 'sanity/cli'

export default defineCliConfig({
  api: {
    // the rest of the config...
  },
  vite: {
    resolve: {
      alias: {
        '@': __dirname,
      },
    }
  },
})
```

You can learn more about configuring Vite [in their documentation](https://vitejs.dev/config/).





# Hosting and deployment

[Sanity Studio](https://sanity.io/docs/content-studio) is an open-source React-based Single Page Application (SPA) that runs entirely in the browser and connects with Sanity's hosted APIs and Content Lake.

There are two primary ways of hosting Sanity Studio:

- We can host this web application for you, giving you a nice `my-company.sanity.studio` URL. With a single Sanity CLI command, you can deploy and manage multiple Studios for different environments or use cases under the same project.
- You can deploy Sanity Studio on any hosting platform that supports SPA routing.

The built-in Sanity Studio hosting is the quickest and easiest way to make the Studio accessible on the web. Self-hosting is usually preferred when you wish to use platform-specific features that are not offered by our hosting and when you want to host the Studio under your own domain.

It's also possible to [embed Sanity Studio](/docs/studio/embedding-sanity-studio) in an application as a dependency – however, depending on your setup and configuration, you might forego certain features that are tied to the build tooling that comes with the Sanity CLI. 

[Upgrading Sanity Studio](/docs/studio/upgrade)



## Hosting with Sanity

```sh
# With the CLI globally installed
sanity deploy

# Using npx
npx sanity deploy
```

Running this command from your Studio project folder builds and deploys your Studio, making it available on a `*.sanity.studio` URL. When you deploy, you will be asked to choose a unique hostname for your Studio. On subsequent deploys, you can select an existing hostname in a prompt or specify it in the CLI config file (`sanity.cli.js|ts)` for automated deployments.

```typescript
import {defineCliConfig} from 'sanity/cli'

export default defineCliConfig({
  api: {
    projectId: 'projectid',
    dataset: 'production'
  },
  // Tip: You can use an environment variable for studioHost if you want to deploy separate Studios for production, staging, testing etc.
  studioHost: 'my-company'
})
```

> [!WARNING]
> Gotcha
> The sanity deploy command works by building the source files in your Studio project into static files, which are then uploaded and served from your chosen sanity.studio domain.
> 
> Logged-in access to your Studio, and private data in your Content Lake, is always secured by authentication. However, no authentication is involved when serving the built Studio's files. Make sure not to include any sensitive data – such as authentication tokens – in your Studio's configuration files.

## Undeploying the Studio

```sh
# With the CLI globally installed
sanity undeploy

# Using npx
npx sanity undeploy
```

Run the command above to change the hostname later or remove the Studio from the web. You may choose a new hostname the next time you deploy.

## Hosting with Sanity in a CI/CD flow

You can host with Sanity automatically with continuous integration tools.  This is convenient for automatically updating the hosted Studio when you push your local changes to source repositories or do manual releases. Add `@sanity/cli` as a development dependency and configure your CI/CD workflow to run the command `sanity deploy`. Remember to have the `sanity.cli.ts` config file in your Studio folder.

If you need to accommodate test, staging, and production deployments, then we recommend that you define the Studio host name (and other configuration) in  [environment variables](/docs/studio/environment-variables) and access them in the config file like this:

```typescript
import {defineCliConfig} from 'sanity/cli'

export default defineCliConfig({
  api: {
    projectId: process.env.SANITY_STUDIO_PROJECT_ID,
    dataset: process.env.SANITY_STUDIO_DATASET,
  },
  studioHost: process.env.SANITY_STUDIO_HOSTNAME
})
```



### Authorizing Studio deployments

You also need to provide an authorization token using the `SANITY_AUTH_TOKEN` environment variable. This is because deploying with the `sanity deploy` command uses your local user session for authenticating with our hosting service, which won't necessarily be available in your CI/CD workflows. You can create a deploy token in [the project management dashboard](https://sanity.io/manage).

## Self-hosting the Studio

Since the Studio consists of static HTML, CSS, and JavaScript files and communicates with Sanity through our HTTP API, it can be hosted anywhere. Popular hosting services like [Vercel](https://vercel.com/guides/deploying-sanity-studio-with-vercel) and [Netlify](https://www.netlify.com) make it possible to automatically deploy new versions of your Studio when you push it to code repositories like GitHub.

There are two things you need to make sure of when hosting the Studio yourself or with a service:

25. The server that delivers the Sanity Studio files needs to be configured for single-page application routing. This means if the requested URL path doesn't exist on the filesystem, it should serve `index.html` to allow the frontend router to handle the request. Most hosting services will have configuration options for this.
25. The domain for where the Studio is hosted must be [added as a valid domain in the project's CORS settings](/docs/content-lake/cors). For security, the Sanity API ensures that only approved Studios can communicate with your project. This is in addition to other security measures such as user authentication, private datasets, and custom access rules.

If you host with Sanity, this is automatically handled for you. If your host does not support single-page-application routing, you can add a redirect rule to make sure non-existent paths are redirected properly. Check the documentation for your provider or server software.

### Specifying the base path

Normally, the Studio expects to be hosted at the root level of its hostname; for instance `https://studio.example.com/`. To serve the studio on a subpath, such as `https://example.com/studio`, you need to edit the CLI configuration file. You'll find it as `sanity.cli.js` or `sanity.cli.ts` in the root of your Studio project.

```javascript
import {defineCliConfig} from 'sanity/cli'

export default defineCliConfig({
  project: {
    basePath: '/studio'
  },
  // ...config continued
})

```

The Studio can now be served from `https://example.com/studio`. This will also change the base path of static files.

Most cases where you embed the Studio in another application will require you to set the basePath.

> [!WARNING]
> Gotcha
> The sanity.config.js file also has a basePath property - this defines the base path of the workspace and not the actual studio.
> 
> In other words, the two basePaths gets joined together: if the CLI base path is set to /studio and the workspace base path is /production, the resulting base path for the "production" workspace will be /studio/production.

Setting the `SANITY_STUDIO_BASEPATH` environment variable is an alternative method of defining the base path for the studio, and will override any value set in the configuration file.

### Building the Studio for hosting

```sh
# With the CLI installed globally
sanity build

# With npx
npx sanity build

# With npx, specifying the build folder name to be "public"
npx sanity build public
```

Run the command above from the studio folder to generate the files for hosting. This will output the files to the `dist/` directory by default. Sometimes your environment requires another directory name, for instance `public`. You can specify this by entering the desired name after the `build` command.

Once the build is complete, the directory can be uploaded and hosted from any web hosting where you can control redirects for a Single-Page Application, like [Vercel](https://vercel.com), [Netlify](https://netlify.com), or [Cloudflare](https://pages.cloudflare.com/).

### Environment variables

Sometimes you want to configure the `projectId` , `dataset` or `studioHost` specified in `sanity.cli.js` and `sanity.config.js` at build time. This is useful for building multiple Studios from the same schema and code to facilitate different environments. See the documentation on [environment variables](/docs/studio/environment-variables) for your options.

## GraphQL

How to deploy the [GraphQL APIs is covered in its own section](/docs/content-lake/graphql).



# Embedding Sanity Studio

Sanity Studio is a React application distributed as [a single dependency on npm](https://www.npmjs.com/package/sanity). In principle, this means you can embed the Studio in any web application, as long as you can control the routing to redirect all Studio URLs to the page it‘s hosted.

## Adding the Studio as a dependency with `npm`

If you work on a project where [node package manager](https://www.npmjs.com/) is supported, you can add the Studio as a dependency using the following command:

```sh
npm install sanity@latest
```

> [!TIP]
> Protip
> If you plan to do Studio customization, then it can be useful to install @sanity/ui and @sanity/icons as well.

## Accommodating the Studio

> [!WARNING]
> Gotcha
> It's easy to forget, but you will always have to add the domain where you host your Studio to your project's CORS origins settings with authenticated requests enabled. 



### Styling

The Studio is built as a responsive web app. For the best editor experience, it should take the full width and height of the browser window. This means that you have to make sure that the DOM node that the Studio is mounted on is styled accordingly:

```css
/* This assumes no margin or padding on #app's parent(s) */
#app {
  height: 100vh;
  max-height: 100dvh;
  overscroll-behavior: none;
  -webkit-font-smoothing: antialiased;
  overflow: auto;
}

```

### Routing

If you embed the Studio inside of another app, it's likely you want to access it on a sub-route, e.g `/studio` or `/admin`. This means that you have to:

14. Add this route to `basePath` in the Studio‘s configuration object
14. Make sure that the app's routing redirects all the Studio's sub routes to the page/view where the Studio is mounted.

Frontend frameworks with file-based routing like [Next.js](https://nextjs.org/docs/routing/dynamic-routes#optional-catch-all-routes), [Nuxt.js](https://nuxtjs.org/docs/features/file-system-routing/#unknown-dynamic-nested-routes), [Svelte](https://kit.svelte.dev/docs/advanced-routing), [Remix](https://remix.run/docs/en/v1/guides/routing#splats), [Astro](https://docs.astro.build/en/core-concepts/routing/#rest-parameters), and others, have conventions for “catch-all”, “rest”, or “splat” routes. These can be used to make sure that all routes under the Studio‘s basePath will be redirected and resolved by the Studio application.

If you are embedding the Studio outside of a framework like this, then you need to control the redirects on the server or hosting level. For example, if you host with [Netlify](https://docs.netlify.com/routing/redirects/), then you need to add a setting like this:

```
# netlify.toml
[[redirects]]
  from = "/admin/*"
  to = "/admin"
  status = 301
  force = true
```

### Rendering the Studio in React applications

The `sanity` package exports a `<Studio />` component that renders a Studio given a `config` object like so:

```jsx
// StudioRoute.tsx
import { defineConfig, Studio } from "sanity";

const config = defineConfig({
  projectId: "your_project_id",
  dataset: "your_dataset",
  basePath: "/some-route-in-your-app"
});

export default function StudioRoute() {
  return <Studio config={config} />
}
```

As mentioned in "Routing" below, you need to ensure all sub-routes of the `basePath` are redirected to this route.

## Embedding Sanity Studio in a Next.js app

If you want to embed Sanity Studio in a Next.js project, then you can use the official next-sanity library. In addition to making embedding the studio easier, it also comes with tools for live preview and other useful things.

[Learn more about next-sanity](https://github.com/sanity-io/next-sanity#next-sanitystudio-dev-preview)



## Rendering the Studio in non-React applications

If you aren't in a React project, then you can use the `renderStudio` function to mount it on a DOM node:

```javascript
const {default: build} = await import("https://esm.sh/build")

const mod = await build({
  dependencies: {
    "sanity": "^3.27.0",
    "@sanity/vision": "^3.27.0",
  },
  source: `
      import { defineConfig, renderStudio } from "sanity";
      import { structureTool } from "sanity/structure";
      import {presentationTool} from 'sanity/presentation';
      import { visionTool } from "@sanity/vision";

      const config = defineConfig({
        basePath: '/',
        projectId: "pv8y60vp",
        dataset: "production",
        schema: {
          types: [
            {
              type: "document",
              name: "post",
              title: "Post",
              fields: [
                {
                  type: "string",
                  name: "title",
                  title: "Title"
                }
              ]
            }
          ]
        },
        plugins: [structureTool(), presentationTool({}), visionTool()]
      });
const div = document.createElement('div')
document.body.innerHTML = ''
document.body.appendChild(div)
export const render = () => renderStudio(div, config);
  `,
  // for types checking and LSP completion
  types: `
    export function render(): string;
  `,
});

// import module
const { render } = await import(mod.bundleUrl);

render()
```





# Upgrading Sanity Studio

Sanity Studio is distributed as [a npm package](https://www.npmjs.com/package/sanity), which means that upgrades are done as with any other dependency in `package.json`.

The simplest way to upgrade the core package for Sanity Studio is:

```sh
  npm install sanity@latest

  ## alternative package managers
  yarn upgrade sanity@latest
  pnpm install sanity@latest
```

Make sure that your lock file (for example, `package-lock.json`) has been updated as well. 

## Upgrading plugins and other dependencies for Sanity Studio

As with the core `sanity` dependency, plugins, and other dependencies are also upgraded by installing newer versions. You can also look into tooling like [npm-upgrade](https://www.npmjs.com/package/npm-upgrade) and [npm-check-updates](https://www.npmjs.com/package/npm-check-updates) that give you interactive CLI workflows for upgrading your dependencies. Code editors like VS Code also have [plugins with UI affordances for managing and updating dependencies](https://marketplace.visualstudio.com/items?itemName=idered.npm).

## Deploying upgrades

After you have upgraded your Studio project's dependencies, you can deploy the new version depending on your deployment strategy:

- Run the `sanity deploy` command in your command line.
- Or, check the changes into git and push/merge to the branch that deploys the Studio to production.

We advise you to always check and track your studio code into git and push it to a remote git repository. That way, you won't accidentally lose your work. 

[Learn more about hosting and deployment](/docs/studio/deployment)



## Automatic studio upgrades 

### Using the built-in `autoUpdates` configuration property

If you deploy your studio using the Sanity build tools you can set up your studio configuration to allow automatically updating to the latest major studio version. 

[Read more about auto-updating studios](/docs/studio/auto-updating-studios)



### Using Renovatebot

If you deploy Sanity Studio from GitHub (or other compatible platforms), then you can automate upgrades by setting up [Renovatebot](https://github.com/renovatebot/renovate). We maintain the preset configuration for Sanity projects that you can reuse. 

Go to [the Sanity config presets on GitHub and follow the instructions](https://github.com/sanity-io/renovate-config?tab=readme-ov-file#usage) to set it up.

## Stay on top of what's new in the changelog 

We publish updates to Sanity Studio up to every week. For each release of the Studio, we'll also post [release notes on GitHub](https://github.com/sanity-io/sanity/releases) and in [our changelog](/changelog) (that also covers other packages and APIs).

Changelog entries on sanity.io will also mark any documentation article that was affected by the upgrade. You will find related changelog entries by expanding the changelog menu above the documentation article.

You will find migration instructions in the changelog entry on the rare occasions where there have been breaking changes.



# Environment Variables

**Note:** Make sure you are using version 3.5.0 or later in order to make full use of environment variables.  

## Exposed variables

Environment variables prefixed with `SANITY_STUDIO_` are automatically picked up by the Sanity CLI tool, development server and bundler.

Any found environment variables are available as `process.env.SANITY_STUDIO_VARIABLE_NAME` - even in browser code. 

By requiring this `SANITY_STUDIO_` prefix, we prevent unrelated (and potentially sensitive) environment variables from getting exposed to the browser bundle.

## Static replacement

It is important to note that these variables are **statically replaced** during production. It is therefore necessary to always reference them using the full static string. For example, dynamic key access like `process.env[key]` will not work (they *might* be accessible this way in development, but will fail in production).

Similarly, logging or iterating over `process.env` will not give you consistent results in development and production - in production, `process.env` will be said to be `undefined`, as all the values are **statically replaced** during the build. In other words:

```javascript
// In development:
const studioTitle = process.env.SANITY_STUDIO_TITLE

// In production:
const studioTitle = "the value of the env var"
```

Note that during a build, it will also replace these references appearing in JavaScript strings. This should be a rare case, but it can have unintended side effects. You may see errors like `Missing semicolon` or `Unexpected token`. One way to work around this behavior is to break the string up with a Unicode zero-width space, e.g. `'process\u200b.env.SANITY_STUDIO_FOO'`.

### Keeping secret things secret

It is common practice to keep secret tokens and API keys as environment variables that are not checked into version control. This is a decent strategy for server-side applications where the runtime is protected. However, since the Studio is a client-side application, you will have to approach secrets differently. We recommend using the [Sanity Secrets library](https://github.com/sanity-io/sanity-studio-secrets) that gives you hooks and UI components for handling secrets.

Make sure that your `.env.local` files are ignored by your version control system (usually git), and do not put sensitive information or keys into the .env files commited.

## Loading variables from `.env` files

Sanity will read `.env` files by default and make its variables available for the development server, production builds, command line actions and similar. Note that you still have to follow the variable naming conventions mentioned above. We also support env loading priorities for situations where you want to differentiate between sharing certain variables in all environments and overwriting them for production builds.

```sh
.env               # loaded in all cases
.env.local         # loaded in all cases, ignored by git
.env.[mode]        # only loaded in specified mode
.env.[mode].local  # only loaded in specified mode, ignored by git

```

Also, Sanity uses [dotenv-expand](https://github.com/motdotla/dotenv-expand) to expand variables out of the box. To learn more about the syntax, check out [their docs](https://github.com/motdotla/dotenv-expand#what-rules-does-the-expansion-engine-follow).

Manually declared environment variables (outside of .env files) take precedence, overriding any values set in .env files.

## Modes

By default, the `build` and `deploy` commands runs in `production` mode, while all other commands run in `development` mode. Other commands can be run in production mode by setting `NODE_ENV` to `production` (note that only this value is supported; for other modes, use `SANITY_ACTIVE_ENV`).

This means when running `sanity build`, it will load the environment variables from `.env.production` if that file exists. For example:

```sh
# .env.production
SANITY_STUDIO_TITLE=My Studio
```

Given this environment variable, you could then render the title in your app using `process.env.SANITY_STUDIO_TITLE`.

In some cases, you may want to run `sanity build` with a different mode to render a different title. You can overwrite the default mode used for a command by setting an environment variable named `SANITY_ACTIVE_ENV`. For example, if you want to build your studio for a staging mode:

```sh
SANITY_ACTIVE_ENV=staging sanity build
```

And create an `.env.staging` file:

```sh
# .env.staging
SANITY_STUDIO_TITLE=My Studio (staging)
```

## Best practices

We encourage you to keep environment variables to a minimum, and not spread them throughout the code base. Common (and valid) use cases are things like configuration files.

To more easily be able to tell which environment variables are used, and keep things as tidy as possible, we recommend having a single file that re-exports environment variables to the rest of your code. For instance:

```typescript
// src/environment.ts
export const myStudioTitle = process.env.SANITY_STUDIO_TITLE
export const myCompanyApiUrl = process.env.SANITY_STUDIO_COMPANY_API_URL

```

Similarly, plugins should generally never use environment variables directly - instead, they should take a configuration object which the user can then choose to pass environment variables to:

```typescript
import {somePluginApiUrl} from './src/environment'

defineConfig({
  plugins: [
    // ...
    somePlugin({
      // process.env.SANITY_STUDIO_SOME_PLUGIN_API_URL
      apiUrl: somePluginApiUrl
    })
  ]
})
```

## Differences from Vite

Sanity's environment variable behavior is heavily inspired by (and partially powered by) Vite. There is however one key difference:

Vite exposes environment variables under `import.meta.env`. While Sanity *also* lets you access them that way, we heavily recommend that you access them using `process.env`.

Using `process.env` allows us to expose the same variables to both the browser and Node.js/Node.js powered tools without too much work. It also eases cross-environment migrations, such as moving from the default Vite-based bundler to (for instance) an embedded setup inside of Next.js.

## Programmatic usage

Should you want to reuse the environment variable handling in other contexts (your own scripts, or using a different bundler etc), you can import and utilize the `getStudioEnvironmentVariables()` method from `sanity/cli`:



```typescript
import {getStudioEnvironmentVariables} from 'sanity/cli'

console.log(getStudioEnvironmentVariables())
// {SANITY_STUDIO_SOME_VAR: 'yourVariableValue'}

```

Note that `.env` files are not loaded by default when using this method. To do so, pass an `envFile` option:

```typescript
import {getStudioEnvironmentVariables} from 'sanity/cli'

console.log(
  getStudioEnvironmentVariables({
    envFile: {
      mode: 'production',
      envDir: '/path/to/some-dotenv-root'
    }
  })
)

```

For usage in bundlers (such as Vite's `define` option or Webpacks `DefinePlugin`), you'll usually want the keys to be fully qualified with the `process.env` prefix, and the values to be JSON-encoded. The method can do all of this for you:

```typescript
import {getStudioEnvironmentVariables} from 'sanity/cli'

console.log(
  getStudioEnvironmentVariables({
    jsonEncode: true,
    prefix: 'process.env.'
  })
)

```

## Built-in Studio Environment Variables

The following environment variables are integrated in the Studio code base and will be picked up when specified in a .env file (see [Loading variables from .env files](https://www.sanity.io/docs/environment-variables#157ecc9db07a) above). Note that they only apply when using the `sanity` CLI; if you render using your own bundler, these will not work.

```text
SANITY_STUDIO_BASEPATH            Sets the base path for the studio
SANITY_STUDIO_SERVER_HOSTNAME     Hostname for the development/preview server
                                  (localhost by default)
SANITY_STUDIO_SERVER_PORT         Port number for the development/preview server
                                  (3333 by default)
SANITY_STUDIO_REACT_STRICT_MODE   Enable React strict mode. Its use is discouraged
                                  unless you know what you're doing, as it leads
                                  to worse performance in development
```



# Using TypeScript in Sanity Studio

TypeScript is a superset of JavaScript that adds optional static typing to the language. You can learn more about TypeScript in [their getting started guide](https://www.typescriptlang.org/docs/handbook/typescript-in-5-minutes.html).

If you initiate a Sanity Studio with the CLI, then TypeScript will be the default. Sanity Studio uses [Vite](https://vitejs.dev/guide/features.html#typescript) to perform the transpilation of TypeScript files.

> [!WARNING]
> Gotcha
> If you customize the Studio with your own React components, that is, using JSX syntax, you will have to (re)name the file as .tsx. 

## Inline documentation with TSDoc

The Sanity Studio codebase uses TSDoc for inline documentation. This is still a work in progress. However, you can already inspect if an API is considered internal, beta, or public:

- `@internal`: Not considered as stable for public consumption. If you rely on internal APIs, they might break between minor [semver](https://semver.org/) releases
- `@beta`: APIs that we intend to ship as public-facing, but that we are testing externally and might be subject to change between minor [semver](https://semver.org/) releases. We will make our best effort to document breaking changes for `@beta` APIs in the release notes.
- `@public`: These are public APIs that you can use confidently. Breaking changes will only happen in major [semver](https://semver.org/) releases and will be documented.

## Default `tsconfig.json`

If you initiate a studio project using the Sanity CLI and don't opt out of TypeScript, then it will generate the following `tsconfig.json`:

```json
{
  "compilerOptions": {
    "target": "es2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "forceConsistentCasingInFileNames": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "node",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true
  },
  "include": ["**/*.ts", "**/*.tsx"],
  "exclude": ["node_modules"]
}
```



# Auto-updating Sanity Studio

Keeping up with frequent releases and making sure your editorial teams get access to all the latest improvements and bug fixes is more convenient than ever before.

With auto updates enabled, the core Sanity Studio app will automatically be kept up to date whenever a patch or minor release drops (3.**X.X**), while your custom code remains untouched and continues to work seamlessly. 

No breaking changes. Just the good stuff.

> [!TIP]
> Protip
> Studio projects initialized before automatic updates were enabled by default in the CLI config (starting with Sanity Studio version 3.57.3) are not automatically opted into this behavior. 
> 
> To enable automatic updates in an existing project, set autoUpdates: true in your studio CLI config (sanity.cli.ts) and then execute the deploy command.



### Prerequisites

- Using auto-updating studios requires a browser that supports [import maps](https://caniuse.com/import-maps). This feature has been considered a [baseline](https://github.com/web-platform-dx/web-features/blob/main/docs/baseline.md) feature in modern browsers since March 2023, and is supported in all the latest versions of common browsers.
- Auto-updates is only supported for studios compiled with Sanity build tooling. I.e., running [sanity deploy](/docs/cli-reference/deploy) or [sanity build](/docs/cli-reference/build) in your command line. Third-party build tools and embedded studios are not currently in scope for this feature.

> [!NOTE]
> Build or deploy?
> For the auto-update feature to work, you must compile your studio using the build tooling provided in the core Sanity Studio package. Somewhat confusingly we refer to both the sanity deploy and sanity build commands in this article, but what's the difference?
> 
> If your studio is hosted on a free *.sanity.studio-domain you probably use sanity deploy to build and deploy your studio in one fell swoop, while if you are hosting the studio elsewhere chances are that you compile the studio with sanity build, and then deal with deployment with a custom workflow.

## Opting Out

With auto-updates, your studio keeps itself current with the latest patch and minor version (i.e., 3.**X.X**) without intervention. Instead of building and re-deploying your studio for each new version, the core Sanity Studio application is streamed on-demand to your browser, which means you'll always get access to the latest updates as they are released. Note that releases are gradually rolled out, so you may not *immediately* see a new release after a new version is out. 

If you need to support older browsers without support for import maps, if you have customized your studio using internal APIs or simply need full control over the studio dependencies, you can opt out of auto-updates. If you decide to opt out of auto-updates, set `autoUpdates: false` in your `sanity.cli.ts` configuration file, and then build and deploy.

```typescript
// sanity.cli.ts
import { defineCliConfig } from 'sanity/cli'

export default defineCliConfig({
  api: {
    projectId: 'your-project-id',
    dataset: 'production',
  },
  autoUpdates: false,
})
```

## Developing auto-updating studios

Your local development process remains unchanged. You will still install the latest version of the Sanity Studio package locally and run your dev server on localhost, quite possibly at port 3333. When you are ready to commit, build and deploy your studio, explicitly setting the feature flag or CLI config property if needed.

If you have auto-updating enabled and are developing locally against a version that is not up-to-date you will receive a warning in your build step to ensure you are aware of any potential discrepancies.

## Caveats and key takeaways

- Auto-updating is currently only supported for studios built with the `sanity build` command. Other build tools and embedded studios are not supported.
- Auto-updates requires a browser that supports [import maps](https://caniuse.com/import-maps), which has been considered a [baseline](https://github.com/web-platform-dx/web-features/blob/main/docs/baseline.md) feature in modern browsers since March 2023, and is supported in all the latest versions of common browsers.
- When developing with auto-updates enabled, you may receive warnings in your build step if your local version is out of sync with the auto-updated deployed version. Keep an eye out for these to ensure you're aware of any discrepancies between your development and production environments.
- Auto-updating keeps your studio current with patch releases and minor updates, but will not automatically update to major releases. Rest assured that breaking changes will be announced clearly and in a timely fashion, as always.
- To opt out of auto-updating, rebuild and redeploy your studio using the `autoUpdates` configuration property set to `false`. Changes will take effect for your editorial teams once the new build has been successfully deployed.
- Auto-updating keeps your studio up-to-date, but you're still responsible for updating and maintaining any custom code, plugins, or configurations layered on top of the base Sanity Studio.



# Introduction

Typically, you find the studio configuration inside a `sanity.config.ts (or js)` file located at the root of your project. The development server for Sanity Studio automatically picks up what's returned from the exported `defineConfig` function. This function takes either a single workspace configuration object or [an array of configuration objects](/docs/studio/workspaces) as its only argument. By implementing the pre-defined properties of this object you are able to customize a range of options and behaviors in the studio, as well as control how plugins and other studio extensions are configured.

> [!TIP]
> Protip
> All these are valid file suffixes for the studio configuration file: .js , .jsx , .ts , .tsx

## Minimal studio configuration example

### Single Studio configuration

For a single Studio configuration, the `defineConfig` function takes a single configuration object. The only *required* properties are `projectId` and `dataset` but, since this won’t make for a very useful studio, we've included the `structureTool()`-plugin and some schemas in our example to reflect a more typical setup.

[More about Schemas and Forms ->](/docs/studio/schemas-and-forms)

```javascript
// Single workspace configuration

import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'
import {schemaTypes} from './schemas'

export default defineConfig({
  projectId: '<projectId>',
  dataset: 'production',
  plugins: [structureTool()],
  schema: {
    types: schemaTypes,
  },
})
```

### Multiple workspace configurations

When configuring multiple workspaces you supply an array of configuration objects. Each of these must, in addition to `projectId` and `dataset`, also include a unique `basePath` and `name` for each workspace.

[More about Workspaces ->](/docs/studio/workspaces)

```javascript
// Multiple workspace configuration
import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'
import {schemaTypes} from './schemas'

export default defineConfig([
  {
    projectId: '<projectId>',
    dataset: 'production',
    name: 'production-workspace',
    basePath: '/production',
    title: 'Default Workspace',
    subtitle: 'production',
    plugins: [structureTool()],
    schema: {
      types: schemaTypes,
    },
  },
  {
    projectId: '<projectId>',
    dataset: 'staging',
    name: 'staging-workspace',
    basePath: '/staging',
    title: 'Another workspace',
    subtitle: 'staging',
    plugins: [structureTool()],
    schema: {
      types: schemaTypes,
    },
  },
])
```

### Property callback functions

Many of the properties of the config object have the option of accepting a callback function instead of a static value. These callbacks are usually invoked with the previous value and a context object.

```javascript
import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'
import {schemaTypes} from './schemas'

export default defineConfig({
  projectId: '<projectId>',
	dataset: 'production',
  plugins: [structureTool()],
	schema: {
    types: (prev, context) => {
      console.log(context);// logs { projectId, dataset }
      return [...schemaTypes, ...prev]
    },
  },
})
```

> [!WARNING]
> Gotcha
> If you choose to use the callback function you need to make sure you return the previous value along with whatever new value you want to add. When using static values this is handled automatically by the studio.

The information included in the context object varies depending on the property in question.

```javascript
import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'
import {RocketIcon} from '@sanity/icons'
import {schemaTypes} from './schemas'

export default defineConfig({
  projectId: '<projectId>',
	dataset: 'production',
  plugins: [structureTool()],
	schema: {
    types: schemaTypes,
  },
	tools: (prev, context) => {
    console.log(context) // logs { getClient, currentUser, schema, projectId, dataset}
    return [
      {
        name: 'my-tool',
        title: 'My super-cool tool',
        icon: RocketIcon,
        component: (props) => <Card>I am a tool, albeit not a useful one</Card>,
      },
      ...prev, // remember to include previous values
    ]
  },
})
```

> [!WARNING]
> Gotcha
> The example above includes some JSX in the inline component declaration. Vite, the default studio bundler, requires files that contain JSX to have a file extension or either .jsx or .tsx.

## Commonly used configuration properties

### Workspace properties

Every workspace configuration needs to at least include appropriate string values for `dataset` and `projectId`. If you are working with multiple workspaces in your studio, each workspace should also include a `name` and `basePath`.

```javascript
//⬇ Required
dataset: 'production',
projectId: '<projectId>',
//⬇ Optional if only using a single workspace
name: 'cool-studio',  
basePath: '/my-default-workspace',
//⬇ Optional 
title: 'My Cool Studio',
subtitle: 'production'
icon: RocketIcon, 
```

[More about Workspaces ->](/docs/studio/workspaces)

### Schema

The `schema` property is where you declare your schema types. You can specify a static array of schema objects or a callback function that returns such an array.

```javascript
schema: {
	types: mySchemas,
}
```

```javascript
schema: {
  types: (prev, context) => {
    console.log(context) // logs { projectId, dataset' }
    return [...mySchemas, ...prev]
  },
},
```

You may also set [initial value templates](/docs/studio/initial-value-templates) using the aptly named `templates` property. You can specify a static array of template objects or a callback function that returns such an array.

```javascript
schema: {
    templates: (prev) => [
		  {
		    id: 'category-child',
		    title: 'Category: Child',
		    schemaType: 'category',
		    parameters: [{name: `parentId`, title: `Parent ID`, type: `string`}],
		    value: ({parentId}) => ({
		      parent: {_type: 'reference', _ref: parentId},
		    }),
		  },
		  {
		    id: 'article-with-author',
		    title: 'Article: Author',
		    schemaType: 'article',
		    parameters: [{name: `authorId`, title: `Author ID`, type: `string`}],
		    value: ({authorId}) => ({
		      author: authorId,
		    }),
		  },
		  ...prev,
		]
  },
```

[More about Schemas ->](/docs/studio/schemas-and-forms)

### Plugins

This is where you declare plugins for your studio. It accepts a static array of plugin config objects or a callback function that returns such an array. The default studio templates come with the `structureTool` plugin included already.

```javascript
plugins: [structureTool()],
```

You’ll notice that the plugin function usually needs to be invoked, not just referred to. This is because plugins, by convention, are functions that can accept configuration options as arguments.

```javascript
plugins: [
    structureTool(),
    visionTool({
      defaultApiVersion: 'v2021-10-21',
      defaultDataset: 'production',
    }),
  ],
```

[More about Plugins ->](/docs/studio/installing-and-configuring-plugins)

### Tools

Tools are full page-components, in that they “take over” most of the studio interface when activated, just like the structure-tool or vision plugin. Because of this behavior they also show up in your studio’s nav bar, and they can be navigated to by appending their `name` to your studio’s URL. E.g. `https://my-cool-site.com/studio/my-tool`.

Tools are declared much in the same way as plugins. The property accepts either a static array of tool configuration objects or a callback function that returns such an array.

```javascript
  tools: [
    {name: 'my-tool', title: 'My Tool', component: MyTool},
    {name: 'tool-2', title: '2nd Tool', component: MyOtherTool},
  ],

 // Example using the callback function with some conditional logic
  tools: (prev, {currentUser}) => {
    if (currentUser.roles.find((r) => r.name === 'admin')) {
      return [
				...prev,
				{name: 'admin', title: 'Admin', component: MyAdminTool},
			]
    }
		return prev
  },
```

[More about Tools ->](/docs/studio/studio-tools)

### Form

The form config property lets you configure asset sources for files and images, as well as override the default rendering of form components. 

```javascript
form: {
	file: {
    assetSources: myFileAssetSourceResolver,
    directUploads: true,
  }
  image: {
    assetSources: myImageAssetSourceResolver,
    directUploads: true,
  },
  components: {
		// TODO: Better example
		input: (props) => isStringInputProps(props) ? <MyCustomStringInput {...props} /> : props.renderDefault(props),
		field: MyCustomField,
	}
},
```

> [!WARNING]
> Gotcha
> Overriding the rendering of inputs and fields in the top level studio configuration will affect all fields in your studio. If you wish to customize the rendering of only certain fields, you probably want to do so by setting the components property of the appropriate fields. More info: Introduction to Component API.

[More about Asset Sources ->](/docs/studio/custom-asset-sources)

[More about Form Components ->](/docs/studio/form-components)

### Document

This property lets you configure [document actions](/docs/studio/document-actions) and [badges](/docs/studio/custom-document-badges), as well as set a `productionUrl` for previews and specify [options for new documents](/docs/studio/new-document-options).

```javascript
document: {
  actions: (prev) =>
    prev.map((previousAction) =>
      previousAction.action === 'publish' ? MyPublishAction : previousAction
    ),
  productionUrl: (prev, context) => {
    return `http://example.com/${context.document?.slug?.current || '404.html'}`
  },
},
```

[More about Actions & Badges ->](/docs/archive/custom-workflows)

### Auth

This property lets you implement custom authentication by providing a configuration object that conforms to the [AuthConfig](https://www.sanity.io/docs/reference/api/sanity/AuthConfig) signature.

```javascript
import {defineConfig} from 'sanity'
/* ... */

auth: {
  redirectOnSingle: false,
  mode: 'append',
  providers: [
    {
      name: 'vandelay',
      title: 'Vandelay Industries',
      url: 'https://api.vandelay.industries/login',
      logo: '/static/img/vandelay.svg'
    }
  ],
  loginMethod: 'dual',
}
```

[More about Authentication ->](/docs/studio/custom-auth)





# Workspaces

Sanity Studio can accommodate multiple workspaces, each with its very own configuration. To set up a studio with more than one workspace, simply supply an array of configurations to `defineConfig` instead of a single config object.

```javascript
// Multiple workspace configuration
import {defineConfig} from 'sanity'
import {RobotIcon, RocketIcon} from '@sanity/icons'
import {structureTool} from 'sanity/structure'
import {schemaTypes} from './schemas'

export default defineConfig([
  {
    projectId: '<projectId>',
    dataset: 'production',
    name: 'production-workspace',
    basePath: '/production',
    title: 'Default Workspace',
    subtitle: 'production',
    icon: RobotIcon,
    plugins: [structureTool()],
    schema: {
      types: schemaTypes,
    },
  },
  {
    projectId: '<projectId>',
    dataset: 'staging',
    name: 'staging-workspace',
    basePath: '/staging',
    title: 'Another Workspace!',
    subtitle: 'staging',
    icon: RocketIcon,
    plugins: [structureTool()],
    schema: {
      types: schemaTypes,
    },
  },
])
```

The studio will pick up your new workspace and display a dropdown next to the studio title in the navbar to let you quickly switch between workspaces.

![Shows an active popover menu next to the studio title in the navbar. The dialog lists two workspaces, whereof the first is indicated to be currently active](https://cdn.sanity.io/images/3do82whm/next/1a5330749ab0b1793ab7bdb1fbd4d55d6dfbe33f-2144x1388.png)

## Workspace configuration properties

[Studio configurations](/docs/studio/configuration) and workspace configurations are the same thing. We refer to them as *studio configs* when there's only one configuration, and as *workspace configs* when there are multiple configurations. 

In practice, all configuration properties are workspace configuration properties. There are a few properties that, while legal and valid also for single workspaces, don't have actual value outside the context of a multi-workspace setup. For more information, see [this reference article](/docs/studio/config-api-reference).



# Schema and forms

You write schemas in plain JavaScript (or TypeScript) objects that let you describe your content model in Sanity Studio. The studio creates the forms and inputs needed to create and edit your content and stores it as structured content in [your dataset in the schema-less Content Lake database](/docs/content-lake/datasets).

[Schema types reference](/docs/studio/schema-types)

[Introduction to schemas](/docs/apis-and-sdks/introduction-to-schemas)



## Anatomy of Schemas:

**Content model or Schema**: More of a conceptual thing like a diagram that details documents and their schema types or attributes present in a studio

**Document Types:** A collection of schema types used to build a standalone piece of content that typically consist of multiple fields, has a revision history, can be published, drafted, and can have queryable references between them. 

> [!WARNING]
> Gotcha
> Document types can be whatever you like, and does not have to map to “a page” or “a post”.

**Form:** The order and structure of the schema types used for a document

**Schema types:** Catch-all for attributes used on a form to make a document and maps to `schema.types` in the config API. Sometimes we use “field types” to talk about the same concept.

### Schemas are Content Models

![A diagram of a schema that describes its parts, Content or Document Types and these types are made of field or schema types. ](https://cdn.sanity.io/images/3do82whm/next/d64b5188da8e1f9d53f2aefab79ac0a1160e1f62-882x372.png)

When we talk about a schema or the content model, we are referring to all the document types created for a studio. All document types are made of schema/field types. In short form:

#### Schema —> All Document Types —> Use Schema/Field Types

### Example: Your first schema

Schemas are declared in the root configuration for your Sanity Studio project, typically found in a file named `sanity.config.js`. 

```javascript
// ./sanity.config.js

import {defineConfig} from 'sanity'
import {deskTool} from 'sanity/desk'
import {schemaTypes} from './schemas'

export default defineConfig({
  name: 'default',
  title: 'my-studio',

  projectId: '<project-id>',
  dataset: 'production',

  plugins: [deskTool()],

  schema: {
    types: schemaTypes,
  },
})

```

You *can* declare your schemas inline in the configuration as an array of JavaScript objects, but the more common practice is to put your schemas in external files and import them into the `schema.types`-array.

The default studio setup when you create a new project in the CLI will include a folder at the root level called `schemas` with a single file called `index.js`. This file exports an array, which – depending on whether you started from a template or with a clean project – might be empty or might already contain some schemas.

## Example schema

Let's look at an example schema for a `person` content type. We'll add a `string` field for the name of the person and an `image` field for a headshot.

```javascript
// ./schemas/person.js

export default {
  name: 'person',
  title: 'Person',
  type: 'document',
  fields: [
    {
      name: 'fullName',
      title: 'Full name',
      type: 'string',
    },
    {
      name: 'portrait',
      title: 'Portrait',
      type: 'image',
      options: {
        hotspot: true,
      }
    }
  ]
}
```

We then import our schema into our main schema file (`./schemas/index.js`) and add it to the schema array.

```javascript
// ./schemas/index.js
import person from './person'

export const schemaTypes = [person]
```

The studio will now let you create a new "Person", and provide the form inputs needed to equip your person with a name and a photo.

![Studio screenshot showing a form with a string input for "Full name" and an image input for "Portrait"](https://cdn.sanity.io/images/3do82whm/next/dfb34aef1e43072e1a8cacd82ab35870c2df7693-800x791.png)

Your studio comes with a range of default schema types, such as the ones we just used to create this document, and these types can be combined to create an endless amount of data structures.



# Conditional fields

Sometimes you want to reduce the cognitive load and the complexity of a content form by controlling the visibility of its fields, or make certain fields read-only under certain conditions. You can make fields in Sanity Studio's form appear and disappear using the `hidden` property, and make them read-only with the `readOnly` property on all field types. This also works on fieldsets. This feature is commonly referred to as “conditional fields”. The `hidden` and `readOnly` properties can take a static `true` or `false` value or a callback function that contains the specific logic you want and returns `true` or `false` accordingly.

## Examples

### Hide based on a value in the current document

Only show the `subtitle` field if the `title` field is [truthy](https://developer.mozilla.org/en-US/docs/Glossary/Truthy):

```javascript
{
  name: 'subtitle',
  type: 'string',
  title: 'Subtitle',
  hidden: ({document}) => !document?.title
}
```

### Set "read-only" based on the current user's role

Only show the `productSKU` field if the current user is an administrator:

```javascript
{
  name: 'productSKU',
  type: 'string',
  title: 'SKU',
  readOnly: ({currentUser}) => {
    return !(currentUser.roles.find(({name}) => name === 'administrator'))
  }
}
```

### Hide based on a value in a sibling field

Show or hide a sibling field if it's empty and the current field has a value:

```javascript
{
  name: 'link',
  type: 'object',
  title: 'Link',
  fields: [
    {
      name: 'external',
      type: 'url',
      title: 'URL',
      hidden: ({ parent, value }) => !value && parent?.internal
    },
    {
      name: 'internal',
      type: 'reference',
      to: [{ type: 'route' }, { type: 'post' }],
      hidden: ({ parent, value }) => !value && parent?.external
    }
  ]
}
```

### Set "read-only" on an entire field set

```javascript
{
  name: 'product',
  type: 'document',
  title: 'Product',
	fieldsets: [
    {
      name: 'links',
      title: 'Links',
      options: {columns: 2},
      readOnly: ({document}) => document?.title === 'Hello world',
    }
  ],
  fields: [
		 {
      name: 'title',
      type: 'string',
      title: 'Title',
    },
    {
      name: 'external',
      type: 'url',
      title: 'URL',
			fieldset: 'links'
    },
    {
      name: 'internal',
      type: 'reference',
      to: [{ type: 'route' }, { type: 'post' }],
			fieldset: 'links'
    }
  ]
}
```

> [!WARNING]
> Gotcha
> You can't return a promise to the hidden or readOnly properties. This is because of performance optimizations.

> [!TIP]
> Editor experience
> Be mindful that Sanity Studio is a real-time collaborative application. That means that someone else can make a condition true that hides the field you're currently working in. You can consider mentioning if a field has a condition in its description, or by letting the content team know.

## Reference

### Callback properties

The `hidden` callback function takes an object as an argument with the following properties:

#### Properties

| Property | Description |
|----------|-------------|
| document | The current state of the document with all its values. Remember that it can return undefined. You can use optional chaining to avoid errors in the console, for example, document?.title. |
| parent | The values of the field's parent. This is useful when the field is part of an object type. Remember that it can return undefined. You can use optional chaining to avoid errors in the console, for example, parent?.title. 

If it's a root field, it will contain the document's values. |
| value | The field's current value. |
| currentUser | The current user with the following fields:

email (string)

id (string)

name (string)

profileImage (string)role (string)

roles (array of objects with name, title, description) |




# Field Groups

When editing documents in the Studio, it can sometimes be helpful to show certain fields together to provide context and alleviate visual input overload. Document and object types accept a `groups` property that you use to define the groups you want and you can assign fields to appear in the groups you have defined using the `group` property on a field. Fields can also appear in more than one group.

Let's, for example, say you have a long document and want to focus on the fields related to SEO. To achieve this, we first define an SEO group in our document's properties and then add the property `group: 'seo'` to a field to make it appear in the SEO group:

![Show a document in the studio in both default and groups view](https://cdn.sanity.io/images/3do82whm/next/149e899dd23c754bb1252fa20e55cce0beb6cee9-2521x1024.png)

> [!TIP]
> Protip
> Adding default: true to the object setup in groups: [] will make it the default field group.

The schema to produce the document structure in the example above might look like this (note the `groups` property on the document itself, as well as the `group` property on the fields related to SEO):

```javascript
export default {
  name: 'article',
  title: 'Article',
  type: 'document',
  groups: [
    {
      name: 'seo',
      title: 'SEO',
    },
  ],
  fields: [
    {name: 'title', title: 'Title', type: 'string'},
    {name: 'icon', title: 'Icon', type: 'image'},
    {
      name: 'related',
      title: 'Related',
      type: 'array',
      of: [{type: 'reference', to: [{type: 'article'}]}],
    },
    {name: 'seoTitle', title: 'SEO title', type: 'string', group: 'seo'},
    {name: 'seoKeywords', title: 'Keywords', type: 'string', group: 'seo'},
    {name: 'seoSlug', title: 'Slug', type: 'slug', group: 'seo'},
    {name: 'seoImage', title: 'Image', type: 'image', group: 'seo'},
  ],
}
```

Fields can belong to more than one group. Expanding on our previous example, let's say we would like another view showing only fields that include images. We might then create a new group called Media and add all the fields with a graphic element to it:

![Shows the studio displaying image fields grouped under "Media"](https://cdn.sanity.io/images/3do82whm/next/99483b322b67a0540797c31e3761330764cd3436-1522x1236.png)

To do this, we'd add another group called Media in `groups`, and change the `group` property on our `icon` and `seoImage` fields to be an array of strings instead of a single string:

```javascript
export default {
  name: 'article',
  title: 'Article',
  type: 'document',
  groups: [
    {
      name: 'seo',
      title: 'SEO',
    },
    {
      name: 'media',
      title: 'Media',
    },
  ],
  fields: [
    {name: 'title', title: 'Title', type: 'string'},
    {name: 'icon', title: 'Icon', type: 'image', group: 'media'},
    {
      name: 'related',
      title: 'Related',
      type: 'array',
      of: [{type: 'reference', to: [{type: 'article'}]}],
    },
    {name: 'field1', title: 'SEO title', type: 'string', group: 'seo'},
    {name: 'field2', title: 'Keywords', type: 'string', group: 'seo'},
    {name: 'field3', title: 'Slug', type: 'slug', group: 'seo'},
    {name: 'seoImage', title: 'Image', type: 'image', group: ['seo', 'media']},
  ],
}
```

> [!TIP]
> Protip
> Using field groups in a document or object does not change the structure of the document, it only affects how and where fields appear in the Studio. 

In addition to `document`s, field groups can also be defined on `object`s.

> [!WARNING]
> Gotcha
> A field inside an object, cannot appear in a group by itself.

## Conditional field groups

It can be useful to make certain groups appear or hide based on certain conditions. A group can be conditionally hidden using the boolean values `true` or `false`, but you can also pass a function. This function passes `currentUser`, `value`, and `parent` as arguments, where `value` is the values of the current group and `parent` is an array of all the groups defined in the document or object.



## Reference

Property: `groups`

Type: `array`

Defined on `document` or `object`

```javascript
groups: [
	{
		name: 'groupName',
		title: 'Group title',
		icon: CogIcon, // optional
		default: true, // optional, defaults to false
	  hidden: ({currentUser, value, parent}) => true // optional
	}
]
```



Property: `group`

Type: `string` or `array`

Defined on a field

```javascript
{
	name: 'fieldName',
	title: 'Field title', 
	type: 'string',
	group: 'groupName' // or ['groupName']
}
```



# List Previews

Sanity Studio will often need to render a compact representation of a document or object for list views and similar situations, and we call this a *list preview*. You can decide which fields should be used and how by configuring the `preview` property on schema types. By default, Sanity Studio tries to guess which fields should be used for preview by introspecting the type's defined fields. For example, if your type has a field of type `string` named `title`, it will infer that this should be used as the title when previewing values of this type.

Sanity Studio offers two ways of customizing how documents and objects are previewed:

3. Specify preview options for the type in the schema for lists and arrays to use automatically
3. Implement a custom preview component to display when used as block content in the Portable Text Editor

> [!TIP]
> Protip
> Looking to create previews inside of the document pane? Read more on creating custom content previews inside split panes with the Structure Builder API.
> 
> For previews of content presentation in front ends, go to the documentation for Visual Editing and Presentation.

## Configuring preview options

Normally, a list preview has three "slots": title, subtitle, and media. If you want to specify which fields should be used for what, you can control this by adding a `preview` key to the type defined in the schema. For example:

```javascript
export default {
  name: 'movie',
  type: 'document',
  fields: [
    {
      title: 'Title',
      name: 'title',
      type: 'string'
    },
    {
      title: 'Release Date',
      name: 'releaseDate',
      type: 'date'
    }
  ],
  preview: {
    select: {
      title: 'title',
      subtitle: 'releaseDate'
    }
  }
}
```

Above, the `preview.select` object will inform the Sanity Studio preview logic that for this document, `movie.title` should be used as `title` and `movie.releaseDate` should be used as `subtitle`.

This might be sufficient in many cases, but sometimes, you want to reformat the selected values. With the `prepare` function, you can access the values that you have selected and customize them.

Say we only want the year for `releaseDate` (e.g., 2016-04-25):

```javascript
export default {
  name: 'movie',
  type: 'document',
  fields: [
    {
      title: 'Title',
      name: 'title',
      type: 'string'
    },
    {
      title: 'Release Date',
      name: 'releaseDate',
      type: 'datetime'
    }
  ],
  preview: {
    select: {
      title: 'title',
      date: 'releaseDate'
    },
    prepare(selection) {
      const {title, date} = selection
      return {
        title: title,
        subtitle: new Date(date).getFullYear() // YYYY-MM-DD --> YYYY
      }
    }
  }
}
```

Above, `title` and `releaseDate` are selected. The result of this selection is passed to the `prepare` function, where you can transform the selection however you like (only keeping the year, in this case).

> [!TIP]
> Protip
> In these examples we have put the preview object after the fields array, however you can also place it before it. This might give you a better idea at first glance of how the document is previewed.

## Preview using fields from referenced documents

You can follow references by using dot notation to the related document field you want to display in `preview.select`. Note that using GROQ joins *is not supported* here (it’s what the Studio will do under the hood).

Here's an example of a preview for a movie document where the `director` field is a reference, and the referenced document has a `name` field:

```javascript
export const movie = {
  name: 'movie',
  type: 'document',
  fields: [
    //...other fields
    {
      name: 'director',
      type: 'reference',
      to: [{ type: 'person' }]
    }
  ],
  preview: {
    select: {
      title: 'title',
      director: 'director.name' // if the movie has a director, follow the reference and get the name
    },
    prepare(selection) {
      const {title, director} = selection
      return {
        title: title,
        subtitle: `Directed by: ${director ? director : 'unknown'}`
      }
    }
  }
}
```

## Previewing from predefined string lists

When using a [predefined list of strings](https://www.sanity.io/docs/string-type#33spwYgc), you can use objects with `title` and `value` keys. This might be useful if you're using a list of U.S. states, for example: The `title` can be the spelled-out state, while the `value` can be a two-letter state code:

```javascript
{
  title: 'U.S. State',
  name: 'state',
  type: 'string',
  options: {
    list: [
      { "title": "Alabama", "value": "AL"},
      { "title": "Alaska", "value": "AK"},
      { "title": "Arizona", "value": "AZ"},
      // ...
    ],
    layout: 'dropdown'
  }
}
```

If you wish to use that value in your preview, Sanity will default to providing the `title`—*unless you use a *`prepare()`* function*. In that case, the `value` (and *only* the `value`) will be passed along to `prepare()`.

If you want to render the `title` in your document preview but need to manipulate it in some way (which is done using `prepare()`, as seen in the [second example above](#770fd57a8f95)), you can specify your list outside of the schema, use it as your list in `options.list`, and then consult that list in your `prepare()` function. This is best explained via an example:

```javascript
const STATES = [
  { "title": "Alabama", "value": "AL"},
  { "title": "Alaska", "value": "AK"},
  { "title": "Arizona", "value": "AZ"},
  // ...
]

export default {
  // ...
  fields: [
    // ...
    {
      name: "state",
      title: "U.S. State",
      type: "string",
      options: {
        list: STATES,
        layout: "dropdown",
      },
    }
  ],
  preview: {
    select: {
      state: 'state',
    },
    prepare: ({ state }) => {
      const stateName = state && STATES.flatMap(option => option.value === state ? [option.title] : [])
      return {
        title: state ? `${state} is ${stateName}` : 'No state selected',
      }
    }
  }
}
```

## Previewing from array values

Fetching entire arrays of values can potentially result in large and complex responses, especially in the case of large arrays. We encourage you only to select a subset of the array values:

```javascript
export default {
  name: 'book',
  type: 'document',
  fields: [...],
  preview: {
    select: {
      title: 'title',
      author0: 'authors.0.name', // <- authors.0 is a reference to author, and the preview component will automatically resolve the reference and return the name
      author1: 'authors.1.name',
      author2: 'authors.2.name',
      author3: 'authors.3.name'
    },
    prepare: ({title, author0, author1, author2, author3}) => {
      const authors = [author0, author1, author2].filter(Boolean)
      const subtitle = authors.length > 0 ? `by ${authors.join(', ')}` : ''
      const hasMoreAuthors = Boolean(author3)
      return {
        title,
        subtitle: hasMoreAuthors ? `${subtitle}…` : subtitle
      }
    }
  }
}
```

> [!WARNING]
> Gotcha
> Resolving references in arrays works the same as covered above, with dot notation.

## Selecting an image field to use for the thumbnail

The easiest way to show an image in the preview is to assign a field containing an image to the `media` property. The different views take care of a proper rendering of the image, including any `hotspot` and `crop` specifics. 

```javascript
export default {
  name: 'person',
  type: 'document',
  fields: [...],
  preview: {
    select: {
      title: 'name',
      media: 'userPortrait' // Use the userPortait image field as thumbnail
    }
  }
}
```

## Rendering React components

You can also use JSX to render a thumbnail. Here's an example of how to show specific emojis based on the status of our document. This example is partly taken from our [Community Studio](/blog/how-we-manage-community-support-with-sanity).

```jsx
// src/schemaTypes/ticket.jsx

export const ticket = {
  name: 'ticket',
  type: 'document',
  fields: [...],
  preview: {
    select: {
      title: 'title',
      summary: 'summary',
      status: 'status'
    },
    prepare({ title, summary, status }) {
      const EMOJIS = {
        open: '🎫',
        resolved: '✅',
        cancelled: '🚫'
      }
      return {
        title: title,
        subtitle: summary,
        media: <span style={{fontSize: '1.5rem'}}>{status ? EMOJIS[status] : '🎫'}</span>
      }
    }
  }
}
```

## Custom preview component 

If you want complete control of how the document or object list preview is rendered, you can also provide a React component invoked when the document or object is previewed in that context. 

> [!WARNING]
> Gotcha
> Custom preview components will only display in lists that appear inside the document pane—not in the top-level Structure tool document list.

To learn more about this option, visit the [article on form components](/docs/studio/form-components).

## Preview in the Studio

Depending on how your schema is set up, here is an example of how `Preview` could look in your Studio. This uses `title`, `subtitle`, and `media`.

![View of Preview in the Studio](https://cdn.sanity.io/images/3do82whm/next/9fc7748d46db973027e0cce787cadc8f5661f23c-1152x700.png)

## Form preview title

![The form title preview displaying "They Are Gutting a Body of Water — Lucky Styles (2022, Smoking Room)](https://cdn.sanity.io/images/3do82whm/next/3013ebad552fa130fb391387d396f47e22e7ea82-1362x560.png)

Since v3.24.1, Sanity Studio has also rendered a large title in the document form to make it easier to discern which document you are currently in. It shares the logic with list previews, looking for a `preview` configuration and returning to the inferred preview title. In cases you don't wish to have this title, you can turn it off: 

```typescript
// src/schemaTypes/location.ts

export const location = {
  name: 'location',
  title: 'Location',
  type: 'document',
  __experimental_formPreviewTitle: false,
  fields: [
    //..fields
  ],
}
```



# Connected Content

## What is a “reference”?

When we talk about references in the context of Sanity, we usually mean one of three related things:

- The reference field type in the studio's schema files
- The field UI that you'll interact within the studio
- The specific data shape that you'll find in the JSON documents that holds your content

You can put reference fields inside a document type, object, and array fields, as well as inside annotations in the [Portable Text](/docs/studio/portable-text-editor-configuration) editor.

### Typical use cases for references:

You can use references pretty much anywhere it makes sense to connect two pieces of content, but let's look at some common patterns and use cases:

- An `author` field that references a `person` document
- An `internalLink` field that references document types like `route`, `product`, `post`, `service`
- Taxonomy fields that reference `tag` and `category` documents
- A `parent` field that establishes a hierarchical structure
- A `related` field that references documents of the same type

### References are always bi-directional

A reference will always point to another *document*. The [Content Lake](/docs/content-lake) will index references bi-directionally, meaning that you can query them from “both sides.” If you are used to database terminology, this means that the Content Lake acts more like a graph database than a relational database. If you're not, it means that any document in your Content Lake can be connected to any other document by a reference!

In other words, as long as you have a reference inside of a document pointing to another, you can ask the Content Lake to return all the data of a document and include the content of the document it refers to, but you can also ask it to return all the documents that contain a reference to a particular document – sometimes referred to as “incoming references.” There are also ways of querying for documents that have a reference to another document in a *given* field.

Hence, where to place a reference field is mostly a consideration for the editorial experience. From where does it makes sense to manage the references? Typically, some document types will be relevant in many different contexts. Typically you want references to point to these.

#### Example

Let's say you have document types for `post` and `person`. While you could put an array of references to `post` on the `person` document type, that would be cumbersome when authoring the post. To connect it to a `person`, that is, its author, you would have to go to the given document and add the new post to the array. Hence, it's "natural" to make a field called `author` that's a reference to the type `person` on the `post` document type.

```javascript
// post.js
export default {
  name: 'post',
  type: 'document',
  fields: [
    // other fields
    // ...
		{
		  name: 'author',
      type: 'reference',
			title: 'Author',
      to: [{type: 'person' }]
    }
  ]
}
```

> [!TIP]
> Protip
> While we're demonstrating a single author field in the code above, it's often wise to make it an array field called authors that can hold multiple references to persons. It's likely that you'll need support for multiple authors at some point.

These fields will produce a data structure that looks like this:

```json
{
  "_type": "post",
  "author": {
     "_type": "reference",
     "_ref": "82b75d44-13af-4351-90ee-13045f84cf3b",
	}
}
```

The value of the `_ref` property is the `_id` of the document it's referencing.

### Referential integrity

Not only are your references indexed and queryable; The Sanity Content Lake will also make sure that they keep their integrity. That means that it will prevent you from deleting a document that is referenced elsewhere. This simplifies implementing connected content, and enables you to have confidence in the structure of your data.

Sometimes you don't need this guarantee while you want to keep the convenience of references. Referential integrity can be turned off by adding the `weak: true` property to a reference field configuration. This will add `_weak: true` to the reference object in the data, which is also the way to convert a strong reference to a weak one programmatically (notice the underscore `_` that signifies a special Content Lake property).

```json
// Example of a weak reference. The referenced document can be deleted because this reference is set as weak.
{
  "_type": "feedback",
	"message": "This was a great article!",
  "article": {
     "_type": "reference",
     "_ref": "4049517c-3258-4747-8e00-2956ca5b894b",
     "_weak": true
	}
}
```

If you use weak references and a reference field points to a non-existent document, this will show up for editors in the studio as a warning:

![Screenshot of the warning displayed in the studio when referencing a non-existent document via a weak reference: “Non-existent document: This is currently referencing a document that doesn’t exist (ID: <id>). You can either remove the reference or replace it with another document.”](https://cdn.sanity.io/images/3do82whm/next/c7a91f5de08b670967c866f15ba0540dc5a8f216-678x297.png)

## Working with references in the studio

A reference field in the studio lets you do mainly four things:

- Search and select a document you want to reference
- Create a new document that can be referenced
- Open an already referenced document in a new pane next to the current one
- Delete a reference to a document

### Adding references

Adding a reference to an existing document is fairly straightforward. You click into the reference field and type into it to search for a document from most of its text-based fields. The studio will create the reference when you select the document. If the document you selected is a draft that has never been published, the studio will still make the connection, but it will block the referring document from publishing until the referenced document has been published (unless the reference field has the `weak` option set to true).

### Create and edit documents in place

Wanting to reference some piece of content you haven't created yet is fairly common. A simple example is when a new author is creating their first post and they don't have their own `author` document yet. In many systems, they'll have to leave the current document that they're editing, create and publish a new author document, and then get back to where they left. The reference fields for Sanity Studio remove this friction by letting you create and edit documents in a new pane next to the current.

![Shows the studio workflow for editing a reference in place](https://cdn.sanity.io/images/3do82whm/next/cc85c96f60addcc0d7b2ea95131d6f299d107089-1304x1220.png)

## Querying connected content

The true potential of connected content is revealed when you start taking advantage of references in your queries. Using GROQ, you can follow any reference and include any value from that document in your result. *Including references*, which means you can follow the trail from a `book` via its reference to an `author` which might include references to `award`s they've won, which might have been referenced by other `author`s who have won that same `award`.

```groq
// For books by any award winning author
// return title of book
// follow the author reference to get their name
// follow the award reference via the author to get the award title
// finally list names of other authors who have received the same award

*[_type == "book" && defined(author->award)] {
  title,
 "By: ": author->name,
 "Winner of: ": author->award->title,
 "Also won by: ": 
		*[_type == "author" && references(^.author->award._ref) ].name
}
```

The query above might yield a result like this:

```json
[
  {
    "title": "One Flew Over The Cuckoos Nest"
    "By: ": "Ken Kesey",
    "Winner of": "Pulitzer",
    "Also won by": [
      "Astrid Lindgren",
      "Niccolo Machiavelli",
      "Terry Pratchett"
    ],
  },
	...
]
```

While this example might seem a bit convoluted (and probably wouldn't stand up to the scrutiny of the Pulitzer board), it also demonstrates how using references can reveal patterns and possibilities in connected content using only a few lines of GROQ.

Delving further into the syntax and features of GROQ is beyond the scope of this article, but rest assured that we have [ample docs and examples](/docs/groq-reference) on the possibilities afforded by references in your queries.

## Further Reading

- [How joins in GROQ work](https://www.sanity.io/docs/how-queries-work#52023b22ca05)
- [Content Modelling in Sanity Studio](https://www.sanity.io/docs/content-modelling)
- [Designing Connected Content: Plan and Model Digital Products for Today and Tomorrow](https://www.pearson.com/us/higher-education/program/Hane-Designing-Connected-Content-Plan-and-Model-Digital-Products-for-Today-and-Tomorrow/PGM1816611.html)



# Validation

Sanity Studio allows you to specify validation rules on your document types and fields. Field-level validation is the most specific and gives the Studio a better chance to help the user understand where the validation failed and why, whereas the document-level validation provides slightly more control since it can validate based on the values of the entire document.

Each schema type has a set of built-in validation methods. [See the schema type documentation for a detailed list →](/docs/schema-types)

> [!TIP]
> Protip
> Validation rules run in the Studio as an editor is working in a document. You can also validate multiple documents in bulk using the CLI.

## Basics

Validation is defined by setting the `validation` property on a document type or field. It takes a function which receives a rule as the first argument. By calling methods on this rule, you add new validation modifiers. Here's an example which validates that a string field has a value and that the string is between 10 and 80 characters long:

```javascript
{
  title: 'Title',
  name: 'title',
  type: 'string',
  validation: rule => rule.required().min(10).max(80)
}
```

Without the `required()` call, the title is also considered valid if it does not have a value.

## Error levels and error messages

By default, values that do not pass the validation rules are considered errors - these will block the draft from being published until they have been resolved. You can also set a rule to be a warning, simply by calling `warning()` on the rule. Similarly, you can customize the error message displayed by passing a string to the `warning()` or `error()` method:

```javascript
{
  title: 'Title',
  name: 'title',
  type: 'string',
  validation: rule => rule.max(50).warning('Shorter titles are usually better')
}
```

If you want to combine both warnings and errors in the same validation set, you can use an array:

```javascript
{
  title: 'Title',
  name: 'title',
  type: 'string',
  validation: rule => [
    rule.required().min(10).error('A title of min. 10 characters is required'),
    rule.max(50).warning('Shorter titles are usually better')
  ]
}
```

## Referencing other fields

Sometimes you may want to build a rule that is based on the value of a different field. By calling the `rule.valueOfField` method, you can achieve this.

```javascript
{
  title: 'Start date',
  name: 'startDate',
  type: 'datetime',
  validation: rule => rule.required().min('2022-03-01T15:00:00.000Z')
},
{
  title: 'End date',
  name: 'endDate',
  type: 'datetime',
  validation: rule => rule.required().min(rule.valueOfField('startDate'))
}
```

Note however that it only allows referencing sibling fields. If you need to refer to things outside of this scope, you will have to use document-level validation.

> [!WARNING]
> Gotcha
> rule.valueOfField() returns the literal value of a field, allowing you to validate that the end date is always equal to or greater than the start date (as in the previous example). However, it cannot be used for inserting a field value into conditional logic and creating a validation based on the result.

## Custom validation

Sometimes you will need to validate values beyond what Sanity provides. The `custom()` method allows you to do this. It takes a function as the first argument, which should return either `true` (in the case of a valid value) or an error message as a string (in the case of an invalid value). You may also return a promise that resolves with one of those values, should you need to do asynchronous operations:

```javascript
{
  name: 'location',
  type: 'geopoint',
  title: 'Location of bar',
  description: 'Required, must be in Norway',
  validation: rule =>
    rule.required().custom(geoPoint =>
      someGeoService
        .isWithinBounds(
          {
            latitude: geoPoint.lat,
            longitude: geoPoint.lng
          },
          someGeoService.BOUNDS_NORWAY
        )
        .then(isWithinBounds => (isWithinBounds ? true : 'Location must be in Norway, somewhere'))
    )
}

```

Please note that custom validators are also run on undefined values, unless the rule is explicitly set as optional by calling `rule.optional()`. This allows for conditionally allowing undefined values based on some external factor, with the slight drawback that you need to make sure your functions check for undefined values. Here's an example:

```javascript
{
  name: 'breweryName',
  type: 'string',
  title: 'Brewery name',
  validation: rule => rule.custom(name => {
    if (typeof name === 'undefined') {
      return true // Allow undefined values
    }
    
    // This would crash if we didn't check
    // for undefined values first
    return name.startsWith('Brew')
      ? 'Please be more creative'
      : true
  }).warning()
}

```

Should you need to reference other fields from within the custom validator function, you can use the second argument (`context`) to the function:

```javascript
{
  name: 'durationInMinutes',
  type: 'number',
  title: 'Duration of talk, in minutes',
  validation: rule => rule.custom((duration, context) => {
    const isShortTalk = duration && duration <= 10
    if (isShortTalk && context.document.talkType !== 'lightning') {
      return 'Only lightning talks should be 10 minutes or less'
    }
    
    return true
  })
}
```

You can also access the closest `parent` from the context, along with the `path` of the current element being validated.

### Asynchronous validation using the client

If you want to base your rule on another part of your content, you can access the client via the validation context.

```javascript
validation: (Rule) =>
  Rule.custom((value, context) => {
    const client = context.getClient({apiVersion: '2021-03-25'}).withConfiguration({perspective: 'previewDrafts'})
    // rest of your rule
},
```

### Validating children

In certain cases, you may want to validate children of an object or array. In this case you can return an object containing a `message` and a `paths` property. Each path is an array of *path segments* leading to the child you want to flag as being the culprit. Let's say you want to disallow empty blocks/paragraphs in a portable text field:

```javascript
{
  name: 'introduction',
  title: 'Introduction',
  type: 'array',
  of: [{type: 'block'}],
  validation: rule => rule.custom(blocks => {
    const emptyBlocks = (blocks || []).filter(
      block =>
        block._type === 'block' &&
        block.children.every(span =>
          span._type === 'span' &&
          span.text.trim() === ''
        )
    )
    
    const emptyPaths = emptyBlocks.map(
      (block, index) => [{_key: block._key}] || [index]
    )

    return emptyPaths.length === 0
      ? true
      : {
          message: 'Paragraph cannot be empty',
          paths: emptyPaths
        }
  })
}
```

For each of the empty blocks we find, we collect the path to it, which can either be the `_key` property (preferably), or the array index if a key cannot be found.

## Document level validation

Sometimes you want to validate a whole document rather than just specific fields in a document. To do this, you can give a document the `validation` property and access fields inside the document by passing a prop. In this example, the validation ensures that editors can't add a "Guest Author" and an "Author."

```javascript
{
  name: 'post',
  type: 'document',
  title: 'Blog Post',
  validation: rule => rule.custom(fields => {
    if (fields.authors.length > 0 && Object.keys(fields.guest).length > 0) return "You can't have an author AND guest author"
    return true
  }),
  fields: [
    // ... 
    {
      name: 'authors',
      title: 'Authors',
      type: 'array',
      of: [
        {
          type: 'authorReference',
        }
      ]
    },
    {
      name: 'guest',
      title: 'Guest Author',
      type: 'object',
      fields: [
        {name: 'name', type: 'string', title: 'Guest Author Name'},
        {name: 'site', type: 'string', title: 'Guest Author Site'},
      ],
    },
  ]
}
```

### Marking nested fields as invalid

Similar to the example in "Validating children" above, you can return an object to specify what field the message should apply to when using document level validation.

```javascript
{
    name: 'post',
    type: 'document',
    title: 'Blog Post',
    validation: (rule) =>
        rule.custom((fields) => {
            if (
                fields.authors.length > 0 &&
                Object.keys(fields.guest).length > 0
            )
                return {
                    message: "You can't have an author AND guest author",
                    path: ['guest'], // add keys to array for nested fields, ex ['guest', 'title'] for guest.title
                }
            return true
        }),
    fields: [
        // ...
        {
            name: 'authors',
            title: 'Authors',
            type: 'array',
            of: [
                {
                    type: 'authorReference',
                },
            ],
        },
        {
            name: 'guest',
            title: 'Guest Author',
            type: 'object',
            fields: [
                { name: 'name', type: 'string', title: 'Guest Author Name' },
                { name: 'site', type: 'string', title: 'Guest Author Site' },
            ],
        },
    ],
}
```

## Disabling validation

You can set `validation: false` in the schema on a document or a field to disable it.



# Initial Value Templates

By default, whenever you create a new document in the studio, the document will be initialized with empty values for every field (actually: `undefined`). Sometimes this is not what you want. In this article, we'll look at some of the ways you can set your documents with some prefilled initial values.



There are two ways of defining initial values, depending on what you want to achieve:

4. Define a single set of initial values to apply to all new documents of the same type
4. Define a set of different templates to choose from when creating a new document

## Define a single set of initial values

If you always want a particular document type to have a single set of initial values, the simplest way to do this is by specifying the `initialValue` property on a document type. In the following example, the `project` schema type is given the value `false` for the `isHighlighted` field.

```javascript
export default {
  name: 'project',
  type: 'document',
  title: 'Project',
  fields: [
    {
      name: 'title',
      title: 'Title',
      type: 'string'
    },
    {
      name: 'isHighlighted',
      title: 'Highlighted',
      type: 'boolean'
    },
    {
      name: 'releaseDate',
      title: 'Release date',
      type: 'datetime'
    }
  ],
  initialValue: {
    isHighlighted: false
  }
}
```

Sometimes you may want to compute property values. For instance, in the example above, you may want to set the `releaseDate` property to be the current date. You can do this by specifying a function for the `initialValue` property:

```javascript
export default {
  // ...
  initialValue: () => ({
    isHighlighted: false,
    releaseDate: (new Date()).toISOString()
  })
}
```

> [!WARNING]
> Gotcha
> The datetime example above will not work for a field of type date, since those should not include the time along with the date string. A similar strategy for date fields might look like this: 
> 
> initialValue: new Date().toISOString().splice(0, 10)

## Set an initial value for a specific field

In the creation of a field in a document, you can specify an `initialValue` for that specific instance of the field.

```javascript
export default {
  name: 'project',
  type: 'document',
  title: 'Project',
  fields: [
    {
      name: 'title',
      title: 'Title',
      type: 'string'
    },
    {
      name: 'isHighlighted',
      title: 'Highlighted',
      type: 'boolean',
      initialValue: false
    },
    {
      name: 'releaseDate',
      title: 'Release date',
      type: 'datetime'
    }
  ]
}
```

### Single-field examples

Initial values can be used on any schema type.

```javascript
export default {
  name: 'project',
  type: 'document',
  title: 'Project',
  fields: [
    {
      name: 'title',
      title: 'Title',
      type: 'string',
      initialValue: 'This string'
    },
    {
      title: 'title',
      name: 'myArray',
      type: 'array',
      initialValue: ['red', 'green'],
      of: [
          {type: 'string'},
      ],
    },
    {
      title: 'Complex Array',
      name: 'myArray',
      type: 'array',
      initialValue: [
        {
          // Required _type to tell the schema what fields to map
          _type: 'mySecondObject', 
          stringField: 'Starting string'
        }
      ],
      of: [
        {type: 'myFirstObject'},
        {type: 'mySecondObject'}
      ],
    },
    {
      name: 'myObject',
      title: 'My custom input',
      type: 'object',
      initialValue: {
        name: "some name",
        someField: "Some other thing"
      },
      fields: [
        {
            name: 'name',
            title: 'Title',
            type: 'string'
        },
        {
            name: 'someField',
            title: 'Something',
            type: 'string'
        },
      ],
    },
    {
      title: 'Custom type',
      type: 'myCustomObjType',
      name: 'myCustomObjType',
      initialValue: {
        customName: "This is a custom name",
        customString: "This is a custom string"
      }
    },
    {
      name: 'isHighlighted',
      title: 'Highlighted',
      type: 'boolean',
      initialValue: false
    },
    {
      name: 'releaseDate',
      title: 'Release date',
      type: 'datetime',
      initialValue: (new Date()).toISOString()
    }
  ]
}
```

## Initial value precedence

Initial values for nested types will be deeply merged. The initial value for the above `project` type will be resolved to something similar to:

```javascript
{
  "releaseDate": "2021-04-28T10:39:17.622Z",
  "isHighlighted": false
}
```

In a case where an object type declares initial values for a field that also defines an initial value, the object type's initial value will "win" and override the field's initial value.

In the following example, the resolved initial value for the project type will be `{"isHighlighted": true}`:

```javascript
export default {
  name: 'project',
  type: 'document',
  title: 'Project',
  fields: [
    {
      name: 'isHighlighted',
      title: 'Highlighted',
      type: 'boolean',
      initialValue: false
    },
    //...
  ],
  initialValue: {
    // this overrides the initial value defined on the field
    isHighlighted: true
  }
}
```

> [!TIP]
> Protip
> If you want to clear the initial value defined for a nested type, you can do this by setting the initial value to undefined.

## Define multiple templates

So far, we’ve only looked at customizing the initial value for all documents of a document type. Quite often, you'll want to provide a set of different templates that an editor can choose from.

> [!WARNING]
> Gotcha
> Initial value templates only applies to document types.

In this example, we'll be working directly in the `defineConfig` configuration of our imaginary studio, but you can of course choose to export your templates/template functions elsewhere as they grow unwieldy.

```javascript
// sanity.config.js|ts

import {defineConfig} from 'sanity'

export default defineConfig({
  //...rest of config
  schema: {
    templates: (prev, context) => {
        console.log(prev)
        // logs array of templates for existing types
        console.log(context);
        // logs: schema, currentUser, getClient, dataset, projectId
        return prev;
      }
    } 
  }
})
```

We’ve now recreated the exact same functionality that you get out of the box, but we also get a look at the templates that exist for our current types. For a document of type `movie` (such as you might find in the default "Movie project" example) the template looks like this:

```javascript
{
    id: "movie",
    schemaType: "movie",
    title: "Movie",
    value: {
        "_type": "movie"
    }
}
```

> [!NOTE]
> Behind the scenes
> This gives you an idea of what happens behind the scenes: each document type will generate a template for its type, which will produce either an empty object as its initial value or whichever value is set in the schema type definitions initialValue property.

Let's take what we've learned and make it useful by defining a custom initial values template.

In the following example, we include all the default templates, and then we add a template for documents of type `person` which will set up each new document with a prefilled value for the `role` field.

```javascript
// sanity.config.js|ts
import {defineConfig} from 'sanity'

export default defineConfig({
  // ...rest of config
  schema: {
    templates: (prev) => [
      ...prev,
      {
      id: 'person-developer',
      title: 'Developer',
      schemaType: 'person',
      value: {
        role: 'developer'
      },
    },
	],
})
```

When creating new documents, an editor will now get the option to create either a Person (the default template for the person type) or a Developer, which is the specific template we defined above with a pre-populated `role` property. The `value` property can also be defined as a function.

![Shows the "new document" option as a dropdown with two options: "Person" and "Developer"](https://cdn.sanity.io/images/3do82whm/next/a745d7910129fa7566bd6668fdd90a320d520981-384x158.png)

To polish this editor experience, it's a best practice to assign custom icons for templates to make them easier to differentiate. You can do this by setting the `icon` property:

```javascript
// sanity.config.js|ts
import {defineConfig} from 'sanity'
import {CogIcon} from '@sanity/icons'

export default defineConfig({
  // ...rest of config
  schema: {
    templates: (prev) => [
      ...prev,
      {
      icon: CogIcon,
      id: 'person-developer',
      title: 'Developer',
      schemaType: 'person',
      value: {
        role: 'developer'
      },
    },
	],
})
```

## Inserting objects or references as default values

Fields that are object types (or is a reference to another document) have a few rules that you need to keep in mind:

- In instances of nested objects, it's necessary to include a `_type` property to define what schema type to reference. For instance, if inserting a `geopoint`, you will set the field value to:
`{_type: 'geopoint', lat: 59.92409, lng: 10.7584}`
- To create a Reference initial value, the `_ref` property must be defined. This means that if you want to reference a specific document, you need to provide that document's ID as the value for `_ref`. `{_ref: 'document-id-to-reference'}`
- [Images](/docs/image-type) and [files](/docs/file-type) are represented as objects with a nested `asset` field (which is a reference to the actual asset document). Combining the two points above, the default value for an image field would look something like this:
`{_type: 'image', asset: {_type: 'reference', _ref: 'image-someId-200x300-jpg'}}`
- While objects in arrays should generally have a `_key` property, the initial value system will generate the `_key` property if it is not included in the provided value (using a randomly generated string).

## Resolving initial values asynchronously

The initial value can also be specified as an asynchronous function (a function returning a promise). This allows exciting things like running a request to an API to get data needed for the initial value. For instance:

```javascript
import axios from 'axios'

export default {
  // ...
  initialValue: async () => {
    const response = await axios.get('https://api.sanity.io/pets')
    return {favoritePetName: response.data[0].name}
  }
}
```

## Parameterized templates

A common use case is to populate fields based on a set of parameters. You can do this by defining a `parameters` array for your template. By defining the `value` property as a function, you'll get the parameters passed to the template as the argument to the function. Each item in the `parameters` array follows the same declaration as fields within a schema object type:

```javascript
// sanity.config.js|ts
import {defineConfig} from 'sanity'

export default defineConfig({
  //...rest of config
  schema: {
    templates: [
      {
        id: 'person-role',
        title: 'Person with role',
        schemaType: 'person',
        parameters: [
          {
            name: 'roleName',
            title: 'Role name',
            type: 'string',
          },
        ],
        value: (parameters) => ({
          role: parameters.roleName,
        }),
      },
    ],
  },
})

```

## Using templates in a structure

When defining your structure for the Structure tool (using the [Structure Builder](/docs/overview-structure-builder)), a common use case is creating filtered document lists. In these cases, you probably want the **new document** action to not start with a totally empty document but have pre-populated values that match the user is current structure.

In a dataset containing many books and their related authors, you may want to segment the books by author. To do this, you might create a structure that looks like this:

```javascript
// structure.js

export const structure = (S) =>
  S.list()
  .id('root')
  .title('Content')
  .items([
    S.listItem({
      id: 'books-by-author',
      title: 'Books by author',
      schemaType: 'book',
      child: () =>
        S.documentTypeList('author').child(authorId =>
          S.documentTypeList('book')
            .title('Books by author')
            .filter('_type == $type && author._ref == $authorId')
            .params({type: 'book', authorId})
        )
    }),
    ...S.documentTypeListItems()
  ])
```

This satisfies the navigation part, but it doesn’t select the author we want when creating a new document. For this to work, we need to first create a parameterized initial value template, then tell the structure to use it. Let’s first define the initial value template. In your `sanity.config.js`:

```javascript
// sanity.config.js|ts
import {defineConfig} from 'sanity'
import {structure} from './structure'

export default defineConfig({
 //...rest of config
  plugins: [
    structureTool({ structure }),
   ],
  schema: {
    templates: [
  		  {
      id: 'book-by-author',
      title: 'Book by author',
      description: 'Book by a specific author',
      schemaType: 'book',
      parameters: [{name: 'authorId', type: 'string'}],
      value: params => ({
        author: {_type: 'reference', _ref: params.authorId}
      })
    }
	],
 },
})
```

Now let’s use it in our structure:

```javascript
// structure.js

export const structure = (S) =>
  S.list()
  .id('root')
  .title('Content')
  .items([
    S.listItem({
      id: 'books-by-author',
      title: 'Books by author',
      schemaType: 'book',
      child: () =>
        S.documentTypeList('author').child(authorId =>
          S.documentTypeList('book')
            .title('Books by author')
            .filter('_type == $type && author._ref == $authorId')
            .params({type: 'book', authorId})
            .initialValueTemplates([
              S.initialValueTemplateItem('book-by-author', {authorId})
            ])
        )
    }),
    ...S.documentTypeListItems()
  ])
```

Note how we’re defining which initial value templates should be valid in this context: by specifying just a single template, that is the only template valid in this context and will be used. If you specify multiple templates, you will get a choice of which template to use. The second argument to `S.initialValueTemplateItem()` is a set of parameters – in this case, we're passing the `authorId` from the parent pane as a parameter.



# Cross Dataset References

_This is a paid feature, available on the Enterprise plan._

A fundamental requirement for enabling a content-driven workflow is having access to the proper tools to help you compartmentalize and then [connect your content](https://www.sanity.io/docs/connected-content). A way of composing sets of fields to create documents, and of connecting documents to create relationships. Boxes and arrows, if you will.

The premier tool for connecting content in Sanity is the [reference schema type](https://www.sanity.io/docs/reference-type), for creating binding relationships between content types. The reference type only allows references within a single dataset. This covers most use cases, but sometimes more complex architectures and content needs present a legitimate case for a way of communicating across datasets.

Enterprise organizations often have different teams working with different content across channels, geographies, and markets. You might have one team managing product data. A handful of others teams each in charge of the digital experience for their specific brand, in their specific market. And a centralized legal team, who supports various brands and markets by providing copy for Terms of Service, Warranties, and other official information. Making sure these teams all refer to the same single source of truth for any specific bit of content is a challenge in most CMSes and often leads to duplication of effort and content debt.

For these scenarios, there is the `crossDatasetReference` schema type! With it comes the ability to make references between documents in different datasets.

While closely related to the `reference` type, the `crossDatasetReference` type has some unique capabilities and some different limitations that you should be aware of.

## The anatomy of a cross-dataset reference

A cross-dataset reference is, as its name suggests, a reference in one dataset to a document in another dataset. In order for this to be possible, there are some requirements that must be filled.

> [!NOTE]
> For the remainder of this article, we’ll use the term referencing dataset when we discussing the dataset where the reference originates – i.e. the document that has a field pointing to a field in a different dataset – and referenced dataset when we talk about the dataset that is being referred to.

- Both datasets must belong to the same project which must be on an enterprise plan and have this feature enabled
- The source and target studios must both be updated to version `2.34.3` or later
- The dataset name of the **referenced dataset** must be known at the time of creating the reference field in the **referencing dataset**
- Similarly, the type of document you wish to refer to in the **referenced dataset**, and one or more of its fields must be known in order to set up previews in the **source dataset**

## Exploring the `crossDatasetReference` schema

> [!NOTE]
> To read details about the crossDatasetReference schema type, visit the schema type reference documentation.

The `crossDatasetReference` type is, as mentioned, closely related to the `reference` type. It supports most of the same properties and options, in addition to some specific ones. Let’s have a look at a minimal example of a `crossdatasetReference` schema, and then go a bit further once we’ve established the basics.

```javascript
//Type definition on the schema of the "referencing" dataset,
//i.e. where the reference originates

{
  name: 'my-reference-field',
  title: 'Field reference to other field in another dataset',
  type: 'crossDatasetReference',
  dataset: 'name-of-the-other-dataset',
  to: [
    {
      type: 'article',
      preview: {
        select: {
          title: 'title'
        },
      },
    },
  ],
}
```

- All fields in the above example, except the title, are required
- The `type` must be set to `crossDatasetReference`
- The `dataset` must have the appropriate value
- The `to` field accepts an array of entries to different document types in the referenced dataset. You may define as many types here as you please, but each `crossDatasetReference` field is limited to connecting to a single referenced dataset.
- Because the entire schema of all document types in the **referenced dataset** is not known to the **referencing dataset**, the following is true for each entry in the `to` array:- In addition to `type`, each entry must specify one or more fields to use when searching for and previewing content in the **referenced dataset**. To learn more about previews and list views, please refer to [this article](https://www.sanity.io/docs/previews-list-views).



Let’s add a few more fields and a little more complexity to our schema:

```javascript
{
  title: 'Reference to a document in a another dataset',
  name: 'myCoolReferenceAcrossDatasets',
  type: 'crossDatasetReference',
  dataset: 'name-of-other-dataset',
	studioUrl: ({ type, id }) => `https://target.studio/desk/${type};${id}`,
  to: [
    {
      type: 'article',
      preview: {
        select: {
          title: 'title',
          media: 'heroImage',
        },
      },
    },
    {
      type: 'person',
      preview: {
          select: {
            name: 'name',
            picture: 'portrait',
            honorific: 'jobTitle',
          },
          prepare({ name, picture, honorific }) {
            return {
              title: name,
              media: picture,
              subtitle: honorific,
            };
          },
        },
      },
    },
  ],
}
```

Let’s look at what we’ve added.

- The `studioUrl` field on line 6 accepts a function, which is invoked with the `type` and `id` of your referenced document, and which should return a string in the shape of a URL to the document editing pane address in the referenced dataset studio. This field is used to create a direct link from the reference preview to its editing environment (providing your editors have access to it, of course).
- Finally, we’ve added a second document type to our `to` array with an expanded preview configuration.

## The Cross-Dataset Reference field in your studio.

Having configured your schema, you should see the `crossDatasetReference` field show up in your studio. While similar to `reference` inputs they differ in some key aspects:

- The “Create New” button and option to open the referenced document in a new pane to the right are not available across datasets. Instead, you will find an intent link that will open the referenced document in the target studio (if you have access to it, and have set the `studioUrl` property).
- Linking to drafts is not available across datasets. Unless the document has been published at some point, it will not show up in search.
- Depending on network conditions, searching and previewing cross-dataset reference fields might be less performant than doing the same operations on internal references.

![Shows input for cross-dataset references](https://cdn.sanity.io/images/3do82whm/next/f4247b1c4574e78c9c9aa7821e8939cd47371343-1800x934.png)



## Editor support for cross-dataset references

> [!TIP]
> Protip
> The visibility of Cross Dataset References depends on the permissions of the current user or token. For private datasets this means that:
> 
> A user or token can see that a reference exists if they have at least read permissions on the source document. If they don’t have read permissions on the target document, they’ll see that the reference exists but not the content of the target document.
> 
> A user or token can fetch the referenced document if they have at least read permissions on the target document.
> 
> A user who wants to create a reference to a document can search for and attach any documents they have read permissions on.

![Shows cross-dataset input with popover](https://cdn.sanity.io/images/3do82whm/next/5481da83a30701082d52a5237da2bc8ec98ee447-1800x720.png)

As with the `reference` schema type, `crossDatasetReference` fields are by default assumed to have bi-directional integrity which means that if you try to delete a document that is referred to by another document, the studio will alert you with a warning.

![Shows warning dialog](https://cdn.sanity.io/images/3do82whm/next/bad0cf6915c6b732624bd378802fdb9c74da7e5f-1800x1016.png)

However, unlike references within a single dataset, the studio will allow you to proceed with deleting or unpublishing documents that have cross-dataset references.

If you go ahead and delete the document despite the warnings, it will show up as unavailable in any studio referencing it and will block publishing until the problem is fixed if any changes are made to the referring document.

![Shows warning about missing content](https://cdn.sanity.io/images/3do82whm/next/5cd229a42e5761bd11f2c7b6913c7ecfcea3941a-1800x720.png)

These measures are in place so that you can feel confident about connecting your content across datasets, and that you will be notified if a referenced document disappears.

Sometimes you don't need this guarantee while you want to keep the convenience of references. This warning can be turned off by adding the `weak: true` property to a reference field configuration.

You will still be notified that the document you are referring to has gone missing, but you will no longer be blocked from publishing.

## Querying cross-dataset references

> [!WARNING]
> Gotcha
> Cross-dataset references require you to use API version v2022-03-07 or later. Read more about the Sanity API versioning scheme here.

> [!WARNING]
> Gotcha
> Cross dataset references can only be dereferenced using GROQ queries. Dereferencing through GraphQL endpoints is not currently supported.

To GROQ, a `crossDatasetReference` behaves similarly to an internal reference, except that dereferencing must always start from the “referencing” document. For example, for these two schemas, each in a different dataset:

```javascript
// Movie type (movies-dataset)

{ 
  name: 'Movie Name',
  ...
},
{
  name: 'Actors',
  title: 'Actors',
  type: 'array',
  of: {
    type: 'crossDatasetReference',
    dataset: 'people-dataset',
    to: [
      {
        type: 'person',
        preview: {
          select: {
            name: 'firstname'
          },
        },
      },
    ]
  }
}
```

```javascript
// Person type (people-dataset)

{ 
  name: 'firstname',
  ...
},
{ 
  name: 'lastname',
  ...
},
...
```

A GROQ query starting at the Movies type can dereference the “actors” field elements to retrieve the Person type field:

```groq
*[_type == "movie"] {
  ...,
  "actors": actors[]->{
    ...
    firstname,
    lastname,
  }
}
```

There are no limitations on the number of levels or nesting of references supported by the dereferencing operation, but dereferencing can only be done through the `->` operator, following the “unidirectionality” of cross-dataset references - for example, if the person type had an “awards” cross-dataset reference field, it could be further dereferenced as follows:

```groq
*[_type == "movie"] {
  ...,
  "actors": actors[]->{
    ...
    firstname,
    lastname,
    awards->name
  }
}
```

However, other ways of dereferencing, for example, using the `references()` function is **not supported**:

```groq
// This is a NOT SUPPORTED query
*[_type == "movie"] {
  ...,
  "actors": actors[]->{
    ...
    firstname,
    lastname,
    "awards": *[_type == "awards" && references(^._id)] // <== here the reference function will not work for a cross-dataset reference
  }
}
```

> [!WARNING]
> Perspectives are limited to the initiating dataset
> Perspectives only apply to the initiating dataset and will not apply to items in the referenced dataset. This can result in only seeing published documents, even when in preview environments.

## In conclusion

The cross-dataset reference schema type is a powerful tool for enabling Shared Content across datasets. It allows you to keep your content connected beyond its original context by extending the reference field with methods for authenticating and querying across datasets.

Further reading:

- [crossDatasetReference schema type](/docs/cross-dataset-reference-type)
- [Connected Content article](/docs/studio/connected-content)



# Sort Orders

When displaying a collection of documents it's useful to be able to sort the collection by different fields. You do this by specifying an `orderings` property in the schema:

```javascript
{
  name: 'movie',
  type: 'document',
  fields: [
    {
      title: 'Title',
      name: 'title',
      type: 'string'
    },
    {
      title: 'Release Date',
      name: 'releaseDate',
      type: 'date'
    },
    {
      title: 'Popularity',
      name: 'popularity',
      type: 'number'
    }
  ],
  orderings: [
    {
      title: 'Release Date, New',
      name: 'releaseDateDesc',
      by: [
        {field: 'releaseDate', direction: 'desc'}
      ]
    },
    {
      title: 'Release Date, Old',
      name: 'releaseDateAsc',
      by: [
        {field: 'releaseDate', direction: 'asc'}
      ]
    },
    {
      title: 'Popularity',
      name: 'popularityDesc',
      by: [
        {field: 'popularity', direction: 'desc'}
      ]
    }
  ]
}
```

The `orderings` above define a list of possible ways to order a collection of movies. To the user these appear as options in the Studio when the movies are listed, with each object in `orderings` being its own sort option (one can sort by `Release Date, New` OR `Release Date, Old` OR `Popularity`).

### Default sort orders

If no sort orders are defined, Sanity will do its best to guess what fields would make sense to sort by.

When no ordering is specified:

- If the document type has *string* fields named `title`, `name`, `label`, `heading`, `header`, `caption` or `description`, we enable options to order by all of these.
- If your type has no fields named any of the above, we will generate ordering configs for *all* fields of primitive types, that is fields of type `string`, `number`, or `boolean`.

If you specify your own ordering, we skip the default heuristics above.





# Configuring the Portable Text editor

[Portable Text](https://www.portabletext.org) is a JSON specification built on the idea of rich text as an array of blocks, where each block is an array of child spans. You can use it instead of Markdown or MDX to store your content in a presentation-agnostic way. This allows transforming, or serializing, content into any markup language or framework.

Portable Text is extendible. Each block can have a style and a set of mark definitions that describe data structures distributed in the child spans. Portable Text also enables inserting arbitrary data objects in the array, only requiring a `_type`-key.
Portable Text allows custom content objects in the root array, enabling editing and rendering environments to mix rich text with custom content types.

You can create as many versions of the editor for Portable Text as you want. A frequent pattern is a simple configuration with selected decorators and annotations, and a more comprehensive configuration with custom block types.
This can help if editors can only use emphasis and annotate text as internal links in some settings (for example, a caption) and have a full toolbox available in another (for example, an article body).

## Minimal configuration example

The following example shows a minimal configuration to implement Portable Text and the editor in Sanity Studio:

```javascript
export default {
  name: 'content',
  type: 'array',
  title: 'Content',
  of: [
    {
      type: 'block'
    }
  ]
}
```

The code renders the Portable Text editor with a default configuration for styles, decorators, and annotations. 

![The default editor configuration](https://cdn.sanity.io/images/3do82whm/next/eb1f7fdd0885f9fc65cd51779ee2490e41655052-516x307.png)

Portable Text is markup-agnostic; however, the default configuration maps easily to HTML conventions. **Bold** and *italics* set the decorators `strong` and `em` (emphasis), and produce a data structure like this:

```javascript
[
  {
    "_type": "span",
    "_key": "eab9266102e81",
    "text": "strong",
    "marks": [
      "strong"
    ]
  },
  {
    "_type": "span",
    "_key": "eab9266102e82",
    "text": " and ",
    "marks": []
  },
  {
    "_type": "span",
    "_key": "eab9266102e83",
    "text": "emphasis.",
    "marks": [
      "em"
    ]
  }
]
```

Portable Text isn't designed for direct human authoring or reading, but to be easily parsed by software. The `_type`-key also makes it queryable in Sanity’s APIs, and by other JSON tools, such as [jq](https://stedolan.github.io/jq/) or [groq-js](https://github.com/sanity-io/groq-js).

## Add custom blocks

Since Portable Text defines block content as an array, adding custom content blocks for images, videos, or code-embeds, means inserting these items between paragraph blocks. 

> [!WARNING]
> Gotcha
> Content blocks for the Portable Text editor must be object-like types, and not primitive types like string, number, or boolean. 
> 
> You can also use types that are installed with plugins.
> Some plugins, such as those for tables, may export an array type. Since it's not possible to store arrays directly within other arrays, you first need to wrap them in an object.

### Example: images

To add images to Portable Text, append a new type object to the array as such:

```javascript
export default {
  name: 'content',
  type: 'array',
  title: 'Content',
  of: [
    {
      type: 'block'
    },
    {
      type: 'image'
    }
  ]
}
```

This configuration will add an insert** **menu with **Image** as the only option:

![The image option is added to the tool bar](https://cdn.sanity.io/images/3do82whm/next/42fe954183c37bb88fe216b729fb308cfc255a4f-520x309.png)

Selecting an image inserts the block with a preview in the Portable Text editor. You can drag the image and drop it in its designated position; you can also edit it by double-clicking the preview box or by selecting the edit option from the kebab menu:

![Edit or delete a custom block](https://cdn.sanity.io/images/3do82whm/next/e26f1b6be0cc28d7272d325f7b67c7f8c2f58d68-634x589.png)

The Portable Text data structure for this example looks like the following:

```javascript
[
  {
    "style": "normal",
    "_type": "block",
    "markDefs": [],
    "_key": "09cc5f099d3b",
    "children": [
      {
        "_type": "span",
        "_key": "09cc5f099d3b0",
        "text": "Kokos is a miniature schnauzer.",
        "marks": []
      }
    ]
  },
  {
    "_type": "image",
    "_key": "a5e9155ee3f5",
    "asset": {
      "_type": "reference",
      "_ref": "image-61991cfbe9182124c18ee1829c07910faadd100e-2048x1366-png"
    }
  },
  {
    "style": "normal",
    "_type": "block",
    "markDefs": [],
    "_key": "54145e9cb006",
    "children": [
      {
        "_type": "span",
        "_key": "54145e9cb0060",
        "text": "Kokos is a good dog!",
        "marks": []
      }
    ]
  }
]
```

The image has its object structure, where `asset` references the asset’s document. While you can derive the image URL from its `_id` (corresponding to the `_ref` value in the code above), you can also join the asset document using [select in GROQ](/docs/specifications/groq-functions):

```json
*[_type == "post"]{
  ...,
  content[]{
    ...,
    _type == "image" => {
      ...,
      asset->
    }
  }
}
```

### Example: code input

Our documentation features many code blocks. They are custom blocks that we added to our editor. You can install the code input as a plugin using the [Sanity CLI](/docs/cli-reference/install):

```sh
npm i @sanity/code-input
# OR
yarn add @sanity/code-input
```

Once installed and [configured](/docs/studio/installing-and-configuring-plugins), you can add the code block to the editor for Portable Text configuration:

```javascript
export default {
  name: 'content',
  type: 'array',
  title: 'Content',
  of: [
    {
      type: 'block'
    },
    {
      type: 'image'
    },
    {
      type: 'code'
    }
  ]
}
```

*Code* (to change the default title to a different one, add `title: "<my-custom-title>"` to the same object) is now available as a selection in the insert menu. Inserting a code block produces a preview and a code editor:

![The code editor with some schema code in JavaScript](https://cdn.sanity.io/images/3do82whm/next/a30a849860f32f198563e0b0c343a31ec4b1fd8d-942x790.png)

You can set [more options](https://www.npmjs.com/package/@sanity/code-input#options) for the code input. 

[How to add a custom YouTube block](/guides/portable-text-how-to-add-a-custom-youtube-embed-block)



## Configuring styles for text blocks

Out of the box, the Portable Text editor includes the following styles: `normal`, `h1–h6`, and `blockquote`. By default, they map to HTML; but a style can be an arbitrary value.

```javascript
// The default set of styles
export default {
  name: 'content',
  title: 'Content',
  type: 'array',
  of: [
    {
      type: 'block',
      styles: [
        {title: 'Normal', value: 'normal'},
        {title: 'H1', value: 'h1'},
        {title: 'H2', value: 'h2'},
        {title: 'H3', value: 'h3'},
        {title: 'H4', value: 'h4'},
        {title: 'H5', value: 'h5'},
        {title: 'H6', value: 'h6'},
        {title: 'Quote', value: 'blockquote'}
      ]
    }
  ]
}
```

If you want to tie Portable Text to specific use cases, that is possible: maybe you are using Sanity to work with content that will be [transformed into InDesign’s XML format](https://medium.com/buro-int/headless-cms-for-a-printed-pizza-book-54b39827e651) where you have ready-made templates with their style names. Or perhaps your organization has a [BEM-based](http://getbem.com/introduction/) CSS design system, and you want to embed certain class names in the rich text data. 

![The default style configuration in the editor](https://cdn.sanity.io/images/3do82whm/next/d194c6084605d055ec365557a77c358543db1fc1-648x523.png)

We recommend keeping the configuration to a reasonably abstract level and following known, established conventions. If you plan to use our Portable Text tooling for rendering web pages, you should probably stay closer to naming conventions in HTML. 

To override the default configuration for styles, add the `style` key and set an array of `title`/`value` objects:



```javascript
export default {
  name: 'content',
  title: 'Content',
  type: 'array',
  of: [
    {
      type: 'block',
      styles: [
        { title: 'Normal', value: 'normal' },
        { title: 'Heading 2', value: 'h2' },
        { title: 'Quote', value: 'blockquote' },
        { title: 'Hidden', value: 'blockComment' }
      ]
    }
  ]
}
```

Here we have set 4 possible styles. The 3 first are from the default settings and are parsed in HTML to `<p>`, `<h2>`, and `<blockquote>`.

`blockComment` is an arbitrary style that we set because we plan to make it possible for editors to hide selected blocks of text from rendering, while keeping them available in the source code as block comments.

[Want to style the blocks in the editor? Read more →](/docs/studio/customizing-the-portable-text-editor)

![Editor with style configuration](https://cdn.sanity.io/images/3do82whm/next/f91121e5420d896b44d5f6b92b8e575bb4cd5e8e-611x404.png)

## Configuring lists for text blocks

The editor supports two types of lists: bullet (unordered) and number (ordered). If your `block` type doesn't contain a `lists` definition, your editor features both a bullet list and a numbered list option:

![](https://cdn.sanity.io/images/3do82whm/next/0a0bc15e8cb00a34be4402fe639d113ee82c8739-451x49.png)

The default is the equivalent of explicitly naming both:

You can override the default by naming the lists you want. If you leave the array empty, you disable lists altogether:

```javascript
// The default set of styles
export default {
  name: 'content',
  title: 'Content',
  type: 'array',
  of: [
    {
      type: 'block',
      lists: [
        {title: 'Bullet', value: 'bullet'},
        {title: 'Numbered', value: 'number'}
      ] // yes please, both bullet and numbered
    }
  ]
}
```

```javascript
// The default set of styles
export default {
  name: 'content',
  title: 'Content',
  type: 'array',
  of: [
    {
      type: 'block',
      lists: [] // no lists, thanks
    }
  ]
}
```

Also, you decide what goes into the title: `{title: 'Prioritized', value: 'number'}` works equally well!

## Configuring marks for inline text

Portable Text enables marks to label inline text with additional data. There are two types of marks: *decorators* and *annotations.
*Decorators are marks as simple string values, while annotations are keys to a data structure. Annotations are a powerful feature of Portable Text in combination with Sanity’s backend, because they allow embedding complex data structures and references in running text. 

### Decorators

Decorators work similarly to styles, but they are applied to spans, that is, inline text. The default configurations are `strong`, `em`, `code`, `underline`, and `strike-through`. If you want to disable some of these and set your own, you do that by adding an array to the `decorators` key, under `marks`:



```javascript
export default {
  name: 'content',
  title: 'Content',
  type: 'array',
  of: [
    {
      type: 'block',
      marks: {
        decorators: [
          {title: 'Strong', value: 'strong'},
          {title: 'Emphasis', value: 'em'},
          {title: 'Code', value: 'code'}
        ]
      }
    }
  ]
}
```

Decorators are displayed as icons in the toolbar. This configuration looks like this:

![Toolbar with custom decorator configuration](https://cdn.sanity.io/images/3do82whm/next/bbeb64ad8cef7d839770996e3196d17116b2d029-608x224.png)

### Annotations

Annotations enable embedding rich content data in inline text. An example can be a reference to another document, typically used for internal linking. A complete guide with front-end implementation [can be found here](/guides/portable-text-internal-and-external-links).

To add an internal link annotation, configure the Portable Text schema like this:

```javascript
export default {
  name: 'content',
  title: 'Content',
  type: 'array',
  of: [
    {
      type: 'block',
      marks: {
        decorators: [
          // ...
        ],
        annotations: [
          {
            name: 'internalLink',
            type: 'object',
            title: 'Internal link',
            fields: [
              {
                name: 'reference',
                type: 'reference',
                title: 'Reference',
                to: [
                  { type: 'post' },
                  // other types you may want to link to
                ]
              }
            ]
          }
        ]
      }
    }
  ]
}
```

> [!WARNING]
> Gotcha
> If you plan to use Sanity’s GraphQL API, you should hoist internalLink as a schema type, and use type: 'internalLink' as the annotation, instead of the anonymous example above.
> 
> Learn more about using GraphQL with Sanity here.

The annotations are displayed in the toolbar as question mark icons if none is set.
For more information on editing toolbar icons, see [Customizing the Portable Text editor](/docs/studio/customizing-the-portable-text-editor).

![Reference modal for internal link annotation](https://cdn.sanity.io/images/3do82whm/next/49ec3c9475f4ddd23c5dd5c32d81132e865e7b10-614x409.png)

The corresponding Portable Text data structure looks like this:

```javascript
[
  {
    "_key": "da9dc50335a0",
    "_type": "block",
    "children": [
      {
        "_key": "da9dc50335a00",
        "_type": "span",
        "marks": [
          "5b86c1132a66"
        ],
        "text": "This is an internal link"
      },
      {
        "_key": "da9dc50335a01",
        "_type": "span",
        "marks": [],
        "text": "."
      }
    ],
    "markDefs": [
      {
        "_key": "5b86c1132a66",
        "_type": "internalLink",
        "reference": {
          "_ref": "1dfa4e95-9f92-4e13-901b-1a769724e23c",
          "_type": "reference"
        }
      }
    ],
    "style": "normal"
  }
]
```



# Customizing the Portable Text Editor

Sanity Studio [Portable Text](https://github.com/portabletext/portabletext) editor is customizable so that it can fit different editorial needs. You can configure and tailor several different editors throughout the studio.
For more information about configuring the editor, see [Configuring the Portable Text Editor](/docs/studio/portable-text-editor-configuration).

In general, customization works by passing React components to the schema definitions of content types that use Portable Text. Alternatively, it's also possible to pass strings. 

## Toolbar icons and span rendering

When you configure custom markers, that is, decorators (simple values) and annotations (rich data structures), they are displayed as icons in the toolbar. The default icon is a question mark. You can customize it to display a different icon. 

If you add custom decorators and annotations, you may want to control their visual presentation in the editor. By default, decorators are invisible, whereas annotations have a gray background and a dotted underline.

### Decorators

Some often-used decorators, such as **strong**, *emphasis*, and `code`, feature rendering out of the box.

For example, let’s say you created a decorator to highlight text using the following configuration:

```jsx
export default {
  name: 'content',
  title: 'Content',
  type: 'array',
  of: [
    {
      type: 'block',
      marks: {
        decorators: [
          { title: 'Strong', value: 'strong' },
          { title: 'Emphasis', value: 'em' },
          { title: 'Code', value: 'code' },
          { title: 'Highlight', value: 'highlight' }
        ]
      }
    }
  ]
}
```

Now, add a custom toolbar icon by passing in an anonymous function that returns `H` as a string to `.icon:`

```javascript
// RichTextEditor.jsx
export default {
  name: 'content',
  title: 'Content',
  type: 'array',
  of: [
    {
      type: 'block',
      marks: {
        decorators: [
          { title: 'Strong', value: 'strong' },
          { title: 'Emphasis', value: 'em' },
          { title: 'Code', value: 'code' },
          { 
            title: 'Highlight',
            value: 'highlight',
            icon: () => 'H'
          }
        ]
      }
    }
  ]
}
```

The string is rendered in the decorator button in the toolbar:

![Toolbar with custom decorator button](https://cdn.sanity.io/images/3do82whm/next/3260a0230e618c96da5db15f00a2c9d33329e105-2562x1768.png)

You can also pass a [JSX component](https://beta.reactjs.org/learn/writing-markup-with-jsx) directly in the schema, or via an import.
The following example adds simple inline styling to a span holding the character *H*.

```javascript
// RichTextEditor.js
import React from 'react'

const HighlightIcon = () => (
  <span style={{fontWeight: 'bold'}}>H</span>
  )

export default {
  name: 'content',
  title: 'Content',
  type: 'array',
  of: [
    {
      type: 'block',
      marks: {
        decorators: [
          { title: 'Strong', value: 'strong' },
          { title: 'Emphasis', value: 'em' },
          { title: 'Code', value: 'code' },
          {
            title: 'Highlight',
            value: 'highlight',
            icon: HighlightIcon
          }
        ]
      }
    }
  ]
}
```

The next step is to render the actual highlighted text in the editor. We do this by passing the props into a React component and wrapping them in a span with some styling.

```javascript
// RichTextEditor.js
import React from 'react'

const HighlightIcon = () => (
  <span style={{ fontWeight: 'bold' }}>H</span>
)
const HighlightDecorator = props => (
  <span style={{ backgroundColor: 'yellow' }}>{props.children}</span>
)

export default {
  name: 'content',
  title: 'Content',
  type: 'array',
  of: [
    {
      type: 'block',
      marks: {
        decorators: [
          { title: 'Strong', value: 'strong' },
          { title: 'Emphasis', value: 'em' },
          { title: 'Code', value: 'code' },
          {
            title: 'Highlight',
            value: 'highlight',
            icon: HighlightIcon,
            component: HighlightDecorator
          }
        ]
      }
    }
  ]
}
```

The rendered presentation in the editor is a yellow background for highlighted text:

![Editor with custom render and icon for the highlight decorator](https://cdn.sanity.io/images/3do82whm/next/3b4821d70974eb08a77c1be9f0aa48fb0d8b82a3-672x340.png)

> [!TIP]
> Protip
> When you create your custom decorators, you can keep all, some, or none of the built-in decorators.
> 
> These are the built-in decorators:
> 
> { "title": "Strong", "value": "strong" },
> { "title": "Emphasis", "value": "em" },
> { "title": "Code", "value": "code" },
> { "title": "Underline", "value": "underline" },
> { "title": "Strike", "value": "strike-through" }
> 
> Make sure you include those you intend to keep.

### Annotations

Customizing annotations works much in the same way as decorations: you pass an icon and a renderer in the schema definition.

A common use case is to have [an annotation for an internal reference, in addition to a link with an external URL](/guides/portable-text-internal-and-external-links).
You can customize the editor to display a custom icon for the internal link, and a renderer that helps recognize external links when they are inline in the text.

The following example imports an icon from the `@sanity/icons`-package. In the example, you configure a user icon to represent internal references to a `person` type.

```javascript
// RichTextEditor.js
import { UserIcon } from '@sanity/icons'

export default {
  name: 'content',
  title: 'Content',
  type: 'array',
  of: [
    {
      type: 'block',
      marks: {
        annotations: [
          {
            name: 'link',
            type: 'object',
            title: 'link',
            fields: [
              {
                name: 'url',
                type: 'url'
              }
            ]
          },
          {
            name: 'internalLink',
            type: 'object',
            title: 'Internal link',
            icon: UserIcon,
            fields: [
              {
                name: 'reference',
                type: 'reference',
                to: [
                  { type: 'person' }
                  // other types you may want to link to
                ]
              }
            ]
          }
        ]
      }
    }
  ]
}
```

Now the user icon replaces the default question mark icon in the toolbar: 

![The editor with a custom user icon for the internal link annotation](https://cdn.sanity.io/images/3do82whm/next/8c1ec5147315ac7d6e2a5b97df048935b278bd3c-627x327.png)

#### Custom components

The next step is to create a custom renderer for external links. The following example appends an "arrow out of a box" icon to mark these links. To do this, you pass a small React component.

In the `/schemas/components` directory, create a file and name it `ExternalLinkRenderer.tsx`.

```jsx
// ExternalLinkRenderer.js
import React from 'react'
import { LaunchIcon } from '@sanity/icons'

const ExternalLinkRenderer = props => (
  <span>
    {props.renderDefault(props)}
    <a contentEditable={false} href={props.value.href}>
      <LaunchIcon />
    </a>
  </span>
)

export default ExternalLinkRenderer

```

Then, import the following component, and pass it to `components.annotation` in the schema type:

```javascript
// RichTextEditor.js
import { UserIcon } from '@sanity/icons'
import ExternalLinkRenderer from './components/ExternalLinkRenderer'

export default {
  name: 'content',
  title: 'Content',
  type: 'array',
  of: [
    {
      type: 'block',
      marks: {
        annotations: [
          {
            name: 'link',
            type: 'object',
            title: 'link',
            fields: [
              {
                name: 'url',
                type: 'url'
              }
            ],
            components: {
              annotation: ExternalLinkRenderer
            }
          },
          {
            name: 'internalLink',
            type: 'object',
            title: 'Internal link',
            icon: UserIcon
            fields: [
              {
                name: 'reference',
                type: 'reference',
                to: [
                  { type: 'person' }
                  // other types you may want to link to
                ]
              }
            ]
          }
        ]
      }
    }
  ]
}
```

As a result, external links now look like this:

![The editor with custom renderer for external links.](https://cdn.sanity.io/images/3do82whm/next/9a23809a820a2e2a5b34d560b9e90536737bf9e4-634x317.png)

## Block styles

The Portable Text editor ships with a set of styles that translate well to their corresponding HTML ones. However, your front end may not be targeting HTML. While it has always been possible to [add and configure block styles](https://www.sanity.io/docs/configuration#configuring-styles-for-text-blocks), you can now also configure how these styles render in the editor using markup and the styling method of your choice. This means you can tune your editor to be aligned with your organization’s design system.

To illustrate this point, the following example produces a custom title style using Garamond as the font face with a slightly increased font size. First, define a custom style called `title`:

```javascript
export default {
  name: 'content',
  title: 'Content',
  type: 'array',
  of: [
    {
      type: 'block',
      styles: [
        {title: 'Normal', value: 'normal'},
        {title: 'Title', value: 'title'},
        {title: 'H1', value: 'h1'},
        {title: 'H2', value: 'h2'},
        {title: 'H3', value: 'h3'},
        {title: 'Quote', value: 'blockquote'},
      ]
    }
  ]
}
```

Without any customization, the block looks exactly like the `normal` one. To apply a custom style to it, create a renderer in React.
It works in the same way as renderers for marks: pass a React component to `component`. The props of the block contain the element to style and the appropriate styling.
In the following example, the React component is added to the configuration file. 

```javascript
// RichTextEditor.js
import React from 'react'

const TitleStyle = props => (
  <span style={{fontFamily: 'Garamond', fontSize: '2em'}}>{props.children} </span>
)

export default {
  name: 'content',
  title: 'Content',
  type: 'array',
  of: [
    {
      type: 'block',
      styles: [
        {title: 'Normal', value: 'normal'},
        {title: 'H1', value: 'h1'},
        {title: 'H2', value: 'h2'},
        {title: 'H3', value: 'h3'},
        {title: 'Quote', value: 'blockquote'},
        {
          title: 'Title',
          value: 'title',
          component: TitleStyle
        },
      ]
    }
  ]
}
```

The `component` prop applies the custom style to the title block, as it's rendered in the editor:

![The editor with a custom title block style](https://cdn.sanity.io/images/3do82whm/next/abcb65d0ba817b6cd74ba5524ee915118f40e785-1008x823.png)



## Validation of annotations

Like other content types, annotations support [content validation](/docs/studio/validation). Warnings are displayed in the margin and in the document. A pointer activates the annotation modal for the editor. Validations help editors structure the content correctly. It's generally a good idea to involve editors in creating validations and testing the warning messages so that they are helpful for them.

Let's say that you are using the same content for multiple websites. In this case, it's important that internal linking use an annotation with a `reference` input. This helps prevent accidental deletion of linked content and resolve internal links in the front-end project.
You can create a simple validation that takes care of this:

```javascript
export default {
  name: 'content',
  title: 'Content',
  type: 'array',
  of: [
    {
      type: 'block',
      validation: Rule => Rule.regex(/.*damnation.*/gi, { name: 'profanity' }),
      marks: {
        annotations: [
          {
            name: 'link',
            type: 'object',
            title: 'link',
            fields: [
              {
                name: 'url',
                type: 'url',
                validation: Rule =>
                  Rule.regex(
                    /https:\/\/(www\.|)(portabletext\.org|sanity\.io)\/.*/gi,
                    {
                      name: 'internal url',
                      invert: true
                    }
                  ).warning(
                    `This is not an external link. Consider using internal links instead.`
                  )
              }
            ]
          },
          {
            name: 'internalLink',
            type: 'object',
            title: 'Internal link',
            fields: [
              {
                name: 'reference',
                type: 'reference',
                to: [
                  { type: 'post' }
                  // other types you may want to link to
                ]
              }
            ]
          }
        ]
      }
    }
  ]
}
```

The regular expression `/https:\/\/(www\.|)(portabletext\.org|sanity\.io)\/.*/i` triggers on all URLs that match all the variations of either `portabletext.org` or `sanity.io` with some sub-paths (this allows linking to the root domain).

![A validation warning for the link annotation in the editor.](https://cdn.sanity.io/images/3do82whm/next/e32117870e43964c12d7c8df5c51e1f50e87792e-2562x1768.png)

## Margin markers

> [!WARNING]
> Gotcha
> We recommend avoiding margin markers. Instead, use form components and the Form API.
> 
> Margin markers are still supported but will be deprecated in a future release.

In addition to validation markers, you can also define custom markers displayed in the right margin. You define these markers with the editor's input prop called `markers`.

Margin markers are objects like this:

```javascript
{
  type: 'comment',
  path: [{_key: 'theBlockKey'}],
  item: {
    _type: 'comment',
    _key: '2f323432r23',
    body: 'I am commenting this block!'
  }
}

```

To show a custom marker, you must wrap the editor component and create a function that renders the custom markers as needed, and returns a React node (or `null`):

```jsx
// renderCustomMarkers.js
import React from 'react'

export default function renderCustomMarkers(markers) {
  return (
    <div>
      {markers.map((marker, index) => {
        if (marker.type === 'comment') {
          return <div key={`marker${index}`}>A comment!</div>
        }
        return null
      })}
    </div>
  )
}

```

Then, create a wrapper for the editor input to give the editor the `renderCustomMarker` prop:

```jsx
// CustomRichTextEditor.js
import React from 'react'
import {PortableTextInput} from 'sanity'
import renderCustomMarkers from './renderCustomMarkers' // From above example

function ArticleBlockEditor (props) {
  const {value, markers} = props
  const customMarkers = [
      {type: 'comment', path: value && value[0] ? [{_key: value[0]._key}] : [], value: 'This must be written better!'}
    ]
  const allMarkers = markers.concat(customMarkers) // [...markers, ...customMarkers] works too

  return (
    <BlockEditor
      {...props}
      markers={allMarkers}
      renderCustomMarkers={renderCustomMarkers}
    />
  )
}

export default ArticleBlockEditor
```

Finally, update the schema to use your wrapper component as `inputComponent`:

```javascript
// content.js
import CustomRichTextEditor from './CustomRichTextEditor.js'

export default {
  title: 'Content',
  name: 'content',
  type: 'array',
  inputComponent: CustomRichTextEditor,
  of: [
    {type: 'block'}
  ]
}
```

## Margin actions

You can also define actions for the blocks; actions also render in the right margin, beside markers. Actions render as buttons that initiate an action on the corresponding block. The pattern is the same as custom markers above, the only difference being that the prop key is `renderBlockActions`. It's a function that gets the block as input, and then returns a React node (or null) for that block.

The following example defines a margin action that renders a button that inserts a new block with a span that contains the text ”*Pong!*”:

```jsx
// marginActions.js
import React from 'react'

function MarginActions (props) {

  const handleClick = event => {
    const {insert} = props
    insert([{
      _type: 'block',
      children: [
        {
          _type: 'span',
          text: 'Pong!'
        }
      ]
    }])
  }
  return (
    <button type="button" onClick={handleClick}>Ping</button>
  )
}

export default MarginActions  
  
```

To add the action to the custom rich text editor, import it and add it as a property:

```jsx
// CustomRichTextEditor.js
import React from 'react'
import {BlockEditor} from 'sanity'
import renderCustomMarkers from './renderCustomMarkers'
import marginActions from './marginActions.js'

function ArticleBlockEditor (props) {
  const {value, markers = []} = props
  const customMarkers = [
      {type: 'comment', path: value && value[0] ? [{_key: value[0]._key}] : [], value: 'Rephrase this section for clarity!'}
    ]
  const allMarkers = markers.concat(customMarkers) // [...markers, ...customMarkers] works too

  return (
    <BlockEditor
      {...props}
      markers={allMarkers}
      renderCustomMarkers={renderCustomMarkers}
      renderBlockActions={marginActions}
    />
  )
}

export default ArticleBlockEditor
```

## Further reading

[Guide: Add inline blocks to your Portable Text Editor and enrich your block content](https://www.sanity.io/guides/add-inline-blocks-to-portable-text-editor)

[Guide: Ultimate Guide on customising Portable Text](https://www.sanity.io/guides/ultimate-guide-for-customising-portable-text-from-schema-to-react-component)





# Customizing block content

## Custom decorators

To render content the way you want it to be shown, you can create custom decorators.

```tsx
import {defineArrayMember} from 'sanity'

defineArrayMember({
  type: 'block',
  marks: {
    decorators: [
      {
        title: 'Highlight',
        value: 'highlight',
        component: (props) => (
          <span style={{backgroundColor: '#0f0'}}>
            {props.children}
          </span>
        ),
        icon: BulbOutlineIcon,
      },
    ],
  },
})
```

The code example defines a member of an array that enables creating a block with decorators. The decorator in the example has props, such as `value`, `component`, and `icon`; these props define how the block is rendered:

- `value`  applies a `highlight` decorator to the component.
- `component` is a React element. It has the same props as `BlockDecoratorProps`. It renders the child props wrapped in a `span` element with a green background color (`#0f0`).
- `icon` is an instance of `BulbOutlineIcon`.

## Custom styles

You can create custom decorators to render custom styles. Custom styles extend the standard set of styles available out of the box.

> [!WARNING]
> Gotcha
> A rendered preview isn't available for custom styles. It's possible to preview only the default built-in styles available in the editor toolbar menu.
> 
> Custom styles don't support the icon prop.

```tsx
import {defineArrayMember} from 'sanity'
import {Card, Text} from '@sanity/ui'

defineArrayMember({
  type: 'block',
  styles: [
    {
      title: 'Section Header',
      value: 'sectionHeader',
      component: (props) => (
        <Card paddingBottom={4}>
          <Text size={4} weight="bold">
            {props.children}
          </Text>
        </Card>
      ),
    },
  ],
})
```

The code example defines a [block](/docs/block-type) array member and adds style options to it:

- The style is `sectionHeader`.
- The child props of the component in the block are rendered as a card with bold text.

## Spellchecking

You can enable and disable the web browser's built-in spell-checker for text blocks. To do so, set `options.spellCheck` to either `true` or `false` for the specified `block` type.

```tsx
defineArrayMember({
  type: 'block',
  options: {
    spellCheck: false,
  },
})
```

The code example defines a `block` array member, and it disables spellchecking text in the block.

## Customizing block content rendering

You can render block content in Sanity Studio using one of the following form components:

- `block`: renders any valid Portable Text block (text or object.)
- `inlineBlock`: renders a Portable Text block inline inside a running piece of text.
- `annotation`: renders text with annotated metadata (for example, a URL link to reference an external resource, or a cross-reference to another document.)

You can modify specific schema types to customize only the corresponding components. Alternatively, you can modify the studio config or create a plugin to apply the customization to all block content in Sanity Studio.

### Customizing specific block content

To customize a specific block content type, use the `components` property associated with that type.
Define a `block` to provide your custom render component for the associated content type.

The following example customizes the rendering of text and image blocks in the `body` field.

```tsx
import {Box} from '@sanity/ui'
import {defineField, defineArrayMember} from 'sanity'

defineField({
  name: 'body',
  title: 'Body',
  type: 'array',
  of: [
    defineArrayMember({
      type: 'image',
      // Replace the preview of all block images
      // with the edit form for that image, bypassing
      // the modal step.
      components: {
        block: (props) => {
          return props.renderDefault({
            ...props,
            renderPreview: () => props.children,
          })
        },
      },
    }),
    defineArrayMember({
      type: 'block',
      // Add extra padding to all text blocks
      // for this type.
      components: {
        block: (props) => {
          return (
            <Box padding={2}>
              {props.renderDefault(props)}
            </Box>
          )
        },
      },
    }),
  ],
})
```

The `defineField` function creates a field called `body`, which is an array of two types of content: `images` and `blocks`.

The `components` property enables customizing the behavior of the field. 

The `block` function is a component that renders the preview of a text or an image block. It takes `props` as an argument, and it returns a rendered version of the content with additional styling:

- It bypasses the modal step when previewing images.
- It adds extra padding to the text blocks.

### Customizing block content with the studio config

To customize the default rendering of all block content in Sanity Studio, modify the studio config, instead of customizing schemas as shown in the previous section.

The following example reuses the customization described in the previous example, but it sets it in the studio config, instead of the schema type. The studio config applies the customization to any text block or image type rendered as block content.

```typescript
import {definePlugin, defineField, BlockProps} from 'sanity'

const BlockComponent = (props: BlockProps) => {
  // Add extra padding to all text blocks
  if (props.schemaType.name === 'block') {
    return (
      <Box padding={2}>
        {props.renderDefault(props)}
      </Box>
    )
  }
  // Inline editing of images
  if (props.schemaType.name === 'image') {
    return props.renderDefault({
      ...props,
      renderPreview: () => props.children,
    })
  }
  // Render default for all other types
  return props.renderDefault(props)
}

// The config in sanity.config.ts
definePlugin({
  ...,
  form: {
    components: {
      block: BlockComponent,
    },
  },
})

// This schema gets the customizations automatically
// added to the 'block' and 'image' types.
defineField({
  name: 'intro',
  title: 'Intro',
  type: 'array',
  of: [
    {type: 'block'},
    {type: 'image'},
  ]
})

```

In the code example:

- `BlockComponent` takes `props` and returns a component that does the following:- Adds extra padding for all text blocks
- Enables skipping the preview and directly editing images inline
- Applies the default rendering to all other types.


- The `defineField` function defines a schema field that automatically adds the customizations to the `block` and `image` types.

### Customizing block content with a plugin

Instead of modifying the studio config, you can use the code in the previous example to create a plugin to achieve the same outcome.

The advantage is that you can install and share the plugin across multiple studios and workspaces. 

For more information about creating plugins, see [Developing plugins](/docs/studio/developing-plugins).

## Customizing the input

Besides customizing block content, you can also customize `PortableTextInput` to change editing block content in Sanity Studio.

This option enables rendering additional information, such as a word counter or supporting custom hotkeys to control editing features.

The following example shows a simple implementation where you can modify the input by assigning custom values to the `props` of `PortableTextInput`.

```typescript
import {
  defineField,
  defineArrayMember,
  PortableTextInput,
  PortableTextInputProps,
} from 'sanity'

defineField({
  name: 'body',
  title: 'Body',
  type: 'array',
  of: [
    {
      type: 'block',
    },
  ],
  components: {
    input: (props: PortableTextInputProps) => {
      return props.renderDefault(props)
      // Alternatively:
      // return <PortableTextInput {...props} />
    },
  },
})

```

Replace the `input` form component with your custom component. To do so, use either a block content schema type definition, or `definePlugin` in the studio config.

### Custom hotkeys

You can also set custom hotkeys by passing your hotkey mapping as `hotkeys` props to `PortableTextInput`.

The following example implements two hotkeys:

- A hotkey for a custom highlight decorator.
- Another hotkey to enable adding a link annotation to the selected text:

```tsx
import {useMemo} from 'react'
import {
  defineField,
  defineArrayMember,
  PortableTextInput,
  PortableTextInputProps,
} from 'sanity'

// The custom input with two custom hotkeys
const CustomInput =
  (props: PortableTextInputProps) => {
    const {path, onItemOpen, onPathFocus} = props
    // Define the hotkey mapping
    const hotkeys: PortableTextInputProps['hotkeys'] = useMemo(
      () => ({
        // Use the 'marks' prop to toggle
        // text decorators on the currently
        // selected text with a hotkey
        marks: {
          'Ctrl+h': 'highlight',
        },
        // Use the 'custom' prop to define custom
        // functions that can access the underlying 
        // editor instance.
        // In this case, the 'Ctrl+l' hotkey toggles
        // a link on the selected text using the
        // PortableTextEditor API with the editor instance.
        custom: {
          'Ctrl+l': (event, portableTextEditor) => {
            const linkType = portableTextEditor.
              schemaTypes.annotations.find((a) => a.name === 'link')
            if (linkType) {
              event.preventDefault()
              const activeAnnotations =
                PortableTextEditor.activeAnnotations(portableTextEditor)
              const isLinkActive =
                activeAnnotations.some((a) => a._type === 'link')
              if (isLinkActive) {
                PortableTextEditor.removeAnnotation(
                  portableTextEditor,
                  linkType
                )
              } else {
                const result = PortableTextEditor.addAnnotation(
                  portableTextEditor,
                  linkType
                )
                if (result?.markDefPath) {
                  // Open the form member
                  onItemOpen(path.concat(result.markDefPath))
                  // Move the focus to the 'href' field in the next tick
                  setTimeout(() => {
                    onPathFocus(result.markDefPath.concat('href'))
                  })
                }
              }
            }
          },
        },
      }),
      [onPathFocus, onItemOpen, path],
    )
    return <PortableTextInput {...props} hotkeys={hotkeys} />
  }

// The schema type to use for the custom input above
defineField({
  name: 'body',
  title: 'Body',
  type: 'array',
  of: [
    defineArrayMember({
      type: 'block',
      marks: {
        decorators: [
          {
            title: 'Highlight',
            value: 'highlight',
            component: (props) => (
              <span style={{backgroundColor: '#0f0'}}>
                {props.children}
              </span>
            ),
            icon: BulbOutlineIcon,
          },
        ],
      },
    }),
  ],
  components: {
    // Return the custom input defined above
    input: CustomInput,
  },
})

```

The example defines a custom input component for the Portable Text editor.

The hotkey mapping includes a `Ctrl+h` key combination to toggle highlighting on the current selected text, and a `Ctrl+l` key combination to toggle a link on the selected text.

The `marks` property defines hotkey mappings for text decorators, whereas the `custom` property defines custom functions that can access the underlying editor instance.

In the example, `custom` adds a link annotation to `PortableTextEditor`:

53. It checks if the link type is an existing schema type.
53. If the link type exists, it prevents the default action that `Ctrl+l` would trigger, and it adds an annotation for the link type.
53. It checks if an active link annotation exists, and it either adds or removes it accordingly.
53. If adding or removing the annotation returns a result, it opens the form member and moves the focus to the `href` field.

### Custom paste handler

The following example implements custom paste handling for any clipboard text that is a valid URL.
It pastes the content as a `resource` type inline block at the current cursor position.

```tsx
import {
  defineField,
  defineArrayMember,
  PortableTextInput,
  PortableTextInputProps,
} from 'sanity'

// The custom paste handler function to pass as props
// to PortableTextInput
const onPaste: PortableTextInputProps['onPaste'] = (data) => {
  let url: URL
  const text =
    data.event.clipboardData.getData('text/plain') || ''
  // Check if clipboard data is a URL
  try {
    url = new URL(text)
    // Insert an inline resource object in the text
    return Promise.resolve({
      insert: [
        {
          _type: 'block',
          children: [{_type: 'resource', url: url.href}],
        },
      ],
      // To set a specific location to insert
      // the pasted content, instead of the current 
      // cursor position, define a 'path' prop
    })
  } catch (_) {
    return undefined
  }
}

// The block content schema type to use
// for the custom paste handler above
defineField({
  name: 'body',
  title: 'Body',
  type: 'array',
  of: [
    defineArrayMember({
      type: 'block',
      of: [
        {
          type: 'object',
          name: 'resource',
          title: 'Resource',
          fields: [{type: 'url', name: 'url'}],
        },
      ],
    }),
  ],
  components: {
    input: (props) => {
      return <PortableTextInput {...props} onPaste={onPaste} />
    },
  },
})

```

### Custom block validation

The following example validates a text block: it checks for a set of disallowed content using regex matching on every `span` node of the text block.
To test the example, type "*foo*" inside a text block.

```tsx
import {
  Path,
  PortableTextSpan,
  defineArrayMember,
  defineType,
  isPortableTextSpan,
  isPortableTextTextBlock,
} from 'sanity'

interface DisallowListLocation {
  matchText: string
  message: string
  offset: number
  path: Path
  span: PortableTextSpan
  level: 'error' | 'info' | 'warning'
}

export default defineType({
  name: 'customValidationExample',
  title: 'Custom block validation example',
  type: 'document',
  fields: [
    {
      name: 'blockContent',
      title: 'Block content with custom validation',
      type: 'array',
      of: [
        defineArrayMember({
          type: 'block',
          validation: (Rule) => [
            Rule.error().custom((value, context) => {
              const disallowList: {regExp: RegExp; message: string}[] = [
                {
                  message: 'Use n-dash (–) instead',
                  regExp: new RegExp(/^- /g),
                },
                {
                  message: 'Use a bullet list instead',
                  regExp: new RegExp(/^\* /g),
                },
                {
                  message: 'Avoid using \'foo\'',
                  regExp: new RegExp(/\bfoo\b/g),
                },
              ]
              const {path} = context
              const locations: DisallowListLocation[] = []
              if (path && isPortableTextTextBlock(value)) {
                value.children.forEach((child) => {
                  if (isPortableTextSpan(child)) {
                    disallowList.forEach((entry) => {
                      const matches = isPortableTextSpan(child) && child.text.matchAll(entry.regExp)
                      if (matches) {
                        Array.from(matches).forEach((match) => {
                          locations.push({
                            span: child,
                            matchText: match[0],
                            path: path.concat(['children', {_key: child._key}]),
                            offset: match.index || 0,
                            message: entry.message,
                            level: 'error',
                          })
                        })
                      }
                    })
                  }
                })
              }
              if (locations.length) {
                return {
                  message: `${locations.map((item) => item.message).join('. ')}.`,
                }
              }
              return true
            }),
          ],
        }),
      ],
    },
  ],
})

```

In the code example, `onPaste` defines a custom paste handler for the `PortableTextInput` component.

- It checks if the clipboard data is a URL; if so, it inserts the URL as an inline resource object in the text block.
- The default insert location is the current cursor position. It's also possible to assign a different insert location by setting an optional `path` prop.



# Create a Portable Text behavior plugin

You can add custom behaviors to your studio's Portable Text Editor (PTE) by creating custom React components, hooking into the editor's behavior API, registering a plugin, and adding it to your configuration or schema.

> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

In this guide we'll create a behavior plugin that auto-closes bracket pairs. For example, when users type an opening bracket (`{`), the editor will automatically add a closing bracket (`}`) and move the cursor in between them. The focus of this guide is on incorporating PTE behaviors in Studio. Additional details for working with the Behaviors API can be found in the [Portable Text Editor documentation](https://portabletext.org).

Prerequisites:

- `sanity` version 3.92.0 or higher is required for Studio to apply plugins.

## Create the custom behavior component

Navigate to your studio's project directory and add the latest version of `@portabletext/editor` with the package manager of your choice. For example:

```sh
npm i @portabletext/editor 
``````sh
pnpm add @portabletext/editor
```

PTE Plugins are React components. This allows them to maintain their own state, handle additional logic, and render any components they need. 

Create a new component file in the location of your choice. We recommend creating a `plugins/pte/` directory or similar. This example names the file `auto-close-brackets-plugin.tsx`.

Start by importing the dependencies you'll need to create the plugin.

```tsx
import {useEditor} from '@portabletext/editor'
import {defineBehavior, execute} from '@portabletext/editor/behaviors'
import {useEffect} from 'react'
```

### Define the behavior

Next, in the same file, define the behavior using the `defineBehavior` helper.

```tsx
// ... imports
const autoCloseBracketsBehavior = defineBehavior({
  on: 'insert.text',
  guard: ({event}) => {
    const bracketPairs: Record<string, string | undefined> = {
      '(': ')',
      '[': ']',
      '{': '}',
    }
    const lastInsertedChar = event.text.at(-1)
    const closingBracket =
      lastInsertedChar !== undefined ? bracketPairs[lastInsertedChar] : undefined
    if (closingBracket !== undefined) {
      return {closingBracket}
    }
    return false
  },
  actions: [
    ({event}) => [
      execute(event),
    ],
    (_, {closingBracket}) => [
      execute({
        type: 'insert.text',
        text: closingBracket,
      }),
      execute({
        type: 'move.backward',
        distance: closingBracket.length,
      }),
    ],
  ],
})
```

All behaviors follow this process:

17. **Listen for an event**. In the example, `on` listens for a `insert.text` event. 
17. **Use a guard to decide if they should run or not**. In the example, guard checks if the last inserted character matches any bracket characters. If it does, it returns the closing character to pass it on to the next step.
17. **Trigger a set of actions to perform on the editor**. In the example, we first send the original action back to insert the first bracket into the editor. Then we send a pair of actions to insert the closing bracket and move the cursor over one place so it rests between the brackets.

This guide doesn't go much further into the Behaviors API. You can learn more about these concepts in the [Portable Text Editor](https://www.portabletext.org/) documentation.

### Create a React component to register the behavior

Next, in the same file, create a function component to register the new behavior with the PTE.

```tsx
// ... imports
const autoCloseBracketsBehavior = defineBehavior({ ... })

export function AutoCloseBracketsBehaviorPlugin() {
  const editor = useEditor()
  
  useEffect(() => {
    const unregisterBehavior = editor.registerBehavior({
      behavior: autoCloseBracketsBehavior,
    })
  
    return () => {
      unregisterBehavior()
    }
  }, [editor])
  
  return null
}
```

Aside from some React conventions, this code does one core task:  it registers the `autoCloseBracketsBehavior` from the previous step with any instance of the PTE it is attached to (we'll do this soon). Other behaviors may use this space to perform additional logic like state management.

Finally, to make adding the plugin to your schema and config files easier, create a file to export a function that wraps the plugin (or group of plugins). This is optional, but makes it easier to use them as [custom form components](/docs/studio/form-components) without changing your file types to support TSX.

```tsx
import type { PortableTextPluginsProps } from 'sanity'
import { AutoCloseBracketsBehaviorPlugin } from './auto-close-brackets-plugin'

export function PortableTextEditorPlugins(props: PortableTextPluginsProps) {
  return (
    <>
      {props.renderDefault(props)}
      <AutoCloseBracketsBehaviorPlugin />
      {/* Add any other plugins here */}
    </>
  )
}
```

Here's our completed `auto-close-brackets-plugin.tsx` and plugin `index.tsx` file:

```tsx
// plugins/pte/auto-close-brackets-plugin.tsx
import {useEditor} from '@portabletext/editor'
import {defineBehavior, execute} from '@portabletext/editor/behaviors'
import {useEffect} from 'react'
/**
 * This Studio Plugin shows how to:
 *
 * 1. Define a standalone and portable Behavior using `defineBehavior`
 * 2. Register the Behavior using `editor.registerBehavior` inside a React component
 * 3. Package the component as a plugin to import into a Studio config
 */

/**
 * This Behavior will auto-close brackets when the user inserts an opening
 * bracket. It will also move the cursor in between the brackets so the user
 * can start typing immediately.
 */
const autoCloseBracketsBehavior = defineBehavior({
  on: 'insert.text',
  guard: ({event}) => {
    const bracketPairs: Record<string, string | undefined> = {
      '(': ')',
      '[': ']',
      '{': '}',
    }
    const lastInsertedChar = event.text.at(-1)
    const closingBracket =
      lastInsertedChar !== undefined ? bracketPairs[lastInsertedChar] : undefined

    if (closingBracket !== undefined) {
      // Pass the closing bracket to the actions for reuse
      return {closingBracket}
    }

    return false
  },
  actions: [
    ({event}) => [
      // Execute the original event that includes the opening bracket
      execute(event),
    ],
    (_, {closingBracket}) => [
      execute({
        type: 'insert.text',
        text: closingBracket,
      }),
      execute({
        type: 'move.backward',
        distance: closingBracket.length,
      }),
    ],
  ],
})

export function AutoCloseBracketsBehaviorPlugin() {
  const editor = useEditor()

  useEffect(() => {
    const unregisterBehavior = editor.registerBehavior({
      behavior: autoCloseBracketsBehavior,
    })

    return () => {
      unregisterBehavior()
    }
  }, [editor])

  return null
}
``````tsx
import type { PortableTextPluginsProps } from 'sanity'
import { AutoCloseBracketsBehaviorPlugin } from './auto-close-brackets-plugin'

export function PortableTextEditorPlugins(props: PortableTextPluginsProps) {
  return (
    <>
      {props.renderDefault(props)}
      <AutoCloseBracketsBehaviorPlugin />
      {/* Add any other plugins here */}
    </>
  )
}
```

### Integrate the plugin with your studio schema

There are two ways to add this PTE plugin to your studio. Globally for all PTE blocks, or locally to specific blocks in your schema.

#### Globally

To apply the plugin to all PTE instances throughout your studio, you can add it globally by setting it as the `pte` form component.

In your studio config file, use the Form Components configuration to add the plugin as shown in this example.

```
import { defineConfig } from "sanity"
import { PortableTextEditorPlugins } from './plugins/pte'

export default defineConfig({
  // ...
  form: {
    components: {
      portableText: {
        plugins: PortableTextEditorPlugins,
      },
    },
  },
  // ...
})
```

#### Locally

Sometimes you want certain plugins for certain PTE fields. In those instances, customize the component in field for the schema type.  For example:

```
import { defineType } from 'sanity'
import { PortableTextEditorPlugins } from './plugins/pte'

export const post = defineType({
  title: 'Blog post',
  name: 'post',
  type: 'document',
  fields: [
    // ...
    {
      type: 'array',
      name: 'content',
      title: 'Post Body',
      of: [
        {
          type: 'block',
        }
      ],
      components: {
        portableText: {
          plugins: PortableTextEditorPlugins,
        }
      }
    },
    // ...
  ],
})
```

## Optional: Composing multiple plugins

The above examples use the `PortableTextEditorPlugins` function to prepare the behavior for inclusion in Sanity schemas and configuration files. You can also do this in-line in the schema or configuration by converting those files to `tsx|jsx` files and using the `AutoCloseBracketsBehaviorPlugin` directly instead. For example:

```tsx
import { defineConfig } from "sanity"
import { AutoCloseBracketsBehaviorPlugin } from './plugins/pte/auto-close-brackets-plugin.tsx'

export default defineConfig({
  // ...
  form: {
    components: {
      portableText: {
        plugins: (props) => {
        return (
          <>
            {props.renderDefault(props)}
            <AutoCloseBracketsBehaviorPlugin />
          </>
        )
        },
      }
    }
  },
  // ...
})
```

Include additional plugins as needed, for example:

```tsx
import { defineConfig } from "sanity"
import { AutoCloseBracketsBehaviorPlugin } from './plugins/pte/auto-close-brackets-plugin.tsx'
import { SomeOtherPlugin } from './plugins/custom/plugins.ts'

export default defineConfig({
  // ...
  form: {
    components: {
      portableText: {
        plugins: (props) => {
        return (
          <>
            {props.renderDefault(props)}
            <AutoCloseBracketsBehaviorPlugin />
            <SomeOtherPlugin />
          </>
        )
        },
      }
    }
  },
  // ...
}) 
```

Additionally, you can use this same approach with the earlier `PortableTextEditorPlugins` technique to package groups of plugins together.



# Introduction

Sanity Studio lets you customize your editorial experience by overriding different parts of the Sanity Studio with your own components written in React. The customized components can be split into two main categories: 

- Studio components- Layout
- Navbar
- Tool menu


- Form components- Fields
- Inputs
- Array items
- Preview



## Studio components

The `studio.components` configuration property accepts replacements for several parts of the studio UI, such as the `layout`, `navbar`, and `toolMenu`. Studio components can be declared in your root workspace configuration, i.e. the `defineConfig` function, or as part of a plugin config, i.e. the `definePlugin` function.

```javascript
// sanity.config.js
import {defineConfig} from 'sanity'

export default defineConfig({
  // ...rest of config
  studio: {
    components: {
      layout: MyLayout,
      navbar: MyNavbar,
      toolMenu: MyToolMenu,
    },
  },
})
```

> [!WARNING]
> Gotcha
> logo is deprecated
> 
> Custom logo components are no longer rendered.
> 
> Instead, provide custom components for individual workspace icons in the Studio configuration.

[Studio components](/docs/studio/studio-components)

[Reference: Studio components](/docs/studio/studio-components-reference)



## Form components

The `form.components` property deals with the rendering of form fields and inputs in the studio. The components available for customizing are `field`, `input`, `item` and `preview`. Form components can be declared in your root workspace configuration, i.e. the `defineConfig` function, as part of a plugin config, i.e. the `definePlugin` function, or individually on any field in your schemas.

```typescript
// sanity.config.js
import {defineConfig} from 'sanity'

export default defineConfig({
  // ...rest of config
  form: {
    components: {
      field: MyField,
      input: MyInput,
      item: MyItem,
      preview: MyPreview,
    },
  },
})
```

[Form components](/docs/studio/form-components)

[Reference: Form components](/docs/studio/form-components-reference)



## Composing components with `renderDefault`

The components available in this API are rendered using a middleware pattern. This means that plugin customizations are applied cumulatively in a chain or cascade. Each component declaration receives a callback function named `renderDefault` which, as the name implies, will defer to the default studio rendering of the component. When you call `renderDefault` you also pass along the `props` needed to render the component, with any changes you care to make.

```typescript
import { Stack, Card, Flex, Text } from '@sanity/ui'

// Adds markup and invokes renderDefault()
function MyEnhancedNavbar(props) {
  return (
    <Stack>
      <Card padding={3} tone="caution">
        <Flex justify="center">
          <Text>Important reminder! Remember this banner!</Text>
        </Flex>
      </Card>
      <>{props.renderDefault(props)}</>
    </Stack>
  )
}
```

![Shows a studio navbar customized to display a yellow background banner on top that says “Import reminder! Remember this banner”](https://cdn.sanity.io/images/3do82whm/next/6bf579c17450e7521b5cddc92a84e451457f8f47-728x421.png)

You may opt not to call `renderDefault` if you want to replace the component in question in its entirety with your own markup, but be aware that doing so in a plugin might result in unexpected behavior as it breaks the middleware chain.

## Typical use cases/problems this solves

- Hide certain tools when the studio is in development mode with a custom `toolMenu`
- Wrap your studio with multiple context providers with a custom `layout` component
- Create a custom `input` to display a range slider on a `number` field, or add a character counter on all `string` fields

## Related and further reading

- [Studio components](/docs/studio/studio-components)
- [Reference – Studio components API](/docs/studio/studio-components-reference)
- [Form components](/docs/studio/form-components)
- [Reference – Form components API](/docs/studio/form-components-reference)





# Custom authentication

Custom authentication can be configured for the studio or individual workspaces. This is done by configuring the root config key `auth` for the studio or workspace with a configuration object that adheres to the [AuthConfig](/docs/reference/api/sanity/AuthConfig) signature.

```javascript
import {defineConfig} from 'sanity'

export default defineConfig({
    ..., // The rest of the other studio config.
    auth: {
      projectId: 'dsf3cqw',
      dataset: 'production',
      mode: 'replace',
      redirectOnSingle: false,
      providers: [
        {
          name: 'enterprise-sso',
          title: 'My Enterprise SSO',
          url: 'https://my-enterprise.com/login',
        },
      ],
    },
  })
```

> [!WARNING]
> Gotcha
> In studio versions prior to v3.15.0 the recommended way to configure custom authentication included using the createAuthConfig helper method. This approach will still work, but is considered deprecated in favor of the more straight forward new method.





# Custom asset sources

Sanity Studio comes with a rudimentary asset selector out of the box. It lets you browse and select images or files you have already uploaded. You can also add multiple asset sources, or replace the default one, globally or for a specific asset field.

## Asset source plugins

You can find available asset source plugins in the [Sanity Exchange](https://www.sanity.io/exchange) or by searching for them on [npmjs.com](https://www.npmjs.com/search?q=sanity+plugin+asset). Just like other plugins, asset source plugins are installed using your preferred package manager. Some asset source plugins will require that you add some configuration, for example, an API token. 

When adding custom asset source plugins to your studio, the Select button for the upload field will become a drop-down button, showing the multiple sources:

![The image asset selector showing both uploaded images, Unsplash and Cloudinary](https://cdn.sanity.io/images/3do82whm/next/510b684ba065041c2b3ce813a2c19774b4d0915b-2086x1400.png)



## Defining asset sources globally

Assets sources that are distributed as npm packages usually come with a plugin definition for easy setup. 

Let's say you want to add the [Unsplash asset source](https://www.sanity.io/plugins/sanity-plugin-asset-source-unsplash). First, install the plugin by running `npm i sanity-plugin-asset-source-unsplash` in your project folder. Then, in `sanity.config.js`, add the following:

```javascript
import {defineConfig} from 'sanity'
import {deskTool} from 'sanity/desk'
import {unsplashImageAsset} from 'sanity-plugin-asset-source-unsplash'
import {schemaTypes} from './schemas'

export default defineConfig({
  name: 'default',
  projectId: '<projectId>',
  dataset: 'production',
  plugins: [
    deskTool(),
    unsplashImageAsset(),
  ],
  schema: {
    types: schemaTypes,
  },
})

```

Adding `unsplashImageAsset()` to the plugins array will deal with registering the asset source and adding it to the list of assets sources for images in your project.

![The Studio default dialog for uploading images with the new Unsplash option added](https://cdn.sanity.io/images/3do82whm/next/0c31eb97488f7d043f32a469ebbd0adeaf2b7cbb-589x184.png)

If you want to *only* allow the Unsplash asset source instead of adding it to the default upload option, you can instead import `unsplashAssetSource` and add it to `form.image` as the sole member of the returned array value.

```javascript
import {defineConfig} from 'sanity'
import {deskTool} from 'sanity/desk'
import {unsplashAssetSource} from 'sanity-plugin-asset-source-unsplash'
import {schemaTypes} from './schemas'

export default defineConfig({
  name: 'default',
  projectId: '<projectId>',
  dataset: 'production',
  plugins: [deskTool()],
  form: {
    image: {
      assetSources: () => [unsplashAssetSource],
      directUploads: false,
    },
  },
  schema: {
    types: schemaTypes,
  },
})

```

> [!WARNING]
> Gotcha
> Many properties of the studio configuration can accept both a static value – an array of asset sources in this case – or a callback function that returns that same value. One crucial difference between the two is that providing a static array of sources will append those sources to the list of existing sources that may have been added by plugins or the studio's default settings, while returning an array of sources from the callback function will replace the current list of sources.
> 
> The callback is invoked with the current list of sources as the first argument, so to append to the list when using the callback option you might do something like this: assetSources:(prev)=>[...prev, unsplashAssetSource]



### Using sources on a single type

You can customize sources for single image or file type field in the schema via the `options.sources` property:

```javascript
{
  name: 'mainImage',
  title: 'Main image',
  type: 'image',
  options: {
    sources: [unsplashAssetSource],
  },
}
```

### Remove the Browse option

You can remove the Browse button on an image field (making the field upload-only) by specifying `options.sources` as an empty array:

```javascript
{
  name: 'uploadedImage',
  title: 'Upload an Image',
  type: 'image',
  options: {sources: []}
}
```

## Anatomy of an asset source plugin

The plugin exports an object with the following shape:

```javascript
export default {
  name: 'cloudinary', // Unique source name
  title: 'Cloudinary', // Title displayed in lists, buttons etc
  component: Cloudinary, // Selection component
  icon: Icon // Icon for lists, buttons etc.
}

```



## The selection component

The plugin must define a **component** that will let the user select some asset(s) from somewhere.

If the user selects something, the component calls the `props.onSelect` function with an array of asset objects like this:

```javascript
type AssetFromSource = {
  kind: 'assetDocumentId' | 'file' | 'base64' | 'url'
  value: string | File
  assetDocumentProps?: ImageAsset
}
```

An asset can be a URL, user agent File object, base64 encoded binary data or an assetDocumentId. It can have `assetDocumentProps` that will end up as properties on the resulting asset document. The allowed document props are:

#### Properties

| Property | Description |
|----------|-------------|
| originalFilename | If you would like to use the original filename, when saving the file etc. |
| source | {name, id, url?} - Optional object identifying the asset in the source, so you can find all assets from that source, or find it back to the specific assets when opening the plugin etc. If set, the object properties  name and id are required, but url is optional. An example for Instagram images: {name: 'instagram', id: '_cjqbJKwZB', url: 'https://www.instagram.com/p/_cjqbJKwZB/'} |
| title | Optional title for the asset. |
| description | Optional description for the asset. |
| creditLine | Optional credit line for the asset. E.g. John Doe by Instragram |
| label | Optional label. |


#### Component Props

#### Properties

| Property | Description |
|----------|-------------|
| selectionType * | If the opening interface selection type is 'single' or 'multiple'. |
| selectedAssets * | An array of Sanity assets if they are selected in the opening interface. These are Sanity asset documents. |
| onSelect * | Accepts an array of asset objects (AssetFromSource[])

When assets are selected and returned to props.onSelect, the Studio will make sure to upload the asset(s). If the selected asset is uploaded previously, the existing asset document and file will be used instead. |
| onClose * | The component must call props.onClose if the select action is canceled or closed somehow. |
| dialogHeaderTitle | A component that serves as the header element for the dialog window. |
| assetType | Either file or image |


## Basic component example

The following code shows how to implement a selection component for an asset source plugin. It's not very useful as it will only allow you to pick one very specific image, but it should serve nicely as an example.

```jsx
import React, {useCallback} from "react";
import {
  Dialog,
  Card,
} from "@sanity/ui";

export default function GitHubAssetSource({ onSelect, onClose }) {
  const handleSelect = useCallback(() => {
    onSelect([
      {
        kind: "url",
        value:
          "https://github.githubassets.com/images/modules/site/sponsors/logo-mona.svg",
        assetDocumentProps: {
          originalFilename: "logo-mona.svg", // Use this filename when the asset is saved as a file by someone.
          source: {
            // The source this image is from
            name: "github.githubassets.com",
            // A string that uniquely idenitfies it within the source.
            // In this example the URL is the closest thing we have as an actual ID.
            id: "https://github.githubassets.com/images/modules/site/sponsors/logo-mona.svg",
          },
          description: "Mona Lisa Octocat",
          creditLine: "By Github.com",
        },
      },
    ]);
  }, [onSelect]);

  const handleClose = useCallback(() => {
    onClose();
  }, [onClose]);

  return (
    <Dialog
      id="github-asset-source"
      header="Select image from Github"
      onClose={handleClose}
      width={4}
      open
    >
      <Card>
        <img
          src="https://github.githubassets.com/images/modules/site/sponsors/logo-mona.svg"
          onClick={handleSelect}
        />
      </Card>
    </Dialog>
  );
}

```

> [!WARNING]
> Gotcha
> CORS headers for image URLs
> 
> When calling onSelect with  kind: 'url' the resource must respond with a access-control-allow-origin header that allows the image to be read by the Studio host. Using * will allow all hosts (including Studio host). 

> [!TIP]
> Protip
> Best practice
> 
> When integrating with an external service, be sure to read the usage guidelines for that service or API. Some will require you to honor the credits for the asset, not expose any API keys etc. Use the assetDocumentProps for onSelect to store any required or relevant information to the resulting asset document. If it is from a service where the asset has an ID and can be displayed in the service, you should use the source key for the assetDocumentProps to store that information. In that way, you can find back to the original asset.



# Diff components

![Annotated Diff screen for default String, Image, and Portable Text fields](https://cdn.sanity.io/images/3do82whm/next/15d75cf306395056afd671d096f7298ccf75aef0-2842x1590.png)



With Sanity Studio, you can see, in real time, any changes happening within a given field. You can see these changes down to the smallest detail. 

Out of the box, this will render the difference in all basic types - strings, numbers, booleans, arrays and similar. If you have your own custom inputs, the changes will be visible, but will only show the changes between two sets of data. 

Often, the difference between two values might not be enough to showcase exactly what happened inside a custom input. A color field changing between two hexadecimal values might not be human readable. A visualization of the two colors would be helpful to humans. A geopoint is an object containing latitude and longitude numbers. Showing the difference between those two numeric numbers won't mean much, but showing two map pins representing those locations can mean a lot.

The Studio API allows you to write your own custom React components that can visualize these changes in ways that make sense to your editors.

## Anatomy of a Diff Component

The components responsible for showing the change of a value are called "**diff components**". These React components receive a structured `diff` object allowing you to inspect the change values deeply. This object contains data on not only the change itself, but also who made the change and when it was made.

When you compare values over time, different parts of a value may have different authors. For instance, one person may have uploaded an image, while a second person provided the caption, and a third changed a crop. In order to provide this information in a fine-grained manner, the diffs contain **annotations**.

**Annotations** can be used to render the name and avatar of the users who did the changes, and also contain the exact timestamp of the change, should you want very fine-grained control.

## Actions

There are four basic actions that might happen to a value:

- `action: "changed"` - The value changed (from value X to value Y)
- `action: "added"` - The value was added (field or array item appeared)
- `action: "removed"` - The value was removed (field or array item disappeared)
- `action: "unchanged"` - The value was unchanged (item was moved within an array)

These actions can be inspected on the `diff` object provided to the custom diff component. All of these actions should be accounted for in a diff component.

## Adding a custom diff component to a field

You can add a custom diff component to any field using the `diff` property of the field's `components` object. To learn more about field components in general, visit the [field component docs](/docs/archive/custom-field-components).

```jsx
import {CustomStringDiff} from '../src/components/field'

export default {
  name: 'product',
  title: 'Product',
  type: 'document',
  fields: [
    {
      name: 'title',
      title: 'Title',
      type: 'string',
      components: {
        diff: CustomStringDiff,
      }
    },
    // ... Additional fields
  ]
}
```

In this example, we assume that our `CustomStringDiff` component is exported from a file located at `../src/components/field/index.jsx` relative to our schema file. A minimalist implementation is shown below.

```jsx
../src/components/field/index.jsx
import {DiffFromTo} from 'sanity'

export function CustomStringDiff({diff, schemaType}) {
  return (
    <DiffFromTo
      diff={diff}
      schemaType={schemaType}
      previewComponent={StringDiffPreview}
      layout="grid"
    />
  )
}

function StringDiffPreview({ value }) {
  return <div style={{borderLeft: '5px solid', padding: '3px', display: 'flex'}}>{value}</div>
}
```

## Creating a basic diff component

The core `sanity` package includes a few helper components and methods to help you build custom diffs. The most basic among these is the `<DiffFromTo />` component. This creates a view that shows an initial state of a field and the new state with an arrow in the middle. You can customize how the those blocks are rendered with a `previewComponent`. 

The following example creates a custom preview for the component to format a country code for a phone number. If a field goes from blank to populated, it will show the addition. If a field goes from populated to empty, it will show a strike-through on the value. If there's a change to the value, it will show a before and after rendered with the preview component. 

```jsx
import {DiffFromTo} from 'sanity'

export function PhoneNumberFieldDiff({diff, schemaType}) {
  return (
    <DiffFromTo
      diff={diff}
      schemaType={schemaType}
      previewComponent={PhoneNumberPreviewComponent}
    />
  )
}

function PhoneNumberPreviewComponent({value}) {
  const prefix = value.countryCode ? `(${value.countryCode}) ` : ''
  const formatted = `${prefix}${value.number}`
  return <span>{formatted}</span>
}

```

## Creating the states of a diff component

In this example, a different layout is used to render each state of the diff. 

> [!WARNING]
> Gotcha
> It might seem strange at first to provide an unchanged state for our custom diff. This is to provide for states when the value itself hasn't changed, but data around the value have, such as when an item has been moved in an array.

```jsx
import {DiffCard} from 'sanity'

export function NumberFieldDiff({diff}) {
  const {fromValue, toValue, action} = diff

  // In certain cases the diff component will be used to render the value, even
  // if there are no changes to the actual _value_. For instance, when an item
  // has been moved within the array, but the actual value did not change.
  if (action === 'unchanged') {
    return <div>{fromValue}</div>
  }

  // If we have both a "from" and "to" value, the value changed
  // "from" and "to" can also be read as "previous" and "next"
  if (typeof fromValue === 'number' && typeof toValue === 'number') {
    return (
      <DiffCard diff={diff}>
        {fromValue} → {toValue}
      </DiffCard>
    )
  }

  // If we only have a "previous" value, the value has been unset
  if (typeof fromValue === 'number') {
    return (
      <DiffCard diff={diff}>
        <del>{fromValue}</del>
      </DiffCard>
    )
  }

  // The only remaining option is that the value was added
  return (
    <DiffCard diff={diff}>
      <ins>{toValue}</ins>
    </DiffCard>
  )
}
```

## Helper components

The `sanity` package includes a range of helpful components and methods for building custom diff views. Let's take a closer look at a few of them.

### `<DiffFromTo />`

Many diff components will follow a pattern where they have a "preview" component that renders the value, and presents a "from → to" layout, setting the background to the "user color" for the change and adding a tooltip when hovering the diff that shows the author information.

You can use this pattern to graphically showcase the change. This can help the editor to quickly understand at a glance the change that happened.

```jsx
import {DiffFromTo} from 'sanity'

export const TelephoneFieldDiff = ({diff, schemaType}) => (
  <DiffFromTo
    diff={diff}
    schemaType={schemaType}
    previewComponent={TelephonePreview}
    layout="inline" // "grid" is also an option
  />
)

function TelephonePreview({value}) {
  const formattedNumber = value.toString().replace(/\d{3}(?=.)/g, '$& ')
  return <>{formattedNumber}</>
}

```

### `<ChangeList />`

The pattern mentioned in the section on `<DiffFromTo />` pairs nicely with the `<ChangeList />` helper. Often when displaying a graphical diff using `<DiffFromTo />`, you may also wish to show the individual fields that were changed to create the overall change. The `<ChangeList />` component takes a `diff`, `schemaType`, and an array of `fields` to render individual fields to show.

In this example, we generate a barcode from two fields on an `object` field type. We use the same visual component used in the custom input, but also use `<ChangeList />` to show the individual field changes for `barcode` and `format`.

```jsx
import {DiffFromTo, getDiffAtPath, ChangeList} from 'sanity'
import Barcode from 'react-barcode' 

export function BarCodeDiff({diff, schemaType}) {
  return (
    <div>
      <DiffFromTo
        diff={diff}
        schemaType={schemaType}
        previewComponent={BarCodeDiffPreviewComponent}
        layout="grid"
      />
      <ChangeList diff={diff} schemaType={schemaType} fields={['barcode', 'format']} />
    </div>
  )
}

function BarCodeDiffPreviewComponent({value}) {
  return (
    <div style={{display: 'flex', padding: '5px', justifyContent: 'center', alignItems: 'center'}}>
      <Barcode textAlign="center" value={value.barcode} format={value.format || ''} width={1} />
    </div>
  )
}
```



### `<FromTo />`

When creating a completely custom diff component, you can use the `<FromTo />` component to specify components to be used for the "From" and "To" states of the component. If you opt for this component, you will need to recreate many of the affordances given, such as tooltips, user background colors, and more.

```jsx
import React from 'react'
import {FromTo} from '@sanity/field/diff'

export const SomeDiff = () => (
  <FromTo
    from={<div>Old value</div>}
    to={<div>New value</div>}
    layout="inline" // "grid" is also an option
  />
)
```

### `<DiffCard />`

Renders a container element styled with the appropriate user color, based on the passed diff or annotation.

```jsx
import {DiffCard} from 'sanity'

export const SomeDiff = ({diff}) => (
  <DiffCard as="pre" diff={diff}>
    <code>{JSON.stringify(diff.toValue, null, 2)}</code>
  </DiffCard>
)
```

### `<DiffTooltip />`

Wraps the passed children with a tooltip when hovered, showing information about the actual change - which authors were involved, and when. This component differs slightly in that it can take multiple annotations instead of just a single one, combining the information in a single tooltip. Example:

```jsx
import {DiffTooltip} from 'sanity'

export function MovieReviewDiff({diff}) {
  const {action, fromValue, toValue} = diff
  if (action === 'unchanged') {
    return (
      <div>
        <StarMeter value={fromValue} diff={diff} />
      </div>
    )
  }
  return (
    <div>
      {fromValue && <StarMeter value={fromValue} diff={diff} />}
      {fromValue && toValue && '→'}
      {toValue && <StarMeter value={toValue} diff={diff} />}
    </div>
  )
}

function StarMeter({value, diff}) {
  const {numStars, comment} = value
  return (
    <div>
      {numStars && (
        <DiffTooltip diff={diff}>
          <div>{'★'.repeat(numStars)}</div>
        </DiffTooltip>
      )}
      {comment && (
        <DiffTooltip diff={diff}>
          <div>{comment}</div>
        </DiffTooltip>
      )}
    </div>
  )
}
```

## Helper Hooks

### `useDiffAnnotationColor(diff, path)`

Takes a diff and an optional path as arguments and returns the corresponding user color for it. A user color is an object with the keys `background`, `text` and `border`, each having a hex color as the value. 

```jsx
import {useDiffAnnotationColor} from 'sanity'

export function MovieReviewDiff({diff}) {
  const {background, text} = useDiffAnnotationColor(diff, 'numStars')
  return <div style={{background, color: text}}>Do some fancy logic here</div>
}
```

### `useAnnotationColor(annotation)`

Like `useDiffAnnotationColor`, but takes an annotation directly instead of a diff.

```jsx
import {useAnnotationColor} from 'sanity'

export function PhoneNumberDiff({diff}) {
  // Note: `diff.annotation` might not be what you want for many diff types!
  // See "diff annotations" section for more information
  const {background, text} = useAnnotationColor(diff.annotation)
  return <div style={{background, color: text}}>Do some fancy logic here</div>
}
```

## Shape of the data on a diff object

The `diff` received in diff components vary depending on the data type represented. All diffs share a common set of properties.

- `action` - either `added`, `removed`, `changed` or `unchanged`.
- `type` - the value type, eg `string`, `number` etc
- `fromValue` and `toValue` - holding the previous and next value
- `isChanged` - a boolean indicating whether or not the actual content changed

Unless the action is `unchanged`, the diff will also have an `annotation` property which is often needed to show information about the author of the change, as well as a timestamp and other similar metadata. Make sure you read the section on "diff annotations"!

### Strings

String diffs have an additional `segments` property, which is an array containing parts of the string which have been added, removed or did not change. This is useful when comparing larger chunks of text, as instead of simply saying the paragraph was "replaced", we can say that a single word was changed. It also allows individual segments of the text to be attributed to different authors.

The `<DiffString />` component can help render a visualization of these changes - or you can iterate over the segments and render them yourself should you want to.

### Arrays

An array rarely has any changes done to "itself", apart from perhaps being set from an undefined state to an empty array. What you are usually interested in is the values it holds, and the locations of those items.

Determining if something was added, removed, or moved within the array is done on a best effort basis, and the algorithm attempts to explain the change with as few "operations" as possible.

Why is it a best guess? From a technical perspective, if an item was at index 0 and now appears at index 1, it has "moved". If that "move" was the result of an item being prepended to the array, the more "natural" way of thinking about it is that it did *not* move.

Array diffs contain an `items` property, which itself is an array of changes to the items within the array. Each item has the following properties:

- `hasMoved` - a boolean indicating whether or not the array item moved, based on the algorithm's best guess.
- `fromIndex` and `toIndex` - containing the previous and next location of the item within the array. As noted above, `fromIndex` and `toIndex` can differ without the `hasMoved` property being `true`, because an add/remove operation could have shifted the indexes.
- `diff` - represents the actual item diff

Note that the `annotation` on the array diff is very coarse. When illustrating changes to data inside an Array item, it's best to do that with a diff component for the item with the item's `annotations` and not for the Array.

### Objects

Like arrays, objects rarely have a value of their own, apart from being set from an undefined state to an empty object. To access the individual fields of an object, the `fields` property is itself an object where the key is the name of the field and the value is the diff for that field. Certain underscore-prefixed fields are ignored when calculating the diff (`_id`, `_type`, `_createdAt`, `_updatedAt`, `_rev`).

## Diff annotations

Diff annotations hold fine-grained information on the change, and are present on individual "leaves" of the diff structure. It contains an `author` property (user ID of the person doing the change) and a timestamp for when the change occurred.

At first glance, this may seem like an unnecessary abstraction, but given the granular structure of annotations, we can create a deeper understanding for our editors. 

```json
{
  "asset": {
    "_ref": "image-someHash-1024x768-png"
  },
  "crop": {
    "bottom": 100,
    "top": 100,
    "left": 20,
    "right": 20
  },
  "caption": "Kokos is a miniature schnauzer"
}
```

When comparing two versions of this image data structure, several things can have changed, for instance:

- The asset reference can have changed
- The numbers in `crop` can have appeared
- The caption may have been edited or created

Using the annotation on the image field itself would give you very coarse information, as if a single author performed the entire set of changes. Instead, we want to look at the annotations on the individual fields, and for the string fields even inspect individual segments of the string.

The `getAnnotationAtPath` function allows for easy retrieval of these annotations at depth.

Note that in certain cases it might make sense to *not* be as fine-grained. Theoretically, two users could have edited the `crop` object above - for instance, one user could have increased the width and a different user only modifying the height, thus touching `bottom`/`top` and `left`/`right`, respectively. Showing individual authors for each side of a rectangle *might* be hard to visualize.

## Usage with TypeScript

Writing diff components with TypeScript is fully supported. The diff tools in the `sanity` package includes not only the helper functions, hooks and React components you may want to use, but also a range of type definitions.

When defining a diff component, you can type it as a `DiffComponent`, and specify which type of diff you expect for the component. For instance, a diff for a geopoint object type might look like this:


```typescript
import {ObjectDiff, DiffComponent} from 'sanity'

interface Geopoint {
  lat: number
  lng: number
  alt?: number
}

export const GeopointDiff: DiffComponent<ObjectDiff<Geopoint>> =
  function GeopointDiff(props) {
    const {diff, schemaType} = props
    
    console.log(diff.fromValue) // Geopoint | null | undefined
    console.log(diff.toValue) // Geopoint | null | undefined
    
    return <div>{/* your diff logic here */}</div>
  }

```



# Form Components



[Reference: Form API](/docs/studio/form-api-reference)



The `form.components` configuration property is available in your root studio configuration, in plugins, and on individual fields. It accepts the following component customizations:

```javascript
// sanity.config.js
import {defineConfig} from 'sanity'

export default defineConfig({
  // ...rest of config
  form: {
    components: {
      input: MyInput,
  		field: MyField,
  		item: MyItem,
  		preview: MyPreview,
    }
  }
})
```

The props for each component available in the API include a callback function called `renderDefault`. As the name implies, `renderDefault` renders the default component. When you call renderDefault, you also pass along the props needed to render the default component. You can modify the props to your liking before passing them along.

```javascript
// ./custom-string.js

import {Stack, Text, Card} from '@sanity/ui'

export function CustomStringInput(props) {
  return (
    <Stack space={3}>
      {props.renderDefault(props)}
      <Text size={1}>Characters: {props.value?.length || 0}</Text>
    </Stack>
  )
}
```

If you want to completely replace the component in question with your own markup, you can do so by neglecting to invoke `renderDefault` in your return. Be aware that doing so in a plugin setup might cause unexpected behavior because of the chainable nature of the components API (discussed in the next section).

## Composing renderDefault()

The rendering of components in this API uses a middleware pattern. This means that plugin customizations are applied in a chain. Each plugin may call `props.renderDefault(props)` to defer to default rendering. If any component in the chain fails to invoke the callback function, the chain breaks! To learn more about `renderDefault`, visit this [article on the component API](/docs/studio/intro-to-custom-studio-components).

## Input and Field components

The `input` and `field` custom components are easiest to understand when examined together. To demonstrate the difference between these two we’ll take a closer look at the anatomy of a field widget in the studio. In the illustration below the *field* includes everything within the purple dashed border conveniently marked “field”, while the *input* includes only what’s within the green dashed border marked “input”.

![Diagram showing that only the actual text field a user enters their input into belongs to the Input component, while elements such as title and description belongs in the Field component](https://cdn.sanity.io/images/3do82whm/next/e22848b0c971f0573db5dd83adc65ae1f7e49477-891x336.png)

Often, developers are chiefly interested in customizing the input widget itself and happy to leave the rest to studio defaults. In these cases, you would opt to replace `components.input`. If you do want to control the field in its entirety, you can do so by replacing the `components.field` component.

In the following example, we assign a custom field component – adding a border and transforming the title and description visually – to *all* studio fields. In contrast, we do a check to only assign a custom input component – adding a character count – if the field has the `string` schema type.



```javascript
// sanity.config.js

import {Stack, Text, Card} from '@sanity/ui'
import {defineConfig} from 'sanity'
import schemaTypes from './schemas'

function CustomStringInput(props) {
  return (
    <Stack space={3}>
      {props.renderDefault(props)}
      <Text size={1} style={{color: 'orange'}}>
        Characters: {props.value?.length || 0}
      </Text>
    </Stack>
  )
}

function CustomField(props) {
  const {description, title, ...restProps} = props
  return (
    <Card border padding={3}>
      <Stack space={3} marginBottom={3}>
        <Text size={1} weight="bold">
          {title?.toUpperCase()}
        </Text>
        {description && (
          <Text size={1} style={{color: 'green'}}>
            {description}
          </Text>
        )}
      </Stack>
      {props.renderDefault(restProps)}
    </Card>
  )
}

export default defineConfig({
  //...rest of config
  form: {
    components: {
      field: CustomField,
      input: (props) =>
				props.schemaType?.name === 'string' ? <CustomStringInput {...props} /> : props.renderDefault(props),
    },
  },
})
```

The result in the studio is that all fields are customized to use the `CustomField` component which transforms the title and description and adds a border around the field, while only the fields of type `string` are affected by the `CustomStringField` a component which adds a character count in bright orange.

![Three studio fields named Title, Author and Alug. All of them have a grey border. The slug and title fields have descriptions in green. The title field alone also has a word count in orange.](https://cdn.sanity.io/images/3do82whm/next/312c8a0db9eb2d8b98061e4d14046b65a0665782-652x438.png)

[Form Component Reference](/docs/studio/form-components-reference)

## Preview components

The preview component decides how an object, image, or reference value is displayed in list views. The illustration below shows an example of an array of objects, where each object has a `string` and an `image` field defined. The default preview component tries its best to guess which fields should be displayed by introspecting the defined fields of the object. 

![Studio screenshot that indeed shows the studio rendering the previews, presumedly by inferring which values to display](https://cdn.sanity.io/images/3do82whm/next/2ed2138b7f654bf3c7c1f55cef16d6d99437857f-2048x652.png)

Just like with input and field components – it is possible to configure a custom preview component. The custom preview component can be configured either in `sanity.config.js`, in a plugin, or directly in the schema definition. In the following example, we will configure a custom preview component directly in the schema definition.

To keep things simple, our custom preview component will be a `div` with a green border that wraps the default preview component rendered using `renderDefault`. However, it is possible to configure a completely custom component and not use `renderDefault`.

The following schema definition is what the illustration above represents. Since we want to configure a custom preview component for each object in the array, we add our component to the `components.preview` property for the object field definition.



```typescript
// ./custom-array.js

import {defineField} from 'sanity'

// Render a div that wraps the default preview component
function MyPreviewComponent(props) {
  return (
    <div style={{border: '1px solid green'}}>
      {props.renderDefault(props)}
    </div>
  )
}

export const arrayOfObjects = defineField({
  type: 'array',
  name: 'arrayOfObjects',
  title: 'Array of objects',
  of: [
    {
      type: 'object',
      name: 'myObject',
      title: 'My object',
      components: {
        preview: MyPreviewComponent, // Add custom preview component
      }, 
      fields: [
        {
          type: 'string',
          name: 'myString',
          title: 'My string',
        },
        {
          type: 'image',
          name: 'myImage',
          title: 'My image',
        },
      ],
    },
  ],
})

```

As you can see in the illustration below, our custom preview component is rendered.

![Shows the array of objects with previews that have a slim green border](https://cdn.sanity.io/images/3do82whm/next/5c13589455833f1a23c96ae0d1aeab71e40a393b-2970x976.png)

## Item components

The item component is the component that represents each item in an array field. The default item component contains a drag handle for sorting, a menu with actions – such as duplicate and delete – and some content. The content, that is, what is between the drag handle and the actions menu, varies based on what type of field(s) the item represents.

In an array of objects, a *preview component* is displayed as content, but in an array of primitive types (e.g boolean or string) – an *input component* is displayed as content.



![Example of items for object field. The object input is displayed in a dialog when clicking the item.](https://cdn.sanity.io/images/3do82whm/next/91ba5f304ed45332fd694a5ecd7f724d9de6e453-2402x836.png)

![Example of items for a string field. The string input is displayed inside the item (inline editing etc)](https://cdn.sanity.io/images/3do82whm/next/bf1b0854c5d4c915c1efa2daf5b6abd49c1f4a9b-2144x754.png)



## Typical use cases/problems this solves

- Decorate the default component, or modify the props passed to the default component, using `renderDefault`
- Create your own completely custom component



# How form paths work

A form path provides a unique and stable address for a value in a Sanity document. This is an important piece of the puzzle that makes Sanity a real-time platform where several people can work on editing the same document simultaneously.

## What are form paths?

Consider a scenario where two people work on the same document. When one of the two users edits the document, the modification is technically represented as a [patch](/docs/studio/from-input-components-to-real-time-safe-patches)—a description of the change.

The patch may look something like this:

`set ["name"] to "Buddy"`

The patch goes places:

- It’s sent to the Sanity API.
- It’s applied to the document in the API datastore.
- It’s redistributed to any other user editing the same document.

In this example, the `["name"]` array is a path that always and uniquely points to the `firstName` field. The contents of this field may change, but its location within the document doesn’t.

The `pets` array in the following example behaves differently:

```json
{"pets": [{"name": "Buddy"}, {"name": "Daisy"}]}
```

To point to Daisy in the array, use the following form path: `["pets", 1, "name"]` What’s the problem here? Let’s find out.

Consider two editors—Alice and Bob—updating the pet list at the same time: Alice moves Daisy to the top of the list, while Bob renames Buddy to Buds.

The patches that the studio generates for these two edits are similar to the following pseudocode:

- Bob: `set ["pets", 0, "name"] to "Buds"`
- Alice: `move ["pets", 1, 0]`

If these two patches are received in the order above, everything is fine: Buddy becomes Buds, and Daisy moves to the top of the list.
However, networks make it hard to set the order of actions reliably. For example, latency may cause Bob’s change to be received and processed after Alice’s:

- Alice: `move ["pets", 1, 0]`
- Bob: `set ["pets", 0, "name"] to "Buds"`

If the Sanity data store receives Alice's patch first, Buddy and Daisy swap places, and Daisy is located at array index `0`. When Bob’s patch is processed, Daisy is renamed to Buds, which isn’t what Bob meant to do! And poor Daisy now has to learn again what humans call her! Luckily, there’s a solution.

## Unique key IDs

To prevent this problem, array objects in Sanity must have a unique `_key` value.

This unique key is generated upon object creation. The key uniquely identifies the object it refers to, and it’s immutable throughout the lifetime of the object.

This is the pet array with the key IDs:

```json
{
  "pets": [
    {_key: "m99vcit2pho", "name": "Buddy"},
    {_key: "v1uf44s5hu8", "name": "Daisy"}
  ]
}
```

Instead of referencing the object by its index number—which may point to a different object if the order in the array changes—you can specify its unique key ID in the form path.
Here’s how you can use `_key` to point to Buddy’s name in a form path: `["pets", {_key: "m99vcit2pho"}, "name"]`

The order of the actions no longer matters. If you rerun the previous wrong-order scenario, everything is ok:

- Alice: `move ["pets", 1, 0]`
- Bob: `set ["pets", {_key: "m99vcit2pho"}, "name"] to "Buds"`

When Bob’s patch is received, Buddy has already been moved on the list. However, the datastore now identifies the pet name to update by its `_key`. Daisy is safe!

You probably noticed that Alice’s `move` patch still refers to the object by its array index.
Currently, this is a bit of a gray area: when an editor moves an item to the top of the array, what is their goal? Do they want to move the item *to the top*, or do they want to move it just *one level up from their current position* in the array?

![Alice and Bob both try to move an array member. In the example, Daisy is the array member they want to move.](https://cdn.sanity.io/images/3do82whm/next/804354b8fd7fad8e9a4afced96e6bdeb142ca649-1600x704.png)

Although this area is a bit ambiguous, changing the order of the array members doesn’t modify the wrong object, and Daisy is safe!

## Using form paths in Sanity Studio

When customizing Sanity Studio, you may sometimes run into situations where you’d like to point to a specific piece of content in a document. This is when form paths are helpful. For example:

- Reading the value of a node inside the active document being edited.
- Deep linking to a specific form node.
- Opening a dialog to edit an array item—for more information about this topic, see the article about [disclosure elements](/docs/studio/focus-and-ui-state-in-custom-inputs).

To support working efficiently with paths, we offer a set of utility functions in the [@sanity/util](https://www.npmjs.com/package/@sanity/util) package at the `@sanity/util/paths` export.

## Known limitations

This approach has limitations with arrays of primitive values and multidimensional arrays:

- It’s not possible to assign a key to a [primitive value](https://developer.mozilla.org/en-US/docs/Glossary/Primitive). Therefore, arrays of primitive values rely on the array index to locate a specific member. Since index references point to the position of an object in the array, not to the object itself, they’re also more likely to produce unexpected effects when multiple users modify the same array at the same time.
- It’s not possible to assign a key to multidimensional arrays, or arrays of arrays. Instead of creating multidimensional arrays, we recommend defining the data as an array of objects, where a field holds the inner array.

Example:

```json
"multiDimensional": [
  {_key: "3u285aqn8uo" inner: [1, 2, 3]},
  {_key: "0c3afift948" inner: [4, 9, 3]},
]
```



# Icons

## Icons

Use icons for types to display in the creation dialogue and when you're missing a media preview.

Helpful icons can improve the editorial experience, and can be applied in several contexts throughout the studio interface, such as in structures created in [Structure Builder](/docs/studio/structure-tool-api), and as [tool icons in the Portable Text Editor](/docs/studio/customizing-the-portable-text-editor).

Each document type can also be assigned an icon to illustrate its purpose. We recommend using an SVG file, but it can be any react component you like.

If you want to have your icons reflect the default style found throughout the Studio, you might opt to install the `@sanity/icons`-package. Another popular package is `react-icons` which includes a plethora of open source SVG icons from collections like [Font Awesome](https://fontawesome.com/), [Material Design](https://material.io/), [Typicons](http://s-ings.com/typicons/), and [Github Octicons](https://octicons.github.com/). 

> [!TIP]
> Protip
> You can browse the icons available in @sanity/icons here. For react-icons, go here.

#### Example

Install the icon package using your favorite package manager, and use them in your schemas! Just remember that any schema file with icons in them should have a `.jsx` or `.tsx` extension.

#### Example 

After including `react-icons` in your `package.json` with `yarn add react-icons` you can add it to your schema:

```javascript
import { PlayIcon } from '@sanity/icons'

export default {
  name: 'movie',
  type: 'document',
  icon: PlayIcon,
  fields: [
    {
      title: 'Title',
      name: 'title',
      type: 'string'
    },
    {
      title: 'Release Date',
      name: 'releaseDate',
      type: 'date'
    }
  ],
  preview: {
    select: {
      title: 'title',
      subtitle: 'releaseDate'
    }
  }
}
```



![A Sanity Studio with sci-fi movies showing unique icons for each document type.](https://cdn.sanity.io/images/3do82whm/next/d82118619cb3bc7caa8ff9fcac6849858048372e-2678x1928.png)





# Favicons

## Favicons/"website icon"



![A browser tab bar, illustrating the use of favicons](https://cdn.sanity.io/images/3do82whm/next/bf96f8116045e5fc2e6ebeb032c415ee93ff1819-316x75.png)

A favicon is a small icon that appears in the browser's tab or bookmarks.  By default, any Sanity studio will use the Sanity logo as its favicon.

Using a favicon that aligns with a company or projects logo or brand colors can help enhance the visual identity of a studio, differentiate it from other open tabs, as well as create a consistent and cohesive look and feel. 

## Individual files

To use a custom favicon that works across all platforms (desktop browsers, mobile browsers and bookmarks and similar), five different files are used:

8. `favicon.svg` - a square vector file in SVG format. This format scales well to many different sizes, and can also support different layouts/colors for [light/dark mode](https://owenconti.com/posts/supporting-dark-mode-with-svg-favicons).
8. `favicon.ico` - a small, square file in ICO format. This serves as the fallback icon for older browsers and environments that do not support resolving the preferred icon from HTML.
8. `favicon-512.png` - a 512x512px image file in PNG format. This is used as the icon defined in a Web Manifest file, and is generally used when bookmarking the studio on mobile devices.
8. `favicon-192.png` - a 192x192px image file in PNG format. Holds the same purpose as the 512px variant, but may be used when smaller icons are needed.
8. `apple-touch-icon.png` - a 180x180px image in PNG format. This is used when bookmarking the studio on iOS devices.

All these files should be placed in the `static` folder within the studio folder. Once they are in place, the studio should automatically use the new icons - though browsers may keep the old icons in their cache for a while.

## Generating the files

The files can either be created by hand, or you can use a tool such as [create-favicon](https://github.com/rexxars/create-favicon) to generate them all based on a source SVG file:



```sh
# From the studio root folder:
npm create favicon <path-or-url-to.svg> static
```

## Troubleshooting

If your custom favicons are not showing up:

- Ensure that all files listed above are present in the static folder, and has the same casing. A common case is that the SVG icon (`favicon.svg`) is missing, in which case it will fall back to the default Sanity favicon.
- When running sanity build, inspect the contents of the `dist/static` folder and see which icons are actually there.
- Your browser might be caching the favicons. Try a hard refresh by pressing `Command+Shift+R` on Mac, or `Ctrl+F5` on Windows/Linux. You can also try the "Disable cache" feature in the network tab of your browser development tools.





# Localizing Sanity Studio

## Studio UI Localization

You can install language plugins that change the Sanity Studio interface to your preferred language. This improves accessibility and user-friendliness and is sometimes a requirement for organizations who wish to adopt Sanity as their content platform.

> [!NOTE]
> Localizing UI vs localizing content
> This article is about the language of the Studio user interface. Visit this article to find documentation on different languages in the content you manage from Sanity Studio.

Languages for the Sanity Studio interface are available as plugins. All available languages can be found in the [sanity-io/locales](https://github.com/sanity-io/locales) repository on GitHub. This repository is regularly updated through artificial intelligence (AI) and manual reviews by our dedicated maintainers, who diligently assess contributions from AI and humans. You can also add project-specific localization and local overrides using the `i18n.bundles` entry-point in the Studio configuration.

The primary official language of Sanity Studio remains American English. However, we encourage and welcome contributions to our community-supported repository. There's a high likelihood your preferred language is already available, and if not, you can request its inclusion. Please visit the repository for more details on [contributing](https://github.com/sanity-io/locales/blob/main/CONTRIBUTING.md#getting-started) or [requesting a language](https://github.com/sanity-io/locales/issues/new?assignees=&labels=&template=new-locale-request.md&title=Locale+request%3A+).

### How to install a language for your Studio interface

Languages are installed as plugins using [npm](https://npmjs.com) or your preferred package manager. To install, for instance, German, run the following command from the root of your studio project:

```bash
npm install @sanity/locale-de-de
```

Once installed, add the plugin to the `plugins` array in your studio configuration:

```jsx
import {deDELocale} from '@sanity/locale-de-de'

export default defineConfig({
  // ...
  plugins: [
		// ...
    // Add German Studio interface language
    deDELocale()
  ],
})
```

Your Studio interface language will now be German.

![Sanity Studio localized in German](https://cdn.sanity.io/images/3do82whm/next/97c24cf8634c10df84b55b9d7165f90599938012-2844x1624.png)

### Overriding translated strings

You can override language strings without going through the process of making an alternate language plugin. This is great if you simply want to change the label of a button in your Studio. Or perhaps you have installed an incomplete language plugin and need to supply some missing strings. In these scenarios, you can use the `defineLocaleResourceBundle` API.

In the following example, the AI translation of “Inspect” into Norwegian is wrong and missed by the human maintainer. As a local fix, we can define a ”locale resource bundle” and add this to the `i18n.bundles` array in the Studio config. The easiest way to find the namespace and key you wish to override is to search for your string in the GitHub [sanity-io/locales](https://github.com/sanity-io/locales) repository and note which file it is stored in, which tells you the namespace, and the key itself.

![Sanity Studio with the document inspector modal open, showing a couple of mistranslated strings](https://cdn.sanity.io/images/3do82whm/next/7369bff5667e9f0259834112238b606c6893f2c0-914x768.png)

```jsx
// sanity.config.ts
import {defineConfig, defineLocaleResourceBundle} from 'sanity'
import {nnNOLocale} from '@sanity/locale-nn-no'

const myCustomOverrides = defineLocaleResourceBundle({
  // make sure the `locale` language code corresponds to the one you want to override
  locale: 'nn-NO',
  namespace: 'structure',
  resources: {
    'document-inspector.menu-item.title': 'Inspiser',
    'document-inspector.dialog.title': 'Inspiserer <DocumentTitle/>',
  },
})

export default defineConfig({
  // ...
  plugins: [
    nnNOLocale(),
  ]
  i18n: {
    bundles: [myCustomOverrides]
  }
})

```

![Sanity Studio with the document inspector modal open, now with correct translations](https://cdn.sanity.io/images/3do82whm/next/198682b2d2427df87e7f1c29f65510432807d20d-914x768.png)

## How to contribute to Studio UI Localization

We're always looking to make Sanity Studio more accessible and user-friendly, and your contributions can make a big difference. Whether you're a seasoned developer or just starting, helping with translations is a fantastic way to get involved.

If you're fluent in a language other than English, we'd love your help reviewing and improving translations. Your expertise can greatly enhance the experience for users worldwide.

### Requesting a new language

You can request the addition of your preferred language by using the issue template provided [here](https://github.com/sanity-io/locales/issues/new?assignees=&labels=&template=new-locale-request.md&title=Locale+request%3A+). Sanity will bootstrap the new locale with AI and community members can then submit suggested improvements. Language maintainers help in reviewing both AI and human contributions.

### Suggesting and reviewing improvements

Visit our [sanity-io/locales](https://github.com/sanity-io/locales) repository and try out a locale you are fluent in. Submit a PR with your suggested improvements following the [contributing guide](https://github.com/sanity-io/locales/blob/main/CONTRIBUTING.md#getting-started). You can also see if there are open PRs involving languages you are fluent in and help review them.

### Quick fix: Use the GitHub built-in editor

If you want to add or change a translated string quickly, the easiest way may be to use GitHub’s built-in editing feature. You can watch the video below to learn how.

![Using the built-in Github editor to submit a quick fix to a localization resource](https://youtu.be/eDCLC1vN4ng)

## Become a language maintainer

Interested in playing a bigger role? You can ask to be added as a maintainer to oversee translations for specific languages, where you will be asked to help review PRs that involve your language. See the [sanity-io/locales README](https://github.com/sanity-io/locales#readme) for more.

Sanity will create and keep the language updated using AI. Human contributors such as yourself help maintain the languages by submitting suggested improvements and helping review pull requests from AI and other contributors.

Your contributions improve Sanity Studio and bring together a diverse and global community of users. We appreciate every effort, big or small, and we can't wait to see what you bring to the table!



# New Document Options

The `newDocumentOptions` API allows you to customize the new document choices users see when they interact with the **Create** buttons in Sanity Studio.

`document.newDocumentOptions` accepts a callback function that returns an array of new document option templates. The callback accepts an array of existing templates, commonly displayed as `prev`, and a context object as arguments. It should return an array of template items.

```typescript
// sanity.config.ts
import {defineConfig} from 'sanity'

export default defineConfig({
  /* ... */
  document: {
    newDocumentOptions: (prev, {currentUser, creationContext}) => {
      /* ... */
      return prev
    }
  }
})
```

> [!TIP]
> Protip
> It's common to return a subset of existing types by filtering and returning the prev array, but you can also add new templates as part of the callback.

## Callback parameters

#### Properties

| Property | Description |
|----------|-------------|
| prev | An array containing all available template items. |
| context | Contains details about the context of the new document creation. Useful for comparing details about the current user and where the document creation event was initiated. |


### Context properties

#### Properties

| Property | Description |
|----------|-------------|
| creationContext | An object containing the type (global, document, or structure) and the schemaType, if one exists. Useful for determining where the document creation action originated. |
| currentUser | An object containing details about the current user such as id, roles, and email. |


## Usage examples

### Example: Limit document types by role

```typescript
// sanity.config.ts
import {defineConfig} from 'sanity'

// Create an array of templateId strings.
// These can match initial value templates, or document names.
const contributor_templates = [
  'guide',
  'blogPost',
  'caseStudy',
]

export default defineConfig({
  /* ... */
  document: {
    newDocumentOptions: (prev, {currentUser}) => {
    
      // Check if the current user is not an administrator
      if (currentUser?.roles.find((role) => role.name !== 'administrator')) {
        return prev.filter(({templateId}) => contributor_templates.includes(templateId))
      }
      
      // All other users (Administrators) see the full document list
      return prev
    }
  }
})
```

### Example: Hide a specific document type from the global create menu

```typescript
// sanity.config.ts
import {defineConfig} from 'sanity'

export default defineConfig({
  /* ... */
  document: {
    newDocumentOptions: (prev, {currentUser, creationContext}) => {
      if (creationContext.type === 'global') {
        // Hide the creation of "settings" documents if the context is global
        return prev.filter((templateItem) => templateItem.templateId != 'settings')
      }
      return prev
    }
  }
})
```



# Studio Components

## Introduction

The `studio.components` config property enables configuration-level customization of your studio. The following components can be overridden:

```typescript
// sanity.config.js

export default defineConfig({
  // rest of config ...
  studio: {
    components: {
      activeToolLayout: MyActiveToolLayout,
      layout: MyLayout,
      navbar: MyNavbar,
      toolMenu: MyToolMenu,
    }
  }
})
```

> [!TIP]
> Protip
> Looking to change the Studio logo? You can customize with the icon property in the workspace configuration.

The props for each component available in the API include a callback function named `renderDefault`. As the name implies, `renderDefault` will render the default component. When you call `renderDefault`, you also pass along the props needed to render the default component. You can modify the props to your liking before passing them along. If you want to completely replace the component in question with your own markup, simply do not invoke `renderDefault` in your return statement.

```jsx
// MyEnhancedNavbar.jsx
import { Stack, Card, Flex, Text } from '@sanity/ui'

// Adds markup and invokes renderDefault()
function MyEnhancedNavbar(props) {
  return (
    <Stack>
      <Card padding={3} tone="caution">
        <Flex justify="center">
          <Text>Important Message: Please Read!</Text>
        </Flex>
      </Card>
      <>{props.renderDefault(props)}</>
    </Stack>
  )
}

// Completely replaces default navbar
function MySuperiorNavbar() {
  return (
    <Stack>
      <Card padding={3} tone="caution">
        <Flex justify="center">
          {/* Custom navbar stuff goes here */}
        </Flex>
      </Card>
    </Stack>
  )
}
```

For some components, like navbar and layout, the `renderDefault()` method is the only prop passed along, while other components receive additional props.

```typescript
function MyTools(props) {
	// MyTools includes the tool titles from project config by default
  const { renderDefault, title } = props;
	// Overwrite the value of `title` after spreading the props object
  return renderDefault({...props, title: title.toUpperCase() });
}
```

```typescript
import { isDev } from 'sanity'

function MyToolMenu(props) {
	// ToolMenuProps includes list of installed tools, and more
  const { tools, renderDefault } = props;
	// Only show the dev-tool if the isDev variable resolves to true
  const availableTools = isDev ? tools : tools.filter(tool => tool.name !== 'dev-tool')
  return renderDefault({ ...props, tools: availableTools })
}
```

## Composing renderDefault()

The rendering of components in this API uses a middleware pattern. This means that plugin customizations are applied in a chain. Each plugin may call `props.renderDefault(props)` to defer to default rendering. If any component in the chain fails to call the callback function, the chain breaks.



# Studio search configuration

> [!WARNING]
> Experimental feature
> This article describes an experimental Sanity feature. The APIs described are subject to change and the documentation may not be completely accurate.

The global search for Sanity Studio lets you search your Content Lake dataset for any document that matches your term, or narrow down by filtering your query by schema types. You can activate the global search from the 🔍 icon in the top toolbar, or with the `ctrl/cmd + k` hotkey. 

## Hide documents from global search

There may be instances where you want to keep specific documents from appearing in the search results. To hide search results for a particular document type and remove it as a selectable filter, set the `__experimental_omnisearch_visibility` property to `false` on the document type:

```javascript
{
  type: 'document',
  name: 'author',
  fields: [
    {name: 'name', type: 'string'},
    // ...
  ],
	// Hide all results for authors (and the author document type filter) in omnisearch
	__experimental_omnisearch_visibility: false,
  // ...
}
```

#### Some example use cases:

- You have workflow-related documents that you don’t wish to expose editors to.
- You want to hide documents that are less frequently used by editors.
- You’re an author of a Sanity plugin that registers its own document schema and would prefer it doesn't appear in user's studios.

> [!TIP]
> Protip
> This only affects visibility within the global studio search (‘omnisearch’). Visibility in both reference and cross dataset reference input fields is unaffected.

## Define custom weighting on fields

You can define specific weights on searchable fields for document types.

Search weights are configurable via `options.search.weight`. Here's an example:

```javascript
{
  type: 'document',
  name: 'author',
  fields: [
    {
      name: 'name',
      type: 'string',
      options: {
        search: { weight: 10 },
        // ...
      }
    },
    {
      name: 'description',
      type: 'array',
      of: [{type: 'block'}],
      options: {
        search: { weight: 10 },
        // ...
      }
    }
    // ...
  ],
  // ...
}
```

## Selecting a search strategy

Define your desired search strategy in the `sanity.config.js|ts` file. Accepted values are `groqLegacy`, `groq2024`, and `textSearch`.

You must be on Studio version `v3.70.0` or greater to define strategy and opt-in to `groq2024.`

```javascript
import { defineConfig } from 'sanity`

export default defineConfig({
 search: {
   strategy: 'groq2024'
 },
})
```

### `groqLegacy`

Default search strategy in current studio versions.

### `groq2024`

Considered experimental and opt-in only until the underlying API is moved to a versioned endpoint.

Updates compared to `groqLegacy` search:

- Improved performance for large datasets.
- The ability to search deeply nested content beyond the default five levels that `groqLegacy` is restricted to today.
- The ability to search all Portable Text content.
- Support for “Google-style” query string parsing. In the below query string, `wine`, `good vintage`, and `fruit*` must all be present in order for the query to match. In other words, there is an implicit “and” between the words. Results with `beer` in will be omitted.

```groq
wine -beer "good vintage" fruit*
```

#### Tokenization and word handling in groq2024

- Text search operates on all tokens that are 1 character or longer, up to 255 character maximum.
- `_` is not a word-breaking character.
- Punctuation is ignored.
- Apostrophes are not word-breaking: `bob` does not match `bob's` or vice versa.
- All words and phrases (with/without wildcards) must appear in at least one attribute value.
- Negations do not match any attribute values.
- Phrases must not match *across* attribute values (i.e. half matching one value and another half matching another value).

#### Exact matching rules for groq2024

##### Text matching rules

| Scenario | Search Term | Behaviour | Should Match  | Should Not Match |
| --- | --- | --- | --- | --- |
| Numbers | 1234 | Match numbers that are separated by punctuation or space | The number is 1234! | X1234Y |
| Floating point numbers | 3.145 | Matches | The answer is 3.145 | The answer is 3 145 |
| Currency numbers | $100 | Match number only (not currency) | The price is $100 | - |
| Prefix | co* | Match prefix of words | co, covid, corvette | Anything not starting with “co” |
| Prefix + wildcard | co*e | Match prefix and suffix | corvette | Anything not having “co” prefix and “e” suffix |
| Middle wildcard | *co* | Match substrings inside words | corvette, acorn, texaco | Anything not containing the substring “co” |
| Special symbols (not punctuation) | ®️, ™️ | Match exactly | - | - |
| Underscores | foo_bar | Match exactly; underscores are part of the word | foo_bar | foo bar |
| Phrases | “hello world” | Match exactly the sequence of words, ignoring punctuation | "hello world", "hello, world" | hello to the world hey world, hello! |
| Phrases with wildcard at the end | “hello world*” | Match exactly the sequence of words, ignoring punctuation, with wildcard match on the last word | "hello world", "hello worlds" | hello to the world hey world, hello! |
| Negation | foo -bar | Search excludes what you specify |  |  |


### `textSearch`

Search strategy some customers may use for historic purposes but not a recommended strategy. This will likely be deprecated in the future. 

> [!WARNING]
> Gotcha
> Each search strategy relies on different underlying query logic. Slightly different search results, including ordering and count is expected. 
> 
> Configure search weights to help tailor your results

### A few things to note

- **Global weight multiplier:** Search weights act as global multipliers across all document types. For instance, if the name field in the customer type has a weight of `2`, and the author type has a weight of `4`, then author documents will rank higher than customer documents for identical name matches in search results.
- **Default search configuration:**- The field designated as the `title` in the preview config automatically receives a search weight of `10`, while the `subtitle` field gets a weight of `5`. 
- Any user-specified weight overrides default settings, ensuring custom search relevance can be achieved as needed.
- If you have customized the preview options using a [prepare function](https://www.sanity.io/docs/previews-list-views#770fd57a8f95 ) search weights cannot be inferred from the preview configuration so explicit custom field weights must be used if you want to boost specific fields.
- If you have a reusable custom type you need to define search weighting on the type definition itself. 
- Fields marked as `hidden: true` in the preview config are scored lower to push them lower in the result set.



> [!TIP]
> Protip
> Defining custom weights on fields applies only to the global search. The document list search is not affected by search weights. 





# Real-time safe patches for input components

Sanity Studio is a real-time application. Therefore, signaling changes from the editor to the back end uses a different strategy than the typical one.

## Real-time syncing with mutations and patches

Traditionally, editing content online is based on the following model:

4. The editor interface reads the content to modify from the database.
4. Users introduce changes to their local copy of the document.
4. After completing their edits, users save their work; all the content of the document is sent back to the server and written to the database.

This is akin to downloading an MS Word document to a local computer, editing it, and sending it back to the server when done.

It’s an approach that works well if there’s only one user working on a single document at the time. If you’re collaborating with someone else, this model breaks down. You risk either overwriting someone else’s work or going through a tedious change conflict resolution process before you can save the document without losing any changes.

Sanity applies a different collaboration model:

8. The editor interface (Sanity Studio) loads the content to modify from the database (Content Lake.)
8. While users edit local versions of the document, the editor emits fine-grained, computer-readable descriptions of what exactly changed. We call these descriptions *mutations*.
8. The editor collects these mutations and sends them to the server, which then applies them directly to the stored document.
8. Then, the server distributes the mutations to any other collaborators who are working on the same document at the same time.
8. Finally, the editor applies the mutations to each concurrent user’s local version so that everything is in sync.

Because of this model, input components in Sanity are designed to work with granular mutations called *patches*.

### Examples

Following the traditional model, an input component for an object may look like this:

```typescript
function MyObjectInput(props) {
  const {fields, value, onChange} = props

  return (
    <>
      {fields.map((field) => (
        <div>
          <label>
            {field.title}
            <input
              type="text"
              value={value[field.name]}
              onChange={(event) => {
                onChange({...value, [field.name]: event.currentTarget.value})
              }}
            />
          </label>
        </div>
      ))}
    </>
  )
}
```

This model is easy to work with when you keep the input value in a state variable: all you need to do is call `setState` with the emitted value, and feed the state variable back to `<MyObjectInput>`.
However, this model doesn’t work as well in a real-time scenario where you don’t want to send and receive full values, but rather *granular change descriptions* (mutations).

In a real-time environment, the following works better:

```typescript
function MyObjectInput(props) {
  const {fields, value, onChange} = props

  return (
    <>
      {fields.map((field) => (
        <div>
          <label>
            {field.title}
            <input
              type="text"
              value={value[field.name]}
              onChange={(event) => {
                onChange({set: {[field.name]: event.currentTarget.value}})
              }}
            />
          </label>
        </div>
      ))}
    </>
  )
}
```

As a bonus, to set a new object input value, you don’t need to consider the current one.

## Patch utilities

The `PatchEvent` class offers a set of utilities for composing real-time safe patches when developing object and array inputs. Instead of manually constructing field paths and patches, you can import a set of patch creators from the Sanity package.

The `PatchEvent` class has several static [methods to declare a granular operation](/docs/content-lake/http-patches):

> [!NOTE]
> path can only accept an array of path segments.

### Patches for all data types

**set**

`set(value: any, path?: Path)`: sets the value at the specified path. It overwrites any existing value.

**unset**

`unset(path: Path?)`: unsets any value at the specified path.

**setIfMissing**

`setIfMissing(value: any, path?: Path)`: performs a [setIfMissing](/docs/content-lake/http-patches) patch on the specified path. 

### Patches for arrays

**insert**

`insert(items: any[], position: "before" | "after", path?: Path)`: performs an [insert](/docs/content-lake/http-patches) patch, inserting the `items` provided before or after the node at the specified path.

### Patches for strings

**diffMatchPatch**

`diffMatchPatch(value: string, path?: Path)`: performs a [diffMatchPatch](/docs/content-lake/http-patches) on the string at the specified path.

### Patches for numbers

inc

`inc(amount, path?: Path)`: performs an [increment](/docs/content-lake/http-patches) operation on the number value at the specified path.

**dec**

`dec(amount, path?: Path)`: performs a `decrement` operation on the number value at the specified path.

## Best practices and considerations

### Consider the user's intention for a change

The change event you emit from the input component needs to consider what users want to achieve when they make a change.

For example:

- Do you want the change to only affect what a user sees on their screen, regardless of the corresponding value in the database (which might not be the same as what is displayed to the user)?
Or do you want to modify the most recent value stored in the database, regardless of what is displayed to the user on the screen?
- When a user changes the value of a number, do they want to increase or decrease the original value? Or do they want to set it to a new arbitrary value?

The differences in the outcomes can be subtle. As a rule of thumb, when creating patches, it’s preferable to avoid reading input values locally. This is possible only when using the `insert`, `inc` and `dec` [patches](/docs/content-lake/http-patches).

Create patches that are as fine-grained as possible. When creating a custom array or object input, you can optionally call [onChange](https://www.sanity.io/guides/usereducer-in-custom-component) with a patch that sets the whole array or object value.

### Avoid array indices

When creating patches that target array elements, avoid targeting the elements with their array index reference. Array indices are unreliable because users may add, remove, and change the order of the items in the array over time. For example: if you have an array with two items `A` and `B` with index `0` and `1`, respectively, their reference array index changes as soon as you or other users modify the order of the elements in the array.

### Diff match patch

Sanity supports [diff-match-patch](https://github.com/google/diff-match-patch), which offers a robust way to describe a change in plain text. Usually, you don’t need to create diff match patches; Sanity Studio does it for you under the hood.

Gotcha: if you implement emitting diff match patches from your custom input, you miss out on built-in optimizations. Therefore, it’s preferable to avoid creating diff match patches from custom input components.





# Sanity UI

When you're creating new tools and custom inputs, it's important for your editor experience to make sure your customizations match the overall design of the studio. To create this consistency, you can use [the Sanity UI component library](https://sanity.io/ui) to create custom experiences without creating custom designs or adding custom CSS.

## Usage of Sanity UI

The Sanity UI package comes bundled for most studio usage, but if you're creating a plugin or tool, you'll want to install the package via NPM.

```sh
npm install @sanity/ui
```

From there, you can import the various components into your custom inputs, tools, or widgets. For example, if you wish to apply a tooltip to a string input, you can create a custom input that uses the `Stack`, `Box`, and `TextInput` design primitives to create one with all the design elements of your studio built right in. 

```javascript
// /components/MyCustomStringInput.jsx
import React, {useCallback} from 'react'
import {Stack, Text, TextInput} from '@sanity/ui'
import {set, unset} from 'sanity'

export const MyCustomStringInput = (props) => {
  const {elementProps, onChange, value = ''} = props

  const handleChange = useCallback((event) => {
    const nextValue = event.currentTarget.value
    onChange(nextValue ? set(nextValue) : unset())
	}, [onChange])

  return (
    <Stack space={2}>
      <TextInput
        {...elementProps}
        onChange={handleChange}
        value={value}
      />
      <Text>Characters: {value.length}</Text>
    </Stack>
  )
}
```

See this guide on [creating custom inputs and tools with Sanity UI](https://www.sanity.io/guides/your-first-input-component-for-sanity-studio-v3).

## Full documentation and playground

Sanity UI comes with a full set of UI primitives that can be mixed, matched, and composed into many different design patterns. The full list of components can be found in [the official Sanity UI documentation](https://sanity.io/ui/docs). To get a better feel for creating design patterns, you can also experiment with all the components in this [interactive component playground](https://www.sanity.io/ui/arcade).



# Studio Tools

A tool is a top-level view in Sanity Studio that you can access through its menu bar. The most common and built-in tool for the Studio is the Structure tool, which lets you browse, edit, and create documents. You can install tools with plugins or create your own. Tools are tied to the Studio’s routing and can be accessed through predictable URLs.

## Recommended tools

To get started, here are some recommended tools to enhance your Studio experience. You may even have a few installed already.

[Structure](/docs/studio/structure-tool)

[Vision](/docs/content-lake/the-vision-plugin)

[Dashboard](/docs/studio/dashboard)

[Presentation](/docs/visual-editing/configuring-the-presentation-tool)



For more from tools and plugins from Sanity and the community, browse the [Exchange](https://www.sanity.io/plugins).

## Manage tools and develop your own

[Tools cheat sheet](/docs/studio/tools-cheat-sheet)

[Create a custom tool](/docs/studio/custom-studio-tool)

[Tool API reference](/docs/studio/tool-api-reference)







# Create a custom Studio tool

A tool is a top-level view in the Sanity Studio application that you can access through its menu bar. New to tools? Visit the [Studio Tools overview](/docs/studio/studio-tools).

Tools are great for custom dashboards and user interfaces for exploring and interacting with content. At their most basic, tools are custom React components that interact with data in Sanity.

## Basic configuration

You can add a custom tool by adding its configuration object to the `tools` array in the studio configuration. A tool needs to have a `title`, `name`, `icon`, and component defined. The `title` controls what appears in the toolbar, while the `name` controls the URL segment that the tool routes to.

```tsx
// sanity.config.tsx
import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'
import {Card, Text} from '@sanity/ui'
import {DashboardIcon} from '@sanity/icons'
import {schemaTypes} from './schemas'

const myCustomTool = () => {
  return {
    title: 'My Custom Tool',
    name: 'my-custom-tool', // localhost:3333/my-custom-tool
    icon: DashboardIcon,
    component: (props) => (
      <Card padding={4}>
        <Text>My custom tool!</Text>
      </Card>
    ),
  }
}

export default defineConfig({
  name: 'default',
  title: 'Studio with custom tool',
  projectId: 'your-project-id',
  dataset: 'production',
  plugins: [structureTool()],
  tools: [myCustomTool()],
  schema: {
    types: schemaTypes,
  },
})
```

> [!TIP]
> Protip
> If you want to use @sanity/ui and @sanity/icons for your own tools, remember to install them as dependencies in your project:
> 
> npm install @sanity/ui @sanity/icons 

## TypeScript

If you're building with TypeScript, then you can use the built-in `Tool` type from the studio package, as well as the `ComponentType` from the react package. You can also extend these to support custom options you might have for your tool:

```tsx
// myCustomTool.tsx
import type {ComponentType} from 'react'
import {type Tool} from 'sanity'
import {Card, Text, Stack} from '@sanity/ui'

export interface myCustomToolOptions {
   customString?: string
}

export interface myCustomToolProps<Options = any> {
  component: ComponentType<{
      tool: Tool<myCustomToolOptions>
  }>
}

export const myCustomTool = (options: myCustomToolOptions | void) => {
  return {
    title: 'My Custom Tool',
    name: 'my-custom-tool', // localhost:3333/my-custom-tool
    icon: DashboardIcon,
    component: () => (
      <Card padding={4}>
        <Stack>
          <Text>My custom tool!</Text>
          <Text>{options?.customString}</Text>
        </Stack>
      </Card>
    ),
  }
}
```

## Share custom tools with others

The best way to share a tool is to make it into a plugin. The guides below will help you package and publish your tool as a plugin.

[Developing plugins](/docs/studio/developing-plugins)

[Publishing your plugin](/docs/studio/publishing-plugins)



Have a tool that you've packaged as a plugin that you think the community would like? Share it on the [Sanity Exchange](https://sanity.io/exchange).



# Tools cheat sheet

Tools are a powerful way to add additional functionality to Sanity Studio. Here are some ways of customizing how tools work within Studio.

## Ordering of tools 

Tools are ordered as they are added in the `tools` array, followed by tools that are added via plugins in the `plugins` array. A tool within a plugin can override the default ordering via the `studio.components.toolMenu.tools` property in the plugin config. And likewise, you can edit this order by accessing this property yourself in the config. **This does not affect which tool opens by default when the Studio loads.**

In the example below, the Structure tool always comes first in the menu. Without this update, the two custom tools would come before the Structure tool in the navigation bar.

```typescript
// sanity.config.ts
export default defineConfig({
  name: 'default',
  title: 'example',
  projectId: '<project-id>',
  dataset: 'production',
  studio: {
    components: {
      toolMenu: (props) => {
        const {tools, renderDefault} = props
        const structureTool = tools.find(({name}) => name == 'structure')
        const otherTools = tools.filter(({name}) => name != 'structure')

        if (!structureTool) {
          return renderDefault(props)
        }

        return props.renderDefault({
          ...props,
          tools: [structureTool, ...otherTools],
        })
      },
    },
  },
  plugins: [structureTool()],
  tools: [myCustomTool, myOtherCustomTool],
  schema: {
    types: schemaTypes,
  },
})
```

## Configure the default tool

Sometimes you need to order the tools in the navigation bar, like in the ordering example above, but you want a specific tool to open when the Studio loads. In this case, use the `tools` property in the configuration to sort the tools array.

In this example, the `(prev, context)` callback pattern [sorts the array](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/sort) and places the vision tool in the front. 

```typescript
// sanity.config.ts
export default defineConfig({
  name: 'default',
  title: 'example',
  projectId: '<project-id>',
  dataset: 'production',
  //... other config
  tools: (prev, context) => {
    return prev.sort((a, b) => {
      if (a.name === 'vision') {
        return -1 // Moves 'vision' tool to the top of the list
      }
      return 1
    })
  }
})
```

This won't change the visual order in the navigation bar, but now the Vision tool opens by default when you visit the Studio. Combine this approach with the tool ordering approach to fully configure the navigation order and default tool.

## Display only in development environments

Sometimes you need to display a tool only in development environments. You can import and use the `isDev` boolean helper to check the state of the environment. In this example, the Studio displays the Vision and Structure tools in development, but just the Structure tool in other environments.

```typescript
import {defineConfig, isDev} from 'sanity'
import {structureTool} from 'sanity/structure'
import {visionTool} from '@sanity/vision'

export default defineConfig({
  // ...
  plugins: isDev
    ? [structureTool(), visionTool()]
    : [structureTool()],
})
```

## Conditionally render tools based on role

Sometimes you want to display tools for specific user roles. There are a few ways to do this. In this example, administrators have access to all tools while all other users can only use the Structure tool.

```typescript
// sanity.config.ts

// define an array of tools 
const userTools = ['structure']

export default defineConfig({
  name: 'default',
  title: 'example',
  projectId: '<project-id>',
  dataset: 'production',

  // This studio includes structure, vision, and any plan-specific tools
  plugins: [structureTool(), visionTool()],
  tools: (prev, context)=>{
    // Retrieve the current user from the context
    const { currentUser } = context
    // Check if the current user is not an admin
    if (currentUser?.roles.find((role)=>role.name != 'administrator')){
      // return an array that only includes tools in the userTools array
      return prev.filter((tool)=>userTools.includes(tool.name))
    }

    // Otherwise, return all tools
    return [...prev]
  },
  //... rest of config
})
```

Alternatively, if you only need to adjust a single tool, this example limits the vision tool to only administrators.

```typescript
export default defineConfig({
  name: 'default',
  title: 'example',
  projectId: '<project-id>',
  dataset: 'production',

  plugins: [structureTool(), visionTool()],
  tools: (prev, context)=>{
    // Retrieve the current user from the context
    const { currentUser } = context
    
    const isAdmin = currentUser?.roles.some((role) => role.name === 'administrator')

    // If the user has the administrator role, return all tools.
    // If the user does not have the administrator role, filter out the vision tool.
    return isAdmin ? prev : prev.filter((tool) => tool.name !== 'vision')
  },
  // ... rest of config
})
```

## Additional resources

[Studio Tools](/docs/studio/studio-tools)

[Tool API reference](/docs/studio/tool-api-reference)





# Theming

## Theming in Sanity Studio

### Using `buildLegacyTheme`

The top-level `theme` config property allows you to customize the color palette of the studio. You can create your own themes using the `buildLegacyTheme` helper function exported from the `sanity` package.

```javascript
import {buildLegacyTheme, defineConfig} from 'sanity'

const props = {
  '--my-white': '#fff',
  '--my-black': '#1a1a1a',
  '--my-blue': '#4285f4',
  '--my-red': '#db4437',
  '--my-yellow': '#f4b400',
  '--my-green': '#0f9d58',
}

export const myTheme = buildLegacyTheme({
  /* Base theme colors */
  '--black': props['--my-black'],
  '--white': props['--my-white'],

  '--gray': '#666',
  '--gray-base': '#666',

  '--component-bg': props['--my-white'],
  '--component-text-color': props['--my-black'],

  /* Brand */
  '--brand-primary': props['--my-blue'],

  // Default button
  '--default-button-color': '#666',
  '--default-button-primary-color': props['--my-blue'],
  '--default-button-success-color': props['--my-green'],
  '--default-button-warning-color': props['--my-yellow'],
  '--default-button-danger-color': props['--my-red'],

  /* State */
  '--state-info-color': props['--my-blue'],
  '--state-success-color': props['--my-green'],
  '--state-warning-color': props['--my-yellow'],
  '--state-danger-color': props['--my-red'],

  /* Navbar */
  '--main-navigation-color': props['--my-black'],
  '--main-navigation-color--inverted': props['--my-white'],

  '--focus-color': props['--my-blue'],
})

export default defineConfig({
  // rest of config...,

  theme: myTheme,
})
```



### Using the Themer app

A potentially quicker and easier way to go about creating themes for your studio is to use [Themer](https://themer.sanity.build/). This web app was created in-house at Sanity and will let you visually create a custom palette that you can then export for your own use.

![Screenshot of web app that lets you define custom color palettes for the Sanity Studio ](https://cdn.sanity.io/images/3do82whm/next/72e1d2538c81f25fc18a4a5ba148e9047d74e73b-1402x939.png)



# The Dashboard tool for Sanity Studio

> [!WARNING]
> Looking for Sanity Dashboard?
> This article is about the Dashboard tool for Sanity Studio. Go here for documentation for Sanity Dashboard, the unified content operations workspace.

Dashboard is a Sanity Studio tool that allows you to add widgets that display information about your content, project details, or anything else you'd want to put there. You can find widgets on the [Sanity Exchange](https://www.sanity.io/exchange/) and install them in your project using your preferred package manager, such as [npm](https://www.npmjs.com/) or [yarn](https://yarnpkg.com/). You can also write your custom project-specific widgets.

Widgets are useful for displaying stats about your content, listing recently edited or stale documents, portraying the daily cat, or whatever sparks joy for those who log in to the Studio.

The Dashboard tool has been designed to be as generic as possible, making few assumptions about its widgets. The Dashboard itself is mostly concerned about the layout of the configured widgets. The layout and order, as well as the widgets’ configurable options can be set in a simple file.

## Installation

If you have started a project from [sanity.io/templates](https://www.sanity.io/templates) you might already have the Dashboard installed. If you wish to install it in existing projects, you follow the same procedure as for any other package:

7. `cd` to your project’s root folder
7. Install the package

```
npm install --save @sanity/dashboard

# OR

yarn add @sanity/dashboard
```

3. Add the widget to your studio configuration (typically found in `sanity.config.js|ts` at the root of your project)

```javascript
import { defineConfig } from "sanity";
import { dashboardTool } from "@sanity/dashboard";
export default defineConfig({
    /* ... */
    plugins: [
        dashboardTool({ widgets: []})
    ]
})
```

To verify that all is well, fire up your Studio (`sanity dev`) and point your browser to [http://localhost:3333/dashboard](http://localhost:3333/dashboard). It should show an empty dashboard with a message encouraging you to add some widgets to the dashboard.

> [!WARNING]
> Gotcha
> Sometimes, you want the Dashboard to be the first thing people see when they log in to the Studio, and sometimes the Structure tool. This depends on what comes first in the plugins-array in sanity.config.ts.

## How to configure the Dashboard

Now, add any widgets you might want. The dashboard plugin provides three widgets out-of-the-box:

```javascript
import { defineConfig } from "sanity";
import {
  dashboardTool,
  sanityTutorialsWidget,
  projectUsersWidget,
  projectInfoWidget,
} from "@sanity/dashboard";


// configure the dashboard tool with widgets
dashboardTool({ 
  widgets: [
    sanityTutorialsWidget(),
    projectInfoWidget(),
    projectUsersWidget(),
  ]
})
```

Widgets can be configured by passing widget-specific config:

```javascript
projectUsersWidget({ layout: { width: 'small' } }),
```

The `widgets` array is how you tell the Dashboard which widgets to render in the order they appear in the array. The ones mentioned above are bundled with Sanity and require no separate installation.

You can play around with the order of the widgets array and see how the layout changes. 

![The Dashboard in Sanity Studio with a feed of tutorials, a project info widget, and a project users widget.](https://cdn.sanity.io/images/3do82whm/next/a8bf55737aad38cd4140f565dd9742745e0ccb32-1217x1046.png)

Some widgets have widget-specific options to change aspects of their behavior. If you install the `sanity-plugin-dashboard-widget-document-list` widget mentioned below, it can be configured with:

```javascript
documentListWidget({
  showCreateButton: true,
  limit: 5,
  types: ["my-document-type"],
})
```

You can add multiple instances of a widget with different configurations. So, if you want your dashboard to display both the newest documents across all document types and another widget showing the last edited books, your dashboard config might look like this:

```javascript
export default {
  widgets: [
    documentListWidget({title: 'New', order: '_createdAt desc'}),
    documentListWidget({title: 'Last edited books', order: '_updatedAt desc', types: ['book']}),
  ]
}
```





# Add widgets to dashboard

## Installing widgets

You install Dashboard widgets the same way you'd install any studio plugin, or indeed any other package, using your preferred package manager. 

You can find some popular widgets at the Sanity Exchange.

[Browse widgets →](/plugins?category=dashboardWidget)

For example, if you want to install the cats example widget mentioned below, proceed as follows:

1. Install the widget in the root folder of your project.

```sh
npm install --save sanity-plugin-dashboard-widget-cats
# OR
yarn add sanity-plugin-dashboard-widget-cats
```

2. Update your `sanity.config.js` file to include the widget in your `plugins` array. 

```javascript
import { dashboardTool } from "@sanity/dashboard";
import { catsWidget } from "sanity-plugin-dashboard-widget-cats";

export default defineConfig({
  // ...
  plugins: [
     dashboardTool({
             widgets: [
                 catsWidget(),
             ],
         }
     ),
  ] 
})
```

3. You've now got a cat in your Studio!

## Changing layout

A widget’s size can be defined by adding a `layout` key to the widget entry:

```javascript
dashboardTool({
        widgets: [
            catsWidget({ layout: { width: "small" } }),
        ],
    }
)
```

The accepted values for `width` and `height` are:

- `auto`
- `small`
- `medium`
- `large`
- `full`

![The Dashboard with four empty widgets of different size configurations](https://cdn.sanity.io/images/3do82whm/next/710b00182e719196413eb9a1ffb73d2d86a8bb64-2560x2160.png)

## Configuring widget options

Some widgets allow options to change aspects of their behavior. The configuration options should be part of the widget’s documentation found in its `README.md`. If you install the document-list widget (install `sanity-plugin-dashboard-widget-document-list` with npm or yarn as shown above), it can be configured with:

```javascript
documentListWidget({
  showCreateButton: true,
  limit: 5,
  types: ["my-document-type"],
})
```

Thus, if you want your dashboard to display both newest documents across all document types and another widget showing the last edited books, your dashboard config would look like this:

```javascript
export default {
  widgets: [
    documentListWidget({title: 'New', order: '_createdAt desc'}),
    documentListWidget({title: 'Last edited books', order: '_updatedAt desc', types: ['book']}),
  ]
}
```

![The Studio with two widgets showing the last edited documents and the last edited posts](https://cdn.sanity.io/images/3do82whm/next/393a85b3be27503fa583c5413444d9941be0ddd9-3200x2400.png)

## 



# Document actions

Document Actions lets you customize and control operations users can perform on a document. When you create a custom action it will be available in the actions menu in the document editor.

![Screenshot of document actions in Sanity Studio](https://cdn.sanity.io/images/3do82whm/next/331e0dc1664ce52e79447e409b33a48cd3f0b07d-2304x1400.png)

## Get started

To set up a new custom action component you need to complete the following steps:

5. Define an action component
5. Register the action component to the `document.actions` array in your workspace configuration

In this first example we'll make an action component that will display an alert window when clicked. 

#### 1. Define a document action component

First, create a file in your local Studio for your action.  Let's call the component `HelloWorldAction` and put it in a file called `actions.js`. 

[Learn about the complete Document Actions API](/docs/studio/document-actions-api)

```javascript
// actions.js

export function HelloWorldAction(props) { 
  return {
    label: 'Hello world',
    onHandle: () => {
      // Here you can perform your actions
      window.alert('👋 Hello from custom action')
    }    
  } 
}
```

#### 2. Register and resolve document actions

Now that you have defined a document action, it can be registered by adding it to `document.actions` in your studio configuration.

```javascript
// sanity.config.js

import {defineConfig} from 'sanity'
import {deskTool} from 'sanity/desk'
import {schemaTypes} from './schemas'
import {HelloWorldAction} from './actions'

export default defineConfig({
  name: 'default',
  projectId: '<project-id>',
  dataset: 'production',

  plugins: [
    deskTool(),
  ],
  document: {
    actions: [HelloWorldAction],
  },
  schema: {
    types: schemaTypes,
  },
})

```

When supplying `document.actions` with a static array of custom actions, the studio will append your customizations to the list of actions provided by plugins and / or the studio defaults.

![Shows the document actions popup menu with our custom action added to the standard list of available actions](https://cdn.sanity.io/images/3do82whm/next/206d8763a36dd1ffaf94d57744f8b1ea2ea9fc3b-557x284.png)

If you want more control over what shows up in the actions menu, you can instead provide a callback function to `document.actions` which should return an array of document action components. The callback will receive as arguments an array of already existing actions, and a context object containing useful info. 

```javascript
import {HelloWorldAction} from './actions'

export default defineConfig({
  // ... rest of config
  document: {
    actions: (prev, context) => {
      // Only add the action for documents of type "movie"
      return context.schemaType === 'movie' ? [HelloWorldAction, ...prev] : prev;
    },
  },
})
```

![Document actions menu in Sanity Studio](https://cdn.sanity.io/images/3do82whm/next/2659ee6276a078696c06612d03a6a98f63a7cc63-557x276.png)

[Document API reference](/docs/studio/document-actions-api)



## Typical use cases

### Showing actions conditionally

In some situations, a document action may not be relevant, and instead of making it *disabled,* you rather want it to not appear at all. For example, some document actions may only be relevant for certain types. In these cases, check the condition and return `null` from the action component if you want to hide the action.

Here's an example of an imaginary "spellcheck" action that will only appear in menus for documents of type `article`:

```javascript
export function SpellCheckArticleAction(props) {
  if (props.type === 'article') {
	return {
		label: 'Spellcheck article'
    //...
  }
    return null
  }
}
```

### Update a value then publish document

Usually a document action provides a way for the user to manipulate the document. To get access to operations that can be done on a document, you can use the `useDocumentOperation` hook from the `sanity` package.

`import {useDocumentOperation} from 'sanity'`

This will give you access to a set of operations that the current document supports. Each operation comes with a `disabled` prop and an `execute` method. 

In this example we update the `publishedAt` value of a document before we publish it. We also provide feedback to the user about the progress of the operation.

Note: Due to current technical limitations, the only way to check whether the publish action has completed is to check for the draft being `null` after the publish action was invoked (i.e., the code in `useEffect()`). We are working on improving this in the future.

```javascript
import {useState, useEffect} from 'react'
import {useDocumentOperation} from 'sanity'


export function SetAndPublishAction(props) {
  const {patch, publish} = useDocumentOperation(props.id, props.type)
  const [isPublishing, setIsPublishing] = useState(false)

  useEffect(() => {
    // if the isPublishing state was set to true and the draft has changed
    // to become `null` the document has been published
    if (isPublishing && !props.draft) {
      setIsPublishing(false)
    }
  }, [props.draft])

  return {
    disabled: publish.disabled,
    label: isPublishing ? 'Publishing…' : 'Publish & Update',
    onHandle: () => {
      // This will update the button text
      setIsPublishing(true)

      // Set publishedAt to current date and time
      patch.execute([{set: {publishedAt: new Date().toISOString()}}])

      // Perform the publish
      publish.execute()

      // Signal that the action is completed
      props.onComplete()
    },
  }
}
```

### Selectively replacing built-in actions

Sometimes you may want to replace just one or a few of the default document actions (publish, duplicate, delete) in your Studio instance. Here's an example of how to replace the built-in publish action with your own:

```javascript
export default defineConfig({
  // ...rest of config
  document: {
    actions: (prev) =>
      prev.map((originalAction) =>
        originalAction.action === 'publish' ? CustomPublishAction : originalAction
      ),
  },
})
```

### Extending built-in actions

You may want to extend a built-in action while retaining its look and functionality, but don't want to re-construct the entire component. After all, that would require constantly monitoring the built-in action for code changes and updating your custom action.

The following is the most basic implementation, and simply logs to the console while retaining all the functionality of the default Publish action (permissions checking, sync state, validation, etc.). 

```javascript
export function createImprovedAction(originalPublishAction) {
  const BetterAction = (props) => {
    const originalResult = originalPublishAction(props)
    return {
      ...originalResult,
      onHandle: () => {
        // Add our custom functionality
        console.log('Hello world!')
        // then delegate to original handler
        originalResult.onHandle()
      },
    }
  }
  return BetterAction
}

```

This method requires you to call the function with the original action as the only argument.

```javascript
import {createImprovedAction} from './actions'

export default defineConfig({
  // ...rest of config
  document: {
    actions: (prev) =>
        prev.map((originalAction) =>
          originalAction.action === 'publish'
            ? createImprovedAction(originalAction)
            : originalAction
        ),
  },
})
```

In this next contrived example, we will extend the Publish action by incrementing a counter on an existing document (`_id: 'publish-counter'`) and then logging the updated counter value to the console:

```javascript
export function createAsyncPublishAction(originalAction, context) {
  const client = context.getClient({ apiVersion: '2022-11-29'})
  const AsyncPublishAction = (props) => {
    const originalResult = originalAction(props)
    return {
      ...originalResult,
      onHandle: async () => {
        await client.patch('publish-counter').setIfMissing({counter: 0}).inc({counter: 1}).commit()
        await client
          .fetch("*[_id == 'publish-counter'][0]{counter}")
          .then((res) => console.log(res))
        originalResult.onHandle()
      },
    }
  }
  return AsyncPublishAction
}
```

In order to make the client available, this function expects the `context` object to be forwarded along with the original action.

```javascript
export default defineConfig({
  // ...rest of config
  document: {
    actions: (prev, context) =>
      prev.map((originalAction) => (originalAction.action === 'publish' ? createAsyncPublishAction(originalAction, context) : originalAction)),
  },
})
```

You can extend more than just the default behavior. This same approach can be used to add a modal, change the button color or icons, and so on. Let's change the default publish button from this:

![Revised publish buttons: enabled on the left (green) and disabled on the right (grey).](https://cdn.sanity.io/images/3do82whm/next/2fc0ddbe5a0df0d8eed298115ee20d3adf867b08-400x34.png)

to this:

![Revised publish buttons: enabled on the left (red with an open eye icon) and disabled on the right (grey with a closed eye icon).](https://cdn.sanity.io/images/3do82whm/next/41d8b5300f540822b3c9cac48207e2f6334ea8c0-400x34.png)

> [!WARNING]
> Gotcha
> Although you can override anything returned from the default actions, the internals of the component are not accessible. This means you can't access component state, internal functions and variables, etc.

```javascript
import {EyeOpenIcon, EyeClosedIcon} from '@sanity/icons'

export function createVisualAction(originalAction) {
  const BetterButtonAction = (props) => {
    const originalResult = originalAction(props)
    return {
      ...originalResult,
      tone: 'critical',
      icon: originalResult.disabled ? EyeClosedIcon : EyeOpenIcon,
    }
  }
  return BetterButtonAction
}

```

### Stateful action components and dialog flows

You can think about the action component as a functional React component and you can use React hooks to give it internal state. This means an action can support all sorts of user interaction, including dialogs. Here's an example of an action that lets the user edit the title from the document actions dropdown:

You can learn more and read about the different kinds of dialogs supported in the [Document Actions API documentation](/docs/studio/document-actions-api).

```javascript
import React from 'react'
import {useDocumentOperation} from 'sanity'

export function DialogAction({id, type, published, draft}) {
  const doc = draft || published

  const [isDialogOpen, setDialogOpen] = React.useState(false)
  const [documentTitle, setDocumentTitle] = React.useState(doc?.title)

  const {patch} = useDocumentOperation(id, type)

  const patchField = (field) => {
    patch.execute([{set: {title: field}}])
  }

  return {
    label: `Edit title`,
    onHandle: () => {
      setDocumentTitle(doc?.title)
      setDialogOpen(true)
    },
    dialog: isDialogOpen && {
      type: 'dialog',
      onClose: () => {
        setDocumentTitle(doc?.title)
        setDialogOpen(false)
      },
      header: 'Edit title field',
      content: (
        <>
          <input
            type="text"
            value={documentTitle}
            onChange={(event) => setDocumentTitle(event.currentTarget.value)}
          />
          <button
            onClick={() => {
              patchField(documentTitle)
              setDialogOpen(false)
            }}
          >
            Update
          </button>
        </>
      ),
    },
  }
}

```







# Custom document badges

A document badge is a small UI component that indicates the status of a document. It currently appears in the Studio next to the toolbar actions. The default set of document badges currently shows `draft` and `published` status.

Depending on how you're implementing your workflows, you may want to control the badges that are displayed here. For example, if you have a workflow that includes reviewing you want to display pending review as a badge here.

![Screenshot from Sanity Studio](https://cdn.sanity.io/images/3do82whm/next/250a4fc9d947827de6e0e1c02777fdec2c2b6908-2304x1400.png)

[Learn more about creating custom workflows →](/docs/studio/document-actions)

## Getting started

[More details in the document badge reference documentation →](/docs/studio/document-badges-api)[](http://localhost:3000/docs/custom-workflows)

In order to implement your own custom badge you need to perform two steps:

8. Create a function that defines the badge
8. Register the badge and resolve which badges should be displayed when

Here's how:

### Define a custom badge

```javascript
export function HelloWorldBadge(props) {
  return {
    label: 'Hello world',
		title: 'Hello I am a custom document badge',
    color: "success"
  }
} 
```

### Register a custom badge

Custom badge definitions like the one above can be added to the `document.badges` property in your workspace configuration.

```javascript
export default defineConfig({
  // ... rest of config
  document: {
    badges: [HelloWorldBadge]
  },
})

```

Adding your badge components as a static array as in the example above will append your custom badges to the list of existing badges, if any. These could be the default set of badges provided by Sanity Studio, and any badges added to the studio via plugins.



The property can also be defined with a callback function that returns an array of badge components. When using the callback option, it's your responsibility to make sure any existing badges are passed along. The callback receives the current array of badges as its first argument and a context object with some useful info as its second.

```javascript
export default defineConfig({
  // ... rest of config
  document: {
    // Use info from the context to decide whether or not
    // to add our badge or just return the current list
    badges: (prev, context) => context.schemaType === 'movie' ? [HelloWorldBadge, ...prev] : prev,
  },
})

```

When editing a document in the studio next time, you should see your badge appear in the toolbar when editing a document:

![Screenshot of document badges in Sanity Studio](https://cdn.sanity.io/images/3do82whm/next/1219c0d0fcc87b88c492d5d9712813651d06c8e8-2304x1400.png)



# Localization

> [!NOTE]
> Localizing UI vs localizing content
> This article is about how to localize the content you manage in Sanity Studio. To learn about how to change the UI language of your studio, visit this article, or visit this article if you want to learn about adding internationalization to your plugins.

## Best practice

Localization in Sanity is performed by storing language data as a value of a field in a document.

We recommend using these two optional plugins to simplify creating and maintaining localized documents and fields in Sanity Studio.

- For **translated documents**, we recommend the [@sanity/document-internationalization](https://github.com/sanity-io/document-internationalization) plugin, which will relate translations as references and handle setting a “language” field value on documents.
- For **translated fields**, the [internationalized-array plugin](https://github.com/sanity-io/sanity-plugin-internationalized-array) can be used with any field type and scales to as many languages as you may need to author.

## Methods of localization

Sanity allows you to model translated content as it makes the most sense to your workflow and content structure. There are two main approaches:

- **Field level localization**  - A single document with content in many languages
- Requires you to publish content in all languages simultaneously
- Achieved by creating an array or object that generates a field for each language value
- Best for documents that have a mix of language-specific and common fields
- Not recommended for Portable Text when using localized objects - you should use localized arrays instead, which helps minimize [attribute usage](/docs/content-lake/attribute-limit).


- **Document level localization**  - A unique document version for every language
- Allows the option to publish each language version independently
- References join language versions together
- Best for documents that have unique, language-specific fields and no common content across languages
- Best for translating content using Portable Text



Your preferred method will depend on your use case, content model, and publishing workflow. Each document’s schema plays a role in deciding the appropriate localization strategy, so you may use both in a single project.

We offer simple plugins for both strategies to improve the authoring experience in Sanity Studio.

## Sanity Studio walkthrough

![Localization approaches in Sanity Studio](https://www.youtube.com/watch?v=6acLvAvvG2w)

## Example repository

This [Course Platform Demo](https://github.com/sanity-io/demo-course-platform) is a Sanity Studio and Next.js front-end showcasing internationalized schema, popular plugin configuration, and how to query for localized content.

## Field-level translations

![](https://cdn.sanity.io/images/3do82whm/next/63887c9523d79a127a77ae43aeb1ef9b8490f83c-1184x750.png)

Instead of using a single field for a single value, field-level translations are created by building arrays or objects with a field for each language representing that value.

This is most useful in document schemas where only some fields in the document require translation. For example, a `person` will use the same `name` and `photo` in every language, but their `title` would need localizing.

So we create a structure like this with a `title` field for each language:

![](https://cdn.sanity.io/images/3do82whm/next/f2f18425284fdeb72cbc26732b9cb0726363b703-1920x1080.png)

### Schema for localized objects

Since Sanity schema definitions are written in JavaScript, you can programmatically define the language properties for these localized versions of fields. Adding languages becomes a task of expanding the list of languages, and all fields will be expanded to include them.

In the following code example, we are also using the [fieldset feature](https://www.sanity.io/docs/object-type#fieldsets-AbjN0ykp) of objects to group every language value except a "base language" into a collapsible group in order to tidy up the Studio document editor.

> [!WARNING]
> Gotcha
> Creating localized objects is fine for a few languages and fields, but it risks rapidly increasing your attribute count with many languages. Consider using arrays instead with the help of a plugin, detailed below.

```typescript
// ./schemas/localeStringType.ts

import {defineType, defineField} from 'sanity'

// Since schemas are code, we can programmatically build
// fields to hold translated values. We'll use this array
// of languages to determine which fields to define.
const supportedLanguages = [
  { id: 'en', title: 'English', isDefault: true },
  { id: 'no', title: 'Norwegian' },
  { id: 'fr', title: 'French' }
]

export const baseLanguage = supportedLanguages.find(l => l.isDefault)

export const localeString = defineType({
  title: 'Localized string',
  name: 'localeString',
  type: 'object',
  // Fieldsets can be used to group object fields.
  // Here we omit a fieldset for the "default language",
  // making it stand out as the main field.
  fieldsets: [
    {
      title: 'Translations',
      name: 'translations',
      options: { collapsible: true }
    }
  ],
  // Dynamically define one field per language
  fields: supportedLanguages.map(lang => ({
    title: lang.title,
    name: lang.id,
    type: 'string',
    fieldset: lang.isDefault ? null : 'translations'
  }))
})
```

> [!TIP]
> Protip
> Generalizing objects into their own “named types” makes them easily reusable across different documents. Many fields with unique name values could use this same localeString type.

```typescript
// ./schemas/presenterType.ts

import {defineType, defineField} from 'sanity'
import {baseLanguage} from './localeStringType.ts'

export const presenterType = defineType({
  title: 'Presenter',
  name: 'presenter',
  type: 'document',
  fields: [
	defineField({
      name: 'name',
      type: 'string'
    }),
	defineField({
      name: 'title',
      type: 'localeString'
    }),
  ],
  preview: {
    select: {
	  title: 'name',
      subtitle: `title.${baseLanguage.id}`
    }
  }
})
```

> [!TIP]
> Protip
> The IANA language tags can be helpful when selecting identifiers for your supported languages.

### Plugin for localized objects

To clean up the UI of a document with localized fields, you may choose to install the [language filter-plugin](https://www.npmjs.com/package/@sanity/language-filter) to allow editors to hide languages they won’t need to interact with.

### Querying localized objects with GROQ

Here is an example of content structured according to this schema, as it appears fetched from our query API:

```groq
*[_type == "presenter"][0]{
  name,
  title
}
```

```json
{
  "name": "Rune Botten",
  "title": {
    "en": "Rune is a solution architect at Sanity.io",
    "es": "Rune trabaja como arquitecto de soluciones en Sanity.io",
    "no": "Rune jobber som løsningsarkitekt hos Sanity.io"
  }
}
```

When using this content in a front-end, you can fetch the full document as above or return a specific `title` field value depending on which locale you are interested in displaying.

This example fetches only the English `title` and returns that value in the `title` property:

```groq
*[_type == "presenter"][0]{
  name,
  "title": title.en
}
```

This will return the following JSON from our previous content example:

```json
{
  "name": "Rune Botten",
  "title": "Rune is a solution architect at Sanity.io"
}
```

If you query the title in a language that might not have a value set yet, you can use the [coalesce function in GROQ](https://www.sanity.io/docs/groq-functions#80d6f2c07a20) to provide a fallback. In this case, the `title.sv` property won't have a value (since no sv language value is set in our example), so the English value is used instead, returning the same JSON as the example above. It would have fallen further back to the string Missing translation if it had not had an English value set either.

```groq
*[_type == "presenter"][0]{
  name,
  "title": coalesce(title.sv, title.en, "Missing translation")
}
```

Lastly, you may prefer to use variables in your GROQ query so that the query does not need to change when your desired language changes – just the values of the parameters supplied to the query:

```groq
*[_type == "presenter"][0]{
  name,
  "title": coalesce(title[$language], title[$baseLanguage], "Missing translation")
}
```

### Localized arrays

![](https://cdn.sanity.io/images/3do82whm/next/fe2c05e0bfc65df9f4a5132bb8a1b2d827afc8e5-1184x566.png)

You may prefer to create localized fields in an array structure for projects with many languages. Arrays [use fewer unique attributes](https://www.sanity.io/docs/attribute-limit) than objects using this method.

Here is a quick explanation of how language objects impact attributes.

An object for a string field with three languages creates these attributes:

```
title
title.en
title.fr
title.es
```

You create another unique attribute in your dataset for every new language you add.

An array of objects to store both a language and field value could create attributes like this:

```
title
title[]
title[]._key
title[].language
title[].value
```

Using the `language` field to store the language and `value` to store the field’s content, you can add many more languages without using more attributes.

The built-in array component is not best suited to authoring like this – as every array item needs to open in a popup dialog – but there is a solution.

### Plugin for localized arrays

The [internationalized-array plugin](https://github.com/sanity-io/sanity-plugin-internationalized-array) has a custom UI that can be used for any field type and renders each field input without a popup dialog.

It also saves an extra attribute by writing the language value to the `_key` field – something you cannot customize in the Studio with a regular schema.

### Querying localized arrays with GROQ

Now performing the same query for name and title but with the title stored in an array, using the [internationalized-array plugin](https://github.com/sanity-io/sanity-plugin-internationalized-array).

```groq
*[_type == "presenter"][0]{
  name,
  title
}
```

You will receive this data:

```json
{
  "name": "Rune Botten",
  "title": [
    {
      "_type": "internationalizedArrayStringValue",
      "_key": "en",
      "value": "Rune is a solution architect at Sanity.io"
    },
    {
      "_type": "internationalizedArrayStringValue",
      "_key": "es",
      "value": "Rune trabaja como arquitecto de soluciones en Sanity.io"
    },
    {
      "_type": "internationalizedArrayStringValue",
      "_key": "no",
      "value": "Rune jobber som løsningsarkitekt hos Sanity.io"
    }
  ]
}
```

To avoid over-fetching, update the query to: 

64. Filter this array to just the language field `_key` you need 
64. Only return the `value` field

```groq
*[_type == "presenter"][0]{
  name,
  "title": title[_key == "en"][0].value
}
```

Now the returned data is filtered down to just what you need:

```json
{
  "name": "Rune Botten",
  "title": "Rune is a solution architect at Sanity.io"
}
```

You can use the `coalesce()` [GROQ function](/docs/specifications/groq-functions) to fall back to another value if the targeted one is not yet set:

```groq
*[_type == "presenter"][0]{
  name,
  "title": coalesce(
    title[_key == "en"][0].value,
    title[_key == "nl"][0].value,
    "Missing translation"
  )
}
```

For the most flexibility, use variables so that your query remains the same but will adapt to whichever parameters you pass into it.

```groq
*[_type == "presenter"][0]{
  name,
  "title": coalesce(
    title[_key == $language][0].value,
    title[_key == $baseLanguage][0].value,
    "Missing translation"
  )
}
```

## Document-level translations

You might have more complex publishing workflows that field-level translations are too simple to solve. You could be working in a base language and want to publish that content as soon as it is ready, then publish translations as they become available from other editors or external translation services. Or you may have content that exists only in a certain locale. It might make the most sense to model localized content as separate documents.

In this example, we have a `lesson` document type where every field is unique to that language variant, so it makes sense to store them as separate documents.

![](https://cdn.sanity.io/images/3do82whm/next/96868d1130de41f2933a9ad9c89c010bd88fdb5d-1920x1080.png)

### Schema for document-level translations

The simplest way to achieve this is to have a language field on documents and set this to whichever language the document's contents correspond to.

```typescript
// ./schemas/articleType.ts

import {defineType, defineField} from 'sanity'

export const articleType = defineType({
  title: "Article",
  name: "article",
  type: "document",
  fields: [
    defineField({
      name: "language",
      type: "string",
      options: {
        list: [
          {title: 'English', value: 'en'},
          {title: 'Spanish', value: 'es'}
        ]
      }
    }),
    defineField({
      name: "title",
      type: "string",
    }),
    defineField({
      name: "body"
      type: "array",
      of: [{type: 'block'}],
    })
  ]
})
```

You can then filter queries for specific locales, thus only presenting the relevant localized content in your front ends.

Using our [Document Actions API](https://www.sanity.io/docs/document-actions) you can further add actions in the Studio for duplicating a document into another locale and then translate the content manually.

Or use [GROQ-powered webhooks](https://www.sanity.io/docs/webhooks) to send the document off to a third-party translation service through their API for automated or professional translation. Once the translation is complete, you can re-import it to your Sanity dataset via, for example, a webhook triggered by the translation service.

You can also use the [Structure Builder API](https://www.sanity.io/docs/structure-builder-introduction) to provide segmented navigation to find and organize localized content in the Structure tool if you wish.

![](https://cdn.sanity.io/images/3do82whm/next/68d1e0ae4825c2b6b0d7a6a4993192a46f5db2a9-1404x612.png)

### Plugin for document-level translations

![](https://cdn.sanity.io/images/3do82whm/next/adf3e48c0e1eb93facfdeaf3c906f028e6aaad9d-2516x894.png)

An integrated solution is to install the [@sanity/document-internationalization](https://github.com/sanity-io/document-internationalization) plugin, which provides most of the above in-Studio features with minimal setup. It handles setting a language field on documents and automatically creates a linked document that stores the translations together so they are more easily queried.

### Querying for localized documents with GROQ

How you query for translated documents will depend on how you have built references between them. If you use the @sanity/document-internationalization plugin, your query will look like the one below.

In this query, you are looking for a `lesson` type document of a specific language, then find the `translation.metadata` type document which contains a reference to it and other language translations.

```groq
*[_type == "lesson" && language == $language]{
  title,
  slug,
  language,
  // Get the translations metadata
  // And resolve the `value` reference field in each array item
  "_translations": *[_type == "translation.metadata" && references(^._id)].translations[].value->{
    title,
    slug,
    language
  },
}
```

The plugin’s page contains more details on [how to query for translations in both GROQ and GraphQL](https://www.sanity.io/plugins/document-internationalization#querying-with-groq).



## Translating content with the AI Assist plugin

The official [AI Assist plugin](/docs/ai-assist) for Sanity Studio offers Large Language Model-powered content translation at the click of a sparkly button.

![Shows the top-level document menu for AI Assist instructions open with a "Translate document" option highlighted](https://cdn.sanity.io/images/3do82whm/next/bdc59e983472bee6853d4cdb81c1e3e235df74e2-610x217.png)

- [Translating content with AI Assist](/docs/studio/ai-assist-content-translation)

## Translation service plugins

In addition to plugins to assist with authoring localized content in Sanity Studio, we offer some adapters to popular translation service providers:

- [Transifex plugin](https://www.sanity.io/plugins/sanity-plugin-transifex)
- [Smartling plugin](https://www.sanity.io/plugins/sanity-plugin-studio-smartling)



# Content Releases Configuration

Content Releases allow teams to organize and schedule updates across multiple documents. Teams can plan, preview, and validate significant changes in advance, ensuring seamless and conflict-free content deployment.

This document explores configuring Content Releases in Sanity Studio. For details on using Releases, or interacting with the API, follow these links:

[User Guide](/docs/user-guides/content-releases)

[Content Releases API](/docs/apis-and-sdks/content-releases-api)



_This is a paid feature, available on the Growth plan._

## Setup and configuration

Content Releases is enabled by default for studios running version 3.75.0 or later. You should update any official plugins and dependencies, such as AI assist, Vision, and any presentation-related plugins to ensure compatibility.

If you'd like to disable content releases for your studio, you can do so in the configuration file.

```typescript
// sanity.config.ts / sanity.config.js

export default defineConfig({
  // ...
  releases: {
    enabled: false
  }
})
```

### Limitations

- In Sanity versions prior to 3.79.0, dataset imports did not work with datasets that contain versions. Please update to 3.79.0 or later before attempting a dataset export/import.
- When you schedule a release, we perform checks in the background to ensure reference integrity between documents. These checks do not take into account [cross-dataset references](/docs/studio/cross-dataset-references).
- GROQ-powered webhooks don't support the latest API version necessary to interact with version documents. You can, however, use the release document type to react to release state. See the [Content Releases API patterns page](https://www.sanity.io/docs/content-releases-cheat-sheet#99b667300c03) for an example. 
- The JS client does not include methods for managing releases, but you can still send actions and queries to modify releases manually. See the [Content Releases API documentation](/docs/apis-and-sdks/content-releases-api) for details on using the API.
- API changes supporting Content Releases introduced changes to perspectives. [See the changelog](https://www.sanity.io/changelog/676aaa9d-2da6-44fb-abe5-580f28047c10) for details on breaking changes.

### Presentation and Visual Editing

Content Release previews in Presentation work with front ends that use Loaders. This includes `@sanity/core-loader`, `@sanity/react-loader`, `@sanity/svelte-loader`, and packages that rely on them such as `next-sanity` (with a loader or `defineLive`) and `@nuxtjs/sanity`. 

Configure your clients to use the `2025-02-19` version of the API to enable previewing.

For applications configured with official loaders and the presentation tool, presentation will preview Content Releases as expected. We're working to provide guidance for custom implementations in the future, however the preferred path is [Presentation tool and Loaders.](/docs/visual-editing-reference-overview)

Follow our guides for [Visual Editing](/docs/visual-editing/introduction-to-visual-editing) to configure presentation in your application.

## Supported plugins

Official plugins have been updated to support Releases. We recommend updating to the latest versions of any official plugins to ensure full compatibility.

[Migrate plugins to support Releases](/docs/developer-guides/migrating-plugins-to-support-content-releases)







# Enable and configure Comments

Comments are available for all paid plans in Sanity Studio. This article walks studio maintainers through enabling and configuring comments for their projects.

[Comments for Sanity Studio](/docs/studio/comments)

[Enabling Tasks for Sanity Studio](/docs/studio/configuring-tasks)



_This is a paid feature, available on the Growth plan._

### Prerequisites:

- Sanity Studio v3.40.0 or later (latest is always recommended)
- Project on a supported plan

> [!NOTE]
> Permissions needed for tasks and comments
> All roles need to have these permissions to be able to use comments and tasks fully:
> 
> Management permissions
> "Project details" read => For feature flag
> "Project members" read => @mention members
> "Project datasets" read => View all comments with count
> 
> Content permissions
> "All documents" read on main dataset(s) used in your Studio/workspaces

## Enabling and disabling comments

Comments are enabled by default for all paid plans. To disable comments, set `document.comments.enabled` to `false` in your studio configuration file:

```typescript
// ./sanity.config.ts|js

export default defineConfig({
  // ... rest of config
  document: {
    comments: {
      enabled: false,
    },
  },
});
```

> [!WARNING]
> Gotcha
> Disabling comments hides them in the studio, but existing comments persist in the add-on comment dataset.

### Enabling comments for specific document types

To enable comments only for specific document types, use an arrow function:

```typescript
// ./sanity.config.ts|js

export default defineConfig({
  // ... rest of config
  document: {
    comments: {
      enabled: (ctx) => {
        return ctx.documentType == 'whitepaper';
      },
    },
  },
});
```

To enable comments for multiple document types, use an array:

```typescript
// ./sanity.config.ts|js
const COMMENTS_ENABLED = ['article', 'blog', 'whitepaper'];

export default defineConfig({
  // ... rest of config
  document: {
    comments: {
      enabled: (ctx) => {
        return COMMENTS_ENABLED.includes(ctx.documentType);
      },
    },
  },
});
```

## Where are comments stored?

To keep everything neat and tidy, comments are stored parallel to your content in a complimentary [dataset](/docs/content-lake/datasets), along with other workflow and collaboration data, such as Comments.

These datasets:

- Do not count toward the data limit of your current plan.
- Do not incur any extra costs for your project.
- Are listed in the [project management pages](https://sanity.io/manage) under **Datasets**, along with all existing datasets for a project.
- Include a distinctive suffix in the dataset name. This is a best-effort attempt, and the result may vary, depending on the character length of the name of the related document dataset. Examples: - `<related-document-dataset>-comments` 
-  `<related-document-dataset>-cmts` 
-  `<related-document-dataset>-cmt` 
-  `<related-document-dataset>-c`


- Are searchable: you can query comment datasets with [GROQ](/docs/groq) or [GraphQL](https://www.sanity.io/glossary/graphql). This means they can also be used with [GROQ-powered Webhooks](/docs/compute-and-ai/webhooks).

> [!WARNING]
> Gotcha
> Deleting an add-on comment dataset permanently removes all comments. To restore commenting after deletion, reload the studio instance to create a new, empty comment dataset.

## Copy comments to a Cloud Cloned dataset

When you export or [Cloud Clone](/docs/content-lake/how-to-use-cloud-clone-for-datasets) a dataset, the comments don’t migrate with the data. To copy comments to a new dataset or studio, you’ll need to perform the following steps:

23. Identify the name of the comment dataset you wish to copy
23. Export the comment dataset
23. Enable comments in the new primary dataset and identify the name of the comments dataset
23. Import the comment dataset file into the new comment dataset
23. Confirm that the process was successful

For the following examples, we’ll use the terms “production” and “staging” to represent the original and new primary datasets. If you haven’t already, make sure to Cloud Clone your primary dataset.

### Identify the name of the comment dataset

Comments live alongside regular datasets in an add-on dataset. You can find the name by visiting [manage](https://www.sanity.io/manage), selecting your project, and selecting the datasets screen. You can also run the `dataset list` command in the CLI.

Input

```sh
npx sanity@latest dataset list
```

Response

```sh
production
production-comments
staging
```

Make note of the name of the comments dataset that matches your primary dataset. In this case, `production-comments`.

### Export the comment dataset

Export the comment dataset to a local file using the `dataset export` command.

```sh
npx sanity@latest dataset export production-comments production-comments-export.tar.gz
```

Make note of the file name and location. You'll need it shortly.

### Enable comments in the new dataset

If you haven't already, reload Sanity Studio with the new primary dataset configured.

```typescript
// sanity.config.ts

export default defineConfig({
  // ... rest of config
  dataset: 'staging'
})
```

If the `dataset list` command does not list a comments add-on dataset for your primary dataset, such as `staging-comments`, you'll need to trigger the creation of the dataset by manually making a comment in Sanity Studio. Once that's done, you should see the new comments dataset in the results of `dataset list`.

Input

```sh
npx sanity@latest dataset list
```

Response

```sh
production
production-comments
staging
staging-comments
```

### Import the comments dataset

Use the `dataset import` command to import the local comments export into the new comments dataset.

```sh
npx sanity@latest dataset import production-comments-export.tar.gz staging-comments
```

### Confirm the import

Reload and visit your Studio. Confirm that the imported comments are as expected and delete the test comment if you created one earlier.



# Configuring Tasks

The Tasks feature for Sanity Studio enables your content creation team to collaborate more effectively right where the work is done. The feature is enabled by default for any eligible project, but can be disabled with a single line of configuration, should you wish to do so.

[Tasks workflow in Sanity Studio](/docs/studio/tasks)

[Comments in Sanity Studio](/docs/studio/configuring-comments)



_This is a paid feature, available on the Growth plan._

## Enable and configure tasks in your studio

Tasks are enabled by default for all eligible projects. If you’d rather opt out for now, you can do so by adding the following property to your main studio configuration:

```tsx
// ./sanity.config.ts|js

export default defineConfig({
  // ... rest of config
	tasks: { enabled: false },
})
```

## Where are tasks stored?

To keep everything neat and tidy, tasks are stored parallel to your content in a complimentary dataset, along with other workflow and collaboration data, such as Comments.

**These datasets:**

- Do not count toward the data limit of your current plan.
- Do not incur any extra costs for your project.
- Are listed in the [project management pages](https://sanity.io/manage) under **Datasets**, along with all existing datasets for a project.
- Include a distinctive suffix in the dataset name. This is a best-effort attempt, and the result may vary, depending on the character length of the name of the related document dataset. Examples: - `<related-document-dataset>-comments` 
-  `<related-document-dataset>-cmts` 
-  `<related-document-dataset>-cmt` 
-  `<related-document-dataset>-c`


- Are searchable: you can query comment datasets with [GROQ](/docs/groq) or [GraphQL](https://www.sanity.io/glossary/graphql).

## Permissions for tasks and comments

All roles need to have these permissions to be able to use comments and tasks fully:

**Management permissions**
"Project details" `read` => For feature flag
"Project members" `read` => @mention members
"Project datasets" `read` => View all comments with count

**Content permissions**
"All documents" `read` on **main** dataset(s) used in your Studio/workspaces



# Scheduled publishing

> [!NOTE]
> Use Releases instead of Scheduled Publishing
> We recommend using Releases as they provide better control and coordination of content updates. With Releases, you can:
> 
> Organize and schedule updates across multiple documents at once.
> 
> Preview and validate changes before they go live.
> 
> Reduce manual effort and risk of conflicting changes.
> 
> Scheduled Publishing is not enabled by default. It can be enabled in the config by setting scheduledPublishing: { enabled: true }. Inversely, you can remove or disable the feature by setting enabled to false or removing the configuration setting.
> 
> If you're using Scheduled Publishing alongside Content Releases, you'll see a warning banner. To disable this warning, you can set scheduledPublishing: { showReleasesBanner: false } to disable it.
> 
> 
> 
> Scheduled Publishing uses the Sanity Scheduling API which is available on Growth plans and above.

![Shows the Scheduled publishing interface in Sanity Studio](https://cdn.sanity.io/images/3do82whm/next/ab8b7f0a2c183d32ef85d36d8886139dddd981e0-2772x1624.png)

![Shows a scheduled post being edited in Sanity Studio](https://cdn.sanity.io/images/3do82whm/next/a4fd9f8169b9e464534257bcf643730c1021260c-2140x1526.png)

## Features

### Create and edit schedules directly from the document editor 

- Create and edit schedules for the document you're working on 
- See current schedule status and potential validation issues 

### View all your schedules with our dedicated tool 

- Filter all schedules by status or use the calendar to browse by date 
- Edit, delete, and immediately publish schedules 
- Automatically validate upcoming schedules, and identify issues before they're published 
- Easily identify who created a schedule 

### View schedule dates in any remote time zone

![Shows a modal dialog for selecting time zones](https://cdn.sanity.io/images/3do82whm/next/c37180974a616b4364f90b8b0ad0d1cdaf077510-1514x840.png)

- Change the time zone you want to preview schedules in by clicking the 🌎 Time Zone button when visible. Great when you need to co-ordinate with a global team or want to time publication to specific regions. 
- Easily select time zones by city, time zone abbreviation or name search. - Selected time zones are automatically stored in your local storage for future use.



## Getting started

If you are starting from scratch, skip the following section on uninstalling the plugin and cleaning up old configuration and jump directly to the [next section on how to configure or disable Scheduled Publishing](https://www.sanity.io/docs/scheduled-publishing#8a69bfded17d).

### Uninstall the Scheduled Publishing plugin

If you are already using Scheduled Publishing plugin, the first step is to get rid of it and [update your studio to the latest release](/docs/studio/upgrade). If you already updated your studio you might have gotten an alert about this.

![Shows an in-studio alert about the plugin deprecation](https://cdn.sanity.io/images/3do82whm/next/d86b559a72fc3845d9036dcf2373b3f090897471-829x389.png)

Run the following command in your project root to uninstall the plugin:

```sh
npm uninstall @sanity/scheduled-publishing
```

Next, remove the plugin from your studio configuration. Typically you'll find this in `./sanity.config.ts|js.` Find and delete the following lines from your configuration:

```typescript
// ./sanity.config.ts|js

import {scheduledPublishing} from '@sanity/scheduled-publishing'

export default defineConfig({
  // ...
  plugins: [
    scheduledPublishing()
  ],
})
```

Your plugin declaration might be a bit more expansive if you've defined a custom time format for the plugin. Delete it all!

```typescript
// ./sanity.config.ts|js

import {scheduledPublishing} from '@sanity/scheduled-publishing'

export default defineConfig({
  // ...
  plugins: [
    scheduledPublishing({
      inputDateTimeFormat: 'MM/dd/yyyy h:mm a',
    }),
  ],
})
```

> [!TIP]
> Protip
> You might also have defined some custom document actions and badges to support Scheduled Publishing. You can keep these around, and they'll continue to work after migrating to the core studio functionality. Refer to the section on document actions and badges further on in this article.

### Add new configuration for Scheduled Publishing

Note that while very similar to the plugin config this goes into the top-level of your studio configuration. Setting `enabled` to `false` will opt you out of using scheduled publishing for the project.

```typescript
// ./sanity.config.ts|js
import {defineConfig} from 'sanity'

defineConfig({ 
  // ....
  scheduledPublishing: {
    enabled: true, 
    inputDateTimeFormat: 'MM/dd/yyyy h:mm a',
  }
)
```

As before, you can add a custom time format if you so wish. If left unspecified, the format will default to `dd/MM/yyyy HH:mm`.

## Document actions and badges

You can further enhance your Scheduled Publishing experience with custom document actions and badges. 

### Configure the document action 

This example assumes you've customized your [document actions](/docs/studio/document-actions) and would like to only show the Schedule button on `movie` documents only.

The Schedule document action allows users to both create and edit existing schedules directly from the form editor. It is added to all document types by the plugin, so you should remove it from types that should NOT have it.

```typescript
// ./sanity.config.ts|js

import {defineConfig, ScheduleAction} from 'sanity'

export default defineConfig({
  // ...
  document: {
    actions: (previousActions, {schemaType}) => {
      /*
       * Please note that this will only alter the visibility of the button in the studio.
       * Users with document publish permissions will be able to create schedules directly
       * via the Scheduled Publishing API.
       */
      if (schemaType.name !== 'movie') {
        // Remove the schedule action from any documents that is not 'movie'.
        return previousActions.filter((action) => action !== ScheduleAction)
      }
      return previousActions
    },
  },
})
```

Note that `ScheduleAction` is now imported from the core `sanity` package.

### Configure the document badge 

This example assumes you've customised your own [document badges](/docs/studio/document-badges-api) and would like to only show the Scheduled badge on `movie` documents only.

The Scheduled document badge displays whether the current document is scheduled and when it will be published if so. It is added to all document types by the plugin, so you should remove it from types that should NOT have it.

```typescript
// ./sanity.config.ts|js

import {defineConfig, ScheduledBadge} from 'sanity'

export default defineConfig({
  // ...

  document: {
    actions: (previousBadges, {schemaType}) => {
      if (schemaType.name !== 'movie') {
        // Remove the schedule badge from any documents that is not 'movie'.
        return previousBadges.filter((badge) => badge !== ScheduledBadge)
      }
      return previousBadges
    },
  },
})
```

Note that `ScheduleBadge` is now imported from the core `sanity` package.

## Frequently Asked Questions

### What's the relationship between Schedules and my dataset?

Schedules sit adjacent to your dataset and can be managed using the [Scheduling API](/docs/http-reference/scheduling) (which this plugin does for you).

Schedules are a unique resource and are linked to, but do not exist within your Sanity project and dataset. It's important to understand the following behavior:

- As schedules are not contained within a project’s dataset, you cannot query them via GROQ or GraphQL.
- Deleting a dataset will immediately delete all schedules.
- Deleting a project will immediately delete all schedules.
- `sanity dataset export` will not include schedules and `sanity dataset import` does not support importing schedules.
- Server-side copying of datasets does not include schedules.
- When a project is disabled or blocked, all scheduled publishes will invariably fail as mutations will not be allowed on the dataset.

More information can be found in the [Scheduling API](/docs/http-reference/scheduling) article.

### Will scheduled documents with a validation errors publish?

**Yes.** Documents scheduled to publish in future will do so, even if they contain validation errors. This also applies to scheduled documents that you manually opt to publish immediately via the tool.



# Structure tool

The Structure tool is included with Sanity Studio and allows you to customize the experience of creating, browsing, and managing documents. 

![Default structure tool layout](https://cdn.sanity.io/images/3do82whm/next/25f3e527146f39ec5abdd8549a51d42cd3b6aeb8-3798x2250.png)

## Install

New projects come pre-configured with the Structure tool. For existing projects, or if it isn’t part of your Studio configuration, you can install it by updating your project’s configuration file.

```typescript
// sanity.config.ts
import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'

export default defineConfig({
  // ...
  plugins: [structureTool()],
})

```

> [!NOTE]
> Is Structure a tool or a plugin?
> Wondering why you’re adding a tool to the plugins array? Plugins are containers for shared tools, components, and other Studio configuration settings.

You can configure the Structure tool beyond the default settings by passing a configuration object to `structureTool` . The [Structure tool API](https://www.sanity.io/docs/structure-tool-api) reference describes the list of available configuration options.

> [!WARNING]
> Gotcha
> The Structure tool's document list has a limited view of 2000 documents. If you find yourself running into this limitation, consider customizing your Structure configuration with Structure Builder to lay out documents in a more categorized way.

## Customize

The Structure tool includes Structure Builder, an API that allows you to customize the way lists, documents, views, and menus are organized within Studio.

![Customized structure tool screenshot](https://cdn.sanity.io/images/3do82whm/next/801e3897cceea68de13a93cb8cbed2fc5cea982c-2288x1388.png)

Start customizing your studio with the [Introduction to Structure Builder](https://www.sanity.io/docs/structure-builder-introduction) series.

## Additional resources

[Structure Tool API](/docs/studio/structure-tool-api)

[Structure Builder cheat sheet](/docs/studio/structure-builder-cheat-sheet)

[Structure Builder API reference](/docs/studio/structure-builder-reference)

[Studio Tools](/docs/studio/studio-tools)







# Introduction to Structure Builder

![Screenshot showing different types of panes that Structure Builder can modify: static list, document list, and document.](https://cdn.sanity.io/images/3do82whm/next/47b805f2167a3da99af4c265fabc36fba3be7f84-1152x700.png)

Structure Builder is an API meant to help reorganize flows and documents inside of Sanity Studio. This article introduces the central ideas and definitions needed to understand how to use the API. If you want to jump ahead, [check out the reference documentation](/docs/studio/structure-builder-reference) for all API methods and functionality.

Structure builder is useful whenever you want to control how documents are grouped and listed in the studio or for adding additional in-studio previews or content to documents.

> [!TIP]
> Protip
> Looking for quick examples of common use cases? See the Structure Builder cheat sheet.

> [!NOTE]
> Learning the Structure Builder API
> This collection of articles will walk you through all the basics of using Structure Builder to create custom editing experiences.
> 
> Introduction to concepts
> 
> Set up structure builder in your project
> 
> Create a link to a single edit page in your main document type list 
> 
> Manually group items in your main document type list 
> 
> Dynamically group documents in a document list
> 
> Create custom document pane

## How does structure builder work?

The Structure Builder API is a collection of classes with methods that you can chain and nest to express how documents should be organized in the Sanity Studio.

This code snippet shows you an example of how to set up a structure for just a form and a custom preview in the Studio. Notice the methods being added to each other. For a detailed explanation of this code, [read the next article on setting up Structure Builder](/docs/studio/set-up-structure-builder-to-override-the-default-list-view). 

## What are collapsable panes?

```javascript
// src/structure.js (.ts)

export const structure = (S) => S.document()
  .schemaType('config')
  .documentId('globalConfig')
  .views([
    S.view.form(),
    S.view.component(ConfigPreview)
  ])
```

When working with the Structure Builder API, you'll primarily be modifying or creating collapsable panes. These panes are the parts within the Structure tool with a title and contain a list of document types, a list of documents, a form, or a custom component. If you make the window narrower, add more panes in the viewport, or click the title area, they will fold down to make more space within the window. This gives authors a quick way to focus on the right things and keep a visual trail of where they are in the different hierarchies.

There are often items within a pane that may open a new pane. The new pane will open to the right, and the current pane will stack to the left. In the Structure Builder API, the pane immediately following another is commonly referred to as a `child`. The initial pane shown when the Structure tool opens is called the `root` pane.

![An annotated screenshot showing a pane with list, divider, list item, and list item icon](https://cdn.sanity.io/images/3do82whm/next/21a287beeb6d4647c6ed76f5942b2aeb274d67a4-1152x700.png)

## Pane types

There are four types of panes:

- List
- Document list
- Document
- Custom component

### List

A list contains one or more list items and is generally considered to be static. An example of this pane type is a list of document types within your schema. 

While generally used for static items, a pane can perform asynchronous calls before determining its items. It can be useful if you need more control over how a small set of items should be rendered in the list. If you're listing documents, you should generally always use a *document list*.

### Document list

Optimized for displaying a list of documents, as the name implies. It differs from a regular list in that it does not simply fetch a list of documents on load but also keeps that list up to date with any changes: documents that match its filter that are deleted will disappear, newly created documents will appear, and changes to the titles will be reflected in real-time.

A document list is given a GROQ filter and then builds an optimized query based on the filter and the pane ordering. It then implements an "infinite scrolling" pattern that lazy-loads the properties needed to display the screen's documents.

A *document type list* is a subset of a document list, which collects documents where the `_type` property matches a given value (in the schema definition, this is the `name` you set for the `type: 'document'`). Document types may also have [Initial Value Templates](/docs/studio/initial-value-templates) attached to them and certain [orderings of their documents](/docs/studio/sort-orders).

### Document (and views)

A document pane (and its corresponding *document node* in structure terms) is a component that holds a document’s values and different states (*published*, *draft*, *historical*, *displayed*). Typically, this will be the editor form with the input fields for the document's data. 

It can also be a component that you import and configure with your structure definition. Typically, a view is helpful when you want to contextualize your document values somehow. It can be used for making different types of previews, statistics, checklists, alternative ways of interacting with the document values, or anything you can build with React.

### Custom component

You may also implement your pane using a custom React component. The component node can be given a set of options passed as an `options` property to the actual React component being rendered.

The component is rendered inside the shell of a pane, with the pane header, menu, and actions available for configuration. You will find more information on how to use this in the [reference documentation](/docs/studio/structure-builder-reference) for the structure builder.

## Path resolution and URL structure

You can open the same document from multiple paths through a structure. This creates an interesting challenge. Sometimes, you know only the document's ID you want to open, but not necessarily with which of the different paths makes sense to open it. In these cases, the API will make a best-effort calculation to figure this out for you, while the fallback will be to open the document to the right of the root pane.

The Structure Builder API also gives you ways to set a default configuration for a document node that's opened outside of a path. This is useful when you want to ensure that a certain document type always has a set of views accessible.

## Child resolvers

Each pane is represented in the URL by an ID. When the studio is first loaded – and on subsequent navigation – the Structure tool looks at the segmented ID in the URL and tries to resolve each ID into a structure node.

It does this by calling the *child resolver* on the parent node. For example, a common pattern is the document type list leading to a document editor. When an editor clicks on any item within the document type list, it will render a document editor as a child of that list. This is usually represented in the URL by something like `documentType;documentId` - for instance, `book;game-of-thrones` represents the `book` type and a document with an ID of `game-of-thrones`.

The Structure tool will call the child resolver of the root node in the structure with an argument containing the first segment (`book`), which will return a document-type list. When that is returned, it will call the child resolver of the document type list with the next segment in the URL (`game-of-thrones`). This will return the document editor pane for this specific document. 

Child resolvers don't *necessarily* care about the ID of the child. In these cases, it's better to define a static structure node instead of a function *returning* that structure node since this will help the Structure tool make certain assumptions.

## URL state

Most states within the structure are represented in the URL bar. This is why you have to specify an `id` – often implicit when setting a `title` – for lists, document nodes, or components. These identifiers are semi-colon-separated in the URL path. Other states within a document node – such as views – can also be parameterized in the URL bar.

This makes it possible to share the Structure tool's exact state more easily between multiple tabs, windows, or users. It also gives you browser history so that editors can use their browser's history affordances to go between different UI states.

## Next steps

Now that you have a solid foundation of the concepts and definitions that make Structure Builder work, let's look at implementing a [basic override of the default structure in a studio](/docs/studio/set-up-structure-builder-to-override-the-default-list-view).





# Set up Structure Builder to override the default list view

![A list view in the Desk tool that shows a new main list title reading "Base" instead of "Content."](https://cdn.sanity.io/images/3do82whm/next/f60a94d0d8c101e25e7afc0afae7f6e731e33164-2482x1378.png)

In this article, we'll use the Structure Builder API to modify the default list view for a Sanity Studio. If you're unfamiliar with the concepts behind Structure Builder, be sure [to read the introductory article](/docs/studio/structure-builder-introduction).

> [!NOTE]
> Learning the Structure Builder API
> This collection of articles will walk you through all the basics of using Structure Builder to create custom editing experiences.
> 
> Introduction to concepts
> 
> Set up structure builder in your project
> 
> Create a link to a single edit page in your main document type list 
> 
> Manually group items in your main document type list 
> 
> Dynamically group documents in a document list
> 
> Create custom document pane

## Setting up Structure Builder for your project

The studio comes with a default structure when it's installed. In order to override the default behavior we'll provide a structure resolving function to the configuration of the `structureTool`-plugin. 

```javascript
// ./sanity.config.js

import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'
import {schemaTypes} from './schemas'

export default defineConfig({
  name: 'default',
  title: 'structure-builder-playground',
  projectId: '<projectId>',
  dataset: 'production',
  plugins: [
    structureTool({
      structure: (S) =>
        S.list()
          .title('Base')
          .items([...S.documentTypeListItems().reverse()]),
    }),
  ],
  schema: {
    types: schemaTypes,
  },
})

```

The `structureTool`'s `structure` property accepts a callback function that receives the builder class, conventionally referred to as capital `S`, as its only argument. In the example above we list out the different types of the project, just like the studio would do by default, except in reversed order.

> [!TIP]
> Protip
> The code can live anywhere in your project. In the previous example we put the structure resolving function directly into the main configuration, but this gets unwieldy fast. A better solution is to externalize structures into their own file or several files.
> 
> For simple structures, having it in the root in one file makes sense. For larger customizations with multiple components, it's considered a best practice to move this code into its own folder.



## Defining a new default structure 

Let's clean up the previous example a bit by moving our structure into a separate file. At the root of your project, create a new file called `deskStructure.js`.

We'll define our structure function as a named export and import it in `sanity.config.js`.

```javascript
// ./deskStructure.js

export const myStructure = (S) =>
  S.list()
    .title('Base')
    .items([...S.documentTypeListItems().reverse()])
```

```javascript
// ./sanity.config.js

import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'
import {schemaTypes} from './schemas'
import {myStructure} from './deskStructure'

export default defineConfig({
  // ...rest of config
  plugins: [
   structureTool({
      structure: myStructure,
    }),
  ],
})

```

Let's break these methods down.

### `S.list()`

The `.list()` method generates a new generic list. Since it's not a child node, it will appear in the first pane of the studio. In most use cases, the `documentTypeList` or `documentList` will be preferred, since they have additional convenience methods. For the first pane of studio, the generic list works the best.

### `.title()`

The `.title()` method exists nested on methods that create various types of panes and accepts a string as its argument. In this case, we'll change the title of our initial panel to be "Base" instead the default "Content."

> [!TIP]
> Protip
> Each panel should have an ID defined. If a title is provided, but no ID, the ID will be generated from the title of the pane.
> 
> This ID will be used to generate routes in the studio.

If we stop here, the studio will now load. It will contain an initial panel with no content. We need to tell our generic list what items should be listed.

### `.items()`

The `.items()` method is used to define the contents of a list panel. The method will take an array of items to populate the list. Typically, it will accept a complementary method from Structure Builder, such as `listItem` or `documentListItem`. In this case, the initial panel of the studio should populate with a list of document types. 

### `S.documentTypeListItems()`

The `documentTypeListItems()` method will find all the document types created by the schema defined in `schema.js` and display links to lists of documents that are of that type.

## Next steps

From here, there's a working instance of Structure Builder in the project. In the next tutorial, we'll look at [adding a link to edit a specific document](/docs/studio/create-a-link-to-a-single-edit-page-in-your-main-document-type-list) in the first panel we just rebuilt.



# Create a link to a single edit page in your main document type list

In some cases, to make a strong editing experience, it's important to create a document type that only serves one document instead of a list of documents. In this article, we'll create the often-used "site settings" pattern that implements a schema to control global variables for our front-end site. To create this pattern, we'll use the Structure Builder API to create a singleton document type.

If you're not familiar with the Structure Builder API, be sure to read through the other articles in this series.

> [!NOTE]
> Learning the Structure Builder API
> This collection of articles will walk you through all the basics of using Structure Builder to create custom editing experiences.
> 
> Introduction to concepts
> 
> Set up structure builder in your project
> 
> Create a link to a single edit page in your main document type list 
> 
> Manually group items in your main document type list 
> 
> Dynamically group documents in a document list
> 
> Create custom document pane

## Creating the site settings schema and document

Before adjusting the studio's main document type list, we'll create a document type and a specific document. For this example, we'll keep it simple, but any global variable you need can be stored in a schema like this. We'll start by creating a new schema named `siteSettings.js` in our `/schemas` directory.

```javascript
// schemas/siteSettings.js
export default {
  name: 'siteSettings',
  title: 'Site Settings',
  type: 'document',
  fields: [
    {
      name: 'title',
      title: 'Site Title',
      type: 'string'
    },
    {
      name: 'description',
      title: 'Site Description',
      type: 'text'
    }
  ]
}
```

Be sure to import and specify this in your project's `/schemas/index.js` file. The example shows the schema setup from the default blog template you can pick when setting up a new project with the Sanity CLI, and which we'll be using as our example studio going forth. Importing and adding your `siteSettings` schema should work the same even if your setup looks different.

```javascript
// /schemas/index.js

import blockContent from './blockContent'
import category from './category'
import post from './post'
import author from './author'
import siteSettings from './siteSettings'

export const schemaTypes = [
  post,
  author,
  category,
  blockContent,
  siteSettings,
]
```

At this point, we have a `siteSettings` document type but no documents. We also have the ability to create multiple site settings documents. This is potentially dangerous and confusing for our editors. 

## Adding the document to the first panel

![A screenshot illustrating a new "Settings" list item that has a child pane of a single document instead of a list of settings documents.](https://cdn.sanity.io/images/3do82whm/next/cd356a83a0bd6dd6bed5651dde2c3e0305674e7c-2482x1378.png)

To add a single document to the first panel, we'll edit the `deskStructure.js` file that we created in [this article](/docs/studio/set-up-structure-builder-to-override-the-default-list-view).

```javascript
// ./deskStructure.js

export const myStructure = (S) =>
  S.list()
    .title('Base')
    .items([
      S.listItem()
        .title('Site Settings')
        .child(
          S.document()
            .schemaType('siteSettings')
            .documentId('siteSettings')),
      ...S.documentTypeListItems(),
    ])
```

[In the last article](/docs/studio/set-up-structure-builder-to-override-the-default-list-view), we overrode the title of our list but showed all of our Document Types with no modifications in the `.items()` method. Now, we need to modify the array that the `.items()` method uses. 

### `S.listItem()`

Since the items will be displayed in array order, we'll start our array with our new custom item. To make a custom list item, we'll use [the .listItem() method](/docs/studio/structure-builder-reference) on the main Structure Builder object.

The `listItem()` method has multiple nested methods that we'll use to define its properties. We'll define the item's title with the `.title()` method. There's an optional `.id()` method, as well, but by default, the ID can be built from the title.

### `.child()`

The `.child()` method will define what the next pane contains when an editor clicks on this item. In our case, we want it to be a single document with a specific schema type and ID.

### `S.document()`

The `.document()` method allows us to specify which document and schema type will be the focus of the next pane. If there's already a document that you want to use, you can use its `_id` value in the `.documentId()` method. By putting a string in this method, it will create a document with that ID if it doesn't already exist. In our case, we'll use the string `siteSettings` to make things as human-readable as possible.

### Listing out all the document types

We still need to show any other document type items in our list. To do this, instead of simply calling the `S.documentTypeListItems()` method like we did in the last article, we need to put each of that method's array items into our current array. To do this, we use [the JavaScript Spread operator](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Spread_syntax): `...`.

When we do this, however, we see the flaw in our plan: The Site Settings document type is listed in this list and our manually defined item.

## Removing singleton document types from the main document type list

To remove our site settings document type from our main list, we need to run a JavaScript filter against our document types. Luckily, we're already spreading all those items.

```javascript
// ./deskStructure.js

export const myStructure = (S) =>
  S.list()
    .title('Base')
    .items([
      S.listItem()
        .title('Site Settings')
        .child(
          S.document()
            .schemaType('siteSettings')
            .documentId('siteSettings')),
            ...S.documentTypeListItems().filter(listItem => !['siteSettings'].includes(listItem.getId()))
    ])
```

The `filter()` method takes an anonymous function as its argument and passes each array item as a property of the function. In our filter function, we'll check to see if each `listItem` has an ID that matches our current string using the `getId()` method on the item. To set this up for more excluded document types, we can make this an array.

## Next steps

We now have a working singleton in our main list pane. In the next article, we'll take a look at [manually grouping multiple list items](/docs/studio/manually-group-items-in-a-pane) to create sections that will make an editor's life easier.





# Manually group items in a pane

![](https://cdn.sanity.io/images/3do82whm/next/16010730db213f3f7f4200b06ed84e54b7c33886-1439x764.png)

We've now learned how to override our studio's default structure and make a list of custom items. Now, let's look at how we can group our single documents in a manually-created group to open a secondary list for our settings.

If you're unfamiliar with setting up the Structure Builder API, be sure to check out the previous articles in this series.

> [!NOTE]
> Learning the Structure Builder API
> This collection of articles will walk you through all the basics of using Structure Builder to create custom editing experiences.
> 
> Introduction to concepts
> 
> Set up structure builder in your project
> 
> Create a link to a single edit page in your main document type list 
> 
> Manually group items in your main document type list 
> 
> Dynamically group documents in a document list
> 
> Create custom document pane

## Creating our new singletons

We'll create a list of "Settings Documents" to allow our editors clear, structured navigation through all the different global settings our frontend will require.

Before we change our structure to group our new documents, we need to create two new singletons. Review [this article's steps on creating a singleton](/docs/studio/create-a-link-to-a-single-edit-page-in-your-main-document-type-list) and create a "Colors" and "Main Navigation" document. These can have whatever schema makes sense for your site (or just a title, if you want to get to this article's main topics). These documents should have a type of `colors` and `navigation` and matching IDs.

## Adjusting the site settings child to show a custom list instead of the settings document

Now that we have more than one document governing our site's settings, it would make sense to group these into one pane instead of having three individual items in our first panel.

To do this, we'll change the `.child()` method on our "Settings" list item to reflect a new list instead of a document.

```javascript
// ./deskStructure.js

export const myStructure = (S) =>
  S.list()
    .title('Base')
    .items([
      S.listItem()
        .title('Settings')
        .child(
          S.list()
            // Sets a title for our new list
            .title('Settings Documents')
            // Add items to the array
            // Each will pull one of our new singletons
            .items([
              S.listItem()
                .title('Metadata')
                .child(S.document().schemaType('siteSettings').documentId('siteSettings')),
              S.listItem()
                .title('Site Colors')
                .child(S.document().schemaType('colors').documentId('colors')),
              S.listItem()
                .title('Main Navigation')
                .child(S.document().schemaType('navigation').documentId('navigation')),
            ])
        ),
      // We also need to remove the new singletons from the main list
      ...S.documentTypeListItems().filter(
        (listItem) => !['siteSettings', 'colors', 'navigation'].includes(listItem.getId())
      ),
    ])
```

Each singleton document is now a specific item under the "Settings Documents" list. They each also need to be removed from the main document type list, as well. To do that, add the singletons' IDs to the array used to filter the `S.documentTypeListItems()`.

## Next steps

Now that we have a manually grouped set of settings for our site, let's [add a set of dynamic groups](/docs/studio/dynamically-group-list-items-with-a-groq-filter) to filter documents by category or author.



# Dynamically group list items with a GROQ filter

![A screenshot illustrating a "Filtered Posts" list that allows an editor to filter by category or author](https://cdn.sanity.io/images/3do82whm/next/4663eac51572134b8723b4ebae92dc31e8ec853b-1854x1010.png)

It's often useful to group documents automatically by some field's value or a combination of field values. Common examples are grouping documents by author, publishing date periods, editorial status, category, or even the dominant background color in a document’s main image. In this article, we'll create lists of filtered blog posts to allow for quicker discovery and editing.

If you're unfamiliar with setting up the Structure Builder API, be sure to check out the previous articles in this series.

> [!NOTE]
> Learning the Structure Builder API
> This collection of articles will walk you through all the basics of using Structure Builder to create custom editing experiences.
> 
> Introduction to concepts
> 
> Set up structure builder in your project
> 
> Create a link to a single edit page in your main document type list 
> 
> Manually group items in your main document type list 
> 
> Dynamically group documents in a document list
> 
> Create custom document pane

## Setting up the schema

In this article, we'll need some basic schema for a blog. For the sake of simplicity, we'll use the default schema that comes from creating a new Sanity project from the Sanity CLI.

To get the schema, run `npx sanity init` and create a new project. When prompted, select `yes` to `Use default dataset configuration?` and `Blog` from the `Select project template` options.

This will give you a project structure that contains the schema for `post`, which contains references for `author` and `category` schema. Combining this with the singletons made in the previous articles, we should have a desk structure that looks like this:

![](https://cdn.sanity.io/images/3do82whm/next/e1c56e3e9f676cba374476185a66508d564688af-1277x361.png)

## Creating a manual group for two filters

To start creating our filters, we'll first create a manual group to house our two dynamic lists. For a review, read this article on [creating a manual group with Structure Builder](/docs/studio/manually-group-items-in-a-pane).

First, we'll create a new `listItem()` for our "Base" list. We'll give it the title "Filtered Posts" and a `.child()` node that will be a static list with the title "Filters."

This list will have two items, our filtered lists "Posts by Category" and "Posts by Author."

```javascript
// /deskStructure.js
// ./deskStructure

export const myStructure = (S) =>
  S.list()
    .title('Base')
    .items([
      S.listItem()
        .title('Filtered Posts')
        .child(
          S.list()
            .title('Filters')
            .items([
              S.listItem().title('Posts By Category').child(),
              S.listItem().title('Posts By Author').child(),
            ])
        ),
      // The rest of this document is from the original manual grouping in this series of articles
      ...S.documentTypeListItems().filter(
        (listItem) => !['siteSettings', 'navigation', 'colors'].includes(listItem.getId())
      ),
      S.listItem()
        .title('Settings')
        .child(
          S.list()
            .title('Settings Documents')
            .items([
              S.listItem()
                .title('Metadata')
                .child(S.document().schemaType('siteSettings').documentId('siteSettings')),
              S.listItem()
                .title('Site Colors')
                .child(S.document().schemaType('colors').documentId('colors')),
              S.listItem()
                .title('Main Navigation')
                .child(S.document().schemaType('navigation').documentId('navigation')),
            ])
        ),
    ])
```

This will create a list of two items. Neither of those items will have children yet. To populate them, we'll use dynamic lists using [GROQ queries](/docs/groq-reference).

## Creating dynamic children with an `S.documentList()` and a GROQ filter

To grab blog posts by category, we need to create a child for our `listItem` that will pull a `documentTypeList`. This list will show all categories in the dataset.

```javascript
// /deskStructure.js
S.listItem()
  .title('Posts By Category')
  .child(
    S.documentTypeList('category')
      .title('Posts by Category')
      .child(),
  )
```

This will create a list of items with a document type that matches the string `'category'`. From here, we need to fill in what this item's child will be. In our case, we want to create a list of all the documents that match the category clicked.

```javascript
// /deskStructure.js
S.listItem()
  .title('Posts By Category')
  .child(
    S.documentTypeList('category')
      .title('Posts by Category')
      .child(categoryId =>
        S.documentList()
          .title('Posts')
          .filter('_type == "post" && $categoryId in categories[]._ref')
          .params({ categoryId })
      )
  ),
```

The `.child()` method can accept an anonymous "arrow function", which will have the `_id` of the current item passed into it. From there, we need to define what type of child we're creating. 

### `S.documentList()`

The `.documentList()` method will pull a list of documents given a filter. It accepts most of the same chained methods as the `.list()` method but has a few special methods. 

### `.filter()`

The `.filter()` method is not the normal JavaScript filter method. In this case, it's a function that will accept a GROQ query as a string and return an array of documents that match that query. We can optionally chain a `.parameter()` method to pass a parameter into our query. In this case, the `categoryId` from our current function scope.

The GROQ query here will match all documents with a `_type` of `post`, containing the `$categoryId` as a reference in its `categories` array.

At this point, we have the post documents that match our query pulling into the next pane.

![](https://cdn.sanity.io/images/3do82whm/next/3a4cf4a9c2cf455f39f1c4ed14922b65d7a3dfd3-1551x492.png)

### Adding "Posts by author" child node

Now, let's do the same process to pull posts by author reference into the "Post by Author" node.

```javascript
// /deskStructure.js
S.listItem()
  .title('Posts By Author')
  .child(
    S.documentTypeList('author')
      .title('Posts by Author')
      .child(authorId =>
        S.documentList()
          .title('Posts')
          .filter('_type == "post" && $authorId == author._ref')
          .params({ authorId })
      )
  ),
```

![](https://cdn.sanity.io/images/3do82whm/next/d7b136d69db1d0f49a963a2fcddb84bcdc3a2ceb-1551x483.png)

## Renaming the "Post" document type list item

Now that we have a "Filtered Posts" group, let's rename our "Post" document type list. To do this, we'll create a new manual list item in the "Base" list group. For a more in-depth explanation, see [this article on creating singleton documents](/docs/studio/create-a-link-to-a-single-edit-page-in-your-main-document-type-list). In this example, we'll create a new `listItem()` for the post document type, give it a new title, and create a child panel with a document list filtering all posts. From there, we'll add the `'post'` ID to our exclusion filter for all other document types in this list.

```javascript
// /deskStructure.js
S.list()
    .title('Base')
    .items([
      S.listItem()
        .title('Filtered Posts')
        .child(/* Dynamic lists */ ),
      S.listItem()
        .title('All Posts')
        .child(
          /* Create a list of all posts */
          S.documentList()
            .title('All Posts')
            .filter('_type == "post"')
        ),
        /* List the other document types adding 'post' to the list to exclude */
      ...S.documentTypeListItems().filter(listItem => !['post', 'siteSettings', 'navigation', 'colors'].includes(listItem.getId())),
      /* Finish with our Settings item */
      S.listItem()
        .title('Settings')
        .child()
    ])
```

This is beginning to look finished. We can help increase an editor's understanding of the grouping by adding dividers between the various sections.

## Create visual sections in the base list with `.divider()`

![](https://cdn.sanity.io/images/3do82whm/next/9fbffa62ad72a2aed01541f7abed0c59610eab66-896x408.png)

To group things together, we'll use the `S.divider()` method in our `.items()` array. We want to group "All Posts" and "Filtered Posts" together, then allow the rest of our document types to flow in the middle, then our "Settings." To do this, we'll insert the divider method in the order we want it to appear.

```javascript
// /deskStructure.js
S.list()
    .title('Base')
    .items([
      S.listItem()
        .title('Filtered Posts')
        .child(/* Dynamic lists */ ),
      S.listItem()
        .title('All Posts')
        .child(
          /* Create a list of all posts */
          S.documentList()
            .title('All Posts')
            .filter('_type == "post"')
        ),
      S.divider(),
      ...S.documentTypeListItems().filter(listItem => !['post', 'siteSettings', 'navigation', 'colors'].includes(listItem.getId())),
      S.divider(),
      S.listItem()
        .title('Settings')
        .child()
    ])
```

## Final code

Putting together all the examples from this series of articles we get the following desk structure.

```javascript
// ./deskStructure.js

export const deskStructure = (S) =>
  S.list()
    .title('Base')
    .items([
      S.listItem()
        .title('Site Config')
        .child(
          S.list()
            // Sets a title for our new list
            .title('Settings Documents')
            // Add items to the array
            // Each will pull one of our new singletons
            .items([
              S.listItem()
                .title('Metadata')
                .child(S.document().schemaType('settings').documentId('siteSettings')),
              S.listItem()
                .title('Site Colors')
                .child(S.document().schemaType('colors').documentId('colors')),
              S.listItem()
                .title('Main Navigation')
                .child(S.document().schemaType('navigation').documentId('navigation')),
            ])
        ),
      S.divider(),
      S.listItem()
        .title('Filtered Posts')
        .child(
          S.list()
            .title('Filters')
            .items([
              S.listItem()
                .title('Posts By Category')
                .child(
                  S.documentTypeList('category')
                    .title('Posts by Category')
                    .child((categoryId) =>
                      S.documentList()
                        .title('Posts')
                        .filter('_type == "post" && $categoryId in categories[]._ref')
                        .params({categoryId})
                    )
                ),
              S.listItem()
                .title('Posts By Author')
                .child(
                  S.documentTypeList('author')
                    .title('Posts by Author')
                    .child((authorId) =>
                      S.documentList()
                        .title('Posts')
                        .filter('_type == "post" && $authorId == author._ref')
                        .params({authorId})
                    )
                ),
            ])
        ),
      S.listItem().title('All Posts').child(
        /* Create a list of all posts */
        S.documentList().title('All Posts').filter('_type == "post"')
      ),
      S.divider(),
      ...S.documentTypeListItems().filter(
        (listItem) => !['settings', 'post', 'colors', 'navigation'].includes(listItem.getId())
      ),
    ])
```

## Next steps

Now that we've created singletons, static lists, and dynamic lists, we need to look at [creating tabs and custom previews for our document views](/docs/studio/create-custom-document-views-with-structure-builder).







# Create custom document views with Structure Builder

![A screenshot illustrating a standard document pane and a custom document pane side by side](https://cdn.sanity.io/images/3do82whm/next/3256c9d6b38c4ade83e2524891c280eb5d2694f8-2482x1378.png)

The Structure Builder API gives you control over how a document node is presented within a collapsable pane. Specifically, it allows you to set up one or more views that either return the default form or a custom React component. Each view receives a collection of props that include the document's values in different states: `draft`, `published`, `historical`, and the currently `displayed` version (for when you have selected a previous revision to a document).

This article will use the Structure Builder API to display the JSON data for a specific document. If you're unfamiliar with setting up a custom structure, [read this article on setting up the basics](/docs/studio/set-up-structure-builder-to-override-the-default-list-view).

> [!NOTE]
> Learning the Structure Builder API
> This collection of articles will walk you through all the basics of using Structure Builder to create custom editing experiences.
> 
> Introduction to concepts
> 
> Set up structure builder in your project
> 
> Create a link to a single edit page in your main document type list 
> 
> Manually group items in your main document type list 
> 
> Dynamically group documents in a document list
> 
> Create custom document pane

## Setting up `deskStructure.js` to create a new default document node structure

If you've been following the earlier articles in this series, we've set our `deskStructure.js` file to export a named function that contains our new structure. Alongside this, we'll now export another named function, as well. 

In `deskStructure.js`, add the following code:

```javascript
// ./deskStructure.js

export const defaultDocumentNodeResolver = (S) =>
  S.document().views([
    S.view.form()
  ])
  
// ...rest of structure
```

Then, in `sanity.config.js`, import this function and add it to the `structureTool` configuration object under the key `defaultDocumentNode`.

```javascript
// ./sanity.config.js

import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'
import {deskStructure, defaultDocumentNodeResolver} from './src/structure'
import {schemaTypes} from './schemas'

export default defineConfig({
  name: 'default',
  projectId: '<projectId>',
  dataset: 'production',
  plugins: [
    structureTool({
      structure: deskStructure,
      defaultDocumentNode: defaultDocumentNodeResolver,
    }),
  ],
  schema: {
    types: schemaTypes,
  },
})

```

In our `defaultDocumentNodeResolver` function, we return an array of views for all the documents. To start us off, we're only returning the default form view. Let's look at the structure builder methods in more detail.

### `S.document()`

The `.document()` method creates the way the Structure tool displays documents. In this example, it changes how all documents are rendered.

### `.views()`

The `.views()` method accepts an array of view elements which can be created using either `S.view.form()` or `S.view.component()`. The view elements define the items that show up in the document’s tab list.

## Adding a second view to all documents

To add a second view, we'll add a second item to the array inside the `.views()` method. For this, we'll use the `.view.component()` method to use a custom component.

> [!TIP]
> Protip
> For simplicity, we're putting our new preview component definition in the deskStructure.js-file where we also keep all our structure builder code. In most real-world cases you probably want to separate it out into another file containing components to keep things tidy.

```javascript
// ./deskStructure.js

const JsonPreview = ({document}) => (
  <h1>JSON Data</h1>
)

export const defaultDocumentNodeResolver = (S) =>
  S.document().views([
     // Give all documents the JSON preview, 
    // as well as the default form view
    S.view.form(),
    S.view.component(JsonPreview).title('JSON')
  ])
  
// ...rest of structure
```

### `.view.component()`

The `.view.component` method takes a custom React component as an argument.  The component can be chained with other methods such as `.title()` to provide a title for the new view.

### Custom component: `JsonPreview()`

Our custom React component is called `JsonPreview`. Custom components have the following props:

- `document` – an object containing the various document states and their data
- `documentId` – the ID of the current document
- `schemaType` – the schema type of the current document 

In this example, we'll only need the `document` object, but to start, let's render an `h1` with the string `JSON Data`. We now have two tabs across the top of our documents. The tab titled "JSON" will render our `h1`.

![](https://cdn.sanity.io/images/3do82whm/next/69073d66cac0d519615e300f73689e6691aedda4-1707x443.png)

## Displaying dynamic data from the document

![](https://cdn.sanity.io/images/3do82whm/next/00496aeed472cf5f06050835871914d830c1dd3c-1706x650.png)

To pull data into our component, we'll need to select which version of the document we want to use. Luckily, the `document` prop contains the various states of the current document. For our uses, we want to show the JSON data for the currently selected version of the document, so we'll choose the `displayed` data.

```javascript
// ./deskStructure.js
const JsonPreview = ({document}) => (
  <> // A React fragment to have sibling elements
  
    // Pulling the currently displayed version's title
    <h1>JSON Data for "{document.displayed.title}"</h1>
    
    // Stringifying a JSON representation of the displayed data
    <pre>{JSON.stringify(document.displayed, null, 2)}</pre>
 
  </>
)

```

## Define views for specific schemas or documents

Sometimes you only want certain tabs to display for certain document types – or even individual documents. For this, the `getDefaultDocumentNode()` method comes with two options passed in: `schemaType` and `documentId`. We can use these with a JavaScript conditional to only build our JSON preview for certain documents.

```javascript
// ./deskStructure.js

const JsonPreview = ({document}) => (
  <>
    <h1>JSON Data for "{document.displayed.title}"</h1>
    <pre>{JSON.stringify(document.displayed, null, 2)}</pre>
  </>
)

export const getDefaultDocumentNode = (S, {documentId, schemaType}) => {
  // Render the JSON preview only on posts or the siteSettings document
  if (schemaType === "post" || documentId === "siteSettings") {
    return S.document().views([
      S.view.form(),
      S.view.component(JsonPreview).title('JSON')
    ])
  }
}

//...rest of structure
```

## Next steps

With all the data available to you in each of your documents, you can put together powerful previews, contextual images, or even custom editor flows for each document or document type.

From here, take a look at the [full reference documentation](/docs/studio/structure-builder-reference) for everything you can do with the Structure Builder API, and build something useful to you or your editors. 





# Cheat sheet

> [!TIP]
> Protip
> Structure Builder can do so much more than the examples on this page show.
> 
> Get a deeper understanding of Structure Builder by reading the introduction guide and API Reference documentation to configure initial value templates and more.

In order to use these code examples you will need to configure the `structureTool` plugin in your `sanity.config.ts` file like below:

```typescript
// ./sanity.config.ts

import {structure} from './structure'

export default defineConfig({
  // ...all other settings
  plugins: [
    structureTool({ structure }),
    // ...all other plugins
  ],
})
```

## All document schema types

Your imported `structure` configuration should have the following set up at a minimum: A list, with a title, and an array passed into `items()`.

The following examples you will paste into this root-level `items()` method.

```typescript
// ./structure/index.ts

import type {StructureResolver} from 'sanity/structure'

export const structure: StructureResolver = (S) =>
  S.list().title('Base').items(
    S.documentTypeListItems() // <= example code goes here
  )
```

![](https://cdn.sanity.io/images/3do82whm/next/09f38477a06bd86a283b0e5022311c91a259eb10-2144x1388.png)

### Filtered list of all document schema types

The `documentTypeListItems` method from above will render a list for every document schema type that is registered in the Studio config. Used together with some clever filtering, this method alone will take you a long way in setting up your document type list to your preference.

In the example below, the `siteSettings` document schema type is filtered out, but all other document types would be listed. Then we insert a divider, and finally the `siteSettings` schema document list is manually inserted.

```tsx
// ./structure/index.ts

import type {StructureResolver} from 'sanity/structure'

export const structure: StructureResolver = (S) =>
  S.list()
    .title('Base')
    .items([
      // list all document types except 'siteSettings'
      ...S.documentTypeListItems().filter(
        (item) => item.getId() !== 'siteSettings',
      ),
      S.divider(),
      // then add the 'sideSettings' type separate
      S.documentTypeListItem('siteSettings').title(
        'Site settings',
      ),
    ])

```

![](https://cdn.sanity.io/images/3do82whm/next/f6e1bca063115a32160132d00995829f5964618f-2144x1388.png)

## All documents of a specific type

`documentTypeListItem()` is a “batteries included” method for showing a list of documents of a given type. Works great for showing complete lists of documents with a custom title. A common usage would be wanting to pluralize the type name in the title.

```tsx
S.documentTypeListItem('lesson').title('Lessons')
```

This would be inserted into the `items` method like this:

```typescript
// ./structure/index.ts

import type {StructureResolver} from 'sanity/structure'

export const structure: StructureResolver = (S) =>
  S.list()
    .title('Base')
    .items([
	    S.documentTypeListItem('lesson').title('Lessons')
	  ])
```

![](https://cdn.sanity.io/images/3do82whm/next/5d62b5edd2b82bedccbfaadddc3d81dd862ddabc-2144x1388.png)

### A note on more complex examples

As these examples grow more complicated, you may wish to exact them into “helper functions” so they can be more easily be reused.

Also, going forth, assume the example code is to be inserted into the array passed into the root level `items()`, as we will exclude the boilerplate code for brevity.

```typescript
// ./structure/index.ts

import type {StructureResolver} from 'sanity/structure'

export const structure: StructureResolver = (S) =>
  S.list()
    .title('Base')
    .items([
      // ⬇ From now on, we will just show this bit 
	    S.documentTypeListItem('lesson').title('Lessons')
      // ⬆ Replace this with the example code
	  ])
```

## Filtered lists of documents

To show documents of a single type, with an additional GROQ filter applied, you will first need to create a `listItem` which has a `documentList` as its `child`. The document list must have an API version if it contains a filter.

These lists are “static” because the values being passed into the filter are known ahead of time.

```typescript
S.listItem()
  .title(`English lessons`)
  .child(
    S.documentList()
      .apiVersion('2024-06-01')
      .title(`English lessons`)
      .schemaType('lesson')
      .filter('_type == "lesson" && language == "en"'),
  )
```

![](https://cdn.sanity.io/images/3do82whm/next/6bbba0780b3bba193eb26b0aa1a0ddac6721c092-2144x1388.png)

You may choose to map over an array of items and use a params method to modify the results of each filtered list.

```tsx
const languages = [
  {id: 'en', title: 'English'},
  {id: 'es', title: 'Spanish'},
]

languages.map((language) =>
  S.listItem()
    .title(`${language.title} lessons`)
    .child(
      S.documentList()
        .apiVersion('2024-06-01')
        .title(`${language.title} lessons`)
        .schemaType('lesson')
        .filter('_type == "lesson" && language == $language')
        .params({language: language.id}),
    )
```

![](https://cdn.sanity.io/images/3do82whm/next/3aa9b2fc75c3aed0da19433cb5fc2b27f8b3a481-2144x1388.png)

### Dynamic filtered lists of documents

Say you have a document type `post` which has an array of references to the document type `category`.

In the example below are unfiltered document lists to show all documents of those types, and then a top level list of all category documents, but instead of rendering those documents as a child element, the ID of each document is used to create a filtered list of every post type document that has a reference to that category.

```tsx
S.documentTypeListItem('post').title('Posts'),
S.documentTypeListItem('category').title('Categories'),
S.listItem()
  .title('Posts By Category')
  .child(
    S.documentTypeList('category')
      .title('Posts by Category')
      .child((categoryId) =>
        S.documentList()
          .apiVersion('2024-06-01')
          .title('Posts')
          .filter('_type == "post" && $categoryId in categories[]._ref')
          .params({categoryId}),
      ),
  )
```

![](https://cdn.sanity.io/images/3do82whm/next/09c0d00695084f305aae8ed5794c408e27e4389e-2144x1388.png)

You may also want to check out this [guide on parent child relationships](https://www.sanity.io/guides/parent-child-taxonomy#91b8b58d75d4) for a more complex setup which includes initial value templates so that new documents created within these lists have filtered values preset.

## Grouped and nested document lists

Some document types may not need to be accessed as often and so to reduce visual noise may be better grouped together into a single menu item.

```typescript
S.listItem()
  .title('Website')
  .child(
    S.list()
      .title('Website')
      .items([
        S.documentTypeListItem('siteSettings').title('Site Settings'),
        S.documentTypeListItem('redirects').title('Redirects'),
        S.documentTypeListItem('labels').title('Labels'),
      ]),
  )
```

![](https://cdn.sanity.io/images/3do82whm/next/9808d945cab0ea57c37e2cb2751a1f536a628cff-2144x1388.png)

## Singleton documents

The structure builder is how you create “singleton” documents with a predetermined ID in Sanity Studio. To create an item with the correct icon, and the narrower height which list items have (compared to the taller height of a document item) the code example below wraps the `editor()` method in a list item of its own. It should inherit the correct icon of the document schema type, and when clicked create or edit a document with the provided ID.

```tsx
S.listItem()
  .id('siteSettings')
  .schemaType('siteSettings')
  .title('Site Settings')
  .child(
    S.editor()
      .id('siteSettings')
      .schemaType('siteSettings')
      .documentId('siteSettings')
  )
```

![](https://cdn.sanity.io/images/3do82whm/next/24e92657705a2c208473fde6dd9465f92178614a-2144x1388.png)

## Custom Structure by user role

The structure configuration contains a second parameter – context – which contains all sorts of valuable information about the current state of the Studio. Including the logged in user and their roles.

In this example, a different set of items is displayed to an Administrator than a user of any other roles.

```typescript
import type {StructureResolver} from 'sanity/structure'

export const structure: StructureResolver = (S, context) =>
  S.list()
    .title('Base')
    .items(
      context.currentUser?.roles.find((role) => role.name === 'administrator')
        ? [
            S.documentTypeListItem('post').title('Posts'),
            S.documentTypeListItem('category').title('Categories'),
            S.divider(),
            S.documentTypeListItem('siteSettings').title('Site Settings'),
          ]
        : [
            S.documentTypeListItem('post').title('Posts'),
            S.documentTypeListItem('category').title('Categories'),
          ],
    )
```

![](https://cdn.sanity.io/images/3do82whm/next/76e2617c1b20215ce421fb1ad092dd852f71fcb0-2144x1388.png)



# Reference

This is the complete reference documentation for Structure Builder. This API lets you configure how the Sanity Studio's Structure tool organizes lists, documents, views, menus, and [initial value templates](/docs/studio/initial-value-templates). 

The Structure Builder API is designed as a collection of methods that can be chained and passed in as arguments/parameters. To learn about the central concepts of Structure Builder, go to the [introduction](/docs/studio/structure-builder-introduction) article and dive deeper into the API in [the TypeScript reference documentation](/docs/reference/api/sanity/structure/structureTool).

> [!TIP]
> Protip
> Looking for quick examples of common use cases? See the Structure Builder cheat sheet.

### Configuring the structure

You can build custom structures by passing the structure to the structure tool configuration. To do this, define the structure resolver function, which receives the Structure Builder instance, by convention referred to as `S`, as its first argument, and a context object as its second.

```javascript
// sanity.config.js

import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'
import {schemaTypes} from './schema'

export default defineConfig({
  name: 'default',
  title: 'My Cool Project',
  projectId: 'my-project-id',
  dataset: 'production',
  plugins: [
    structureTool({
      structure: (S, context) => {
        console.log(context) // returns { currentUser, dataset, projectId, schema, getClient, documentStore }
        return S.documentTypeList('post')
      },
    })
  ],
  schema: schemaTypes
})



```

Where you define your structure is up to you - you could define it inline (as in the above example), or you could place it in a separate file and import/use it in your structure tool config:

```javascript
// src/structure.js

export const structure = (S) => S.documentTypeList('post')

// sanity.config.ts
import { defineConfig } from 'sanity'
import { structureTool } from 'sanity/structure'
import { structure } from './src/deskStructure'

export default defineConfig({
  // ...
  plugins: [
    structureTool({
      structure
    })
  ]
})
```

In the Structure Builder API, you'll find “convenience methods.” We recommend using these unless you want more fine-grained control.

![](https://cdn.sanity.io/images/3do82whm/next/b75ff5eb51228251b3a136ec1415ceba67c3ce82-2304x1400.png)

## Context properties

#### Properties

| Property | Description |
|----------|-------------|
| dataset | Name of the current dataset |
| projectId | Unique ID for the project |
| schema | The schema registry of your project. Use `schema.get("schemaTypeName") to retrieve any schema by name. |
| currentUser | An object with info about the currently logged in user. |
| getClient | Callback function that returns a configured client |
| documentStore |  |
| perspectiveStack | The stacked array of perspective ids ordered chronologically to represent the state of documents at the given point in time. Can be used as the perspective param in the client to get the correct view of the documents.  

Example values: ["published"] \| ["drafts"] \| ["releaseId2", "releaseId1", "drafts"] |
| i18n | Contains information about the current and avaliable locale configuration. |


## Lists

These methods define how lists and list items appear in the collapsible panes within the Studio’s structure tool. There are methods you can consider as “primitives” and methods that take a document schema type and automatically configure menus, initial value templates, and similar from the schema configuration.

> [!TIP]
> Protip
> You should generally opt for documentTypeList and documentList when you can since these give you good defaults and sets up a lot of things automatically for you.

list(): List

![](https://cdn.sanity.io/images/3do82whm/next/09b2f3697cb3d929f6a40341825f3acc241fd325-1152x700.png)

A primitive for defining the list content of the collapsible pane, including its title and representation in the URL bar. Typically used when you want to group different list items within a pane.

> [!TIP]
> Protip
> Are you getting the error Structure node id cannot contain character ...? This is typically caused when the title contains a character that is not in the domain of (or cannot be automatically converted to the domain of) web-safe characters.
> 
> The solution is to explicitly specify an id that includes only web-safe characters.

#### **Methods**

| Method | Description |
|--------|-------------|
| id(id): List | Set the id for the list. |
| title(title): List | Set the title for the list. |
| items(items): List | Set the list items to display.

You would typically use a method that returns an array of list items, such as documentTypeListItems, or list item methods inserted into their own array, such as listItem and documentListItem.
 |
| showIcons(showIcons): List | Set whether or not to show the icons of the list items.




 |
| initialValueTemplates(templateItems): List | Sets which initial value templates should be available for this pane (which items appear when using "new document" on the pane).

Use S.initialValueTemplateItem(templateId, parameters) to get a reference to a template. By passing an object of parameters, you can contextualize the template for the specific pane. See the initial value template documentation for more information. |
| menuItemsGroups(groups): List | Defines which groups of menu items should be available for the pane. This also defines the order of the groups.

You can either build these groups by using the builder method:
[S.menuItemGroup().id('some-id').title('Some title')]

or by passing an array of objects containing id and title properties:
[{id: 'some-id', title: 'Some title'}] |
| menuItems(menuItems): List | Sets the list of menu items to appear in the pane menu. |
| defaultLayout(layout): List | Sets the default layout for this list. Currently the only supported layouts are default and detail. |
| child(child): List | Sets which structure node to use as the child of this list, when an item in the list is selected.

Can either be a structure node (any list, document, component etc) or a child resolver - a function that either syncronously or asyncronously resolves to a structure node.

Read more about child resolvers in the conceptual guide for the structure builder. |
| canHandleIntent(intentChecker): List | Sets the method used to determined whether or not the pane can handle an intent of a certain type.

The intent checker receives three arguments: intentName, params and context. It should return a boolean indicating whether or not it can handle the intent.

intentName is generally create or edit.

params usually contains id and type, representing the document ID and schema type to be created. Often it will also have template, a string representing the ID of an initial value template.

context is an object containing pane and index |
| getCanHandleIntent(): function | Returns the configured intent checker |
| getChild(): node | function | Returns the configured child or child resolver for this pane |
| getDefaultLayout(): string | Returns the defined default layout for this pane (if any) |
| getId(): string | Returns the configured ID for this pane, if any |
| getInitialValueTemplates(): InitialValueTemplateItem[] | Returns the list of configured initial value templates, if any |
| getItems(): node[] | Returns the list of configured list items, if any |
| getMenuItemGroups(): MenuItemGroup[] | Returns an array of the configured menu item groups, if any |
| getMenuItems(): MenuItem[] | Returns an array of the configured menu items, if any |
| getShowIcons(): boolean | Returns whether or not the pane is configured to show icons for the list items |
| getTitle(): string | Returns the configured title for the pane, if any |


documentTypeList(schemaType): DocumentList

Convenience method for `documentList()`. Returns a list node for the specified document type with its configuration for list items, menus, initial value templates, and views. 

#### Arguments

#### Properties

| Property | Description |
|----------|-------------|
| schemaType * | The schema type name of an existing document type that's defined in the Studio’s schema file. |


#### Methods

[See the methods summary for documentList](#methods-38c3f64ec08f)

#### Example

```javascript
// src/structure.js (.ts)

// Exports a list of documents with the schema type “post”
export const structure = (S) => S.documentTypeList('post')


```

documentList(): DocumentList

A variant of the list type is made specifically for displaying a list of documents. It lets you define which documents to list by using a GROQ filter expression (`filter`) plus optional parameters (`params`).  Note that this list type does not have an `items()` method - if you want to use specific list items, use the [list()](#list-ab6eb182896e) method instead.

![](https://cdn.sanity.io/images/3do82whm/next/ff6751a8703f644f0f6df1a1ef0c458c98671e75-1152x700.png)

#### Methods

[Shares methods from list](#list-ab6eb182896e)

| Method | Description |
|--------|-------------|
| filter(filter): DocumentList | Filters the document list based on a GROQ filter expression. |
| params(params): DocumentList | Sets the parameters to use when executing the query specified by the provided GROQ-filter |
| apiVersion(apiVersion): DocumentList | Sets the API version to use for the given filter. Available since studio version v2.20.0. |
| schemaType(schemaType): DocumentList | Sets the schema type of the documents expected to be in the list. This helps provide context for the tooling, but is not strictly required. Set this if you are expecting a single document type to be returned. |
| defaultOrdering(orderings): DocumentList | Sets the default ordering for this document list:
documentList.defaultOrdering([{field: 'priority', direction: 'desc'}])  |
| getFilter(): string | Returns the configured GROQ-filter for this list, if any |
| getParams(): object | Returns the configured parameters for the GROQ-filter, if any |
| getApiVersion(): string | Returns the configured API version for this list, if any |
| getSchemaType(): string | Returns the configured schema type for this pane, if any |
| getDefaultOrdering(): SortItem[] | Returns the configured default ordering for this document list, if any |


#### Example

```javascript
// ./structure.js (.ts)

export const structure = (S) =>
  S.list()
    .title('Content')
    .items([
      S.listItem()
        .title('Future projects')
        .schemaType('sampleProject')
        .child(
          S.documentList()
            .title('Future projects')
            .filter('_type == "sampleProject" && publishedAt > now()')
        )
      ])
```

> [!WARNING]
> Gotcha
> Selecting a custom sort order in the Studio will override defaultOrdering and retain that custom sort order in local storage. If your defaultOrdering configuration doesn't appear to be working, try clearing your local storage or opening the Studio in a different browser.

#### Examples

```javascript
// src/structure.js

export const structure = (S) => S.documentTypeList('sampleProject')
  S.list()
    .title('Content')
    .items([
      S.listItem('category')
        .title('Projects by category')
        .child(
          S.documentList()
            .title('Projects by category')
            .schemaType('sampleProject') // Because we want menu items for “sampleProjects”
            .filter('_type == "category"')
            .child(id => // Returns the id for the selected category document
              S.documentList()
                .title('Projects by category')
                .schemaType('sampleProject')
                .filter('_type == "sampleProject" && $id in categories[]._ref')
                .params({id}) // use the id in the filter to return sampleProjects that has a reference to the category
            )
        )
      ])
```



#### Example

```javascript
export const structure = (S) =>
  S.list()
    .title('Content')
    .items([
      /* list item(s) goes here */
    ])
```

## List items

listItem(): ListItem

A primitive for defining a list item within a list in a collapsible pane. Usually used within the array of `S.list().items([/* here */])`.

By using the `child` method on the item, you can control what opens in the next pane when you interact with a `listItem`. If not defined, it will use the parents' child resolver to determine the next pane.

#### Methods

| Method | Description |
|--------|-------------|
| id(id): ListItem | Sets the ID for this list item.

Setting an ID is highly recommended, but if no ID is provided it will be inferred from the title of the list item. |
| title(title): ListItem | Set the title of the list item. |
| icon(icon): ListItem | Set an icon for the list item. |
| child(child): ListItem | Sets the child that should be rendered if this item is selected. If you have many list items that should resolve to the same type of child, you should usually set the child resolver on the parent (usually a list) and use the ID to resolve the correct child. |
| schemaType(schemaType): ListItem | Sets the schema type of this list item, if any. Only used if the list item represents a document. |
| showIcon(showIcon): ListItem | Decide whether or not to show the icon for the list item. |
| getId(): string | Get the id of the list item, if any |
| getTitle(): string | Gets the title of the list item, if defined |
| getChild(): node | function | Get the child or child resolver of the list item, if defined |
| getSchemaType(): string | Gets the schema type for the list item, if defined |
| getShowIcon(): boolean | Gets whether or not the icon is shown. |


documentListItem(): DocumentListItem

Convenience method for returning a list item representing a document.

Prefer this over a regular *listItem* when manually building a list of items representing documents.

#### Methods

[Shares the same methods as listItem](#methods-7566a867c0a3).

documentTypeListItem(schemaType): ListItem

Convenience method for returning a list item representing a document type. In other words, if you want a list item that opens a list of documents of a specific type when clicked, use this. It will automatically configure the title, icon, schema type, and similar. 

#### Arguments

#### Properties

| Property | Description |
|----------|-------------|
| schemaType | Name of the schema type. |


documentTypeListItems(): ListItem[]

Convenience method. Returns an array of list items for all defined document types in your schema, and configure them with the correct titles, icons, initial value templates and similar.

#### Example

```javascript
export const structure = (S) =>
  S.list()
    .title('Content')
    .items(
      // List all document types except "siteSettings"
      S.documentTypeListItems().filter(
        item => item.getId() !== 'siteSettings'
      )  
    )
```

## Dividers

divider(): void

Inserts a visual divider in a list.

#### Methods

| Method | Description |
|--------|-------------|
| title(title): Divider | Set the title of the divider. |
| i18n(i18n): Divider | Set the i18n key and namespace used to populate the localized title. |
| getTitle(): string | Gets the title of the divider, if defined |
| getI18n(): object | Gets the internationalized title of the divider, if defined |


#### Example

```javascript
//sanity.config.js

import {structure} from './structure'

export default defineConfig({
  ...
  plugins: [
    structureTool({
      structure
    }),
  ],
  schema: schemaTypes
})

// ./structure.js (.ts)

export const structure = (S) =>
  S.list()
    .title('Content')
    .items([
      // Make a singleton of the document with ID “siteSettings”
      S.documentListItem()
        .id('siteSettings')
        .schemaType('siteSettings'),
      // Add a visual divider
      S.divider()
        .title('Divider title')
        .i18n({title: {key: 'text-divider-title', ns: structureLocaleNamespace}}),
      // Add the rest of the document types, but filter out the siteSettings type defined above
      ...S.documentTypeListItems().filter(
        item => item.getId() !== 'siteSettings'
      )
    ])

```



## Document nodes

The document node type represents (as the title implies) a document. Often, it is referred to as "the editor node" since it's common for it to render a form allowing you to edit the document in question. However, it can also render other "views" of the document, which is why it has a more generic name.

The following methods let you configure what happens when you open a document. If no views are specified for a document node, it will return the default form view.

document(): Document

A primitive for defining a document node.

#### **Methods**

| Method | Description |
|--------|-------------|
| id(id): Document | Set the id for the document node. Usually picking something like document or documentView is enough to differentiate it. Alternatively, you can use the document ID. |
| title(title): Document | Set the title for the document. Leave blank to use the document title. |
| documentId(documentId): Document | Sets the document ID this document node represents |
| schemaType(schemaType): Document | Sets the schema type for this document |
| initialValueTemplate(templateId, parameters): Document | Sets which initial value template should be used for this document (if any).

 |
| views(views): Document | Defines which views should be rendered for this document. If not defined, it will use the default form view as the only view.

Currently, there are two view types: form and component.

The S.form method renders the form for a given document, allowing it to be edited.

A component renders a custom React component. See the document node views documentation for example usage. |
| child(child): Document | Sets which structure node to use as the child of this document, in the case where a view allows navigating to one.

Can either be a structure node (any list, document, component etc) or a child resolver - a function that either syncronously or asyncronously resolves to a structure node.

Read more about child resolvers in the conceptual guide for the structure builder. |
| getId(): string | Returns the configured ID for this pane, if any |
| getTitle(): string | Returns the configured title for the pane, if any |
| getDocumentId(): string | Returns the configured document ID for this document node |
| getSchemaType(): string | Returns the configured schema type for this editor, if any |
| getInitialValuesTemplate(): string | Returns the configured initial value template ID, if any |
| getInitialValueTemplateParameters(): object | Returns the configured parameters for the initial value template, if any |
| getViews(): View[] | Returns an array of the configured views, if any |
| getChild(): node | function | Returns the configured child or child resolver for this pane |


documentWithInitialValueTemplate(templateId, parameters): Document

Convenience method for building a document node with a specific initial value template and set of parameters. Automatically configures the document with the correct schema type.

Returns a document node and as such, has the same builder methods as [document()](#document-cbba11c7a572). 

## Default document node

Many of the nodes in the structure builder (such as the document type list)  automatically render a document node as a child if none is provided. While you can take full control of every node in the structure, this is sometimes a bit tedious if you usually return the same document node for every document or schema type.

The structure definition allows for defining a function which is called when the "default" should be resolved.  

> [!WARNING]
> Gotcha
> Note that this is not a function exposed on the Structure Builder API itself - rather, it is a function you declare and export for the Structure Builder to use as a fallback.

While the structure definition you provide is exported as the *default* export, you can also export a function named *getDefaultDocumentNode* that will be called in these situations:

getDefaultDocumentNode(options): Document

Export a function named `getDefaultDocumentNode` that returns an `S.document()` node to set the default configuration for document nodes (also often referred to as an "editor node" since it often contains the form view used to edit a document).

#### Arguments

#### Properties

| Property | Description |
|----------|-------------|
| options | Object of contextual information that can be used to determine which properties the document node should have. Properties:

schemaType - The value of a document’s _type.

documentId - The ID of the document. |


#### Example

```javascript
// sanity.config.ts (.js)

import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'
import {structure, defaultDocumentNode} from './structure'
import { schemaTypes } from './schema'

export default defineConfig({
  name: 'default',
  title: 'My Cool Project',
  projectId: 'my-project-id',
  dataset: 'production',
  plugins: [
    structureTool({
      structure,
      defaultDocumentNode,
    }),
  ],
  schema: schemaTypes
})

// ./structure.js (.ts)
import { WebPreview, JsonView } from './previews'

export const structure = (S, context) =>
  S.list()
    .title('Content')
    .items([
      S.listItem()
        .title('Settings')
        .child(
          S.document()
            .schemaType('siteSettings')
            .documentId('siteSettings')
        ),
      ...S.documentTypeListItems()
    ])

export const getDefaultDocumentNode = (S, {schemaType}) => {
 // Conditionally return a different configuration based on the schema type
 if (schemaType === "post") {
   return S.document().views([
     S.view.form(),
     S.view.component(WebPreview).title('Web')
   ])  
 }
 return S.document().views([
   S.view.form(),
   S.view.component(JsonView).title('JSON')
 ])
}

export default S.defaults()
```

## Document node views

Within a document pane, there can be one or more *views*. If there are more than one view node, they will appear as tabs. The views can be built using methods exposed on `S.view` - eg `S.view.form()`

![](https://cdn.sanity.io/images/3do82whm/next/4c2e2f1dfdd5795db958fb7ef2d9f728283b1da5-1152x700.png)

### View types

form(): FormView

The form method returns the default form-based editor for the specified document type. 

#### Methods

| Method | Description |
|--------|-------------|
| id(id): FormView | ID for the view, used for the URL |
| title(title): FormView | Sets the title of the view. Appears when there are more than a single view for a document node. |
| icon(icon): FormView | Sets an icon for the views tab |
| getId(): string | Returns the ID of the view, if any |
| getTitle(): string | Returns the title of the view, if any |
| getIcon(): function | Returns the defined icon for this view, if any |


#### Example

```javascript
S.view.form()
```

component(reactComponent): ComponentView

The `component` method takes a React component and renders it into the document view. It's most often used for showing the values of a document in alternative ways to the default document form.

#### Methods

| Method | Description |
|--------|-------------|
| id(id): ComponentView | ID for the view, used for the URL |
| title(title): ComponentView | Sets the title of the view. Appears when there are more than a single view for a document node. |
| canHandleIntent(intentChecker): ComponentView | Sets the method used to determined whether or not the pane can handle an intent of a certain type. This method is especially useful for routing to custom components from global search results or other links.

The intent checker receives three arguments: intentName, params and context. It should return a boolean indicating whether or not it can handle the intent.

intentName is generally create or edit.

params usually contains id and type, representing the document ID and schema type to be created. Often it will also have template, a string representing the ID of an initial value template.

context is an object containing pane and index |
| component(component): ComponentView | Sets the React component |
| icon(icon): Component | Sets an icon for the views tab |
| options(options): ComponentView | Sets additional user-defined options that will be passed to the React component as the options property. |
| getId(): string | Returns the ID of the view, if any |
| getTitle(): string | Returns the title of the view, if any |
| getComponent(): function | Component | Returns the defined React component to use for the view, if any |
| getIcon(): function | Returns the defined icon for this view, if any |
| getOptions(): object | Returns the user-defined options for this view, if any |


#### Example

```jsx

export const structure = (S) =>
  S.documentTypeList('sampleProject')
    .child(id =>
      S.document()
        .schemaType('sampleProject')
        .documentId(id)
        .views([
          // The default form for editing a document
          S.view.form(),
          
          // Render the current selected document’s values as JSON
          S.view.component(({document}) => (
            <pre>{JSON.stringify(document.displayed, null, 2)}</pre>
          )).title('View JSON')
        ])
    )

```

## Menus

![](https://cdn.sanity.io/images/3do82whm/next/87ef6d281e4279d8080657277b02c93592515c94-1152x700.png)

Each pane may have a menu in its upper right corner. This is where you usually find ordering options and affordances for adding new documents. Menus can also hold initial value templates. The convenience list methods will automatically set up default menus for you (some of these may also require that you add `schemaType()`).

menuItem(): MenuItem

#### Methods

| Method | Description |
|--------|-------------|
| title(title): MenuItem | Sets the title for the menu item. |
| icon(icon): MenuItem | Sets the icon for the menu item. |
| action(action): MenuItem | Sets the action that should be performed when the menu item is selected. Can either be a function or a string.

If specifying a function, that function will be called with any configured params as the first argument when the item is clicked.

If specifying a string, the menu will attempt to look for an object named actionHandlers on the React pane component holding the menu, and find a method with a matching name there. The function will be called with any configured params as the first argument when the item is clicked. |
| params(params): MenuItem | Sets an arbitrary object of parameters to pass to the action handler defined by action(), when clicked. |
| intent(intent): MenuItem | Sets an intent that should be performed when clicking this menu item.

An intent is an object with a type key with either the value create or edit, and an optional set of parameters specified as params.

For the edit intent, the type and id properties are required (document type name and document ID, respectively): {type: 'edit', params: {id: 'documentID', type: 'person'}}

For the create intent, specifying a type is required. If you want to use a specific Initial Value Template, you can specify a template parameter with the ID of the template you want to use: {type: 'create', params: {type: 'person', template: 'book-author'}}

If you want to pass arbitrary parameters to the initial value template, you can use the array form of specifying parameters, where the first element of the array defines the actual intent parameters, while the second element defines the arbitrary template parameters: {type: 'create', params: [{type: 'person', template: 'book-author'}, {some: 'template-param'}]}

Note that intent and action are mutually exclusive. |
| showAsAction(showAsAction): MenuItem | Sets whether or not the menu item should be displayed as an action instead of being part of the dropdown menu.

Actions appear next to the pane title and must be given an icon, since they have no room for a title unless hovered. |
| getTitle(): string | Returns the title of the menu item. |
| getIcon(): function | Returns the icon for the menu item.
 |
| getAction(): function | string | Returns the defined action for the menu item, if any |
| getParams(): object | Returns the parameters for the menu item. |
| getIntent(): Intent | Returns the intent for the menu item. |
| getShowAsAction(): boolean | Returns whether or not the menu item is set to show as an action or not. |


orderingMenuItem({title, by}): MenuItem

Convenience method for populating the menu for selecting different fields and directions for ordering a list of documents.

#### Arguments

#### Properties

| Property | Description |
|----------|-------------|
| title * | Title of the ordering to show in the menu |
| by * | An array of objects consisting of the field to sort by and the direction of the ordering (asc or desc). See example below. |


#### Example

```javascript
 S.documentList()
  .title("Products")
  .filter("_type == $type")
  .params({ type: "product" })
  .menuItems([
    ...S.documentTypeList("product").getMenuItems(),
    S.orderingMenuItem({title: 'Title ascending', by: [{field: 'title', direction: 'asc'}]}),
    S.orderingMenuItem({title: 'Title descending', by: [{field: 'title', direction: 'desc'}]})
  ])
```

orderingMenuItemsForType(typeName): MenuItem[]

Convenience method for returning an array of menu items used to order documents of a given schema type, based on the orderings defined in the schema definition.

#### Arguments

#### Properties

| Property | Description |
|----------|-------------|
| schemaType * | Schema type name to return orderings for |


menuItemsFromInitialValueTemplateItems(items): MenuItem[]

Convenience method for returning an array of menu items used to create new documents based on the given initial value template items.

Note: You should consider using the `initialValueTemplates()` method on the pane instead of this method unless you want the menu items to be nested inside the actual menu.

#### Arguments

#### Properties

| Property | Description |
|----------|-------------|
| items * | Initial value template items. Use S.initialValueTemplateItem() to generate these. |


#### Example

```javascript
 S.documentList()
  .title('Craft beer & cider')
  .filter('_type in $types && isCraft == true')
  .params({types: ['beer', 'cider']})
  .menuItems(
    S.menuItemsFromInitialValueTemplateItems([
      S.initialValueTemplateItem('beer', {isCraft: true}),
      S.initialValueTemplateItem('cider', {isCraft: true}),
    ])
  )
```





# Installing and configuring plugins

Plugins for Sanity Studio are installed just like any other dependency in your project, using your package manager of choice – such as [yarn](https://yarnpkg.com/) or [npm](https://npmjs.com). For the remainder of this article, we'll assume you're using npm.

After installation, a plugin must be imported from the package and added to the `plugins` array in the studio configuration, commonly found at the root of your project in a file named `sanity.config.js` or `.ts`.

### Example: Install @sanity/color-input

Let’s install [@sanity/color-input](/plugins/color-input). It adds `color` as a schema type and provides a handy color picker to select the color value.

> [!WARNING]
> Gotcha
> Whether you are installing a plugin or looking at the source code to learn how it works, make sure you are looking at the correct version! Many plugins have versions for both v2 and v3 of Sanity Studio. The two are not interchangeable!

Navigate to your project's root folder and run the following command in your terminal to install the plugin:

```sh
npm install @sanity/color-input
```

Then open your project configuration and import the `colorInput` function from the plugin package. Then add the `colorInput()` function call to the plugins array. Take care to include the parenthesis, as shown in the example. Then, finally, add a `color` field to any of your schemas.

```javascript
// sanity.config.js
import {defineConfig} from 'sanity'
import {colorInput} from '@sanity/color-input'

export default defineConfig({
	// here we add the @sanity/color-input plugin to the Studio
  plugins: [colorInput()],

  // example schema
  schema: {
    types: [
      {
        type: 'document',
        name: 'color-demo',
        title: 'Document with color field',
        fields: [
          {
           // The 'color' schema type was added by the plugin
            type: 'color',
            name: 'mySwatch',
            title: 'Swatch',
          },
        ],
      },
    ],
  },
})
```

You should be rewarded with a color picker widget in your studio.

![](https://cdn.sanity.io/images/3do82whm/next/99b77ec52cd2eac4377a09cc2b461219f42fb342-517x237.png)

## Configure

Some plugins accept a configuration object when created. Configuration is provided as an argument to the plugin function.

### Example: Configure

Let's add and configure the `@sanity/dashboard` plugin. It adds a new `tool` to the studio, which can be configured with various widgets. (You can even add your own!)

Remember to install the plugin first!

```javascript
// sanity.config.js
import {defineConfig} from 'sanity'
import {dashboardTool, projectUsersWidget, projectInfoWidget} from '@sanity/dashboard'

export default defineConfig({
  // ...

  // add and configure @sanity/dashboard
  // notice how we provide a configuration object with widgets
  plugins: [
		dashboardTool({ 
			widgets: [
			  projectInfoWidget(),
		    projectUsersWidget(),
			]
		}
	],
})
```

## Get organized

Plugins are just snippets of Studio configuration. As your studio configuration grows, it can be helpful to organize distinct features into plugins. This reduces clutter and avoids huge inline functions in `studio.config.js`. It also makes your config portable!

Plugins can be published to npm (we recommend using [@sanity/plugin-kit](https://github.com/sanity-io/plugin-kit)) or copy-pasted into other studios as you see fit.

For more on this topic, take a look at the [plugin development docs](/docs/studio/developing-plugins).



# Developing plugins

## Config to plugin

Most of the properties from the [defineConfig API](/docs/studio/configuration) can be expressed as a plugin with the `definePlugin` function.

Note that plugins can also have plugins! This can be a nice way to indicate inter-dependencies in plugins, and build features on top of each other in a portable way.

### Example

Given the following Studio configuration, let's move the code related to `productionUrl` into a plugin:

```javascript
// before: sanity.config.js
import {defineConfig} from 'sanity'

export default defineConfig({
  // ...
  
  plugins: [],

  document: {
    productionUrl: async (prev, { document }) => {
      // assume there is 20+ lines of code here
      const useCustomUrl = !!document?.slug?.current; 
      if(useCustomUrl) {
					return `https://some-custom-url.xyz/${document.slug.current}`
			}
			return prev
    }
	}
})
```

Start by extracting the relevant code into a new file and replace `defineConfig` with `definePlugin`.

```javascript
// after: productionUrlPlugin.js
import {definePlugin} from 'sanity'

export const productionUrlPlugin = definePlugin({
  name: 'custom-production-url'
 
  // code remains exactly the same, but is now contained by the plugin
  document: {
    productionUrl: async (prev, { document }) => {
      // assume there is 20+ lines of code here
      const useCustomUrl = true; 
      if(useCustomUrl) {
					return ‘https://some-custom-url.xyz’
			}
			return prev
    }
	}
})
```

Then import the plugin and add it to the plugins-array in `defineConfig`:

```javascript
// after: sanity.config.js
import {defineConfig} from 'sanity'
import {productionUrlPlugin} from './productionUrlPlugin'

export default defineConfig({
  // ...
   
  // now we have the productionUrl plugin neatly wrapped up in a plugin
  plugins: [productionUrlPlugin()],
})
```

## Studio plugin to package

Some plugins are so useful that you want to use them in multiple studios or share them with everyone. For that, you first need to create an npm package, then publish it to npm.

By organizing your code using `definePlugin` you are already halfway there! The other part of the equation is all about creating an npm package repository.

There are many ways to go about this, but we highly recommend [@sanity/plugin-kit](https://github.com/sanity-io/plugin-kit) as a way to get started.

### Let @sanity/plugin-kit do the work

> [!NOTE]
> Opinionated
> @sanity/plugin-kit is an opinionated way to create an npm package for Sanity. It aims to be a one-stop-shop for creating a Sanity plugin package. It will handle a lot of the tedium that goes into preparing a package, such as bundling for both commonjs and esm Javascript runtimes. You do not need to use it if you prefer other ways to work with npm package repositories.

**Initialize a new package**

To create a new plugin package, run the following command in a shell:

```sh
npx @sanity/plugin-kit init <plugin-name>
```

This will initialize a new plugin package in the current directory, and will prompt for various details that will go into `package.json`.

At this point, you have a fully functioning Sanity plugin package that can be tested in your studio.

Feel free to confer the manual pages for the init command with `npx @sanity/plugin-kit init --help` for available options.

**Test your plugin in a studio**

In your plugin package directory run:

```sh
npm run link-watch
```

This will set up your plugin to build whenever the code changes, and publish the package to a local [yalc](https://github.com/sanity-io/plugin-kit#q-why-use-yalc) repository.

> [!TIP]
> Protip
> yalc is a replacement for npm link that makes testing plugins locally easier.

In the command log, there should be a note that reads something like this:

```sh
# To test this package in another repository directory run:
npx yalc add <sanity-plugin> && npx yalc link <sanity-plugin> && npm install
```

Copy the command, paste it into your Studio directory shell and run it. This will install the plugin from the local yalc repository, which will be updated whenever the plugin code changes.

You can now import the plugin from your package in `sanity.config.js`, start the studio and it should appear there. It will look something like this:

```javascript
//sanity.config.js
import {defineConfig} from 'sanity'

// export name depends on what is exported from index.ts in the plugin
import {myPlugin} from '<sanity-plugin>'

export default defineConfig({
  // ...
   
  plugins: [myPlugin()],
})
```

> [!WARNING]
> Gotcha
> The default plugin implementation created by plugin-kit only logs “hello” in the web-console, so take a look there if you are not seeing any changes.

For more, please confer @sanity/plugin kit [testing guidelines](https://github.com/sanity-io/plugin-kit#testing-a-plugin-in-sanity-studio).

**Add your plugin code**

Now you can add your plugin code. Remember to add any dependencies used to `package.json`

`src/index.ts` is the entry point to the plugin and the place where `definePlugin` is configured.

Changes you make will be reloaded in the studio as long as you have the `link-watch` command running.

## Publish a package

After developing and testing a plugin, it is time to publish it to npm.

In a package using `@sanity/plugin-kit`, the `prepublishOnly` script will ensure that the package is validated and builds according to plugin-kit expectations. These checks are in place to prevent an array of common errors from slipping through when publishing.

### The manual way

If you are comfortable with publishing from your local development environment, in your package directory run:

```sh
npm run publish
```

This will build and publish your package to npm. It will ask for a one-time code if you have 2-factor authentication enabled on your npm account (you should).

When manually publishing like this, you are responsible for bumping the `version` field in `package.json` manually between releases. You also have to manually tag and create a release on Github if you want that sort of thing.

> [!WARNING]
> Gotcha
> Scoped packages (package name starts with @ ie: @orgOrNpmUser/package) are private by default. This implies:
> 
> Only users with access to the organization (or user) can download the package.
> 
> You have to be a paid npm user to be allowed to publish.
> 
> To make your package public (which will circumvent the above limitations), add the following to your package.json:
> 
> "publishConfig": { "access": "public" }

### The opinionated semantic-release way

> [!NOTE]
> Opinionated
> This section uses a @sanity/plugin-kit preset template to do most of the work. It puts certain guardrails on the development process to lower the chance of a faulty publish event. Feel free to make changes to the setup, look to it for inspiration or completely disregard it.

If you want to do automated releases using Github actions, `@sanity/plugin-kit` has this covered via the injectable `semver-workflow` preset.

Consider using this preset if you want the following:

- [husky](https://github.com/typicode/husky) for pre-commit hooks to ensure that:- all commits follow [conventional-commits](https://www.conventionalcommits.org/en/v1.0.0/#summary) format
- all files in a commit pass eslint


- [semantic-release](https://semantic-release.gitbook.io/semantic-release/) automation for npm publish
- [GitHub workflow](https://docs.github.com/en/actions/using-workflows) (Action) that does continuous integration and has publish-on-demand support

That said, all of these can be opted-out of by simply reverting the changes you don’t care for.

Before continuing, ensure that your package has no local changes, so it is easy to check what changes are applied to your code.

In your plugin directory run:

```sh
npx @sanity/plugin-kit inject --preset-only --preset semver-workflow && npm i
```

This command will configure the plugin package with files and dependencies that accommodate an automated plugin workflow on GitHub.

Keep in mind that this setup is tailored to the needs of the Ecosystem team at Sanity. Feel free to modify any and all files injected by the preset, or use it as a basis for creating your own workflow.

For more on this, refer to the [semver-workflow preset](https://github.com/sanity-io/plugin-kit/blob/main/docs/semver-workflow.md) docs.



# Publishing plugins

## Publish a package

After [developing and testing](/docs/studio/developing-plugins) a plugin, it is time to publish it to npm.

If you use `@sanity/plugin-kit` for your package, the `prepublishOnly` script will ensure that the package is validated and builds according to plugin-kit expectations. These checks are in place to prevent an array of common errors from slipping through when publishing.

### The manual way

If you are comfortable with publishing from your local development environment, in your package directory run:

```sh
npm run publish
```

This will build and publish your package to npm. It will ask for a one-time code if you have 2-factor authentication enabled on your npm account (you should).

When you manually publish like this, you are responsible for bumping the `version` field in `package.json` manually between releases. You also have to manually tag and create a release on GitHub if you want that sort of thing.

> [!WARNING]
> Gotcha
> Scoped packages (package name starts with @ ie: @orgOrNpmUser/package) are private by default. This implies:
> 
> Only users with access to the organization (or user) can download the package.
> 
> You have to be a paid npm user to be allowed to publish.
> 
> To make your package public (which will circumvent the above limitations), add the following to your package.json:
> 
> "publishConfig": { "access": "public" }

### The opinionated semantic-release way

> [!NOTE]
> Opinionated
> This section uses a @sanity/plugin-kit preset template to do most of the work. It puts certain guardrails on the development process to lower the chance of a faulty publish event. Feel free to make changes to the setup, look to it for inspiration or completely disregard it.

If you want to do automated releases using GitHub Actions, `@sanity/plugin-kit` has this covered via the injectable `semver-workflow` preset.

Consider using this preset if you want the following:

- [husky](https://github.com/typicode/husky) for pre-commit hooks to ensure that:- all commits follow [conventional-commits](https://www.conventionalcommits.org/en/v1.0.0/#summary) format
- all files in a commit pass eslint


- [semantic-release](https://semantic-release.gitbook.io/semantic-release/) automation for npm publish
- [GitHub workflow](https://docs.github.com/en/actions/using-workflows) (Action) that does continuous integration and has publish-on-demand support

That said, all of these can be opted-out of by simply reverting the changes you don’t care for.

Before continuing, ensure that your package has no local changes, so it is easy to check what changes are applied to your code.

In your plugin directory run:

```sh
npx @sanity/plugin-kit inject --preset-only --preset semver-workflow && npm i
```

This command will configure the plugin package with files and dependencies that accommodate an automated plugin workflow on GitHub.

Keep in mind that this setup is tailored to the needs of the Ecosystem team at Sanity. Feel free to modify any and all files injected by the preset, or use it as a basis for creating your own workflow.

For more on this, refer to the [semver-workflow preset](https://github.com/sanity-io/plugin-kit/blob/main/docs/semver-workflow.md) docs.



# Internationalizing plugins



> [!NOTE]
> Looking for Studio localization docs?
> This article is aimed at plugin authors who wish to add localization capabilities to their plugins. If your aim is rather to enable a new language in your studio UI, visit this article.

The `v3.23.0` release of Sanity Studio includes the tools maintainers need to internationalize their Studio code, as well as a range of tools for plugin developers to enable i18n in their plugins. This article will introduce you to the core concepts and tooling with examples to get you started.

Before proceeding, make sure you are running the latest version of Sanity Studio. Your Studio needs to be `v3.23.0` or later to work with the internationalization (i18n) APIs.

> [!WARNING]
> Gotcha
> Minimum Sanity peer dependency
> 
> Since there is no easy way to make these changes backward compatible, your plugin will now have to bind to a minimum version of Sanity where i18n is introduced. As this may be considered a breaking change, consider implementing semantic versioning if not already in use.

## Glossary

- **Locale** - “English (US)”, or “Norwegian (Bokmål)”. Has an ID (`en-US`, `nb-NO`), a title and an icon. In most cases it should also have one or more *resource bundles* defined (strings).
- **Locale namespace** - “studio”, “desk”, “vision” etc. This makes it simpler to use the translation in a plugin (no need to prefix all strings), and allows for dynamic loading of namespaces when needed.
- **Resource bundles** - represents the strings available for a locale/namespace combination. The “resources” (strings) can be defined statically or as an async function, allowing for dynamic imports. Studio only loads/merges resources for a namespace/locale when used. (not thrilled about the name, but i18next calls strings “resources”, and PoC used “bundle” terminology - suggestions welcome).

## Defining the resource bundle

Start by creating a “resource bundle”, and define a namespace for your plugin using the [defineLocaleResourceBundle](/docs/reference/api/sanity/defineLocaleResourceBundle) helper function. A resource bundle is an object specifying a namespace and locale for a localization, as well as pointing to where the files containing your localized strings – or resources – can be found. By convention the namespace should be the same as the name of your plugin, e.g., `@sanity/vision` or `@sanity/google-maps-input`. The locale should be specified following the [BCP-47 naming convention](https://en.wikipedia.org/wiki/IETF_language_tag) extended to include both language – e.g. `en` for English – and area – i.e. `US` for USA. See the list of [available locales](https://github.com/sanity-io/locales) for reference.

```tsx
import {defineLocaleResourceBundle} from 'sanity'

export const googleMapsInputResourceBundle = defineLocaleResourceBundle({
  locale: 'en-US',
  namespace: '@sanity/google-maps-input',
  resources: () => import('./resources'),
})
```

If you aim to add support for multiple locales, you should export an array of these bundles with different `locale` properties.

The `resources` key is a function that resolves to an object of resources, which are the files containing your translated strings - this allows for only loading the namespace and locale combination if the user has chosen the given locale and is currently using the plugin that depends on it.

### The resources file

The `resources` file should have a default export that exposes an object of key-value pairs. The keys should be identical for each supported locale, while the translated strings are kept in values. E.g.:

```tsx
export default {
  /** --- PUBLISH ACTION --- */
  /** Tooltip when action is disabled because the studio is not ready.*/
  'action.publish.disabled.not-ready': 'Operation not ready',

  /** Label for action when there are pending changes.*/
  'action.publish.draft.label': 'Publish',

  /** Label for the "Publish" document action while publish is being executed.*/
  'action.publish.running.label': 'Publishing…',

  /** Label for the "Publish" document action when there are no changes.*/
  'action.publish.published.label': 'Published',

  /** Label for the button to rev up the engine of your Tardis.*/
	'action.time-travel': 'Allons-y!'
}
```

### Naming conventions for resource keys

While you are technically free to name your keys however you like, we do have a few recommendations to keep everything consistent and easy to reason about:

- Keys should be in lowercase and kebab-case - no camelCase!
- Keys are namespaced, so you don’t need to worry about conflicts outside of your plugin.
- Put a comment on top of each key explaining what it does/what it means. Helps translating the key, both for humans and/or an AI.
- Use separate keys for aria labels, suffixed with `-aria-label`.

### Registering the resource bundle

In your `definePlugin` call, include an `i18n.bundles` property that points to your resource bundles:

```tsx
import {definePlugin} from 'sanity'
import {mySwedishBundle, myNorwegianBundle} from './i18n'

export const myPlugin = definePlugin(() => ({
  // ...rest of plugin config
  i18n: {
    bundles: [mySwedishBundle, myNorwegianBundle],
  },
}))
```

## Translating the user interface

The workhorse of the i18n toolkit is the `useTranslation` hook. Initialize it with your plugin namespace and use it to retrieve localized string values in your plugin user interface.

### Basic translation

```tsx
import {useTranslation} from 'sanity'
import {travelInTimeAndSpace} from 'tardis'

function MyComponent() {
  // Argument is the locale namespace - if omitted, it will use the `studio`
  // namespace, which is probably not what you want. Specifying a namespace
	// might become a requirement in the future. For now it's just good practice.
  const {t} = useTranslation('myPlugin')

  return <button onClick={travelInTimeAndSpace}>{t('action.time-travel')}</button>
}
```

### Interpolation

Interpolation is how you dynamically integrate variable data, like perhaps a username or file type, into translated strings. Surrounding a term with a double set of curly braces will mark it as a `{{placeholder}}` for interpolation. You’d then supply the term to be interpolated as a second argument to the `useTranslation` function.

```tsx
// resources.ts
export default {
  // ...
  'greetings.my-name-is': 'My name is {{userName}}'
}

// MyComponent.tsx
function MyComponent() {
  const {t} = useTranslation('myPlugin')
  return <div>{t('greetings.my-name-is', {userName: 'Kokos'})}</div>
}
```

This object can take any number of key:value-pairs, allowing for quite complex string-crafting.

> [!TIP]
> Protip
> Note that values passed for interpolation should generally never be language-specific terms. For instance, sending Image or File to a message that reads {{assetType}} not found will not work in other languages, i.e. Kunne ikke finne Image is not valid Norwegian. See the section below titled Context for more info.

### Pluralization/counts

That second argument object to the `useTranslation` hook is no one-trick-pony. In addition to accepting interpolation variables, passing a `count` parameter to the `t` function will allow it to be pluralized:

```tsx
// resources.ts
export default {
  'search.result-summary_zero': 'Nothing matched your query',
  'search.result-summary_one': 'Found one match for your query',
  'search.result-summary_other': 'Found {{count}} matches for your query',
}

// MyComponent.tsx
function MyComponent() {
  const {t} = useTranslation('myPlugin')
	// ⬇ returns 'Found 4 matches for your query
  return <div>{t('search.result-summary', {count: 4})}</div>
}

Note that the underscore works as a delimiter for matching terms. We’ll see the same pattern when working with variants in the next section. For more information, consult the [i18next documentation](https://www.i18next.com/translation-function/plurals).
```

### Context

Similarly to the count feature, the context feature allows you to create several variants of a translated term. Pass a string to the `context` parameter, and it will be matched against the underscore-delimited suffix of the keys:

```tsx
// resources.ts
export default {
  // ...
  'error.asset-not-found_image': 'Image not found',
  'error.asset-not-found_file': 'File not found',
  // Fallback in case context cannot be found:
  'error.asset-not-found': 'Asset not found',
}

// MyComponent.tsx
function MyComponent(props) {
  const {t} = useTranslation('myPlugin')
	// ⬇ returns either 'file' or 'image' depending on the assets type
  const assetType = props.document._type === 'sanity.imageAsset' ? 'image' : 'file'1
  return <div>{t('error.asset-not-found', {context: assetType})}</div>
}
```

### Using React components as part of strings

In certain cases, you may need to use a component as part of the string. For instance, formatting/highlighting a part to indicate that it is user input, or showing a localized timestamp with an aria-label and a computer-readable ISO-timestamp attached to it.

For this, you can use the `Translate` component. Note that it is heavier to execute and should therefore only be used when necessary.

```tsx
// resources.ts
export default {
  'event-listing.summary': '{{host}} is organizing <Emphasis>{{name}}</Emphasis> at <Location/>, <Time />'
}

// EventListing.tsx
import {useRelativeTime, Translate} from 'sanity'

function EventListing(event) {
  const {t} = useTranslation('myPlugin')
  const time = useRelativeTime(event.isoTime)
  return (
		<Translate
		  t={t}
	    i18nKey="event-listing.summary"
      values={{host: event.host, name: event.name}}
	    components={{
			Time: () => <time dateTime={event.isoTime}>{time}</time>,
        Location: () => <a href={event.location.url}>{event.location.name}</a>,
	      Emphasis: ({children}) => <em>{children}</em>
	    }}
	  />
  )
}
```

> [!WARNING]
> Gotcha
> The tags in components can only receive a single prop: children - all other props should be passed to the Translate-component when defining it. This helps minimize the parsing logic, as well as keeping it safer in terms of injections.

## I18n outside of React

The available and chosen languages are currently resolved as part of the *workspace matcher*, which means in some areas of the Studio user interface, i18n is not yet supported.

When you need to use i18n outside of React, such as in validation or structure definitions, we pass an `i18n` ”source” through context, which has a `t` function available for use. If you encounter areas that do not have access to this context where you would need it, please let us know so we can find a suitable workaround or find a way to pass it down.

```jsx
defineField({
  type: 'string',
  name: 'Model',
  validation: (Rule) =>
    Rule.custom((value, context) => {
      if (!value || !value.startsWith('Model ')) {
        return context.i18n.t('some-namespace:some-error-message', {
          modelValue: value,
        })
      }
      return true
    }),
})
```

### Localizing validation messages directly in your schema

It’s worth noting that you can also add localized strings directly in the schema if you know what languages should be supported at authoring time.

```jsx
import {defineType, defineField} from 'sanity'

export const gallery = defineType({
  name: 'gallery',
  type: 'document',
  fields: [
    defineField({
		name: 'photos',
		type: 'array',
		of: [{type: 'image'}],
		// The parameters here have to be documented
		validation: Rule => Rule.required().min(4).max(100).error({
			'en-US': 'Needs at least {{min}}, at most {{max}} items',
			'no-NB': 'Kan ikke ha flere enn {{max}}, og ikke færre enn {{min}}'
		})
    }),
  ]
})
```

## Hooks

We are working on exposing more hooks, and some of these might change names. Currently they are

- [useTranslation](/docs/reference/api/sanity/useTranslation) - described above in some detail
- [useCurrentLocale](/docs/reference/api/sanity/useCurrentLocale) - returns a `Locale`, which contains the locale `id` and `title`. Useful if you want to send this as part of a request to a server or something to localize the response. Also, for passing into the `Intl.X` APIs
- [useListFormat](/docs/reference/api/sanity/useListFormat) - provides cached access to `Intl.ListFormat` instances based on the passed options and the current locale.
- [useNumberFormat](/docs/reference/api/sanity/useNumberFormat) - provides cached access to `Intl.NumberFormat` instances based on the passed options and the current locale.
- [useFormattedDuration](/docs/reference/api/sanity/useFormattedDuration) - not strictly an i18n API, but is localized. Give it a duration in milliseconds and it will format it with localized units.

### Debugging

If you want to see which parts of the user interface are localized and which are not, you can run the Studio with `SANITY_STUDIO_DEBUG_I18N=triangles npm run dev`. In this mode all strings that have been localized will be framed by little ◤ “Triangles” ◢, and your console will have all sorts of useful information logged to help you locate missing or incorrect translations.



# Reference

[Introduction to plugins](/docs/archive/plugins)



The `plugin` configuration property accepts an array of plugin definitions. Plugin configuration accepts most of the same properties as the workspace config API, the notable exceptions being `dataset`, `projectId`,  `auth` and `theme`.

> [!TIP]
> Protip
> While entirely optional, wrapping your plugin configuration object with the definePlugin()-helper function (exported from 'sanity') will make most editors show helpful type information and autocomplete suggestions even if you're not using TypeScript!

```javascript
import { definePlugin } from 'sanity'

export const previewUrlPlugin = definePlugin({
  name: 'preview-url-plugin'
  document: {
    productionUrl: async (prev, { document }) => {
      const slug = document.slug?.current;
			return slug ? ‘https://some-custom-url.xyz/${slug}’ : prev
    }
	}
})
```

### Properties

#### Properties

| Property | Description |
|----------|-------------|
| name * | Unique identifier for the plugin |
| document | Accepts custom components for document actions and badges, as well as a custom productionUrl resolver and default configuration for new documents. Read more about the document API. |
| form | Extensions / customizations to the studio forms. Accepts configurations for image and file asset sources as well as custom components to override the default studio rendering. Read more about the form API. |
| plugins | Studio plugins - takes an array of plugin declarations that can be called with or without a configuration object. Read more about plugins. |
| tools | Studio tools – takes an array of tool declarations that can be called with or without a configuration object. Read more about the tool API. |
| schema | Schema definition - takes an array of types and an optional array of templates (initial value templates). While defining a schema is not required, there are few things inside the studio that works without one. Read more about the schema API. |
| studio | Accepts a components object which will let you override the default rendering of certain bits of the studio UI. Read more about studio components. |
| title | Human-readable name for the plugin |
| onUncaughtError | Accepts a callback function containing an error: Error and and errorInfo: ErrorInfo arguments. Commonly used by plugin developers to implement customized error handling, external logging, and telemetry. |




# Installation

Sanity AI Assist puts the power of Large Language Models (LLM) right at your fingertips in the studio, where your content lives. Write reusable instructions in natural human language to a document-aware AI assistant that can handle chores and repetitive tasks while you focus on the creative stuff.

> [!NOTE]
> Paid feature
> This article is about a feature currently available for all projects on the Growth plan and up.

[Create and run instructions with AI Assist](/docs/user-guides/ai-assist-working-with-instructions)

[Common instructions for AI Assist](/docs/user-guides/ai-assist-cheat-sheet)

[Content translation with AI Assist](/docs/studio/ai-assist-content-translation)

[AI Assist plugin page ](https://www.sanity.io/plugins/ai-assist)



If you're interested in running programatic AI instructions, check out [Agent Actions](/docs/agent-actions).

## Installing the AI Assist plugin

AI Assist is a [plugin](https://www.sanity.io/plugins/ai-assist) for Sanity Studio and is installed using your favorite package manager, such as `npm`, `yarn`, or `pnpm`. It’s a good idea to ensure your studio is up to date while you’re at it. AI Assist requires your studio to be v3.26.0 or later to work.

```bash
npm install sanity@latest @sanity/assist

```

### Add the plugin to your studio configuration

Once installed in your project, you must activate the plugin by importing it and adding it to your main studio configuration. In `sanity.config.ts`, add `assist` to the `plugins` array:

```tsx
import { defineConfig } from 'sanity'
import { assist } from '@sanity/assist'
/* other imports */

export default defineConfig({
  /* other config */
  plugins: [
    /* other plugins */
    assist(),
  ]
})

```

We’ll look at some configuration options for the `assist` plugin further on in this article, but for now, this is everything you need to get started.

#### Additional configuration settings

You can also configure AI Assist for more control.

```typescript
assist({
  //Showing defaults
  assist: {
    localeSettings: () => Intl.DateTimeFormat().resolvedOptions(),
    maxPathDepth: 4,
    temperature: 0.3
  },
})
```

- `localeSettings`: Enables the AI to understand natural language date and time, and know what timezone the language refers to. See the next section for more details.
- `maxPathDepth`: The max depth for document paths AI Assist can write to. Increase if you need deeper traversal, but large and complex schemas may result in decreased performance.
- `temperature` (from 0 to 1): Influences how much the output of an instruction will vary between runs. Higher values result in more varied results, while lower values are more repeatable.

#### Date and datetime

Starting from v3.0.0, AI Assist can write to date and datetime fields. Instructions can use language like "tomorrow at noon" or "next year", and when Assist writes to the field, it will be converted to a field-compatible value.

Language about time is `locale` and `timeZone` dependant. By default, instructions will use the locale and timezone provided by the browser (`Intl.DateTimeFormat().resolvedOptions()`).

Alternatively, you can configure the plugin per user with an `assist.localeSettings` function that should return `LocaleSettings`.

Example

```typescript
assist({
  assist: {
    localeSettings: ({currentUser, defaultSettings}) => {
      if (currentUser.roles.includes('admin')) {
        // forces locale and timeZone for admins
        return {
          locale: 'en-US',
          timeZone: 'America/New_York'
        }
      }
      // defaultSettings is the same as using:
      // const {locale, timeZone} = Intl.DateTimeFormat().resolvedOptions()
      return defaultSettings
    }
  }
})

```

For a list of allowed values for these parameters, see the following resources:

- `locale`: [Mozilla on Intl](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Intl#getcanonicalocales)
- `timeZone`: [Wiki on time zones](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones)

### Enabling the AI Assist API

After installing the AI Assist package and importing and adding the plugin to your studio configuration, you need to create a token to allow the plugin to access the AI Assist API. This needs to be done by a project member with token creation permissions (typically someone with an admin or developer role):

- Start the studio and open any document
- Click *the sparkle icon*(✨) in the document header near the close document X-button
- Then select **Manage instructions**



![](https://cdn.sanity.io/images/3do82whm/next/342f1cc71229f06723a35b5462d1b8bd57fbf1c7-2000x1352.png)

Selecting **Manage instructions** will open an inspector pane to the right side of the current document with a button prompting you to **Enable Sanity AI Assist**.

![](https://cdn.sanity.io/images/3do82whm/next/a8a01ad145fae14a70bd229333f0e03c644b7ec2-2000x861.png)

Click the **Enable Sanity AI Assist** button to create a token and enable AI Assist for everyone accessing the project.

You will find a new API token entry for your project named “Sanity AI” has been created in your project's API settings, which you can examine at [sanity.io/manage](https://sanity.io/manage).

AI Assist will now work for any dataset in your project.

> [!TIP]
> Protip
> You can revoke this token at any time to disable Sanity AI Assist service. A new token has to be generated via the plugin UI for it to work again.

At this point, AI Assist should be operational and ready for your perusal. You might want to take a detour and check out the article linked below for a closer look at where and how you can interact with the assistant in the Studio interface, or keep reading to learn more about how AI Assist works.

[Create and run instructions with AI Assist](/docs/user-guides/ai-assist-working-with-instructions)



## Schema configuration

By default, unless otherwise specified, AI Assist is enabled for all compatible fields and document types. We will look at how you can selectively exclude fields or document types from being affected by the assistant further on in the article.

## Supported field types

AI Assist can use most fields in your schema as a context in an instruction.

These are the field types it can write content to, including custom schema types based on the following:

- String and text
- Objects and the fields within them
- Arrays with inline objects and references
- Portable Text, including formatting and custom blocks
- Image assets (and image fields)
- References (requires additional configuration)
- Booleans
- Numbers
- Slugs
- URLs
- Date and DateTime

### Conditionally hidden and read-only fields

Fields and field sets that are conditionally visible or read-only can have instructions and can be written to by an instruction, as long as the field is non-hidden when the instruction is initiated.

> [!WARNING]
> Gotcha
> AI Assist does not re-evaluate the hidden and readOnly status of fields after the instruction has started running. This means that even if a field has its hidden  property changed from true to false as a side-effect of something the assistant does, it will still regard that field as hidden, even if the status was changed before the assistant "gets to it".

### Unsupported fields

There are some field types that AI Assist can use as context but not write content for:

- Geolocation
- Cross Dataset References
- File assets

### Image asset generation

AI Assist can create assets for images configured with a prompt field.

Image generation can be done directly using the **Generate image from prompt** command on the prompt field or indirectly whenever an AI Assist instruction modifies the image prompt field.

To enable image generation for an image field, you must:

- Set `options.aiAssist.imageInstructionField` to a child-path relative to the image
- Have a `string` or `text` field that corresponds to the `imageInstructionField` path

This will add a "Generate image from prompt" instruction to the image prompt field. Executing this instruction will generate an image.

```typescript
defineType({
  type: 'document',
  name: 'article',
  fields: [
    defineField({
      type: 'image',
      name: 'articleImage',
      fields: [
        defineField({
          type: 'text',
          name: 'promptForImage', 
          title: 'Image prompt',
          rows: 2,
        }),
      ],
      options: {
        aiAssist: {
          imageInstructionField: 'promptForImage',
        },
      },
    })
  ]
})
```

An image will be generated each time an AI Assist instruction modifies the image prompt field. This modification could come from a document instruction, an instruction for the image field or parent object, or directly on the image prompt field.

### Enabling automatic image captions

In addition to generating images from a prompt field, the assistant can also be set to generate descriptions from image assets. To enable the assistant to auto-generate descriptions that can be used for alt text or captions, supply a valid field path in `options.aiAssist.imageDescriptionField`.

```tsx
defineType({
  type: 'document',
  name: 'article',
  fields: [
    defineField({
      type: 'image',
      name: 'articleImage',
      fields: [
        defineField({
          type: 'text',
          name: 'alt',
          title: 'Alternative text',
          rows: 2,
        }),
      ],
      options: {
        aiAssist: {
          imageDescriptionField: 'alt',
        },
      },
    })
  ] 
})


```

This will add a **Generate Caption** instruction to the `caption` field that will produce a description of the image.

![](https://cdn.sanity.io/images/3do82whm/next/9a51f1a050f5202704f78a17ed43aad51575c46b-2000x1503.png)

### Enabling support for related content in references

To work with a `reference` field, the AI Assist plugin must consult an embedding index that includes the types it will refer to. To learn about embedding indexes and how to set them up, visit [this article](/docs/compute-and-ai/embeddings-index-api-overview).

You can manage your indexes directly in the studio using the [Embeddings Index Dashboard plugin](https://github.com/sanity-io/embeddings-index-ui#embeddings-index-api-dashboard-for-sanity-studio). Once you have an index configured, you can enable `reference` fields for AI Assist by setting `options.aiAssist.embeddingsIndex` to whatever you named your index.

```tsx
import { defineField } from 'sanity'

defineField({
  type: 'reference',
  name: 'articleReference',
  title: 'Article reference',
  to: [{ type: 'article' }],
  options: {
    aiAssist: {
      embeddingsIndex: 'all-our-stuff-index'
    },
  },
})

```

Reference fields with this option set can have instructions attached and will be included when running instructions for object fields and arrays. An example instruction might look like this:

```
Given <Document field: Title> suggest a related article

```

AI assist will use the embeddings index, filtered by the types specified in the field declaration, to look up contextually relevant references. One or more references can be added for arrays or portable text fields with references. 

## Selectively exclude fields and document types

AI Assist defaults to inclusivity and will target every supported field it comes across. This may not always be desirable, so it comes with the option to exclude fields and document types selectively by setting the `options.aiAssist.exclude` option to `true`.

### **Disable AI Assist for a schema type**

```tsx
// disable AI assistance wherever it is used,
// ie: as field, document, array types
defineType({
 name: 'policy',
  type: 'document',
  options: {
    aiAssist: {exclude: true}
 },
  fields: [
    // ...
  ]
})

```

### Disable for a nested field type

```tsx
// disables AI assistance for the specific array member
// if all types in the `of` array are excluded, the array type is also considered excluded
defineType({
 name: 'myArray',
  type: 'array',
  of: [
    defineArrayMember({
      type: 'someType',
      options: {
        aiAssist: {exclude: true}
     }
  })
  ]
})

```

### Disable for an array type

```tsx
// disables AI assistance for the specific array member
// if all types in the `of` array are excluded, the array type is also considered excluded
defineType({
 name: 'myArray',
  type: 'array',
  of: [
    defineArrayMember({
      type: 'someType',
      options: {
        aiAssist: {exclude: true}
     }
  })
  ]
})

```

## Support for conditionally hidden and read-only fields

It’s worth spending a moment examining how AI Assist deals with conditional fields. Any field that has `hidden` or `readOnly` set to `true` when the relevant instruction starts running will be skipped. An important word in that previous statement is “starts.” If a field has its `hidden` or `readOnly` value changed while the assistant is doing its thing, the new value will not be considered for that running process, even if the assistant has yet to reach that field.

Fieldsets with `hidden` and `readOnly` states are also accounted for.

## The AI Context document type

This plugin adds an `AI Context` document type.

If your Studio uses [Structure Builder](https://www.sanity.io/docs/structure-builder-introduction) to configure the studio structure, you might have to add this document type to your structure.

The document type name can be imported from the plugin:

```tsx
import {contextDocumentTypeName} from '@sanity/assist'

// put into structure in structure  
S.documentTypeListItem(contextDocumentTypeName)

```

## Troubleshooting

> [!NOTE]
> Caveats
> Large Language Models (LLMs) are a new technology. Constraints and limitations are still being explored, but some common caveats to the field that you may run into using AI Assist are:
> 
> Limits to instruction length: Long instructions on deep content structures may exhaust model context
> 
> Timeouts: To be able to write structured content, we're using the largest language models - long-running results may time out or intermittently fail
> 
> Limited capacity: The underlying LLM APIs used by AI Assist are resource constrained

There are limits to how much text the AI can process when processing an instruction. Under the hood, the AI Assist will add information about your schema, which adds to what's commonly called “the context window.”

If you have a very large schema (many document and field types), it can be necessary to exclude types to limit how much of the context window is used for the schema itself.

We recommend excluding all types that would rarely benefit from automated workflows. A quick win is typically to exclude array types. It can be a good idea to exclude most non-block types from Portable Text arrays. This will ensure that the Sanity Assist outputs mostly formatted text.

### Third-party sub-processors

This version of the feature uses OpenAI.com as a third-party sub-processor. Sanity's security team has vetted its security posture and approved it for use.



# Translation

AI Assist is a powerful tool for projects that need to serve content in multiple languages. To reflect the two most popular strategies for content localization in Sanity projects, two distinct APIs are ready to help you with document-level or field-level translation. Let’s look at each in turn.

> [!NOTE]
> Paid feature
> This article is about a feature currently available for all projects on the Growth plan and up.

[Install and configure AI Assist](/docs/studio/install-and-configure-sanity-ai-assist)

[Create and run instructions with AI Assist](/docs/user-guides/ai-assist-working-with-instructions)

[Common instructions for AI Assist](/docs/user-guides/ai-assist-cheat-sheet)

[Localization](/docs/studio/localization)



## Full document translation

This workflow assumes one document per language. It is designed to work especially well with the Sanity-maintained [Document internationalization](https://www.sanity.io/plugins/document-internationalization) plugin, but it will work without it if, for some reason, you aren’t willing or able to install said plugin.

### Configuring the AI Assist plugin for full document translation

To set up your project for full document translation with AI Assist, you need to pass a configuration object to the `assist()` plugin declaration in `sanity.config.js|ts`.

```tsx
plugins: [
  assist( {
    translate: {
      document: {
          // The name of the field that holds the current language
          // in the form of a language code e.g. 'en', 'fr', 'nb_NO'.
          // Required
          languageField: 'language',
          // Optional extra filter for document types.
          // If not set, translation is enabled for all documents
          // that has a field with the name defined above.
          documentTypes: ['article', 'blogpost'],	
      }
    }
  })
]
```

The `languageField` should correspond with the `name` of a field present in any document type that should be translation-enabled. AI Assist will use the value of this field to determine the language to which it should translate the relevant document.

```tsx
export default {
  name: 'article',
  title: 'Article',
  type: 'document',
  fields: [
    {
      name: 'title',
      title: 'Title',
      type: 'string',
    },
		{
      name: 'language',
      title: 'Language',
      type: 'string',
      options: {
        list: [
          {title: 'English', value: 'en_US'},
          {title: 'Norwegian Bokmål', value: 'nb_no'},
          {title: 'Esperanto', value: 'eo'},
          {title: 'Lojban', value: 'jbo'},
          {title: 'Toki Pona', value: 'tok'},
        ],
      },
    },
    {
      name: 'body',
      title: 'Body',
      type: 'array',
			of: [{type: 'block'}],
    },
  ],
}

```

Any document type that is enabled for translation will now have a translation instruction added to its document-level AI Assist menu. This instruction is not available to edit for the user, but in all other regards works and behaves the same as user-created instructions.

![Shows the top-level document contextual menu for AI Assist](https://cdn.sanity.io/images/3do82whm/next/3bb2bd5b1c6eeec67cb3b8bc71adf2df7bffbf37-617x446.png)

After first setting the desired language, selecting the **Translate document **instruction will put the assistant to work. Within a few moments, you should have a translated document. Note that the assistant will replace the current field values with the translated values, so unless your aim was to replace the original, you probably should make a copy before running the translation.

> [!WARNING]
> Gotcha
> As with all current Large Language Model (LLM)-based tools, AI Assist should never be relied on for critical content without a human being reviewing the results. It’s pretty good, but it does make mistakes!

![Shows multiple fields with a purple icon indicating the assistant is currently working on that field](https://cdn.sanity.io/images/3do82whm/next/86984b4d10f801dd710674ded6d5d76e94956ab1-1084x808.png)

While the assistant is working, you’ll see purple spinning AI presence icons indicating which fields are currently being translated. The assistant can work on several fields simultaneously, as shown in the screenshot above.

## Field level translations

Another popular strategy for multi-language content wrangling in Sanity is to keep all the different language variants in the same document, using objects with a set of fields of the same type to represent each translation.

```typescript
{
	
type: 'document',
name: 'article',
fields: [
	{
		type: 'object',
		name: 'localeTitle',
		fields: [
			{type: 'string', name: 'en', title: 'English'},
			{type: 'string', name: 'de', title: 'German'},
		]
		}
	]
}
```

This method is greatly facilitated by using the Sanity-maintained [Internationalized Array](https://www.sanity.io/plugins/internationalized-array)-plugin, and AI Assist affordances for field-level translation have been designed to work with the same setup and configuration this plugin presumes.

![Shows a field named "Subtitle" with two fields marked with NB and EN respectively, each containing a string in that respective language](https://cdn.sanity.io/images/3do82whm/next/1e8f550c25d4acddbaf48b5537f9bef1233b5b00-1338x418.png)

Setting up AI Assist to support field-level translation for this workflow is done in the `translate.field` configuration property. A minimal example for the schema in the previous example, might look something like this:

```typescript
assist({
  translate: {
    field: {
      documentTypes: ['article'],
      languages: [
        { id: 'en', title: 'English' },
        { id: 'de', title: 'German' },
      ],
    },
  },
});

```

`documentTypes` expects an array of document names for which the translation instruction should be activated. `languages` expects an array of language definitions, which should consist of an `id` in the form of a locale code and a human-readable `title` for rendering labels and such in the UI. An async callback function can also be used to return the same structure of data.

```typescript
assist({
  translate: {
    field: {
      languages: async () => {
        const response = await fetch('https://example.com/languages');
        return response.json();
      },
    },
  },
});

```

The async function contains a configured Sanity client as its first argument, allowing you to store language options as documents. Your query should return an array of objects with an `id` and `title`.

```typescript
assist({
  translate: {
    field: {
      languages: async (client) => {
        const response = await client.fetch(
          `*[_type == "language"]{ id, title }`
        );
        return response;
      },
    },
  },
});

```

Additionally, you can pick specific fields from a document to pass into the query. For example, if you have a concept of "markets" where only certain language fields are required in certain markets.

In this example, each language document has an array of strings named `markets` to declare where that language can be used. And the document being authored has a string field named `market`.

```typescript
assist({
  translate: {
    field: {
      selectLanguageParams: {
        market: 'EU',
      },
      languages: async (client, { market = `` }) => {
        const response = await client.fetch(
          `*[_type == "language" && $market in markets]{ id, title }`,
          { market }
        );
        return response;
      },
    },
  },
});

```

### Custom language fields

As mentioned the translation capabilities of AI Assist have been designed to work with the content paradigm recommended by the official Sanity-maintained plugins for working with multi-language content. I.e. the [Document internationalization](https://www.sanity.io/plugins/document-internationalization) and [Internationalized Array](https://www.sanity.io/plugins/internationalized-array) plugins. If following the conventions of these plugins is not feasible for your project, you have the option of tailoring the relationship and structure between language fields in your setup using the `translationOutputs` property.



By providing a function to `translate.field.translationOutputs` you can manually map the structure of your internationalized fields.

This function is invoked when an editor uses the **Translate fields** instruction, and determines the relationships between document paths: Given a document path and a language, it should return the sibling paths into which translations are output.

`translationOutputs` is invoked once per path in the document (limited to a depth of 6), with the following arguments:

- `documentMember` - the field or array item for a given path; contains the path and its schema type
- `enclosingType` - the schema type of the parent holding the member
- `translateFromLanguageId` - the languageId for the language the user wants to translate from
- `translateToLanguageIds` - all languageIds the user can translate to

The function should return an array that contains all the paths where translations from `documentMember` (in the language given by `translateFromLanguageId`) should be output.

The function should return `undefined` for all documentMembers that should not be directly translated, or are nested fields under a translated path.

### Default function

The default `translationOutputs` is available using `import {defaultTranslationOutputs} from '@sanity/assist`.

### Example

Given the following document:

```typescript
{
  titles: {
    _type: 'languageObject',		
    en: {
      _type: 'titleObject',
      title: 'Some title',
      subtitle: 'Some subtitle'
    },
    de: {
      _type: 'titleObject',
    }
  }
}
```

When translating from English to German, `translationOutputs` will be invoked multiple times.

The following parameters will be the same in every invocation:

- `translateFromLanguageId` will be `'en'`
- `translateToLanguageIds` will be `['de']`

`documentMember` and `enclosingType` will change between each invocation and take the following values:

49. `{path: 'titles', name: 'titles', schemaType: ObjectSchemaType}`, `ObjectSchemaType`
49. `{path: 'titles.en', name: 'en', schemaType: ObjectSchemaType}`, `ObjectSchemaType`
49. `{path: 'titles.en.title', name: 'title', schemaType: StringSchemaType}`, `ObjectSchemaType`
49. `{path: 'titles.en.subtitle', name: 'subtitle', schemaType: StringSchemaType}`, `ObjectSchemaType`
49. `{path: 'titles.de', name: 'de', schemaType: ObjectSchemaType}`, `ObjectSchemaType`

To indicate that you want everything under `title.en` to be translated into `title.de`, `translationOutputs` needs to return `[id: 'de', outputPath: 'titles.de']` when invoked with `documentMember.path: 'titles.en'`.

The function to enable this behavior might look like this:

```typescript
function translationOutputs(
  member,
  enclosingType,
  translateFromLanguageId,
  translateToLanguageIds
) {
  const parentIsLanguageWrapper =
    enclosingType.jsonType === 'object' &&
    enclosingType.name.startsWith('language');

  if (parentIsLanguageWrapper && translateFromLanguageId === member.name) {
    // [id: 'de', ]
    return translateToLanguageIds.map((translateToId) => ({
      id: translateToId,
      // in this example, member.path is 'titles.en'
      // so this changes titles.en -> titles.de
      outputPath: [...member.path.slice(0, -1), translateToId],
    }));
  }

  // ignore other members
  return undefined;
}

```

## Adding translation actions to fields

By default, **Translate document** and **Translate fields…** instructions are only added to the top-level document instruction menu.

These instructions can also be added to fields by setting `options.aiAssist.translateAction: true` for a field or type.

This allows editors to translate only parts of the document, and can be useful to enable for `internatinoalizedArrays` or `locale` wrapper object types.

For document types configured for full document translations, a **Translate** action will be added. Running it will translate the field to the language set in the language field

For document types configured for field translations, a **Translate fields...** action will be added. Running it will open a dialog with language selectors.

```typescript
defineField({
    name: 'subtitle',
    type: 'internationalizedArrayString',
    title: 'Subtitle',
    options: {
        aiAssist: {
            translateAction: true
        }
    },
})
```



# Custom field actions

In this guide, you will create a field action that uses [Agent Action Transform](/docs/agent-actions/transform-quickstart) to fix the spelling of a field.

_This is a paid feature, available on the Growth plan._

Prerequisites:

- AI Assist plugin (`@sanity/assist`) v4.3.0 or higher is required to enable custom field actions.
- A studio configured with the AI Assist plugin. See the following guide for details on installation and setup.
- Available usage to run an Agent Action Transform query.

[Install and configure Sanity AI Assist](/docs/studio/install-and-configure-sanity-ai-assist)



## Add your first field action

You configure field actions in the `assist` plugin configuration. Open the `sanity.config.ts` file in your studio and add the `fieldActions` key to your `assist` configuration.

```
import { defineConfig } from "sanity"
import { assist } from "@sanity/assist"

export default defineConfig({
// ... other settings
  plugins: [
    assist({
      fieldActions: {
        // <-- Field actions are configured here.
      }
    })
  ]
})
```

For this example, you'll need a few imports as well. Update your config to include the new imports. The dependencies should already be a part of your studio.

```
import { defineConfig } from "sanity"
import { assist, defineAssistFieldAction } from '@sanity/assist'
import { useMemo } from 'react'
import { useClient } from 'sanity'

export default defineConfig({
// ... other settings
  plugins: [
    assist({
      fieldActions: {
      }
    })
  ]
})
```

### Set up `useFieldActions`

Start by defining `useFieldActions`. It is called for the document itself and for all fields. It can call React hooks. Actions returned by the hook are added to the corresponding document or field menu. It is recommended to wrap the returned actions in `useMemo`.

```
import { defineConfig } from "sanity"
import { assist, defineAssistFieldAction } from '@sanity/assist'
import { useMemo } from 'react'
import { useClient } from 'sanity'

export default defineConfig({
// ... other settings
  plugins: [
    assist({
      fieldActions: {
        title: "My Actions", // Optional: sets the group title
        useFieldActions: (props) => {
          const {
            actionType
          } = props
          return useMemo(() => {
            if (actionType === 'field') {
              return [
                defineAssistFieldAction({
                  title: "Fix spelling",
                  onAction: async () => {
                    // ... Action logic here
                  }
                })
              ]
            }
            return []
          }, [actionType])
        }
      }
    }),
  ]
})
```

This checks if the action, stored as `actionType`, is a field or the document, then returns an array of actions to apply to the field's action menu.

The `defineAssistFieldAction` helper adds a single action. `onAction` *cannot* call hooks. If you need any state form a hook, it should be pre-assembled in `useFieldActions` before returning `useMemo`.

### Gather props and use the client to call Transform

This action uses Agent Action Transform, so you need a client instance, as well as some contextual details to pass in to Transform. 

18. Destructure additional props that you need for Transform. `useFieldActions` gives you access to details about the schema, the type of action, the path to a field (if available) and more.
18. Set up the client with `useClient`. You already imported `useClient` in a previous step.
18. Configure Transform to pass the existing contents of the field to the AI, then replace it with the response.

Here's the final code:

```
import { defineConfig } from "sanity"
import { assist, defineAssistFieldAction } from '@sanity/assist'
import { useMemo } from 'react'
import { useClient } from 'sanity'

export default defineConfig({
// ... other settings
  plugins: [
    assist({
      fieldActions: {
        title: "My Actions", // Optional: sets the group title
        useFieldActions: (props) => {
          const {
            actionType,
            schemaId,
            documentIdForAction,
            path,
            getConditionalPaths
          } = props
          const client = useClient({apiVersion: 'vX'})
          return useMemo(() => {
            if (actionType === 'field') {
              return [
                defineAssistFieldAction({
                  title: "Fix spelling",
                  onAction: async () => {
                    await client.agent.action.transform({
                      schemaId,
                      documentId: documentIdForAction,
                      instruction: "fix any spelling mistakes",
                      instructionParams: {
                        field: { type: 'field', path }
                      },
                      target: path.length ? {path} : undefined,
                      conditionalPaths: { paths: getConditionalPaths()}
                    })
                  }
                })
              ]
            }
            return []
          }, [actionType, schemaId, documentIdForAction, path, getConditionalPaths, client])
        }
      }
    }),
  ]
})
```

### Run the field action

Save and run your studio, and you should see the field action in the AI Assist menu for any supported fields.

![Animated gif of a user running the fix spelling field action](https://cdn.sanity.io/images/3do82whm/next/a168ebfd6080a42a5f5a9d6b00911a12a8efe194-800x281.gif)

## Contextually-aware example

The following example adds a "Fill field" action to all fields in the document by calling [Agent Action Generate](/docs/agent-actions/generate-quickstart).

The action will:

- Create the document as a draft if it does not exist, respecting initial values (`targetDocument`)
- Use existing document state to determine what should be put in the field (`instruction`, `instructionParams`).
- Pass the current readOnly and hidden state currently used by the document form to the Agent Action, so it respects it (`conditionalPaths`).
- Output to the field the action started from (`target.path`).

```
assist({
  fieldActions: {
    title: 'Custom actions',
    useFieldActions: (props) => {
      const {
        documentSchemaType,
        actionType,
        schemaId,
        getDocumentValue,
        getConditionalPaths,
        documentIdForAction,
        path,
        schemaType,
      } = props

      // hook usage has to happen outside onAction, so preassemble state in useFieldActions and pass to useMemo
      const client = useClient({apiVersion: 'vX'})

      return useMemo(() => {
        if (actionType === 'document') {
          // in this case we dont want a document action
          return []
        }

        return [
          defineAssistFieldAction({
            title: 'Fill field',
            icon: EditIcon,
            onAction: async () => {
              await client.agent.action.generate({
                schemaId,
                targetDocument: {
                  operation: 'createIfNotExists',
                  _id: documentIdForAction,
                  _type: documentSchemaType.name,
                  initialValues: getDocumentValue(),
                },
                instruction: `
                        We are generating a new value for a document field.
                        The document type is ${documentSchemaType.name}, and the document type title is ${documentSchemaType.title}
                        The document language is: "$lang" (use en-US if unspecified)
                        The document value is:
                        $doc
                        ---
                        We are in the following field:
                        JSON-path: ${pathToString(path)}
                        Title: ${schemaType.title}
                        Value: $field (consider it empty if undefined)
                        ---
                        Generate a new field value. The new value should be relevant to the document type and context.
                        Keep it interesting. Generate using the document language.
                     `,
                instructionParams: {
                  doc: {type: 'document'},
                  field: {type: 'field', path},
                  lang: {type: 'field', path: ['language']},
                },
                target: {
                  path,
                },
                conditionalPaths: {
                  paths: getConditionalPaths(),
                },
              })
            },
          }),
        ]
      }, [
        client,
        documentSchemaType,
        schemaId,
        getDocumentValue,
        getConditionalPaths,
        documentIdForAction,
        actionType,
        path,
        schemaType,
      ])
    },
  },
})
```

## Define helpers

The following are the available helpers for defining actions. You've seen the first one in use above.

### `defineAssistFieldAction`

Adds a single action that will appear in the document/field action menu.

`onAction` *cannot* call hooks. If state from hook is needed, it should be pre-assembled by `useFieldActions`

```
defineAssistFieldAction({
  title: 'Do something',
  icon: ActionIcon,
  onAction: async () => {
    //perform actions
  },
})
```

### `defineAssistFieldActionGroup`

Adds a group to hold one or more actions (or nested groups).

By default, any actions returned by `useFieldActions` will be grouped under `title`.

```
useFieldActions: (props) => {
  return [
    defineAssistFieldAction({/* ... */}), 
    defineAssistFieldActionGroup({
      title: 'More actions',
      children: [
        defineAssistFieldAction({/* ... */}),
      ],
    })
  ]
}
```

#### Only groups in `useFieldActions`

If `useFieldActions` *only* returns groups, the default wrapper group will be omitted. This allows full control over each group title.

### `defineFieldActionDivider`

Adds a divider between actions or groups. Takes no arguments:

```
useFieldActions: (props) => {
  return useMemo(() => [
    defineAssistFieldAction({/* ... */}),
    defineFieldActionDivider(),
    defineAssistFieldAction({/* ... */}),
  ], [])
}
```

## `useUserInput`

For certain actions, it is useful to have the user provide additional information or details that can be used as parameters for the action.

`useUserInput` returns a `getUserInput` function that can be called and awaited to return input from the user.

The `getUserInput` function takes input configuration and will display an input dialog to the user. When the user completes the dialog, the user-inputed text will be available (or undefined if the user closed the dialog).

```
assist({
  fieldActions: {
    title: 'Custom actions',
    useFieldActions: (props) => {
      const getUserInput = useUserInput()

      return useMemo(
        () => [
          defineAssistFieldAction({
            title: 'Do something with user input',
            onAction: async () => {
              const inputResult = await getUserInput({
                title: 'What do you want to do?', // dialog title
                inputs: [
                  {
                    id: 'topic',
                    title: 'Topic',
                  },
                  {
                    id: 'facts',
                    title: 'Facts',
                    description: 'Provide additional facts that will be used by the action',
                  },
                ],
              })
              if (!inputResult) {
                return // user closed the dialog
              }

              //use the result from each input
              //const [{result: topic}, {result: facts}] = inputResult
            },
          }),
        ],
        [getUserInput],
      )
    },
  },
})
```



# Comments

![Shows a comment about to be posted](https://cdn.sanity.io/images/3do82whm/next/83eeef7375c7da21cd2c3a162ed97e5de6d3c502-352x139.png)

Comments for Sanity Studio enables effective collaboration workflows right where the work is done. Leave comments on specific document fields or even single words in Portable Text, *@mention* your colleagues, and streamline your content workflow without ever leaving the studio.

_This is a paid feature, available on the Growth plan._

[Enable Comments for Sanity Studio](/docs/studio/configuring-comments)

[Enabling Tasks for Sanity Studio](/docs/studio/configuring-tasks)



## Comments workflow

Once Comments has been enabled for your project, open any document in your studio to start exploring how they work. If someone has already left comments on any field in the document you will notice a small speech bubble icon 💬 adorning the input showing how many comments have been posted. If no comments have yet been posted, hover any field to bring up the speech bubble to leave the first!

### Leaving comments

Hover over any comment-enabled field and click on the comment icon 💬 to open a popover dialog, then type your comment in the input field and hit **Send** to post it.

![](https://cdn.sanity.io/images/3do82whm/next/f75b3ef413eb78a529ec0b52e33638e96368272a-614x171.png)

To mention a colleague, type **@** followed by their name. A list of users with access to the document will appear. Click on the user you want to mention, and they will receive an email notification.

Your comment will now be visible to others with access to the document, and any mentioned users will receive a notification by email.

![Shows a string field with an icon indicating it has 1 comment attached](https://cdn.sanity.io/images/3do82whm/next/333aac237fe95c447fa1ea002f0ead1396c31096-501x158.png)

Unlike their closely related cousin [Tasks](/docs/studio/tasks), comments are always directly coupled with a specific piece of content in your studio. Comments can be attached to any compatible field, or even to distinct sentences or words within Portable Text!

![](https://cdn.sanity.io/images/3do82whm/next/3affb600efe308c532305c0b53730f477a24d2e5-536x204.png)

Clicking the 💬 comments icon on a field, will open the comment inbox for the document so you can easily browse through existing comments. Comments are neatly grouped into the fields they correspond to.

![](https://cdn.sanity.io/images/3do82whm/next/a69088d21908c24452a3d7297c77bd81593f9c74-354x608.png)

### Resolving comments

When a comment has been addressed or is no longer relevant, you can mark it as resolved. To do this, click on the **Resolve** option in the popover menu that appears when hovering. Resolved comments will be hidden from the main view but can still be accessed in the **Resolved Comments** list.

![Comment being resolved](https://cdn.sanity.io/images/3do82whm/next/8acabde302218caf03c6d7f865c28bdfaf8e9bef-315x311.png)

### Reactions, editing, and deleting comments

In addition to resolving comments, the popover menu includes a few more options. You can leave a reaction emoji for effective communication, copy a direct link to the comment, and you have options to edit or delete your comment. These options all work as you'd expect.

![Shows options for reacting to, editing, and deleting comments](https://cdn.sanity.io/images/3do82whm/next/d11b21191a257b198bf9616caf0e514dd109e9cb-649x175.png)



# Task

Tasks for Sanity Studio are perfect for collaborating on content with your team, or even for solo content creators who need to keep track of their outstanding to-do’s in the same environment where the work is to be done. Assign tasks to the appropriate team member, and they will get a notification alerting them to the new item in their inbox. Keep the discussion going in dedicated comment threads for every task, and tag in those who might be missing out with *@mention*’s.

_This is a paid feature, available on the Growth plan._

[Configuring Tasks](/docs/studio/configuring-tasks)

[Comments in Sanity Studio](/docs/studio/configuring-comments)



## Working with tasks

### Find your tasks inbox

Your tasks inbox is located in the top-right corner of your studio, next to your profile picture. Here, you’ll find any new tasks assigned to you, any in-progress tasks that you’ve subscribed to, and all open tasks for the currently active document, whether or not you’ve been tagged in yet.

![Shows the tasks inbox in Sanity Studio](https://cdn.sanity.io/images/3do82whm/next/a215b192a21a60726137274df4e2e44ffccc6390-360x448.png)

### Create a task

Click the link aptly labeled **+ New task** to create a new task. You can give your task a deadline, and assign it to the appropriate person who will then receive a notification email. You can also *@mention* studio users to notify them that their input is requested.

> [!TIP]
> Protip
> Memo to self? Assigning a task to yourself, or @mentioning yourself in a task will not trigger any notifications, so talking to yourself in the studio is perfectly fine, and won’t flood your inbox. 

![Shows an un-published task with a deadline requesting a review from a colleague on the target article](https://cdn.sanity.io/images/3do82whm/next/b2cb65d7be8edd2e277891c0564c82f6510abeb0-359x642.png)

You can also choose to attach your task to a target document or leave it empty if that’s more appropriate. Adding a target document facilitates discovery and contextualizing, and will also put a handy notice next to the publish button for the relevant document, listing unfinished tasks.

### Comment on tasks

Tasks can have comment threads attached so you can keep related discussions in one easy to find place. Just as with comments elsewhere you can *@mention* your team members to let them know about discussions they should be aware of.

![Shows a comment in a task thread tagging a tam member with a @mention](https://cdn.sanity.io/images/3do82whm/next/bd10f58b6cd0d3683a2d0815aee22df7a7438601-359x254.png)

### Resolve tasks

Once dealt with, a task can be satisfactorily checked off your to-do list. Resolved tasks are still available by accessing the list of **Done** tasks at the bottom of your inbox.

![Shows a popover allowing users to mark a task as done](https://cdn.sanity.io/images/3do82whm/next/bf65a2958195f26ab55fb6dada339e3febca1850-370x217.png)



# Copy and paste fields

The field copy and paste feature in Sanity Studio enables you to copy and paste field values or entire documents within your studio. This feature can be a significant time saver when you need to duplicate content or move it between different document types.

You can access these specialized copy-and-paste actions in the following ways:

- Through the **Field Actions** menu on individual fields.
- Using the standard **Ctrl/Cmd+C** and **Ctrl/Cmd+V** keyboard shortcuts on supported field types.

## Copy and paste fields

To copy and paste individual fields within a document:

6. Hover over a field to reveal the **Field Actions** menu. 

![](https://cdn.sanity.io/images/3do82whm/next/2885ad78b489aa44a290a88452e726116f2c5e28-492x235.png)

8. Select **Copy field** to copy the contents of that field.
8. Navigate to another field of the same type and select **Paste field** in the **Field Actions** menu to paste the copied content.

Additionally, certain field types support using the standard **Ctrl/Cmd+C** and **Ctrl/Cmd+V** keyboard shortcuts for copying and pasting:

- Array fields
- Object fields
- Reference fields
- Image and File fields 

Using keyboard shortcuts can be a quick way to duplicate content within these field types.

## Copy and paste documents

To copy and paste entire documents:

14. Open the **Document Actions** menu and select **Copy** to copy the current document to your clipboard.

![Shows the Document Actions menu with options for copying and pasting documents](https://cdn.sanity.io/images/3do82whm/next/37300fc59a415e495fe7baaafdcae1bddeeae6d9-491x330.png)

16. Navigate to the document list where you want to create a new document.
16. Create a new document.
16. Select **Paste** from the **Document Actions** menu or use the keyboard shortcut **Ctrl/Cmd+V**.

Another advantage of the copy/paste workflow over using the **Duplicate** action is that you can paste documents across different document types. Sanity Studio will try to map the fields from the source to the destination document.

## Examples

Here are some examples where copy-and-paste for fields can come in handy.

### Copying between array types

There might be cases where it’s more efficient to copy-start existing items from an array into a new one and edit them. For example, if you use array fields to build landing pages, newsletters, etc, and want to keep the same structure or have minor variations between them.

**Note that pasting into an array will replace all the items in it. **However, if you do this accidentally, you can use Review changes and restore to the content you want to keep.

### Copying between object types

Say you have an `object` field of type `bio` with the fields `name`, `image`, and `history`. If you copy that entire object and paste it into an object field of type `author` which has the fields `name` and `image`, Sanity Studio will transfer over the field values that are in common between the two types (`name` and `image`) and discard the field that doesn't exist in the destination (`history`).

### Copying between document types

Similarly, if you copy a whole document of type `author` and paste it into a document of type `person`, Sanity Studio will copy over any fields that the two document types have in common (e.g., `name` and `image`). Fields that do not exist in the destination type (e.g., `publicationsList` in `author`) will be discarded.

If there are no fields in common between the source and destination, you will see a warning listing the fields that were discarded. If there is nothing currently on your clipboard when you try to paste, you'll get a notification informing you there's nothing to paste.

## Limitations

There are a few known limitations to be aware of with the new copy-and-paste feature:

- References cannot be fully validated when pasting except to check that the reference type matches the target field. Complete validation would require making the paste operation asynchronous to fetch the referenced document, adding significant complexity. Full validation will happen after the reference value has been added.
- For images and files, validating the MIME type against the field's accept configuration is impossible without fetching the full asset document.
- When focused inside a text input, copy/paste is handled by the input's own clipboard event management to avoid interfering with the native editing experience, undo/redo functionality, etc.
- Pasting into an array field will overwrite the entire array rather than appending the pasted content. This behavior might be unexpected for users accustomed to appending when pasting in other contexts.



# Content Releases

Content Releases allow teams to organize and schedule updates across multiple documents. Teams can plan, preview, and validate significant changes in advance, ensuring seamless and conflict-free content deployment.

Content Releases provide several key benefits:

- **Coordinate Updates:** Simultaneously manage and publish updates across multiple pages and channels, ensuring consistency.
- **Reduce Manual Effort and Risk:** Automate scheduling to minimize manual tracking and prevent errors or conflicting changes.
- **Gain Confidence with Previews and Validation:** Preview and validate scheduled releases to guarantee readiness before going live

For developer documentation on how to configure, integrate, and interact with Content Releases programmatically, go here:

[Configure content releases](/docs/studio/content-releases-configuration)

[Content Releases API](/docs/apis-and-sdks/content-releases-api)



_This is a paid feature, available on the Growth plan._

## Before you begin

Content Releases requires that your studio and plugins be up to date. If you're experiencing issues using Content Releases, check with your administrator and direct them to the [Studio configuration](/docs/studio/content-releases-configuration).

## The Content Releases workflow

Content Releases introduces the concept of a **release**. Releases are a way to group multiple document changes together into a single unit that can be previewed, validated, scheduled, and published as one.

The most basic workflow is as follows:

12. Create a release.
12. Add documents to a release to create new document versions.
12. Make changes to documents.
12. Publish the release.

### The document view

![content releases document screen](https://cdn.sanity.io/images/3do82whm/next/9b0426bfd894bfeae9393ab1af169efde4bd21fb-2142x1820.jpg)

When you're working on a release, the document screen displays details about **versions** of the document. Document version names correspond to release names. Published and drafts are always enabled, but additional versions are displayed as documents are added to releases.

Select a version name to switch between versions.** **Right click the names to copy versions between releases, or discard a version.

> [!TIP]
> Protip
> The release color highlights the global toolbar and document list to remind you that you're working on a specific release.

### The releases view

![the content releases screen](https://cdn.sanity.io/images/3do82whm/next/30ead86756557f74b0ba1e8b06e46104a9a7d236-2130x1820.jpg)

The releases screen displays any upcoming releases. Bold dates in the calendar indicate releases with date estimates. You can also see the number of changes in each release, and warnings if there are validation errors.

### Release layering

Release layering is the concept of displaying documents based on where a release falls in the release timeline and which perspective is active.

This allows editors to preview document changes across multiple releases. You can see a simplified version of this in how *drafts* override published documents in Presentation. 

In Studio, release layering works on a timeline. The type and time of release indicates where a release falls on the timeline. You already know *published* and *draft*, but there are also *as soon as possible (ASAP)*, *timed*, and *undecided*.

The layer follows this order, starting at 1 and adding changes.

26. Published
26. Draft
26. As soon as possible (ASAP)
26. Timed (e.g., a set time in the future)
26. Undecided

When viewing a release with an undecided release time, you will see all changes from drafts, ASAP releases, and timed releases stacked atop published documents—plus any changes on the undecided release(s). These views of your content in Studio are the *global perspective.*

> [!NOTE]
> Documents override their earlier versions
> The global view and document list will show changes across releases based on the layering order, but multiple changes in the same document across releases will be overridden by the highest selected layer level.

### Global perspective

The global perspective is your view into the state of all documents at a point in time. By selecting a release, you're viewing not only its changes, but all changes in prior releases. You can hide individual releases form view, or view just the Published perspective.

> [!WARNING]
> Gotcha
> Does it seem like all documents are read-only? You might be in the Published perspective. Select Draft or a release from the document screen to make changes to a document.

### How do drafts fit in with releases?

You can work directly on a draft and publish it without needing to create a release. You can also work on a draft, then copy it to a release. 

One important thing to keep in mind. Publishing a release will not reset a draft. If you created a draft and made changes, then copied it to a release, that draft still exists. There are two ways you can keep these leftover drafts in check:

- If you know you're working on a release, start the draft in the release. This way a draft document is never created.
- After copying a draft to a release, return to the draft document and discard the draft version.

## Create a release

To add new documents and changes to a release, you first need to create a release.

![](https://cdn.sanity.io/images/3do82whm/next/9f0e6c6a20bea7f40490ae8e91c4a61a82c43c50-774x758.jpg)

39. Locate the **calendar** icon in the top right corner of the toolbar. 
39. Select the **down arrow** icon to reveal a list of releases.
39. Select **new release** to create a new release.4. Select an approximate time of release.
4. Enter a release title (optional).
4. Enter a description for the release (optional).



![](https://cdn.sanity.io/images/3do82whm/next/dc780f442bb5fa52f63adfeab058d7ecd08bdc82-1410x962.jpg)

You can change these values later by navigating to the release on the **Releases** screen.

> [!TIP]
> Protip
> You can also create a release from the Releases screen by selecting New release in the top right corner.

## Add a document to a release

There are two ways to add a document to an existing release.

### On the main releases screen

46. Navigate to the **releases** screen by selecting the **calendar** icon in the top right of Studio.
46. Select the **release name** to navigate to its detail screen. 
46. At the bottom of the list of documents, select **Add document**.
46. Search for and select a document.

### On the document screen

48. Ensure you are in a release perspective by pinning a release. You'll know you've pinned a release if the release name is next to the **calendar icon** in the toolbar.
48. In a document's editor view, select the **Add to release** button in the top bar. This button and bar should match the color scheme associated with the release perspective.

Once a document is part of a release, you'll be able to edit the release version by ensuring the release is selected at the top of the document.

## Remove a document from a release

Removing a document from a release discards any changes unique to that version. This action won't remove the document from other releases.

There are three ways to remove a document from a release.

### On the main releases screen

54. Select the **release name** to navigate to its details screen.
54. Identify the document you wish to remove and select the **"..." icon** to reveal additional options.
54. Select **Discard version** and confirm the selection when prompted.

### On the document screen

56. Confirm you are in the perspective for the desired release. You should see the release name next to the calendar icon in the toolbar, as well as the highlighted release name at the top of the document.
56. At the bottom right of the document screen, select the **"..." icon**. 
56. Select **Discard version** and confirm the selection when prompted.

or

58. On the document header, find the chip with the version you wish to discard.
58. Right click the chip, a context menu will open.
58. Select **Discard version** and confirm the selection when prompted.

## Copy a document from one release to another

You can copy a document version to a different release from the document view.

![Version action user interface](https://cdn.sanity.io/images/3do82whm/next/7f067c31c8845d0650b1a15320125eb13b5a6bd9-1608x820.jpg)

62. Navigate to the document you want to copy.
62. Right click the release name you want to copy from.
62. Hover over **"Copy version to"**.
62. In the popover menu, select the destination release.

## Unpublish a document as part of a release

Sometimes you want a release to unpublish, or remove a live document. This converts a published document back to a draft once the release is published. 

65. Add the document to a release.
65. In the bottom right corner of the document screen, select the **"..." icon**.
65. Select **Unpublish when releasing** and confirm the selection when prompted.

![Document screen popover menu](https://cdn.sanity.io/images/3do82whm/next/a4f0e9c5e58bcf2fa020b8996ce7e5b9d26f06de-846x400.jpg)

## Discard a draft version

To discard a document version, follow the steps listed in *Remove a document from a release*. 

To discard changes from the **Draft** version, select the **"..." icon **at the bottom right of the document screen and select **Discard changes**.

## Publish a release

After creating a release, you can choose to publish it on-demand or schedule a publish. 

72. Navigate to the **release screen** for the release you want to publish.
72. Select **Publish all documents.**

## Schedule a release

To schedule a release, first set a release time and date. You can do this when creating a release, or by selecting the **release time** label and selecting **At time** from the **release** screen. You can adjust this time later if needed.

![](https://cdn.sanity.io/images/3do82whm/next/75b7b10a61d63b82240f3937c2a8a65774719131-1816x986.jpg)

Next, select **Schedule for publishing** in the bottom left of the **release** screen.

![](https://cdn.sanity.io/images/3do82whm/next/aadb1c9274e77f04d984ada034df5c50e90fb9e3-2134x1232.jpg)

Confirm the release time and date, then select **Yes, schedule for publishing**.

> [!WARNING]
> Gotcha
> Setting a release time alone does not schedule the release. You must set a time, and schedule the release using the Schedule for publishing button.

## Unschedule a release

To unschedule a release, select the **Unschedule** button in the bottom right of the **release** screen.

## Archive a release

Archived releases are releases that were published, but you can also archive unpublished releases to preserve them for future reference.

> [!WARNING]
> Gotcha
> You cannot archive a scheduled release. First, unschedule it. Then follow the steps below to archive the release.

There are two ways to manually archive a release.

### On the main releases screen

87. Select the **"..." icon** for the release you want to archive.
87. Select **Archive release**.

### On the individual release screen

89. In the bottom right, next to the Publish / Schedule button, select the **"..." icon**. 
89. Select **Archive release**.

## Unarchive a release

You may unarchive an archived, unpublished release. Published releases cannot be unarchived.

There are two ways to manually unarchive a release.

### On the main releases screen

94. Select the **"..." icon** for the release you want to unarchive.
94. Select **Unarchive release**.

### On the individual release screen

96. In the bottom right select the **"..." icon**. 
96. Select **Unarchive release**.

## Change the release order

Release order is determined by when the release will be live, with exceptions for *ASAP* and *Undecided*. 

- ASAP releases come first, in order of creation.
- Dated releases come next, ordered by date.
- Undecided releases come last, ordered by creation.

To change the order of a release, change the date and time associated with it.

## Pin a release (global perspective)

Pinning a release sets the global perspective in Studio. This is indicated by the color change in the toolbar, as well as the highlighted release name throughout Studio. 

You can only pin one release at a time.

![](https://cdn.sanity.io/images/3do82whm/next/686434212666f7c5f8175ddb7cb64adedb95d523-2132x1330.jpg)

There are three ways to pin a release.

### Anywhere in Studio

107. In the top toolbar, select the dropdown arrow next to the **calendar icon**. If a release is currently pinned, the arrow will display next to the pinned release.
107. Select the **release name** for the release.

### On the main releases screen

109. Locate the release to pin.
109. Select the **pin** **icon **to the left of the release name.

### On the Release detail screen

111. Navigate to the release that you wish to pin.
111. Select the pin icon on the top left, above the release name

## View Release history

You can view past releases, including unpublished ones, from the **Archived** tab on the **main releases screen**. 

## Edit properties of an existing release

You can edit the name, estimated release time, or description directly on the **release** screen. 

To change the title or description, select the field and begin typing. 

To change the estimated publish time, select the **release time** label and choose a new time.

> [!WARNING]
> Gotcha
> You can edit the name and description of scheduled releases, but in order to change the schedule date or time you first need to unschedule the release.

## Hide releases from the global perspective view

When viewing a future release, you can choose to hide earlier releases from the global perspective view. This lets you hide document changes made by specific releases, while still previewing a subset of changes across releases.

![](https://cdn.sanity.io/images/3do82whm/next/2ff2a670d4b12d4cd311d5170b82e65273622949-1128x734.jpg)

122. To hide versions from a specific release, first set your global perspective.
122. In the release dropdown view, select the **open eye icon** next to any release you wish to hide.
122. To reveal a hidden release, select the **closed eye icon**.

## Preview releases in Presentation

If your team has enabled Presentation, you can preview a release by **pinning it** and then selecting the Presentation tool in Studio. 

Keep the release layering concept in mind, and use the *hide release* feature to customize your preview perspective.



# Compare document versions

Use the document comparison view to compare document versions. This includes drafts, published, and release versions. To get started, open a document in Sanity Studio that contains multiple document versions.

2. In the top left right corner of the document view, select the **"..."** icon.
2. Next, select "Compare versions".

![](https://cdn.sanity.io/images/3do82whm/next/cdb7012fcd612207338ade2426a87d897c623d57-1474x896.jpg)

The document comparison view appears over your studio window. It contains a version selection at the top, and two panels that correspond to each document version on the left and right.

> [!WARNING]
> Gotcha
> This view only works when multiple document versions exist. If you don't see the option, ensure that changes are made to a draft or release version in addition to the published version.

![](https://cdn.sanity.io/images/3do82whm/next/4f834f0e91d3b28a0861c0fad7303e5d3808e982-2962x1710.png)

Differences between the fields in the right version are highlighted in yellow. You may recognize this from other history or "diff component" tools. In the screenshot above, the *Overview* field has a yellow highlight along the right edge indicating changes.

You can adjust the compared versions with the version selector at the top of the window.

![](https://cdn.sanity.io/images/3do82whm/next/5c140b6852622c79f2e4a310db0f211b7729d910-1706x872.png)

> [!TIP]
> Protip
> You cannot leave comments or tasks from within the selector. It's best to save major changes and workflows for the document view.





# Studio schema configuration

The top level `schema` configuration accepts an object with two properties: `templates` and `types:`

- The `templates` property accepts an array of Initial Value Template configuration objects or a callback function returning the same.
- The `types` property accepts an array of schema definition objects or a callback function returning the same. 

In both cases, the callback function is called with the current value as the first argument and a context object as the second. Thus, you can access schema definitions and Initial Value Templates implemented by plugins.

#### Properties

| Property | Description |
|----------|-------------|
| templates | An array of initial value templates, or a callback function that resolves to the same. |
| types | An array of schema definitions or a callback function that resolves to the same. |


The `templates` property is discussed in greater detail [in this article](/docs/studio/initial-value-templates), and a reference article can be found [here](/docs/studio/initial-value-templates-api). The rest of this article will deal with the default set of schema types supported in the Sanity Studio.

All schema types are listed below or in the documentation menu.

[Array](/docs/studio/array-type)

[Block](/docs/studio/block-type)

[Boolean](/docs/studio/boolean-type)

[Cross Dataset References](/docs/studio/cross-dataset-references)

[Date](/docs/studio/date-type)

[Datetime](/docs/studio/datetime-type)

[Document](/docs/studio/document-type)

[File](/docs/studio/file-type)

[Geopoint](/docs/studio/geopoint-type)

[Image](/docs/studio/image-type)

[Number](/docs/studio/number-type)

[Object](/docs/studio/object-type)

[Reference](/docs/studio/reference-type)

[Slug](/docs/studio/slug-type)

[String](/docs/studio/string-type)

[Span](/docs/studio/span-type)

[Text](/docs/studio/text-type)

[URL](/docs/studio/url-type)

[Global Document Reference](/docs/studio/global-document-reference-type)



## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Name of any valid schema type. This will be the type of the value in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | Required. The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | Takes a static or a callback function that resolves to a boolean value (truthy or falsy) and hides the given field based on it. You can use this property for conditional fields. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a document type or a field as deprecated. This will render the field(s) as read-only with a visual deprecation message defined by the reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

## Validation

**Note**: The properties listed above are common for all data types. For a more thorough description of how to use them, have a look at the [Object Type](/docs/object-type).

The studio loads all schemas defined under `schema.types` in `studio.config.js`.

```javascript
//sanity.config.js
import {defineConfig} from 'sanity'

export default defineConfig({
  /* ... */
  schema: {
    types: [
      {
        title: "My Example Document Type",
        name: "exampleDocumentType",
        type: "document",
        fields: [
          {
            title: "Greeting",
            name: "greeting",
            type: "string"
          }
        ]
      }  
    ]
  }
})

```

To keep things organized, consider keeping the types array in a separate file and import it into `studio.config.js`. 

```javascript
//schemaTypes.js
export const schemaTypes = [
  {
    title: "My Example Document Type",
    name: "exampleDocumentType",
    type: "document",
    fields: [
      {
        title: "Greeting",
        name: "greeting",
        type: "string"
      }
    ]
  }  
]

//sanity.config.js
import {defineConfig} from 'sanity'
import {schemaTypes} from './schemaTypes'

export default defineConfig({
  /* ... */
  schema: {
    types: schemaTypes
  }
})

```

You should also consider using the `defineType`, `defineField` and `defineArrayMember` helper functions when working with schemas. These will give you better IDE auto-suggestions and provide type-safety when used in TypeScript files. Using these functions is *completely optional.*

```javascript
import {defineType, defineField, defineArrayMember} from 'sanity'

export const someDocumentType = defineType({
  title: "Some Document Type",
  name: "exampleDocumentType",
  type: "document",
  fields: [
    defineField({
      title: "String array",
      name: "strings",
      type: "array",
      of: [
        defineArrayMember({ type: "string" })  
      ]
    })
  ]
})  

```

### Plugins

Plugins may also provide types. They will be available in the studio exactly like studio configured types. 

Using plugins to organize your code can be helpful as the studio codebase grows.

```javascript
// pluginWithSchema.js
import {definePlugin, defineType, defineField} from 'sanity'

export const pluginWithSchema = definePlugin({
  name: 'plugin-with-schema',
  schema: {
    types: [
      defineType({
        title: "Plugin object",
        name: "exampleObject",
        type: "document",
        fields: [
          defineField({
            title: "Title",
            name: "title",
            type: "string"
          })
        ]
      })    
    ]
  }
})

//sanity.config.js
import {defineConfig} from 'sanity'
import {pluginWithSchema} from './pluginWithSchema'

export default defineConfig({
  /* ... */
  plugins: [pluginWithSchema()]
})

```



# Array

![Screenshot of an array from Sanity Studio](https://cdn.sanity.io/images/3do82whm/next/befae1ca226723422da18e04b241f25c7b0a466a-3456x2100.png)

An ordered list of data. The `of` property specifies which value types the array may hold.

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Value must be set to array. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | Required. The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| of * | Defines which types are allowed as members of the array. |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | If set to true, this field will be hidden in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| initialValue | The initial value that will be used when creating new arrays from this type. Can be either the literal array value or a resolver function that returns either the literal value or a promise resolving to the initial value. |


#### Properties

| Property | Description |
|----------|-------------|
| components | Lets you provide custom components to override the studio defaults in various contexts. The components available are field, input, item, preview. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

#### Properties

| Property | Description |
|----------|-------------|
| sortable | Controls whether the user is allowed to reorder the items in the array. Defaults to true. |


#### Properties

| Property | Description |
|----------|-------------|
| layout | If set to tags, renders the array as a single, tokenized input field. This option only works if the array contains strings.

If set to grid it will display in a grid. 

If the array uses the list option, it will display the values as a vertical list of checkboxes. Use grid layout to place the checkboxes horizontally. |


#### Properties

| Property | Description |
|----------|-------------|
| list | Renders checkboxes for a predefined list of values.

For arrays of primitives the following formats are supported:

[ {value: <value>, title: <title>}, { … } ]

[ <value1>, <value2>, … ]

For arrays of objects the format is

[ {_type: <mandatory-object-type>, _key: <optional-key>,  /* optionally any fields that exist in <object-type>*/}, { … } ]

Objects will be rendered using the object types preview config. |


#### Properties

| Property | Description |
|----------|-------------|
| modal | Controls how the modal (dialog for array content editing) is rendered. Takes an object with type and width property. 

type can be dialog or popover, width can be 'auto' or a number. 

Default is {type: 'dialog', width: 'auto'}. |


#### Properties

| Property | Description |
|----------|-------------|
| insertMenu | Allows configuring the insert menu for array items with the following properties:

filter: boolean \| 'auto'
Enable or disable filtering of types. Defaults to 'auto' which will enable filtering automatically if more than 5 types are present.

groups: array of group definitions { name: string, title: string, of: string[] }
Groups allowable types for easier access.

showIcons: boolean
Show or hide icons for types.

views: array of view options: {name: 'list'} \| {name: 'grid', previewImageUrl: function }

See examples further on in this article. |


#### Properties

| Property | Description |
|----------|-------------|
| disableActions | Accepts a list of actions that can be selectively disabled from the array inputs action menu. The avaialable options are:

add – Removes the ability to add new items to the array

addBefore – Removes the "Add item before"-menu item from the array item menu

addAfter – Removes the "Add item after"-menu item from the array item menu

remove – Removes the ability to remove items from the array

duplicate – Removes the ability to duplicate array items

copy – Removes the ability to copy items from the array

options: { disableActions: ['add', 'addAfter'] } |


## Validation

#### Properties

| Property | Description |
|----------|-------------|
| required() | Ensures that this field exists. |


#### Properties

| Property | Description |
|----------|-------------|
| unique() | Requires all values within the array to be unique. Does a deep comparison, only excluding the _key property when comparing objects. |


#### Properties

| Property | Description |
|----------|-------------|
| min(minLength) | Minimum number of elements in array. |


#### Properties

| Property | Description |
|----------|-------------|
| max(maxLength) | Maximum number of elements in array. |


#### Properties

| Property | Description |
|----------|-------------|
| length(exactLength) | Exact number of array elements to allow. |


#### Properties

| Property | Description |
|----------|-------------|
| custom(fn) | Creates a custom validation rule. |


## Options Example

### Example: Customizing the insert menu

The `insertMenu` option allows you to configure several aspects of the array input's insert menu. It accepts the following properties:

#### `showIcons`

Set to `false` to hide the icons for schema types.

```javascript
{
  insertMenu: {
    showIcons: false,
  }
}
```

![Shows array insert menu with and without icons for schema types](https://cdn.sanity.io/images/3do82whm/next/d13f247073ff9d14645f9d85af86e42b6a2afd39-646x545.png)

#### `filter`

Enable or disable filtering of available schema types. Can be set to `true`, `false` or `'auto'` (default). When set to `'auto'` filtering will kick in once the list of allowable types has five or more options.

```javascript
{
  insertMenu: {
    filter: true,
  }
}
```

![Shows array insert menu with a search field to filter types](https://cdn.sanity.io/images/3do82whm/next/292e34dc0ef32251b725bd14e037cade22a6e996-701x599.png)

#### `groups`

Define groups of related schema types for improved findability.

```javascript
{
  insertMenu: {
    groups: [
      {
        name: 'intro',
        title: 'Intro',
        of: ['hero'],
      },
      {
        name: 'storytelling',
        title: 'Storytelling',
      },
      {
        name: 'upsell',
        title: 'Upsell',
        of: ['testimonials', 'hero'],
      },
    ],
  },
}
```

![Shows insert menu with groups of schema types](https://cdn.sanity.io/images/3do82whm/next/28d0139dbbcc9f7feacfc6872bd5e6dcd12bf375-686x613.png)

#### `views`

Allows for selecting between the classic select menu `{ name: 'list' }` or an expanded grid view `{ name: 'grid' }` with optional preview images for each type. If both are included, the first item in the array will be shown by default, and a button to toggle between views will be included.

![](https://cdn.sanity.io/images/3do82whm/next/2cee344492badd128f2148d37087edd10a76b762-731x633.png)

```javascript
{
  insertMenu: {
    groups: [
      {
        name: 'intro',
        title: 'Intro',
        of: ['hero'],
      },
      {
        name: 'storytelling',
        title: 'Storytelling',
      },
      {
        name: 'upsell',
        title: 'Upsell',
        of: ['testimonials', 'hero'],
      },
    ],
    views: [
      {name: 'list'},
      {name: 'grid', previewImageUrl: (schemaTypeName) => `/static/preview-${schemaTypeName}.png`},
    ],
  },
}
```

If `previewImageUrl'` is not defined, the icon associated with the respective schema types will be shown instead.

![](https://cdn.sanity.io/images/3do82whm/next/f7d249f4e6857cf03166bc19eba6e816a1ce6b9c-680x723.png)

#### `disableActions`

Allows for selectively disabling and removing actions from the actions menu.
The following actions can be disabled. 

- `add` – Removes the ability to add new items to the array 
- `addBefore` – Removes the "Add item before"-menu item from the array item menu 
- `addAfter` – Removes the "Add item after"-menu item from the array item menu 
- `remove` – Removes the ability to remove items from the array 
- `duplicate` – Removes the ability to duplicate array items 
- `copy` – Removes the ability to copy items from the array

```typescript
{
      name: 'someArrayField',
      options: {
        disableActions: ['add', 'duplicate'],
      },
      title: "Array you can't add elements to",
      type: 'array',
      of: [
        {
          type: 'object',
          name: 'something',
          title: 'Something',
          fields: [{name: 'first', type: 'string', title: 'First string'}],
        },
      ],
    }
```

A few things to note:

- These changes are only about UI affordances, and doesn't imply any form of write protection for the actual data. Items can still be added or removed to an array by sending mutations to the API. 
- This affordance only applies to arrays of objects and arrays of primitive values. Not to portable text arrays. 
- Disabling add will also implicitly disable addBefore and addAfter, thus disable inserting new items to the array entirely (although items can still be inserted via duplicate).



## Validation Examples

### Example: Array of strings

Vanilla, reorderable array of strings:

Input

```javascript
{
  title: 'Names',
  name: 'names',
  type: 'array',
  of: [{type: 'string'}]
}
```

API response

```json
{
  "names": ["Wilma", "Håvard"]
}
```

Tokenized field (tags) is data-wise an ordinary array

Input

```javascript
// Presented as a tokenizing tag-field
{
  title: 'Tags',
  name: 'tags',
  type: 'array',
  of: [{type: 'string'}],
  options: {
    layout: 'tags'
  }
}
```

API response

```json
{
  "tags": ["clever", "unexpected"]
}
```

### Example: Array containing both crew members and cast members

Both `crewMember` and `castMember` are custom types defined in our schema. In this example we want an array of `employees` to be able to contain both crew members or cast members. The resulting array will end up containing inline instances of the actual objects. Note that in a real world example, you may want this to be references to existing cast members or crew members.

Input

```javascript
{
  title: 'Employees',
  name: 'employees',
  type: 'array',
  of: [{type: 'crewMember'}, {type: 'castMember'}]
}
```

API response

```json
[
  {
      "_key": "5ead6b7c7dcc55ae66d504e2d9bfeff5",
      "_type": "castMember",
      "characterName": "Mark Watney",
      "externalCreditId": "53e7e85e0e0a266f9a0029aa",
      "person": {
        "_ref": "person_matt-damon",
        "_type": "reference"
      }
  },
  {
    "_key": "8c1dd384a3ab34befbe5fdd93478fc8e",
    "_type": "castMember",
    "characterName": "Melissa Lewis",
    "externalCreditId": "5466c78eeaeb8172820008e4",
    "person": {
      "_ref": "person_jessica-chastain",
      "_type": "reference"
    }
  },
  {
    "_key": "76a7e8c2547ce445294c581564bc7d75",
    "_type": "crewMember",
    "department": "Camera",
    "externalCreditId": "5607a946c3a3681218003eef",
    "externalId": 1404244,
    "job": "Helicopter Camera",
    "person": {
      "_ref": "person_john-marzano",
      "_type": "reference"
    }
  }
]
```

### Example: Array of references

In this example, we want `castMember` and `crewMember` to be stored as separate documents, and our array should contain references to these documents instead of the actual data.

Input

```javascript
{
  title: 'Employees',
  name: 'employees',
  type: 'array',
  of: [
    {
      type: 'reference',
      to: [
        {type: 'castMember'},
        {type: 'crewMember'}
      ]
    }
  ]
}
```

API response

```json
[
  {
    "_ref": "person_harrison-ford",
    "_type": "reference"
  },
  {
    "_ref": "person_ridley-scott",
    "_type": "reference"
  }
  //...
]
```

### Example: Array of both references and non-references

Arrays can contain mixed types – subject to the [second limitation](https://www.sanity.io/docs/array-type#fNBIr84P) identified below.  This includes mixing references and non-references (e.g., objects).

Let's consider the previous example once more. This time, we want `castMember` to be stored as a separate document that our array should reference, while `crewMember` is a type where we want to store an inline instance of the actual object.

Input

```javascript
{
  title: 'Employees',
  name: 'employees',
  type: 'array',
  of: [
    {
      type: 'reference',
      to: [
        {type: 'castMember'},
      ]
    },
    {type: 'crewMember'}
  ]
}
```

API response

```json
[
  {
    "_ref": "person_harrison-ford",
    "_type": "reference"
  },
  {
    "_ref": "person_ridley-scott",
    "_type": "reference"
  },
  {
    "_key": "76a7e8c2547ce445294c581564bc7d75",
    "_type": "crewMember",
    "department": "Camera",
    "externalCreditId": "5607a946c3a3681218003eef",
    "externalId": 1404244,
    "job": "Helicopter Camera",
    "person": {
      "_ref": "person_john-marzano",
      "_type": "reference"
    }
  }
  //...
]
```

Notice that `{type: 'crewMember'}` is inside the `of` array but outside the reference.

> [!TIP]
> Protip
> Looking to query an array of mixed references and non-references? You could specify what to return from each _type in the array (e.g., crewMember, castMember, etc.) using projections, but you can also distinguish between references and non-references and either return the inline instance of the object or return the referenced document.

```groq
*[] {
  'employees': employees[] {
    _type == 'reference' => @->,
    _type != 'reference' => @
  }
}
```

### Example: Predefined strings

Sometimes you need an array of strings presented as a set of predefined values. By using the list option, the field is presented as an array of check boxes where the editor can toggle which strings are in the array. This handles the array as a set and the ordering is not defined.

Input

```javascript
{
  title: 'Category Set',
  name: 'categorySet',
  type: 'array',
  of: [{type: 'string'}],
  options: {
    list: [
      {title: 'Building', value: 'building'},
      {title: 'Master plan', value: 'masterPlan'},
      {title: 'Infrastructure', value: 'infrastructure'},
      {title: 'Private Home', value: 'privateHome'}
    ]
  }
}
```

API response

```json
{
  "categorySet": ["building", "privateHome"]
}
```

### Example: Predefined objects

Input

```javascript
{
  title: "Example object list",
  type: "array",
  name: "example",
  of: [
    {
      type: "object",
      name: "inline",
      fields: [
        { type: "string", name: "title" },
        { type: "number", name: "amount" }
      ]
    }
  ],
  options: {
    list: [
      { _type: "inline", title: "Big amount", amount: 100 },
      { _type: "inline", title: "Small amount", amount: 1 }
    ]
  }
}
```

API response

```json
{
  "example": [
    {
      "_type": "inline",
      "title": "Big amount",
      "amount": 100,
      "_key": "auto-generated-0"
    },
    {
      "_type": "inline",
      "title": "Small amount",
      "amount": 1,
      "_key": "auto-generated-1"
    }
  ]
}
```

### Example: Unique values

A common use case is to only want unique items in an array. This can be enforced by adding a validation function and using the `unique()` method.

Input

```javascript
{
  title: 'Category Set',
  name: 'categorySet',
  type: 'array',
  of: [{type: 'string'}],
  validation: Rule => Rule.unique()
}
```

API response

```json
{
  "categorySet": ["building", "privateHome"]
}
```



## Why the `_key`?

When adding data with `type: 'object'` to an array, each item gets assigned a persistent and unique `_key` property. This is to ensure that each item can be addressed uniquely in a collaborative, real time setting. This allows one user to edit an array item while another user simultaneously reorders the array.

> [!WARNING]
> Gotcha
> When using the initialValue property in Sanity Studio to initialize a field with a predefined array of objects, setting the _key property of those objects manually will not work.
> 
> This: { type: 'array', initialValue: [{_key: 'monday', day: 'Monday'}] }
> 
> Will result in this: [{_key: '<random string>', day: 'Monday'}]
> 
> 

### Two Limitations

112. Due to a limitation in the data store, arrays may not currently contain arrays. Solve this by wrapping nested arrays in objects.
112. It is not possible to define arrays that contains **both** object types and primitive types. Arrays that hold values of *primitive* types (e.g. *strings* or *numbers*) cannot be addressed uniquely by a key in real time. As a consequence, when defining an array of primitive values, the content studio will switch to a simpler array input widget for editing. This simpler input widget will **not** be able to handle object types, which is why it is not possible to define arrays that contains **both** object types and primitive types. If you should ever need an array that contains both primitive types (e.g., *strings*) and *objects* (e.g., `movie`), you should instead create an object as an item in the array and give it properties that hold the primitive values.

This **will not** work:

```javascript
{
  type: 'array',
  of: [
    {
      type: 'actor', /* This is an object type */
      title: 'Actor'
    },
    {
      type: 'string', /* Will not work! */
      title: 'Actor name'
    }
  ]
}
```

This **will** work:

```javascript
{
  type: 'array',
  of: [
    {
      title: 'Actor',
      type: 'actor'
    },
    {
      title: 'Actor name',
      type: 'object',
      fields: [
        {
          title: 'Name',
          name: 'value',
          type: 'string'
        } 
      ]
    }
  ]
}
```



# Block



![A block field in Sanity Studio](https://cdn.sanity.io/images/3do82whm/next/dd4138b9407ac3ebe5939a391d220c040a9263fd-3456x2100.png)

In order to activate the block content editor for Sanity Studio, you must make an *array of blocks*. In the schema, it looks like this in its simplest form:

```javascript
{
  title: 'Content', 
  name: 'content',
  type: 'array', 
  of: [{type: 'block'}]
}
```

In other words, rich text is modeled as an *array of content* following the [specification for Portable Text](https://www.portabletext.org). What is stored in the database is an array of JSON objects describing the rich text content. This JSON data can later be used to [produce HTML, React components, or other formats depending on the target requirements](/docs/developer-guides/presenting-block-text). This provides a lot of flexibility if you should later want to re-use your content across the web, apps, print, set-top-boxes, consoles, etc.

The block text type supports block styles, lists, decorators (bold, italic, etc.), custom content types (embedded objects), inline objects, and even marking up text with arbitrary object data (annotations). [Learn more about how to configure the rich text editor](/docs/studio/portable-text-editor-configuration).

> [!WARNING]
> Gotcha
> You can't currently use block as a standalone field outside of an array.

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Value must be set to block. Also, blocks only make sense as member of an array, see examples below. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | Required. The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| styles | This defines which styles that applies to blocks. A style is an object with a title (will be displayed in the style dropdown) and a value, e.g.: styles: [{title: 'Quote', value: 'blockquote'}]. If no styles are given, the default styles are H1 up to H6 and blockquote. A style named normal is reserved, always included and represents "unstyled" text. If you don't want any styles, set this to an empty array e.g.: styles: []. |


#### Properties

| Property | Description |
|----------|-------------|
| lists | What list types that can be applied to blocks. Like styles above, this also is an array of "name", "title" pairs, e.g.: {title: 'Bullet', value: 'bullet'}. Default list types are bullet and number. |


#### Properties

| Property | Description |
|----------|-------------|
| marks | An object defining which .decorators (array) and .annotations (array) are allowed. See example below. |


#### Properties

| Property | Description |
|----------|-------------|
| of | An array of inline content types that you can place in running text from the Insert menu. |


#### Properties

| Property | Description |
|----------|-------------|
| icon | To return an icon that is shown in the menus and the toolbar. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | If set to true, this field will be hidden in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| components | Lets you provide custom components to override the studio defaults in various contexts. The components available are field, input, item, preview. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

#### Properties

| Property | Description |
|----------|-------------|
| spellCheck | Enables or disables spellchecking in the Portable Text Editor. Defaults to true. |


## Validation

#### Properties

| Property | Description |
|----------|-------------|
| required() | Ensures that this field exists. |


#### Properties

| Property | Description |
|----------|-------------|
| custom(fn) | Creates a custom validation rule. |


> [!WARNING]
> Gotcha
> A block represents a single paragraph. To make sense, your blocks must live inside an array.

#### Example schema: Default block array

With no custom configuration, the block editor supports:

- Block styles: Normal, Heading 1 to Heading 6, and blockquotes
- Decorators: Strong, emphasis, code, underline and strikethrough
- Lists: bullet list and ordered list
- Link: An annotation that is an object with a `href` with type `url`

Input

```javascript
{
  title: 'Rich text example',
  name: 'myRichTextExample',
  type: 'array',
  of: [{type: 'block'}]
}
```

Response

```json
{
  "myRichTextExample": [{
    "style": "normal",
    "_type": "block",
    "markDefs": [],
    "children": [
      {
        "_type": "span",
        "text": "That was ",
        "marks": []
      },
      {
        "_type": "span",
        "text": "bold",
        "marks": [
          "strong"
        ]
      },
      {
        "_type": "span",
        "text": " of you.",
        "marks": []
      }
    ]
  },
  {
    "style": "normal",
    "_type": "block",
    "markDefs": [],
    "children": [
      {
        "_type": "span",
        "text": "Amazing, actually.",
        "marks": []
      }
    ]
  }]
}
```

#### Example schema: Block array with custom types

This defines a block array that can include both text, actors, and (inline) images.

```javascript
{
  title: 'Rich text',
  type: 'array',
  of: [
    {type: 'block'},
    {type: 'actor'},
    {type: 'image', icon: myIcon}
  ]
}
```

The editor will now get an insertion (`+`) icon in the text editor that can be used to insert actors or images as content blocks in the text. The data stored in the array for these objects are exactly as if they were in a regular array of objects, because they are.

These objects are embedded on the block level, but you may also need objects that appear inline with text useful for stuff like footnotes, ticker-symbols or [sparklines](https://en.wikipedia.org/wiki/Sparkline). Add these to an array under the `of` key in the block type object:

```javascript
{
  title: 'Rich text',
  type: 'array',
  of: [
    {
      type: 'block',
      of: [
        {type: 'footnote'}
      ]
    }
  ]
}

```

### Customizing

Almost every aspect of the block editor and the content it produces is [configurable](/docs/studio/portable-text-editor-configuration). You may want to restrict certain types of decorators or add your own, use your own list styles, annotate text with custom data (e.g. a citation or reference), or support highlighted text.

You can add a `component` property to a block, decorator, or annotation that contains callback functions to control how the content is rendered in the studio, and you can add an `icon` property to render in the tool bar of the editor.

> [!WARNING]
> Gotcha
> Note that customizations made in the studio will not affect how content is rendered elsewhere, such as your front end. That gets handled via portable text serialization.

```javascript
{
  name: 'customized',
  title: 'Customized block type',
  type: 'array',
  of: [
    {
      type: 'block',
      // ...
      marks: {
        decorators: [
          { title: "Strong", value: "strong" },
          { title: "Emphasis", value: "em" },
          {
            title: "Sup",
            value: "sup",
            icon: () => <div>x<sup>2</sup></div>,
            component: ({ children }) => <sup>{children}</sup>
          },
        ],
      },
      // ...
    }
  ]
}
```

#### Example schema: Block array with custom types

```javascript
{
  name: 'customized',
  title: 'Customized block type',
  type: 'array',
  of: [
    {
      type: 'block',
      // Only allow these block styles
      styles: [
        {title: 'Normal', value: 'normal'},
        {title: 'H1', value: 'h1'},
        {title: 'H2', value: 'h2'}
      ],
      // Only allow numbered lists
      lists: [
        {title: 'Numbered', value: 'number'}
      ],
      marks: {
        // Only allow these decorators
        decorators: [
          {title: 'Strong', value: 'strong'},
          {title: 'Emphasis', value: 'em'}
        ],
        // Support annotating text with a reference to an author
        annotations: [
          {name: 'author', title: 'Author', type: 'reference', to: {type: 'author'}}
        ]
      }
    }
  ]
}

```

> [!TIP]
> Protip
> Looking to query for the occurence of a string in an array of blocks? Try *[pt::text(body) match "aliens"] (where body is the name of your array).
> 
> 



# Boolean

A boolean, `true` or `false`.

![Screenshot of a boolean field in Sanity Studio](https://cdn.sanity.io/images/3do82whm/next/5780e4036ae661fa86d2749813f81af0ee1dd841-3456x2100.png)

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Value must be set to boolean. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | Required. The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | If set to true, this field will be hidden in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| initialValue | The initial value used when creating new values from this type. Can be either a literal value or a resolver function that returns either a literal value or a promise resolving to the initial value. |


#### Properties

| Property | Description |
|----------|-------------|
| components | Lets you provide custom components to override the studio defaults in various contexts. The components available are field, input, item, preview. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

#### Properties

| Property | Description |
|----------|-------------|
| layout | Either switch (default) or checkbox

This lets you control the visual appearance of the input. By default the input for boolean fields will display as a switch, but you can also make it appear as a checkbox. |


## Validation

#### Properties

| Property | Description |
|----------|-------------|
| required() | Ensures that this field exists. |


#### Properties

| Property | Description |
|----------|-------------|
| custom(fn) | Creates a custom validation rule. |


Input

```javascript
{
  title: 'Has the movie been released?',
  name: 'released',
  type: 'boolean'
}
```

Response

```json
{
  "_type": "movie",
  "released": true,
  ...
}
```

New documents are created without schema-defined fields. This means that a boolean field in your schema will not immediately result in documents containing the boolean key. The key must be assigned a value for it to appear in a document. Make sure your front-end code treats a missing boolean value as false.

> [!TIP]
> Protip
> In GROQ you can handle missing booleans and false values equally like this *[_type == 'story' && featured != true] which would match stories where featured is false or missing (or to be fair, any other value that is not true).





# Cross Dataset Reference

Cross Dataset References allow you to connect documents across datasets. While similar to the [reference type](/docs/reference-type), they have their own, distinct schema type of `crossDatasetReference` and the two can not be used interchangeably. 

To learn about how to set up your datasets for cross-dataset referencing, please refer to [the introduction article Cross Dataset References](/docs/studio/cross-dataset-references).

_This is a paid feature, available on the Growth plan._

> [!WARNING]
> Gotcha
> Cross dataset references can only be dereferenced using GROQ queries. Dereferencing through GraphQL endpoints is not currently supported.

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Value must be set to crossDatasetReference. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| to * | Must contain an array of objects that name all the types from the referenced dataset that should be available in the referencing studio. type and preview are required properties. icon and title are optional properties. For example, [{type: 'someTypeFromAnotherDataset', preview: { select: { title: 'title' }}}]. See more examples below.

Note: While you may refer to several types in your referenced dataset in the to array, you are limited to types from a single dataset for each field. |


#### Properties

| Property | Description |
|----------|-------------|
| dataset * | The name of the referenced dataset. |


#### Properties

| Property | Description |
|----------|-------------|
| studioUrl | A function that is invoked with the type and id of the referenced document, and returns a string that can be used to construct a URL directly to the item referenced in its studio environment.

Example: 

studioUrl: ({ type, id }) => `https://<your-studio-url>/desk/intent/edit/id=${id};type=${type}/` |


#### Properties

| Property | Description |
|----------|-------------|
| weak | Default false. If set to true the reference will be made weak. This allows references to point at documents that may or may not exist, such as a document that has not yet been published or a document that has been deleted (or indeed an entirely imagined document). |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | If set to true, this field will be hidden in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| initialValue | The initial value that will be used when creating new values from this type. Can be either the literal value or a resolver function that returns either the literal value or a promise that resolves to the initial value. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

#### Properties

| Property | Description |
|----------|-------------|
| filter | Additional GROQ-filter to use when searching for target documents. The filter will be added to the already existing type name clause.

If a function is provided, it is called with an object containing document, parent and parentPath properties, and should return an object containing filter and params. As of v2.4.0 this function can optionally be async and return a Promise that resolves to an object containing filter and params.

Note: The filter only constrains the list of documents returned at the time you search. It does not guarantee that the referenced document will always match the filter provided. |


#### Properties

| Property | Description |
|----------|-------------|
| filterParams | Object of parameters for the GROQ-filter specified in filter. |


## Validation

#### Properties

| Property | Description |
|----------|-------------|
| required() | Ensures that this field exists. |


#### Properties

| Property | Description |
|----------|-------------|
| custom(fn) | Creates a custom validation rule. |


## Cross Dataset Reference

A minimal example of a `crossDatasetReference` field:

Input

```javascript
{
  title: `Person in another dataset"`,
  name: 'personReference',
  type: 'crossDatasetReference',
  dataset: 'production',
  to: [
    {
      type: 'person',
      preview: {
        select: {
          title: 'name',
          media: 'image',
        },
      },
    },
  ],
}
```

Response

```json
{
  "_type": "crossDatasetReference",
  "_ref": "person_andrew-stanton",
  "_dataset": "production"
}
```

## Weak reference

Defining the `crossDatasetReference` as `weak`, will unblock publishing of documents that has a (cross-dataset) reference to a non-existing document.

Input

```javascript
{
  title: `Person in another dataset"`,
  name: 'personReference',
  type: 'crossDatasetReference',
  dataset: 'production',
  weak: true,
  to: [
    {
      type: 'person',
      preview: {
        select: {
          title: 'name',
          media: 'image',
        },
      },
    },
  ],
}
```

Response

```json
{
  "_type": "crossDatasetReference",
  "_ref": "person_andrew-stanton",
  "_dataset": "production",
  "_weak": true,
}
```

## Reference  multiple types

The `directors` field is an array that can contain both `person` and `bovinae` (in the rare occasion a cow would direct a movie) references:

Input

```javascript
{
  title: `Person or cow in another dataset"`,
  name: 'personOrCowReference',
  type: 'crossDatasetReference',
  dataset: 'production',
  to: [
    {
      type: 'person',
      preview: {
        select: {
          title: 'name',
          media: 'image',
        },
      },
    },
    {
      type: 'bovinae',
      preview: {
        select: {
          title: 'name',
          media: 'avatar',
        },
      },
    },
  ],
}
```

Response

```json
[
  {
    "_type": "crossDatasetReference",
    "_ref": "person_andrew-stanton",
    "_dataset": "production"
  },
  {
    "_type": "crossDatasetReference",
    "_ref": "bovinae_ferdinand-bull",
    "_dataset": "production"
  }
]
```

## Additional static filter

If providing a target schema type is not enough to provide a meaningful set of search results, you may want to further constrain the search query:

Input

```javascript
{
  title: `Person in another dataset"`,
  name: 'personReference',
  type: 'crossDatasetReference',
  dataset: 'production',
  options: {
    filter: 'role == $role',
    filterParams: {role: 'director'}
  },
  to: [
    {
      type: 'person',
      preview: {
        select: {
          title: 'name',
          media: 'image',
        },
      },
    },
  ],
}
```

Response

```json
{
  "_type": "crossDatasetReference",
  "_ref": "person_steven-spielberg",
  "_dataset": "production",
}
```

## Additional dynamic filter

If you want to further constrain the search result, but need properties from the surrounding document or object/array, you can use the function form for `filter`:

Input

```javascript
{
  title: `Person in another dataset"`,
  name: 'personReference',
  type: 'crossDatasetReference',
  dataset: 'production',
  to: [
    {
      type: 'person',
      preview: {
        select: {
          title: 'name',
          media: 'image',
        },
      },
    },
  ],
  options: {
  filter: ({document}) => {
    // Always make sure to check for document properties
    // before attempting to use them
    if (!document.releaseYear) {
      return {
        filter: 'role == $role',
        params: {role: 'director'}
      }
    }
    
    return {
      filter: 'role == $role && birthYear >= $minYear',
      params: {
        role: 'director',
        minYear: document.releaseYear
      }
    }
  }
}
```

Response

```json
{
  "_type": "crossDatasetReference",
  "_ref": "person_steven-spielberg",
  "_dataset": "production",
}
```

## Nonexistent reference

Sometimes the reference field may show an error message like `<nonexistent reference>`. This usually happens when creating documents with a client library and can mean one of two things:

- The document with the ID you are referencing does not exist
- The field does not allow references to the document type of the document ID you tried to reference





# Date

An ISO-8601 formatted string containing date. E.g.  `2017-02-12`.

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Required. Value must be set to date. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | Required. The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | If set to true, this field will be hidden in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| placeholder | Placeholder text that appear within the input when it is empty. |


#### Properties

| Property | Description |
|----------|-------------|
| initialValue | The initial value used when creating new values from this type. Can be either a literal date string value or a resolver function that returns either a literal date string value or a promise resolving to the initial date string value. |


#### Properties

| Property | Description |
|----------|-------------|
| components | Lets you provide custom components to override the studio defaults in various contexts. The components available are field, input, item, preview. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

#### Properties

| Property | Description |
|----------|-------------|
| dateFormat | Controls how the date input field formats the displayed date. Use any valid Moment format option. Default is YYYY-MM-DD. |


## Validation

#### Properties

| Property | Description |
|----------|-------------|
| required() | Ensures that this field exists. |


#### Properties

| Property | Description |
|----------|-------------|
| custom(fn) | Creates a custom validation rule. |


#### Properties

| Property | Description |
|----------|-------------|
| min(minDate) | Minimum date (inclusive). minDate should be in ISO 8601 format. |


#### Properties

| Property | Description |
|----------|-------------|
| max(maxDate) | Maximum date (inclusive). maxDate should be in ISO 8601 format. |


![Screenshot of Date field with a title, description, and value.](https://cdn.sanity.io/images/3do82whm/next/a4780c2c8594ddb523fcf824d3cff6c011be05e9-1152x500.png)

The stored date is represented as a string in compliance with [ISO 8601](http://en.wikipedia.org/wiki/ISO_8601) (often described as `YYYY-MM-DD`).

> [!TIP]
> Protip
> If you need to store information about both date and time, use the datetime type instead.

Input

```javascript
{
  title: 'Release date',
  name: 'releaseDate',
  type: 'date'
}
```

Response

```json
{
  "releaseDate": "2017-02-12"
}
```

#### Example: All options set

```javascript
{
  title: 'Release date',
  name: 'releaseDate',
  type: 'date',
  options: {
    dateFormat: 'YYYY-MM-DD',
    calendarTodayLabel: 'Today'
  }
}
```



# Datetime

An ISO-8601 formatted string containing date and time stored in UTC. E.g.  `2017-02-12T09:15:00Z`.

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Value must be set to datetime. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | Required. The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | If set to true, this field will be hidden in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| initialValue | The initial value used when creating new values from this type. Can be either a literal datetime string or a resolver function that returns either a literal datetime string value or a promise resolving to a datetime string value. |


#### Properties

| Property | Description |
|----------|-------------|
| components | Lets you provide custom components to override the studio defaults in various contexts. The components available are field, input, item, preview. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

#### Properties

| Property | Description |
|----------|-------------|
| dateFormat | Controls how the date input field formats the displayed date. Use any valid Moment format option. Default is YYYY-MM-DD. |


#### Properties

| Property | Description |
|----------|-------------|
| timeFormat | Controls how the time input field formats the displayed date. Use any valid Moment format option. Default is HH:mm. |


#### Properties

| Property | Description |
|----------|-------------|
| timeStep | Number of minutes between each entry in the time input. Default is 15 which lets the user choose between 09:00, 09:15, 09:30 and so on. |


#### Properties

| Property | Description |
|----------|-------------|
| allowTimeZoneSwitch | Determines whether the user is allowed to set a personalized time zone for viewing and interacting with the field in Studio. Defaults to true. |


#### Properties

| Property | Description |
|----------|-------------|
| displayTimeZone | Set a specific time zone to be used when viewing and interacting with the field in Studio. Expects a string in the shape of a valid time zone identifier. Note: the timestamp stored in the dataset is always UTC. |






## Validation

#### Properties

| Property | Description |
|----------|-------------|
| required() | Ensures that this field exists. |


#### Properties

| Property | Description |
|----------|-------------|
| min(minDate) | Minimum date (inclusive). minDate should be in ISO 8601 format. |


#### Properties

| Property | Description |
|----------|-------------|
| max(maxDate) | Maximum date (inclusive). maxDate should be in ISO 8601 format. |


#### Properties

| Property | Description |
|----------|-------------|
| custom(fn) | Creates a custom validation rule. |


The date+time is represented as a string in a *simplified* extended ISO format ([ISO 8601](http://en.wikipedia.org/wiki/ISO_8601)). This is the same format as [date.toISOString()](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date/toISOString) and **date.toJSON()** returns.

Input

```javascript
{
  title: 'Launch Scheduled At',
  name: 'launchAt',
  type: 'datetime'
}
```

Response

```json
{
  "launchAt": "2017-02-12T09:15:00Z"
}
```

## Example: All options set

```javascript
{
  title: 'Launch Scheduled At',
  name: 'launchAt',
  type: 'datetime',
  options: {
    dateFormat: 'YYYY-MM-DD',
    timeFormat: 'HH:mm',
    timeStep: 15,
    calendarTodayLabel: 'Today',
    allowTimeZoneSwitch: true, // default value, could be omitted
    displayTimeZone: 'Europe/Berlin'
  }
}
```



# Document

Everything in the Studio starts with the `document`. A document is what you create and edit in the studio—all the other types you may define live inside the `document`s. In the default studio configuration, the document-types are the ones that will be listed in the content-column.

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| name * | The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| type * | Value must be set to document. |


#### Properties

| Property | Description |
|----------|-------------|
| fields * | The fields of this object. At least one field is required. Documented here. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Show a description to editors with context about the document type. |


#### Properties

| Property | Description |
|----------|-------------|
| fieldsets | A list of fieldsets that fields may belong to. Documented here. |


#### Properties

| Property | Description |
|----------|-------------|
| groups | Groups fields into tabs. 

On document: groups: [{name: 'seo', title: 'SEO'}], 

On field: group: 'seo',

For details, see this reference doc. |


#### Properties

| Property | Description |
|----------|-------------|
| initialValue | The initial value that will be used for all new documents created from this document type. Can be either a literal document value or a function that returns either a literal value or a promise that resolves to a document value. |


#### Properties

| Property | Description |
|----------|-------------|
| liveEdit | Turns off drafts when set to true. Changes to documents will publish immediately. |


#### Properties

| Property | Description |
|----------|-------------|
| orderings | A declaration of possible ways to order documents of this type, documented here. |


#### Properties

| Property | Description |
|----------|-------------|
| preview | Use this to implement an override for the default preview for this type. Documentation here. |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the document. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, documents of this type will not be editable in the Studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| components | Lets you provide custom components to override the studio defaults in various contexts. The components available are field, input, item, preview. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


#### Properties

| Property | Description |
|----------|-------------|
| __experimental_formPreviewTitle | Hides the document title heading in the studio form pane. |


## Options

## Validation

At its core, a document is a JSON-object that has a unique `_id`, timestamps (`_createdAt`, `_updatedAt`) and revision-marker `_rev`.

The `document` type is used to define the structure of a document that can be stored in our data store. You can think of a document as an object that, in addition to the fields you define, also has a unique id, (`_id`), a field for tracking created time and last updated time (`_createdAt` and `_updatedAt`) and a revision marker (`_rev`). Only *documents* can be referred to from other documents or retrieved by id and only *document* types will be listed and create-able in the studio.

Apart from the above, documents are defined just like regular objects, so see the documentation of the object type for more info about how to define documents.

Input

```javascript
{
  title: 'Movie',
  name: 'movie',
  type: 'document',
  fields: [
    {
      title: 'Title',
      name: 'title',
      type: 'string'
    },
    {
      title: 'Poster',
      name: 'poster',
      type: 'image'
    },
    {
      title: 'Directors',
      name: 'directors',
      type: 'array',
      of: [{type: 'string'}]
    }
  ]
}
```

Response

```json
{
  "_type": "movie",
  "_id": "2106a34f-315f-44bc-929b-bf8e9a3eba0d",
  // ... _createdAt, _updatedAt, _rev omitted
  "title": "Alien",
  "poster": {... <an image object> ...},
  "directors": ["Ridley Scott"]
}
```





# File

A `file` is a special kind of [object](/docs/object-type) that includes an implicit asset field, which is a reference to a file asset document. This is useful for storing any kind of non-image files (pdf, mpeg, docx etc).



> [!WARNING]
> Gotcha
> You shouldn't use the file type for images. Use image instead. Images uploaded as files will not have the associated metadata for images and you won't be able to scale and crop them in the image pipeline.

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Required. Value must be set to file. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | Required. The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| fields | An array of optional fields to add to the file field. The fields added here follow the same pattern as fields defined on objects. This is useful for allowing users to add custom metadata related to the usage of this file (see example below). |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | If set to true, this field will be hidden in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| initialValue | The initial value used when creating new values from this type. Can be either a literal value or a resolver function that returns either a literal value or a promise resolving to the initial value. |


#### Properties

| Property | Description |
|----------|-------------|
| components | Lets you provide custom components to override the studio defaults in various contexts. The components available are field, input, item, preview. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

#### Properties

| Property | Description |
|----------|-------------|
| storeOriginalFilename | This will store the original filename in the asset document. Please be aware that the name of uploaded files could reveal potentially sensitive information (e.g. top_secret_planned_featureX.pdf). Default is true. |


#### Properties

| Property | Description |
|----------|-------------|
| accept | This specifies which mime types the file input can accept. It functions just like the accept attribute on native DOM file inputs and you can specify any valid file type specifier.

It is recommended to use MIME types ("application/pdf") over file extensions (".pdf") in order for hover notifications to work for drag and drop as browsers do not send the file name while hovering. |


#### Properties

| Property | Description |
|----------|-------------|
| sources | Lock the asset sources available to this type to a specific subset. Import the plugins by their part name, and use the import variable name as array entries. 

Read more about custom asset sources |


## Validation

#### Properties

| Property | Description |
|----------|-------------|
| required() | Ensures that this field exists. |


#### Properties

| Property | Description |
|----------|-------------|
| assetRequired() | Like required but more specific. Requires that an actual asset is referenced to validate. Must be used together with required, i.e.: 
validation: (Rule) => Rule.required().assetRequired(), |


#### Properties

| Property | Description |
|----------|-------------|
| custom(fn) | Creates a custom validation rule. |


Input

```javascript
{
  title: 'Manuscript',
  name: 'manuscript',
  type: 'file',
  fields: [
    {
      name: 'description',
      type: 'string',
      title: 'Description'
    },
    {
      name: 'author',
      type: 'reference',
      title: 'Author',
      to: {type: 'person'}
    }
  ]
}
```

Response

```json
{
  "_type": "file",
  "asset": {
    "_type": "reference",
    "_ref": "file-5igDD9UuXffIucwZpyVthr0c"
  },
  "description": "First draft",
  "author": {
    "_type": "reference",
    "_ref": "1osKfX-49GLPg-2EeuOe-3ufEFE"
  }
}
```

## Download file

In order to download a file from your front-end you need to append `?dl=<filename-of-your-choice.pdf>` to the file URL. If you leave the filename blank, the original filename will be used if present. If the original filename is not available, the id of the file will be used instead. 

```groq
// GROQ query
*[_type == 'movie'] {
  title,
  "manuscriptURL": manuscript.asset->url
}

// Then you can use the URL in HTML for example like this:
// <a href={`${manuscriptURL}?dl=`}>Manuscript</a>
```

## Uploading files via Drag & Drop or Paste

When you drag and drop files into the Portable Text Editor or an Array field in Sanity Studio, it will automatically pick the most suitable field to add the file to based on the `accept` option configured on the file fields. If multiple fields match the dropped file type, it will use the first matching field.

```javascript
// Field with accept option set to PDF
defineField({
  name: 'pdfFile',
  type: 'file',  
  options: {
    accept: 'application/pdf'
  }
})
```

```javascript
// Field with accept option set to Excel
defineField({
  name: 'excelFile',
  type: 'file',
  options: {
    accept: 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
  }
})
```

When dropping an Excel file, it will be added to the `excelFile` field that accepts Excel files.



# Geopoint

An object signifying a global latitude/longitude/altitude coordinate. Longitude and latitude is stored as decimal degrees, while altitude is stored as a floating point representing meters above sea level.

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Value must be set to geopoint. |


#### Properties

| Property | Description |
|----------|-------------|
| name | Required. The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | If set to true, this field will be hidden in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| initialValue | The initial value used when creating new values from this type. Can be either a literal value or a resolver function that returns either a literal value or a promise resolving to the initial value. |


#### Properties

| Property | Description |
|----------|-------------|
| components | Lets you provide custom components to override the studio defaults in various contexts. The components available are field, input, item, preview. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

## Validation

#### Properties

| Property | Description |
|----------|-------------|
| required() | Ensures that this field exists. |


#### Properties

| Property | Description |
|----------|-------------|
| custom(fn) | Creates a custom validation rule. |


Input

```javascript
{
  title: 'Launchpad Location',
  name: 'location',
  type: 'geopoint'
}
```

Response

```json
{
  "_type": "geopoint",
  "lat": 58.63169011423141,
  "lng": 9.089101352587932,
  "alt": 13.37
}
```

While the `geopoint` type is available in Sanity by default, you will probably want to install a plugin that provides a more visual way to input the coordinates. For instance, you could use the [@sanity/google-maps-input](https://www.npmjs.com/package/@sanity/google-maps-input) plugin:

```sh
cd my-project
npm install --save @sanity/google-maps-input
```

Then add the plugin to your `sanity.config.ts|js` with your Google maps API key:

```typescript
import { googleMapsInput } from "@sanity/google-maps-input";

export default defineConfig({
  // ...
  plugins: [
      googleMapsInput({
          apiKey: "my-api-key"
     })
  ] 
})
```

Make sure the key has access to all of the following APIs:

- Google Maps JavaScript API
- Google Places API Web Service
- Google Static Maps API

You can create such keys and grant API access in the [Google Developer Console](https://console.developers.google.com/apis).

> [!WARNING]
> Gotcha
> Be careful with your API keys. If you use this functionality, it's a good idea to make your repository private.



# Global Document Reference

Global document references expand the concept of the reference type to support referencing documents in other datasets.

> [!NOTE]
> Global document references are limited to Media Library
> Global document references are a new schema type. At this time, only Media Library's  aspects feature supports use of global document references.

Like standard [references](/docs/studio/reference-type), global document references can be either *strong* (default) or *weak*. A strong reference will ensure that the document it points to actually exists and will not allow the deletion of a document that any other document refers to. A weak reference allows pointing to documents that may not exist (yet) or have been deleted.

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Value must be set to globalDocumentReference. |
| name * | Required. The field name. This will be the key in the data record. |
| resourceType * | The resource type containing the connected documents. At the moment, Media Library can reference the dataset resource type. |
| resourceId * | The ID of the target resource. A resourceId is made up of the projectId and the dataset name, connected by a .. For example:

projectId.datasetName. 

w3dbef.production

wm2efj.staging |
| to * | Must contain an array naming all the types which may be referenced. For example: [{type: 'person'}]. See more examples below. |
| weak | If set to true the reference will be made weak. This allows references to point at documents that may or may not exist, such as a document that has not yet been published or a document that has been deleted (or indeed an entirely imagined document). |
| title | Human-readable label for the field. |
| options | Further configure the schema type. See details in the "options" section below. |


### Options

#### Properties

| Property | Description |
|----------|-------------|
| filter | A GROQ filter string (the contents between the square brackets), such as language == "en-US". |
| filterParams | Object of parameters for the GROQ-filter specified in filter. |


## Example

```
import { defineAssetAspect } from 'sanity'

export default defineAssetAspect({
  name: 'photographer',
  title: 'Photographer',
  type: 'globalDocumentReference',
  description: 'Select the photographer.',
  resourceType: 'dataset',
  resourceId: '3do82whm.example',
  weak: true,
  to: [
    {
      type: 'photographer',
      preview: {
        select: {
          title: 'name'
        }
      }
    }
  ]
})
```



# Image

When you create a field with the `image` type, the user is presented with a standard file dialog that allows normal uploads, as well as drag and drop and pasting of images. Arrays of images accept batches of files to be dropped on them. 

When uploading an image the reference to the file itself is not stored in the image field in a given document. Instead, it adds a reference to the asset metadata document. This allows you to separate between context-specific data, like hotspot, crop, and captions – and the image asset itself, which you might want to re-use in many contexts.

Image assets also contain metadata such as Low-Quality Image Previews (LQIP), palette information, and original image dimension as well as aspect ratio. 

Have a look at the articles on [presenting images](/docs/apis-and-sdks/presenting-images) and [image URLs](/docs/apis-and-sdks/image-urls) for how to use images in practice. 

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Value must be set to image. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| fields | An array of optional fields to add to the image record. The fields added here follow the same pattern as fields defined on objects. This is useful for adding custom properties like caption, attribution, etc., to the image record itself (see example below). |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | If set to true, this field will be hidden in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| initialValue | The initial value used when creating new values from this type. Can be either a literal value or a resolver function that returns either a literal value or a promise resolving to the initial value. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

#### Properties

| Property | Description |
|----------|-------------|
| metadata | This option defines what metadata the server attempts to extract from the image. The extracted data is written into the image asset. This field must be an array of strings where accepted values are image, exif, location, lqip, blurhash and palette. Read more about image metadata in this reference document. |


#### Properties

| Property | Description |
|----------|-------------|
| hotspot | Enables the user interface for selecting what areas of an image should always be cropped, what areas should never be cropped, and the center of the area to crop around when resizing. The hotspot data is stored in the image field itself, not in the image asset, so images can have different crops for each place they are used.

Hotspot makes it possible to responsively adapt images to different aspect ratios at display time. The default value for hotspot is false. |


#### Properties

| Property | Description |
|----------|-------------|
| storeOriginalFilename | This will store the original filename in the asset document. Please be aware that the name of uploaded files could reveal potentially sensitive information (e.g. top_secret_planned_featureX.pdf). Default is true. |


#### Properties

| Property | Description |
|----------|-------------|
| accept | This specifies which mime types the image input can accept. Just like the accept attribute on native DOM file inputs, you can specify any valid file type specifier: https://developer.mozilla.org/en-US/docs/Web/HTML/Element/input/file#Unique_file_type_specifiers |


#### Properties

| Property | Description |
|----------|-------------|
| sources | Lock the asset sources available to this type to a specific subset. Import the plugins by their part name, and use the import variable name as array entries. 

Read more about custom asset sources |


## Validation

#### Properties

| Property | Description |
|----------|-------------|
| required() | Ensures that this field exists. |


#### Properties

| Property | Description |
|----------|-------------|
| assetRequired() | Like required but more specific. Requires that an actual asset is referenced to validate. Must be used together with required, i.e.: 
validation: (Rule) => Rule.required().assetRequired(), |


#### Properties

| Property | Description |
|----------|-------------|
| custom(fn) | Creates a custom validation rule. |


## Custom asset sources

You can [customize what asset sources are available](/docs/studio/custom-asset-sources) via plugins. This way, you can integrate with your preferred digital asset management system (DAM). Check out the [current list of asset sources](/plugins?category=assetSource).

## Supported image formats

Sanity allows you to upload 256-megapixel archival originals of the image types JPG, SVG, PNG, GIF, or TIFF. These formats can be transcoded into JPG, PNG, GIF, AVIF, and [WebP](https://en.wikipedia.org/wiki/WebP). Learn how in [the chapter on image URLs](/docs/apis-and-sdks/image-urls).

## Examples of image-related data structures

The `image` field type is similar to an object `field`, in that it can have additional fields appended to it using the `fields` configuration.

When an asset is uploaded to an image field, an asset metadata document is created, and a reference to that document is added to the `asset` field within the image field.

### Example of an image type object

Input

```javascript
defineField({
  name: 'poster',
  type: 'image',
  // 👇 Enables crop and hotspot tools
  options: {
    hotspot: true
  },
  // 👇 Optionally append additional fields to the image object
  fields: [
    defineField({
      name: 'caption',
      type: 'string',
    }),
    defineField({
      name: 'attribution',
      type: 'string',
    })
  ]
})
```

Response

```json
{
  "_type": "image",
  "asset": {
    "_type": "reference",
    "_ref": "image-S2od0Kd5mpOa4Y0Wlku8RvXE"
  },
  "caption": "This is the caption",
  "attribution": "Public domain",
  "crop": {
    "top": 0.028131868131868132,
    "bottom": 0.15003663003663004,
    "left": 0.01875,
    "right": 0.009375000000000022
  },
  "hotspot": {
    "x": 0.812500000000001,
    "y": 0.27963369963369955,
    "height": 0.3248351648351647,
    "width": 0.28124999999999994
  }
}
```

### Example of an image asset metadata document

The asset metadata document created when an asset is uploaded includes details such as location, `lqip` (low quality image placeholder), palette and dimensions.

```json
{
  "_createdAt": "2018-06-27T10:46:48Z",
  "_id": "image-223c27c1f0e75fe1ef494333738e2d16a8539e6a-1365x1364-svg",
  "_rev": "MGbYJ9NCiEIKUXQcjjXmmw",
  "_type": "sanity.imageAsset",
  "assetId": "223c27c1f0e75fe1ef494333738e2d16a8539e6a",
  "extension": "svg",
  "metadata": {
    "dimensions": {
      "aspectRatio": 1.000733137829912,
      "height": 1364,
      "width": 1365
    },
    "location": {
      "_type": "geopoint",
      "lat": 59.92399340000001,
      "lng": 10.758972200000017
    },
    "lqip": "data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAUABQDASIAAhEBAxEB/8QAGQABAAIDAAAAAAAAAAAAAAAAAAYHAwUI/8QAKBAAAQQCAQIEBwAAAAAAAAAAAgEDBAUABhEHExQhQVESIiMxYXGB/8QAFQEBAQAAAAAAAAAAAAAAAAAABAX/xAAgEQACAgEEAwEAAAAAAAAAAAABAgADEQQSITETUXGB/9oADAMBAAIRAxEAPwCqej0eqhVtneWMLx0mOn0GeOfP9Zv2upVFsDcmv3GkCIwoqjbgAqqK5BdFh7RHrpFpRRvEQQ57o88/b8ZJ9ZtQ3KcVZNo07pCqk4I+Q8e/tgrCysSRkfeRL+lFNlSIrbG9EZDfsqizCO3YSBhGrkVDXtkqcKo+mMz7DCCuupkRpeQacUUxjFOQCJDsUo5U9iSnpVtNpRXQxoLo+Gkrw404PxCv8y6N92GTQa45LqmIceQ6PzGLPC+eMYa0DeJU0bHwNz1OYZDzkh9x54lJxwlIiX1VcYxipIPM/9k=",
    "palette": {
      "darkMuted": {
        "background": "#482d2c",
        "foreground": "#fff",
        "population": 15,
        "title": "#fff"
      },
      "darkVibrant": {
        "background": "#68201e",
        "foreground": "#fff",
        "population": 22,
        "title": "#fff"
      },
      "dominant": {
        "background": "#f34b3c",
        "foreground": "#fff",
        "population": 1292,
        "title": "#fff"
      },
      "lightMuted": {
        "background": "#c5837e",
        "foreground": "#000",
        "population": 31,
        "title": "#fff"
      },
      "lightVibrant": {
        "background": "#f9948c",
        "foreground": "#000",
        "population": 3,
        "title": "#fff"
      },
      "muted": {
        "background": "#ac736c",
        "foreground": "#fff",
        "population": 24,
        "title": "#fff"
      },
      "vibrant": {
        "background": "#f34b3c",
        "foreground": "#fff",
        "population": 1292,
        "title": "#fff"
      }
    }
  },
  "mimeType": "image/svg+xml",
  "originalFilename": "logo-s-red-1365x1365.svg",
  "path": "images/3do82whm/production/223c27c1f0e75fe1ef494333738e2d16a8539e6a-1365x1364.svg",
  "sha1hash": "223c27c1f0e75fe1ef494333738e2d16a8539e6a",
  "size": 1378,
  "url": "https://cdn.sanity.io/images/3do82whm/production/223c27c1f0e75fe1ef494333738e2d16a8539e6a-1365x1364.svg",
  "_updatedAt": "2018-07-30T08:07:49.238Z"
}
```

## Uploading images via Drag & Drop or Paste

When you drag and drop images into the Portable Text Editor or an Array field in Sanity Studio, it will automatically pick the most suitable field to add the image to based on the `accept` option configured on the image fields. If multiple fields match the dropped image type, it will use the first matching field.

```javascript
// Field with accept option set to PNG
defineField({
  name: 'pngImage', 
  type: 'image',
  options: {
    accept: 'image/png'
  }
})
```

```javascript
// Field with accept option set to JPEG
defineField({
  name: 'jpegImage',
  type: 'image', 
  options: {
    accept: 'image/jpeg'
  }
})
```

When dropping a JPEG image, it will be added to the `jpegImage` field that accepts JPEG images.



# Number



![A current popularity indicator with a number field](https://cdn.sanity.io/images/3do82whm/next/6a8dc39442ae4dbc12c8579c0dd2c3d54b778c48-1152x474.png)

Any number, e.g. `900`, `900.0`, `9E+2` or `9.0E+2`.

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Value must be set to number. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | Required. The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | If set to true, this field will be hidden in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| initialValue | The initial value used when creating new values from this type. Can be either a literal number value or a resolver function that returns either a literal number value or a promise resolving to a number value. |


#### Properties

| Property | Description |
|----------|-------------|
| components | Lets you provide custom components to override the studio defaults in various contexts. The components available are field, input, item, preview. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

#### Properties

| Property | Description |
|----------|-------------|
| list | A list of predefined values that the user may pick from. The array can include numeric values [1, 2] or objects [{value: 1, title: 'One'}, ...]. |


#### Properties

| Property | Description |
|----------|-------------|
| layout | Controls how the items defined in the list option are presented. If set to 'radio' the list will render radio buttons. If set to 'dropdown' you'll get a dropdown menu instead. Default is dropdown. |


#### Properties

| Property | Description |
|----------|-------------|
| direction | Controls how radio buttons are lined up. Use direction: 'horizontal\|vertical' to render radio buttons in a row or a column. Default is vertical. Will only take effect if the layout option is set to radio. |


## Validation

#### Properties

| Property | Description |
|----------|-------------|
| required() | Ensures that this field exists. |


#### Properties

| Property | Description |
|----------|-------------|
| min(minNumber) | Minimum value (inclusive). |


#### Properties

| Property | Description |
|----------|-------------|
| max(maxNumber) | Maximum value (inclusive). |


#### Properties

| Property | Description |
|----------|-------------|
| lessThan(limit) | Value must be less than the given limit. |


#### Properties

| Property | Description |
|----------|-------------|
| greaterThan(limit) | Value must be greater than the given limit. |


#### Properties

| Property | Description |
|----------|-------------|
| integer() | Value must be an integer (no decimals). |


#### Properties

| Property | Description |
|----------|-------------|
| precision(limit) | Specifies the maximum number of decimal places allowed. |


#### Properties

| Property | Description |
|----------|-------------|
| positive() | Requires the number to be positive (>= 0). |


#### Properties

| Property | Description |
|----------|-------------|
| negative() | Requires the number to be negative (< 0). |


#### Properties

| Property | Description |
|----------|-------------|
| custom(fn) | Create a custom validation. |


Input

```javascript
{
  title: 'Current popularity',
  name: 'popularity',
  type: 'number'
}

```

Response

```json
{
  "_type": "movie",
  "popularity": 12.5,
  ...
}
```

> [!WARNING]
> Gotcha
> Never use number for storing a phone-number. Minimize pain down the road and use string instead.



# Object

The `object` type is the bread and butter of your data model. You use it to define custom types with fields of strings, numbers, and arrays, as well as other object types.

By default, object types cannot be represented as standalone documents in the data store. To define an object type to represent it as a document with an ID, revision, as well as created and updated timestamps, you should define with the [document](/docs/document-type) type. Apart from these additional fields, there's no semantic difference between a document and an object.

> [!TIP]
> Protip
> If you plan to use your schemas with the GraphQL API, you'll need to import object types on the top-level (called “hoisting”). Learn more about how to make “strict schemas” in our GraphQL documentation.

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Value must be set to object. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | Required. The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| fields * | The fields of this object. At least one field is required. See documentation below. |


#### Properties

| Property | Description |
|----------|-------------|
| fieldsets | A list of fieldsets that fields may belong to. Documentation below. |


#### Properties

| Property | Description |
|----------|-------------|
| groups | Groups fields into tabs. 

On object: groups: [{name: 'seo', title: 'SEO'}], 

On field: group: 'seo',

For details, see this reference doc. |


#### Properties

| Property | Description |
|----------|-------------|
| preview | Enables specifying a preview option that replaces the default preview for the document type. For more information, see List Previews. |


#### Properties

| Property | Description |
|----------|-------------|
| inputComponent |  |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | If set to true, this field will be hidden in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| initialValue | The initial value that will be used when creating new objects from this type. Can be either the literal value or a function that returns either the literal value or a promise that resolves to the initial value. |


#### Properties

| Property | Description |
|----------|-------------|
| components | Lets you provide custom components to override the studio defaults in various contexts. The components available are field, input, item, preview. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

#### Properties

| Property | Description |
|----------|-------------|
| collapsible | If set to true, the object will make the fields collapsible. By default, objects will be collapsible when reaching a depth/nesting level of 3. This can be overridden by setting collapsible: false |


#### Properties

| Property | Description |
|----------|-------------|
| collapsed | Set to true to display fields as collapsed initially. This requires the collapsible option to be set to true and determines whether the fields should be collapsed to begin with. |


#### Properties

| Property | Description |
|----------|-------------|
| columns | An integer corresponding to the number of columns in a grid for the inputs to flow between. |


#### Properties

| Property | Description |
|----------|-------------|
| modal | Controls how the modal (for object content editing) is rendered. The types you can choose between is dialog or popover. Default is dialog. You can also set the width of the modal. 

modal?: {

   type?: 'dialog' \| 'popover'

   width?: number \| number[] \| 'auto'

} |


## Validation

#### Properties

| Property | Description |
|----------|-------------|
| required() | Ensures that this field exists. |


#### Properties

| Property | Description |
|----------|-------------|
| custom(fn) | Creates a custom validation rule. |


## Fields

Fields are what gives an object its structure. Every field must have a name and a type. An object can reference [any field type](/docs/schema-types). You may specify the properties and options that are supported for the given type, e.g.:

```javascript
{
  title: 'Address',
  name: 'address',
  type: 'object',
  fields: [
    {name: 'street', type: 'string', title: 'Street name'},
    {name: 'streetNo', type: 'string', title: 'Street number'},
    {name: 'city', type: 'string', title: 'City'}
  ]
}
```

Once a type is added to the schema, it can be reused as the type for other fields, so lets use it in our screening:

Input

```javascript
{
  title: 'Screening',
  name: 'screening',
  type: 'document',
  fields: [
    // ... 
      {
      title: 'Cinema address',
      name: 'address',
      type: 'address'
    }
    // ... 
  ]
}

```

Response

```json
{
  "_type": "screening",
  "_id": "2106a34f-315f-44bc-929b-bf8e9a3eba0d",
  "title": "Welcome to our premiere of Valerian and the City of a Thousand Planets!",
  //...
  "address": {
    "_type": "address",
    "street": "Dronningens gate",
    "streetNo": "16",
    "city": "Oslo"
  }
  //...
}
```

### Field names

A field name must start with a letter from a-z, and can **can only include:**

- Letters
- Numbers
- Underscores

This means field names can't contain hyphens. We also [recommend](/docs/apis-and-sdks/naming-things) using the camel case naming convention for field names.

### Additional Field options

Sometimes you may have fields which are not meant to be exposed to the editors through the studio, but are populated by backend services or scripts. By setting the `hidden` property to `true`, you can make sure that the field is still included in the schema but not displayed in the studio. Example:

```javascript
{
  title: 'Movie',
  name: 'movie',
  type: 'document',
  fields: [
    // ... other fields
    {
      title: 'Last synchronized',
      name: 'lastSynced',
      description: 'Timestamp the movie was last synced with external service. Not shown in studio.',
      type: 'datetime',
      hidden: true
    }
  ]
}
```

## Fieldsets

Sometimes it makes sense to group a set of fields into a fieldset. Say you want the `social` fieldset, to be grouped together in Sanity Studio like this:

![Screenshot of a fieldset in Sanity Studio](https://cdn.sanity.io/images/3do82whm/next/dbdd62dcf92d6305853f1d96f8295610a48b8285-2304x1400.png)

Input

```javascript
{
  type: 'object',
  name: 'person',
  fieldsets: [
    {name: 'social', title: 'Social media handles'}
  ],
  fields: [
    {
      title: 'Name',
      name: 'name',
      type: 'string'
    },
    {
      title: 'Twitter',
      name: 'twitter',
      type: 'string',
      fieldset: 'social'
    },
    {
      title: 'Instagram',
      name: 'instagram',
      type: 'string',
      fieldset: 'social'
    },
    {
      title: 'Facebook',
      name: 'facebook',
      type: 'string',
      fieldset: 'social'
    }
  ]
}
```

Response

```json
// Values will still appear at the same level in the data
{
  "name": "Somebody",
  "twitter": "@somebody",
  "instagram": "@somebody",
  "facebook": "somebody"
}
```

Fieldsets takes the same collapsible options as described for objects above, as well as the `hidden` and `readOnly` properties, e.g.:

```javascript
{
  title: 'Social media handles',
  name: 'social',
  hidden: false, // Default value
  readOnly: true,
  options: {
    collapsible: true, // Makes the whole fieldset collapsible
    collapsed: false, // Defines if the fieldset should be collapsed by default or not
    columns: 2, // Defines a grid for the fields and how many columns it should have
    modal: {type: 'popover'} //Makes the modal type a popover
  }
}
```



# Reference

![A GIF of a showing the behaviour of a reference field in Sanity Studio](https://cdn.sanity.io/images/3do82whm/next/61ee34b6a0512c9d187a318eaddea2ffd53af7d6-664x329.gif)

Relations between documents are modeled using the `reference` type. To model a one-to-many relation, store the references in an array.

References can be either *strong* (default) or *weak*. A strong reference will enforce that the document it points to actually exists, and will not allow deletion of a document that any other document refers to. A weak reference allows pointing to documents that may not exist (yet) or may have been deleted.

> [!WARNING]
> Gotcha
> Whether a reference should be strong or weak is configured by setting the weak property on the reference field. Note that merely changing this property won't automatically update reference fields in the data store.

When working in Sanity Studio, the reference input allows you to search for already existing documents, or create and publish new documents of the appropriate type inline from the place of referral. In order to secure referential integrity, the referring document will be blocked from publishing until the new, referenced, document has been published. The exception is if the reference has the property `weak: true`.

> [!TIP]
> Protip
> For a more in-depth discussion on how to think about references in Sanity, we recommend reading the supplementary article Connected Content.





## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Value must be set to reference. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | Required. The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| to * | Required. Must contain an array naming all the types which may be referenced e.g. [{type: 'person'}]. See more examples below. |


#### Properties

| Property | Description |
|----------|-------------|
| weak | Default false. If set to true the reference will be made weak. This allows references to point at documents that may or may not exist, such as a document that has not yet been published or a document that has been deleted (or indeed an entirely imagined document). |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | If set to true, this field will be hidden in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| initialValue | The initial value that will be used when creating new values from this type. Can be either the literal value or a resolver function that returns either the literal value or a promise that resolves to the initial value. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

#### Properties

| Property | Description |
|----------|-------------|
| disableNew | Disables inline creation of new documents from the reference field. Defaults to false. |


#### Properties

| Property | Description |
|----------|-------------|
| filter | Additional GROQ-filter to use when searching for target documents. The filter will be added to the already existing type name clause.

If a function is provided, it is called with an object containing document, parent and parentPath properties as well as a convenient getClient() method, and should return an object containing filter and params. As of v2.4.0 this function can optionally be async and return a Promise that resolves to an object containing filter and params.

Note: The filter only constrains the list of documents returned at the time you search. It does not guarantee that the referenced document will always match the filter provided. |


#### Properties

| Property | Description |
|----------|-------------|
| filterParams | Object of parameters for the GROQ-filter specified in filter. |


## Validation

#### Properties

| Property | Description |
|----------|-------------|
| required() | Ensures that this field exists. |


#### Properties

| Property | Description |
|----------|-------------|
| custom(fn) | Creates a custom validation rule. |


## Default reference

Define the movie's `director` as a reference to a person:

Input

```javascript
{
  name: 'movie',
  type: 'object',
  fields: [
    {
      title: 'Director',
      name: 'director',
      type: 'reference',
      to: [{type: 'person'}]
    }
  ]
}
```

Response

```json
{
  "_type": "reference",
  "_ref": "ffda9bed-b959-4100-abeb-9f1e241e9445" /* This could be the id of Jessica Chastain */
}
```

## Weak reference

Define the screening's `movie` as a weak reference to a movie, thereby allowing the movie to be deleted without deleting the screening first:

Input

```javascript
{
  name: 'screening',
  type: 'document',
  fields: [
    {
      name: 'movie',
      title: 'Movie',
      type: 'reference',
      weak: true,
      to: [{type: 'movie'}],
      description: 'Which movie are we screening'
    },
  ]
}
```

Response

```json
{
  "_type": "reference",
  "_ref": "93f3af18-337a-4df7-a8de-fbaa6609fd0a" /* Movie id */
  "_weak": true
}
```

## Reference  multiple types

The `directors` field is an array which can contain both `person` and `bovinae` (in the rare occasion a cow would direct a movie) references:

Input

```javascript
{
  title: 'Directors',
  name: 'directors',
  type: 'array',
  of: [
    {
      type: 'reference',
      to: [
        {type: 'person'},
        {type: 'bovinae'}
      ]
    }
  ]
}
```

Response

```json
[
  {
    "_type": "reference",
    /* this could be the id of Yvonne, the escaped cow */
    "_ref": "9b711031-3744-47ab-9bb7-1bceb177d0d0"
  },
  {
    "_type": "reference",
    /* this could be the id of Matt Damon */
    "_ref": "ffda9bed-b959-4100-abeb-9f1e241e9445"
  }
]
```

## Additional static filter

If providing a target schema type is not enough to provide a meaningful set of search results, you may want to further constrain the search query:

Input

```javascript
{
  title: 'Director',
  name: 'director',
  type: 'reference',
  to: [{type: 'person'}],
  options: {
    filter: 'role == $role',
    filterParams: {role: 'director'}
  }
}
```

Response

```json
{
  "_type": "reference",
   /* this could be the id of some director */
  "_ref": "9b711031-3744-47ab-9bb7-1bceb177d0d0"
},

```

## Additional dynamic filter

If you want to further constrain the search result, but need properties from the surrounding document or object/array, you can use the function form for `filter`:

Input

```javascript
{
  title: 'Director',
  name: 'director',
  type: 'reference',
  to: [{type: 'person'}],
  options: {
    filter: ({document}) => {
      // Always make sure to check for document properties
      // before attempting to use them
      if (!document.releaseYear) {
        return {
          filter: 'role == $role',
          params: {role: 'director'}
        }
      }
      
      return {
        filter: 'role == $role && birthYear >= $minYear',
        params: {
          role: 'director',
          minYear: document.releaseYear
        }
      }
    }
  }
}
```

Response

```json
{
  "_type": "reference",
   /* this could be the id of some director,
    * born after the movie was released */
  "_ref": "9b711031-3744-47ab-9bb7-1bceb177d0d0"
}
```

## Additional async filter

If you want to constrain your filter based on factors available elsewhere in your content lake, you can specify your filter as an asynchronous function.

Input

```javascript
{ 
  // Somewhat contrived example that will make the reference field accept any document of a valid type except the most recently published
  name: 'personRef',
  type: 'reference',
  to: [{type: 'director'}, {type: 'actor'}, {type: 'producer'}],
  options: {
    filter: async ({getClient}) => {
      const client = getClient({apiVersion: '2023-01-01'})
      const latestPersonId = await client.fetch(
        '*[title in ["director", "actor", "producer"] && _id in path("*")] | order(_createdAt desc) [0]._id'
      )
      return {
        filter: '_id != $latestPersonId',
        params: {latestPersonId: latestPersonId},
      }
    },
  },
}
```

Response

```json
{
  "_type": "reference",
   /* this could be the id of some director, actor, or producer */
  "_ref": "9b711031-3744-47ab-9bb7-1bceb177d0d0"
}
```

## Disable new document creation

If you wish to disable the inline creation of new document from the reference field. This is done by setting the `disableNew` option to `true`. 

```javascript
{
  title: 'Director',
  name: 'director',
  type: 'reference',
  to: [{type: 'person'}],
  options: {
    disableNew: true,
  }
}
```

## Nonexistent reference

Sometimes the reference field may show an error message like `<nonexistent reference>`. This usually happens when creating documents with a client library and can mean one of two things:

- The document with the ID you are referencing does not exist
- The field does not allow references to the document type of the document ID you tried to reference

## Create reference programmatically

If you want to create a reference to another document when using our APIs, you need to know the ID of the document you want to create a reference to. Then you need to add that to an object with the following form: 

```json
{
  _type: 'reference',
  _ref: 'id-of-reference-document'
}
```

 Here's an example using the [Javascript client](/docs/js-client):

```javascript
import {createClient} from '@sanity/client'

export const client = createClient({
  projectId: 'your-project-id',
  dataset: 'your-dataset-name',
  useCdn: true,
  apiVersion: '2023-05-03',
  token: process.env.SANITY_SECRET_TOKEN // Must have write access
})

client.create({
  _type: 'book',
  title: 'Some book title',
  author: {
    _type: 'reference',
    _ref: 'id-of-author-document'
  }
})
.then(result => {
  console.log(`Created book with id: ${result._id}`)
})


```

## Writing GROQ queries for references

References by default are **bi-directional** and can be queried from either side of their relationship. For a movie that has an actors array referencing multiple `person` documents, we can join the person data to the `movie` by dereferencing its data, but we can also query all movies associated with a `person`.

### Join the actor data onto movie data

```groq
*[_type == "movie"] {
  ...,
  "actors": actors[]{
    ...
    person->
  }
}
```

### Get all movies for a person

```groq
*[_type=="person"]{
  name,
  "relatedMovies": *[_type=='movie' && references(^._id)]{ 
  	title,
  	slug,
  	releaseDate
	}
}
```







# Slug

![Screenshot of a slug field from Sanity Studio](https://cdn.sanity.io/images/3do82whm/next/2097cb0ab8784b87b95e4e4f092ebe623af7538d-4608x2800.png)

A slug is a unique string (typically a normalized version of title or other representative string), often used as part of a URL. The input form will render an error message if the current slug field is not unique (see note on uniqueness below).

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Value must be set to slug. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | Required. The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | If set to true, this field will be hidden in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| initialValue | The initial value used when creating new values from this type. Can be either a literal value or a resolver function that returns either a literal value or a promise resolving to the initial value. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

#### Properties

| Property | Description |
|----------|-------------|
| source | The name of the field which the slug value is derived from. If a string is provided, it should match the name of the source field in your schema. If a function is provided, the source function is called with two parameters: doc (object - the current document) and options (object - with parent and parentPath keys for easy access to sibling fields). |


#### Properties

| Property | Description |
|----------|-------------|
| maxLength | Maximum number of characters the slug may contain when generating it from a source (like a title field) with the default slugify function. Defaults to 200. If you include your own slugify function, or manually enter your slug this option will be ignored. |


#### Properties

| Property | Description |
|----------|-------------|
| slugify | Supply a custom override function which handles string normalization. slugify is called with three parameters: input (string), type (object - schema type) and context (object). If slugify is set, the maxLength option is ignored. |


#### Properties

| Property | Description |
|----------|-------------|
| isUnique | Supply a custom function which checks whether or not the slug is unique. Receives the proposed slug as the first argument and an options object. |


## Validation

#### Properties

| Property | Description |
|----------|-------------|
| required() | Ensures that this field exists. |


#### Properties

| Property | Description |
|----------|-------------|
| custom(fn) | Creates a custom validation rule. |


By *default*, the slug input will check for uniqueness based on the document type and the path to the slug field. For instance, a document of type `article` and a document of type `product` can have the same slug. You can customize this behavior by defining an `isUnique` function in the field options.

The value of the slug field is stored on the `current` property.

Input

```javascript
{
  title: 'Slug',
  name: 'slug',
  type: 'slug',
  options: {
    source: 'title',
    maxLength: 200, // will be ignored if slugify is set
    slugify: input => input
                         .toLowerCase()
                         .replace(/\s+/g, '-')
                         .slice(0, 200)
  }
}
```

Response

```json
{
  "_type": "slug",
  "current": "this-is-the-title"
}
```

#### Custom slugify function

```javascript
import slugify from 'some-off-the-shelf-slugifier'

async function myAsyncSlugifier(input, schemaType, context) {
  const slug = slugify(input)
  const {getClient} = context
  const client = getClient({apiVersion: '2022-12-07'})
  const query = 'count(*[_type=="movie" && slug.current == $slug]{_id})'
  const params = {slug: slug}
  return client.fetch(query, params).then((count) => {
    console.log('Movies with identical slug', count)
    return `${slug}-${count + 1}`
  })
  return slug
}

//…
// schema field
{
  title: 'Slug',
  name: 'slug',
  type: 'slug',
  options: {
    source: 'title',
    slugify: myAsyncSlugifier
  }
}
```

#### Custom isUnique function

By default the `isUnique` function checks for uniqueness across **all documents of the same type**. Here's an example of an `isUnique` function that checks for uniqueness across **all documents in your dataset**:

```javascript
// /lib/isUniqueAcrossAllDocuments.js

// Note: this assumes that every document that has a slug field
// have it on the `slug` field at the root
export async function isUniqueAcrossAllDocuments(slug, context) {
  const {document, getClient} = context
  const client = getClient({apiVersion: '2022-12-07'})
  const id = document._id.replace(/^drafts\./, '')
  const params = {
    draft: `drafts.${id}`,
    published: id,
    slug,
  }
  const query = `!defined(*[!(_id in [$draft, $published]) && slug.current == $slug][0]._id)`
  const result = await client.fetch(query, params)
  return result
}
```

```javascript
// post.js
import {isUniqueAcrossAllDocuments} from '../lib/isUniqueAcrossAllDocuments'

export default {
  name: 'post',
  type: 'document',
  title: 'Post',
  fields: [
    {
      name: 'title',
      type: 'string',
      title: 'Title'
    },
    {
      name: 'slug',
      type: 'slug',
      title: 'Slug',
      options: {
        isUnique: isUniqueAcrossAllDocuments
      }
    }
  ]
}
```

#### Custom source function

It's also possible to provide the source as a function, that will be called with a first argument containing the whole document, and a second containing a context object.

```javascript
{
  title: 'Slug',
  name: 'slug',
  type: 'slug',
  options: {
    // include category if dataset is production
    source: (doc, context) => context.dataset === 'production' ? `${doc.category}-${doc.title}` : doc.title
  }
}
```

The source function also receives an `options` object containing the parent object/array, if any. It can be useful if you want to derive the slug from a sibling field instead of a property on the document root:

```javascript
{
  title: 'Slug',
  name: 'slug',
  type: 'slug',
  options: {
    source: (doc, context) => context.parent.title
  }
}
```

To query for a document with a given slug, make sure you put the constraint on the `current` key:

```groq
*[_type == "your-document-type" && slugFieldName.current == "your-slug"]
```



# Span

A `span` is a text range within a `block`. It is a child of the `block` type’s children.

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | The value must be set to span. |


## Options

## Validation

The span type is created automatically by the rich text editor as part of the [block type](/docs/block-type). It’s not something you will define yourself. 

When you configure [decorators and annotations](/docs/studio/customizing-the-portable-text-editor) for the rich editor, these will be stored as values in a span’s marks. Decorators as simple text strings, and annotations as keys that reference entries in the block’s mark definitions (`markDefs`). In the example below, there are marks in the third span with the values `strong` and `cbe9d12c6af9`. Most presentation layers will represent the `strong` as in a bold typeface. The other value is a key that corresponds to an object entry under `markDefs` that describes a link.

```json
{
  "_key": "9d2d1ed68d84",
  "_type": "block",
  "children": [
    {
      "_type": "span",
      "marks": [],
      "text": "I am "
    },
    {
      "_type": "span",
      "marks": [
        "strong"
      ],
      "text": "strong and "
    },
    {
      "_type": "span",
      "marks": [
        "strong",
        "cbe9d12c6af9"
      ],
      "text": "annotated"
    },
    {
      "_type": "span",
      "marks": [],
      "text": ""
    }
  ],
  "markDefs": [
    {
      "_key": "cbe9d12c6af9",
      "_type": "link",
      "href": "https://www.google.com/?q=annotation"
    }
  ],
  "style": "normal"
}
```

## Render Spans on the frontend

Spans and Blocks are defined in the Portable Text specification for data storage. They provide insight into how a frontend might use the data provided. If you want to see how to render custom annotations and decorators, see this guide on [Presenting Portable Text](/docs/developer-guides/presenting-block-text).



# String

![Screenshot from Sanity Studio of a string field](https://cdn.sanity.io/images/3do82whm/next/a6c032005fefd5fdfc0f5e177ea5659819dc1971-4608x2800.png)

Short string. Typically used for titles, names, and labels. If you need a basic multi-line string input, use the [text](/docs/text-type). If you need text with markup and structured data, use [block](/docs/block-type).

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Required. Value must be set to string. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | Required. The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | If set to true, this field will be hidden in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| initialValue | The initial value used when creating new values from this type. Can be either a literal string value or a resolver function that returns either a literal string value or a promise resolving to the initial string value. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

#### Properties

| Property | Description |
|----------|-------------|
| list | A list of predefined values that the user can choose from. The array can either include string values ['sci-fi', 'western'] or objects [{title: 'Sci-Fi', value: 'sci-fi'}, ...].

String values will automatically be made uppercase in the Studio. To prevent this, use object values instead. |


#### Properties

| Property | Description |
|----------|-------------|
| layout | Controls how the items defined in the list option are presented. If set to 'radio' the list will render radio buttons. If set to 'dropdown' you'll get a dropdown menu instead. Default is dropdown. |


#### Properties

| Property | Description |
|----------|-------------|
| direction | Controls how radio buttons are lined up. Use direction: 'horizontal\|vertical' to render radio buttons in a row or a column. Default is vertical. Will only take effect if the layout option is set to radio. |


## Validation

#### Properties

| Property | Description |
|----------|-------------|
| required() | Ensures that this field exists. |


#### Properties

| Property | Description |
|----------|-------------|
| min(minLength) | Minimum length of string. |


#### Properties

| Property | Description |
|----------|-------------|
| max(maxLength) | Maximum length of string. |


#### Properties

| Property | Description |
|----------|-------------|
| length(exactLength) | Exact length of string. |


#### Properties

| Property | Description |
|----------|-------------|
| uppercase() | All characters must be uppercase. |


#### Properties

| Property | Description |
|----------|-------------|
| lowercase() | All characters must be lowercase. |


#### Properties

| Property | Description |
|----------|-------------|
| email() | Value must be a valid email-address. |


#### Properties

| Property | Description |
|----------|-------------|
| regex(pattern[, options]) | String must match the given pattern.

options is an optional object, currently you can set options.name and options.invert.

Providing a name will make the message more understandable to the user ("Does not match the <name>-pattern").

Set invert to true in order to allow any value that does NOT match the pattern. |


#### Properties

| Property | Description |
|----------|-------------|
| custom(fn) | Creates a custom validation rule. |


## Examples

### Field configuration

```javascript
{
  title: 'Title',
  name: 'title',
  type: 'string',
  description: 'Make it catchy',
  validation: Rule => Rule.max(120).warning(`A title shouldn't be more than 120 characters.`)
}
```

### List of predefined strings

Input

```javascript
{
  title: 'Genre',
  name: 'genre',
  type: 'string',
  options: {
    list: [
      {title: 'Sci-Fi', value: 'sci-fi'},
      {title: 'Western', value: 'western'}
    ], // <-- predefined values
    layout: 'radio' // <-- defaults to 'dropdown'
  }
}
```

Response

```json
{
  "_type": "movie",
  "_id": "23407q-qwerqyt12",
  "genre": "sci-fi",
  ...
}
```

> [!TIP]
> Protip
> Want to make a multi-select for strings? Check out the Array schema type to see how you can build an array of strings, references, objects, and more. 

For details on how to access the `title` value of a list in your document list preview, please see the documentation on [previewing from predefined string lists](https://www.sanity.io/docs/previews-list-views#dbe115bb1388).

### Setting initial value for string fields

You can use [initial values](/guides/getting-started-with-initial-values-for-new-documents) to preset string fields on document creation:

```javascript
export default {
  name: 'post',
  type: 'document',
  title: 'Post',
  initialValue: {
    title: 'The initial title'
  },
  fields: [
    {
      name: 'title',
      type: 'string',
      title: 'Title'
    }
  ]
}
```



# Text

A basic string expected to contain multiple lines. Typically used for a summary, short bio etc. If you need text with markup and structured data, use [block text](/docs/block-type).

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Value must be set to text. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | Required. The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| rows | Controls the number of rows/lines in the rendered textarea. Default number of rows: 10. |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | If set to true, this field will be hidden in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| initialValue | The initial value used when creating new values from this type. Can be either a literal string value or a resolver function that returns either a literal string value or a promise resolving to the string initial value. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

## Validation

#### Properties

| Property | Description |
|----------|-------------|
| required() | Ensures that this field exists. |


#### Properties

| Property | Description |
|----------|-------------|
| min(minLength) | Minimum length of string. |


#### Properties

| Property | Description |
|----------|-------------|
| max(maxLength) | Maximum length of string. |


#### Properties

| Property | Description |
|----------|-------------|
| length(exactLength) | Exact length of string. |


#### Properties

| Property | Description |
|----------|-------------|
| uppercase() | All characters must be uppercase. |


#### Properties

| Property | Description |
|----------|-------------|
| lowercase() | All characters must be lowercase. |


#### Properties

| Property | Description |
|----------|-------------|
| email() | Value must be a valid email-address. |


#### Properties

| Property | Description |
|----------|-------------|
| regex(pattern[, options]) | String must match the given pattern.

options is an optional object, currently you can set options.name and options.invert.

Providing a name will make the message more understandable to the user ("Does not match the <name>-pattern").

Set invert to true in order to allow any value that does NOT match the pattern. |


#### Properties

| Property | Description |
|----------|-------------|
| custom(fn) | Creates a custom validation rule. |


Input

```javascript
{
  title: 'Description',
  name: 'description',
  type: 'text'
}
```

Response

```json
{
  "_type": "movie",
  "_id": "23407q-qwerqyt12",
  "description": "...rather long text here....\n  yes.. long",
  ...
}
```



# URL

A string which represents a URL.

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| type * | Value must be set to url. |


#### Properties

| Property | Description |
|----------|-------------|
| name * | Required. The field name. This will be the key in the data record. |


#### Properties

| Property | Description |
|----------|-------------|
| title | Human readable label for the field. |


#### Properties

| Property | Description |
|----------|-------------|
| hidden | If set to true, this field will be hidden in the studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| readOnly | If set to true, this field will not be editable in the content studio. You can also return a callback function to use it as a conditional field. |


#### Properties

| Property | Description |
|----------|-------------|
| description | Short description to editors how the field is to be used. |


#### Properties

| Property | Description |
|----------|-------------|
| initialValue | The initial value that will be used when using this type to create new values. Can be either the literal value or a resolver function that returns either the literal value or a promise that resolves to the initial value. |


#### Properties

| Property | Description |
|----------|-------------|
| deprecated | Marks a field or document type as deprecated in the studio interface and displays a user-defined message defined by the single required reason property.

If you deploy a GraphQL API schema, this property will translated into the @deprecated directive. |


## Options

## Validation

#### Properties

| Property | Description |
|----------|-------------|
| required() | Ensures that this field exists. |


#### Properties

| Property | Description |
|----------|-------------|
| uri(options) | scheme - String, RegExp or Array of schemes to allow (default: ['http', 'https']).

allowRelative - Whether or not to allow relative URLs (default: false).

relativeOnly - Whether to only allow relative URLs (default: false). |


#### Properties

| Property | Description |
|----------|-------------|
| custom(fn) | Creates a custom validation rule. |


The URL type is basically just a string input, but the rendered HTML input field will have the `type` attribute set to `url`, like so:

```html
<input type="url">
```

Input

```javascript
{
  title: 'Image URL',
  name: 'imageUrl',
  type: 'url'
}
```

Response

```json
{"imageUrl": "https://example.com/img.jpg"}
```



To allow more protocols than http/https, you can specify validation options:

```javascript
{
  title: 'Link',
  name: 'href',
  type: 'url',
  validation: Rule => Rule.uri({
    scheme: ['http', 'https', 'mailto', 'tel']
  })
}
```



# Asset Source

The form API includes options for working with assets. The `file` and `image` properties will both let you add to or override the list of available asset sources for their respective form inputs, as well as enable or disable direct uploads.

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| assetSources | Accepts either a static array of asset source definitions or a callback function that returns the same. The callback is called with the current list of active asset sources as its first argument and a context object as the second. |
| directUploads | Whether or not to allow direct uploading of images/files. Defaults to true. |


## Example 

```javascript
import {defineConfig} from 'sanity'
import {unsplashAssetSource} from 'sanity-plugin-asset-source-unsplash'
import {customSource} from './src/custom-asset-source'

export default defineConfig({
  // ...rest of config
  form: {
    image: {
      assetSources: (prev) => [...prev, unsplashAssetSource],
    },
    file: {
      assetSources: [customSource],
      directUploads: false,
    },
  },
})
```

## Context properties

These are the properties provided in the context object when defining asset sources using the callback function.

#### Properties

| Property | Description |
|----------|-------------|
| dataset | Name of the current dataset |
| projectId | Unique ID for the project |
| schema | The schema registry of your project. Use `schema.get("schemaTypeName") to retrieve any schema by name. |
| currentUser | An object with info about the currently logged in user. |
| getClient |  |


[Read more about asset source plugins](/docs/studio/custom-asset-sources)



## Asset source properties

#### Properties

| Property | Description |
|----------|-------------|
| name * | Unique identifier for the asset source |
| title * | Human-readable name for the asset source |
| component * | A component that will let users browse and select images or files |
| icon | An icon for the asset source |


## Asset source selection component props

#### Properties

| Property | Description |
|----------|-------------|
| selectionType * | If the opening interface selection type is 'single' or 'multiple'. |
| selectedAssets * | An array of Sanity assets if they are selected in the opening interface. These are Sanity asset documents. |
| onSelect(Asset[]) * | When assets are selected and returned to props.onSelect, the Studio will make sure to upload the asset(s). If the selected asset is uploaded previously, the existing asset document and file will be used instead. |
| onClose * | The component must call props.onClose if the select action is canceled or closed somehow. |
| dialogHeaderTitle | A component that serves as the header element for the dialog window. |
| assetType | Either file or image |




# Configuration

[Introduction to Studio configuration](/docs/studio/configuration)



## Workspaces

The root [configuration](https://www.sanity.io/docs/reference/api/sanity/SanityFormConfig) of your studio is created by supplying either a single workspace configuration object or an array of the same type to the `defineConfig`-function, and returning the result as the default export of the configuration file – typically found at the root of your project in a file named `sanity.config.js|ts`.

```javascript
// The absolute minimum viable studio configuration
import { defineConfig } from 'sanity'

export default defineConfig({
  projectId: '<project-id>',
  dataset: 'production',
})
```

## Properties

The following table shows all the top-level properties available for configuring and customizing a single workspace studio.

#### Properties

| Property | Description |
|----------|-------------|
| projectId * | The ID of the Sanity project to use for the studio |
| dataset * | The name of the dataset to use for the studio |
| auth | Lets you implement custom authentication by providing a configuration object. Read more about configuring auth providers. |
| document | Accepts custom components for document actions and badges, as well as a custom productionUrl resolver and default configuration for new documents. Read more about the document API. |
| form | Extensions / customizations to the studio forms. Accepts configurations for image and file asset sources as well as custom components to override the default studio rendering. Read more about the form API. |
| plugins | Studio plugins - takes an array of plugin declarations that can be called with or without a configuration object. Read more about plugins. |
| tools | Studio tools – takes an array of tool declarations that can be called with or without a configuration object. Read more about the tool API. |
| schema | Schema definition - takes an array of types and an optional array of templates (initial value templates). While defining a schema is not required, there are few things inside the studio that works without one. Read more about the schema API. |
| studio | Accepts a components object which will let you override the default rendering of certain bits of the studio UI. Read more about studio components. |
| theme | Accepts a theme configuration object. Read more about theming. |
| i18n | Accepts a config object for localizing the studio UI. Read more about studio localization. |


## Additional properties for multiple workspace-configurations

#### Properties

| Property | Description |
|----------|-------------|
| name * | Name of the workspace - by convention in lowercase/camelCase |
| basePath * | URL base path to use, for instance /myWorkspace |
| title * | Title of the workspace |
| subtitle | Subtitle to show under the name of the workspace |
| icon | React component to use as icon for this workspace |


## Examples

### Minimal example

```javascript
// A more plausible minmalist configuration
import { defineConfig } from 'sanity'
import { structureTool } from 'sanity/structure'
import { schemaTypes } from './schemas'


export default defineConfig({
  title: 'My cool project',
  projectId: '<project-id>',
  dataset: 'production',
  plugins: [structureTool()],
  schema: {
    types: schemaTypes,
  },
})
```

### Multiple workspace example



```javascript
import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'
import {visionTool} from '@sanity/vision'
import {LaunchIcon, RobotIcon} from '@sanity/icons'
import {schemaTypes} from './schemas'

export default defineConfig([
  {
    name: 'my-prod-space',
    title: 'My production workspace',
    basePath: '/production',
    icon: LaunchIcon,
    projectId: '<project-id>',
    dataset: 'production',
    plugins: [structureTool()],
    schema: {
      types: schemaTypes,
    },
  },
  {
    name: 'my-staging-space',
    title: 'My staging workspace',
    basePath: '/staging',
    subtitle: 'The world is a stage',
    icon: RobotIcon,
    projectId: '<project-id>',
    dataset: 'staging',
    plugins: [structureTool(), visionTool({defaultApiVersion: '2022-10-21'})],
    schema: {
      types: schemaTypes,
    },
  },
])

```





# Document

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| actions | Accepts an array of document action components, or a callback function that resolves to the same. The callback function receives the existing actions array as its first argument and a context object as its second. Read more about document actions. |
| badges | Accepts an array of document badge components, or a callback function that resolves to the same. The callback function receives the existing badges array as its first argument and a context object as its second. Read more about document badges. |
| productionUrl | Constructs a production URL for previews or other purposes. Accepts a static string or a more helpful callback function called with the existing value as the first argument and a context object as the second.

If specified, an "Open preview" option in the document context menu of your studio. |
| newDocumentOptions | Accepts a callback function that returns an array of new document options templates. The callback is called with the array of existing templates and a context object as arguments. Read more about new document options. |






# Document Badges

A document badge is a small UI component that indicates the status of a document. It currently appears in the Studio next to the toolbar actions. The default set of document badges currently shows `draft` and `published` status.

![](https://cdn.sanity.io/images/3do82whm/next/88aaa1f0926b0209f181c2c4653514e8d2a38ec0-2304x1400.png)

[Introduction to using document badges →](/docs/studio/custom-document-badges)

[Learn how to use document badges when building custom workflows →](/docs/studio/document-actions)

## Document badge properties

These are the properties returned to a badge component.

#### Properties

| Property | Description |
|----------|-------------|
| id | An id for the current document (e.g. the id of the published document) |
| type | The schema type of the current document. |
| draft | Returns the draft document (a document with unpublished changes) if any. Returns null if there is no draft document. |
| published | The version of the document that is currently published, if available. Returns null if the document isn't published. |


## Document badge description

These are the properties a badge description object must follow.

#### Properties

| Property | Description |
|----------|-------------|
| title | Title of the badge. This will be displayed when hovering the badge. |
| label | The label that the badge will display. |
| color | The color for the badge. Can be one of the following values: primary, warning, success, danger |


## Example

```javascript
export function HelloWorldBadge(props) {
  return {
    label: 'Hello world',
		title: 'Hello I am a custom document badge',
    color: "success"
  }
} 
```

[See a complete example of implementing custom badges →](/docs/studio/custom-document-badges)



# Document Actions

You can use the Document Actions API for Sanity Studio to customize and control operations that can be done to documents. When you create a custom action, it will be available in the actions menu in the document editor. You create custom actions by adding a *Document action component* to the `document.actions` array of your workspace configuration.

[Learn how to create custom workflows with the Document Actions API](/docs/studio/document-actions).

![The action bar with a badge, an action button, and the action menu](https://cdn.sanity.io/images/3do82whm/next/250a4fc9d947827de6e0e1c02777fdec2c2b6908-2304x1400.png)

`document.actions` accepts either a static array of document action components or a callback function returning the same. When supplied with a static array, Sanity Studio will append your actions to the list of already existing actions.

> [!TIP]
> Protip
> Sanity Studio comes with a set of predefined document actions enabled that are helpful for manipulating documents. These are:
> 
> Publish
> 
> Unpublish
> 
> Delete
> 
> Duplicate
> 
> Discard changes
> 
> Restore to history state
> 
> You are free to swap any or all of these out with your own custom actions, or conditionally disable or enable them in your studio configuration.

```javascript
import {CustomAction} from './actions'

export default defineConfig({
  // ...rest of config
  document: {
    actions: [CustomAction],
  },
})
```

In contrast, when using the callback method, you will need to make sure you return the exact set of actions you want to register. Helpfully, the callback function receives the current array of registered action components as its first argument and a context object as its second and final argument. 

```javascript
import {HelloWorldAction} from './actions'

export default defineConfig({
  // ... rest of config
  document: {
    actions: (prev, context) => {
      // Only add the action for documents of type "movie"
      // for other types return the current array of actions as is
      return context.schemaType === 'movie' ? [HelloWorldAction, ...prev] : prev;
    },
  },
})
```

### Callback context properties

#### Properties

| Property | Description |
|----------|-------------|
| currentUser | An object containing information about the currently logged in user |
| schemaType | Schema type of the current document |
| dataset | Name of the dataset |
| projectId | Unique ID of the project |
| getClient | Returns a configured SanityClient |
| documentId | ID of the document |
| schema | The schema registry of your project. Use `schema.get("schemaTypeName") to retrieve any schema by name. |


### Example

```javascript
document: {
    actions: function (prev, context) {
      console.log('context: ', context);
      return prev.map((originalAction) => (originalAction.action === 'publish' ? HelloWorldAction : originalAction));
    }
  },
```

## Document Action components

This table describes the values a document action component receives as properties:

### Properties

#### Properties

| Property | Description |
|----------|-------------|
| id | The current document’s id. |
| type | The schema type of the current document. |
| draft | The draft document (e.g. unpublished changes) if any. 

Returns null if there are no unpublished changes. |
| published | The version of the document that is currently published (if any).

Returns null if the document isn't published. |
| onComplete | A callback function that the action components must call to signal that the action has completed |
| liveEdit | Whether the document is published continuously (live) or not. liveEdit-enabled documents skip the draft workflow. This is not to be confused with the Live Content API, which handles how published changes are handled by queries. |


## Document Action description

Every Document Action component must return either `null` or an action description object. An action description describes the action state that can be used to render action components in different render contexts (e.g. in a toolbar, as a menu item, etc.). This table describes the different properties of an action description object.

#### Properties

| Property | Description |
|----------|-------------|
| label * | This is the action label. If the action is displayed as a button, this is typically what becomes the button label. |
| onHandle * | This allows the action component to specify a function that gets called when the user wants the action to happen (e.g. the user clicked the button or pressed the keyboard shortcut combination). The implementation of the onHandle must either make sure to start the dialog flow or to execute the operation immediately. |
| icon | In render contexts where it makes sense to display an icon, this will appear as the icon for the action. Default is null |
| disabled | This tells the render context whether to disable this action. Default is false. |
| shortcut | A keyboard shortcut that should trigger the action. The keyboard shortcut must be compatible with the format supported by the is-hotkey-package. |
| title | A title for the action. Depending on the render context this will be used as tooltip title (e.g. for buttons it may be passed as the title attribute). Default is null. |
| dialog | If this is returned, its value will be turned into a dialog by the render context. More about dialog types below. Default is null. |
| group | Allow users to specify whether a specific document action should appear in the footer ("default")  or in the document's context menu ("paneActions"). |
| tone | Allows changing the tone of the action when displayed. |


## Document Action dialog types

Dialogs can notify and inform users about the outcome of an action, or they can collect confirmation before executing the action. You can define the following dialog types:

- [confirm](#ef8f04ebc9f1)
- [popover](#6f849687ff57)
- [dialog](#037f877ad3f1)
- [custom](#3d31280433b7)

![Screenshots of dialog types from Sanity Studio](https://cdn.sanity.io/images/3do82whm/next/4f7e7b70f92b7b39586c09cb9dea49301359cff9-2304x1400.png)

### `confirm`

This tells the render context to display a confirm dialog.

#### Properties

#### Properties

| Property | Description |
|----------|-------------|
| type | Must be confirm. |
| color | Support the following values warning, success, danger, info. |
| message | The message that will be shown in the dialog. |
| onConfirm | A function to execute when the the user confirms the dialog. |
| onCancel | A function to execute when the user cancels the dialog. |


#### Example

```javascript
export function ConfirmDialogAction({onComplete}) {
  const [dialogOpen, setDialogOpen] = React.useState(false)
  return {
    label: 'Show confirm',
    onHandle: () => {
      setDialogOpen(true)
    },
    dialog: dialogOpen && {
      type: 'confirm',
      onCancel: onComplete,
      onConfirm: () => {
        alert('You confirmed!')
        onComplete()
      },
      message: 'Please confirm!'
    }
  }
}
```

### `popover`

This will display the value specified by the `content` property in a popover dialog. The `onClose` property is required, and will normally be triggered by click outside or closing the popover.

#### Properties

| Property | Description |
|----------|-------------|
| onClose * | A function to execute when the dialog is closed. |
| type | Must be popover. |
| content | The content to be shown in the popover dialog. |


#### Example

```javascript
export function PopoverDialogAction({onComplete}) {
  const [dialogOpen, setDialogOpen] = React.useState(false)
  return {
    label: 'Show popover',
    onHandle: () => {
      setDialogOpen(true)
    },
    dialog: dialogOpen && {
      type: 'popover',
      onClose: onComplete,
      content: "👋 I'm a popover!"
    }
  }
}
```

### `dialog`

This will display the value specified by the `content` property in a dialog window. The `onClose` property is required.

#### Properties

| Property | Description |
|----------|-------------|
| onClose * | A function to execute when the user closes the dialog. |
| type | Must be dialog. |
| header | Text to show in the header field of the dialog. |
| content | The content to show in the dialog. |
| footer | Text to show in the footer field of the dialog. |


#### **Example**

```javascript
export function ConfirmDialogAction({onComplete}) {
  const [dialogOpen, setDialogOpen] = React.useState(false)
  return {
    label: 'Show confirm',
    onHandle: () => {
      setDialogOpen(true)
    },
    dialog: dialogOpen && {
      type: 'dialog',
      onClose: onComplete,
      content: <div>
        <h3>👋 ... and I'm a dialog</h3>
        <img src="https://source.unsplash.com/1600x900/?cat" style={{width: '100%'}}/>
        <p>
          I'm suitable for longer and more diverse forms of content.
        </p>
      </div>
    }
  }
}
```

### `custom`

This will display the value specified by the `component` property in a custom dialog window. The `onClose` property is required.

#### Properties

| Property | Description |
|----------|-------------|
| onClose * | A function to execute when the user closes the dialog. |
| type | Must be custom. |
| component | The content to show in the dialog. Pass a React component with the custom properties you want to render in the custom modal. |


#### **Example**

```javascript
import {Button, Card, Dialog, Stack, Text} from '@sanity/ui'

export function CustomDialogAction({onComplete}) {
  const [dialogOpen, setDialogOpen] = React.useState(false)
  return {
    label: 'Custom modal',
    tone: 'primary',
    onHandle: toggleOpen,
    dialog: {
      type: 'custom',
      component: open && (
        <Dialog
          header="Custom action component"
          id="custom-modal"
          onClickOutside={toggleOpen}
          onClose={toggleOpen}
          width={1}
          footer={
            <Stack padding={2}>
              <Button onClick={toggleOpen} text="Close" />
            </Stack>
          }
        >
          <Card padding={5}>
            <Text>This dialog is rendered using a custom dialog component.
            </Text>
          </Card>
        </Dialog>
      ),
    }
  }
}
```



# Form



#### Properties

| Property | Description |
|----------|-------------|
| components | Accepts custom component overrides for the following form components: input, field, preview, and item. The components can be declared in the root studio configuration, in plugins, or directly in a schema definition.

Form components API -> |
| file | Accepts an object with the following properties: assetSources and directUploads.

assetSources accepts an array of valid asset source configuration objects, or a callback function resolving to the same. The callback function is called with the current list of registered asset sources as its first argument and a context object as the second.

directUploads accepts a boolean true or false. |
| image | Accepts an object with the following properties: assetSources and directUploads.

assetSources accepts an array of valid asset source configuration objects, or a callback function resolving to the same. The callback function is called with the current list of registered asset sources as its first argument and a context object as the second.

directUploads accepts a boolean true or false. |




# Form Components

The following components are available for customization:

```javascript
// ./sanity.config.js

export default defineConfig({
  // rest of config ...
  form: {
    components: {
      field: MyCustomField,
      input: MyCustomInput,
      item: MyCustomItem,
      preview: MyCustomPreview,
    },
  },
})
```

For a description of how these different components map to the different parts of a form field, visit the [Form Components article](/docs/studio/form-components).

Custom form components can be declared either at the configuration level, i.e. either in `defineConfig` or `definePlugin`, or in a schema. Components added at configuration level will affect all forms in the studio while components added to a schema will only affect the field or fields specified in that schema.

```javascript
// ./schemas/myDocument.jsx

import {defineType} from 'sanity'

function MyStringInput(props) {
  return (
    <div style={{border: '4px solid magenta'}}>
      {props.renderDefault(props)}
    </div>
  )
}

const myDocument = defineType({
  name: 'myDocument',
  type: 'myDocument',
  title: 'My document',
  fields: [
    {
      name: 'myTitle',
      type: 'string',
      title: 'My title',
      components: {input: MyStringInput},
    },
  ],
})
```

## Shared Properties

#### Properties

| Property | Description |
|----------|-------------|
| changed |  |
| level |  |
| path |  |
| presence |  |
| renderDefault |  |
| schemaType |  |
| validation |  |
| value |  |


All form components receive the `renderDefault` method which will defer to the default studio rendering of the component when called with the component's props. 

#### Properties

| Property | Description |
|----------|-------------|
| renderDefault | A callback function that renders the default layout component. The function takes the component's properties as an argument, and these properties can be modified. |


In addition, each form component receives a set of props that varies in shape depending on the type of field they are assigned to. 

## Input components

In addition to the shared properties (above), input components have the following:

### Properties

#### Properties

| Property | Description |
|----------|-------------|
| elementProps |  |
| focused |  |
| id |  |
| onChange |  |
| path |  |
| readOnly |  |
| validationError |  |


## Array item components

In addition to the shared properties (above), array item components have the following:

### Properties

#### Properties

| Property | Description |
|----------|-------------|
| id |  |
| elementProps |  |
| focused |  |
| focusPath |  |
| members |  |
| onChange |  |
| onItemExpand |  |
| onItemCollapse |  |
| onItemClose |  |
| onItemOpen |  |
| onInsert |  |
| onItemMove |  |
| onItemRemove |  |
| onItemAppend |  |
| onItemPrepend |  |
| onPathFocus |  |
| onUpload |  |
| resolveInitialValue |  |
| resolveUploader |  |
| renderInput |  |
| renderField |  |
| renderItem |  |
| renderPreview |  |


## Field components

In addition to the shared properties (above), field components have the following:

### Properties

#### Properties

| Property | Description |
|----------|-------------|
| children |  |
| description |  |
| index |  |
| inputId |  |
| inputProps |  |
| name |  |
| presence |  |
| title |  |
| validation |  |


## List preview components

In addition to the shared properties (above), list preview components have the following:

### Properties

#### Properties

| Property | Description |
|----------|-------------|
| actions |  |
| error |  |
| isPlaceholder |  |
| layout |  |
| media |  |
| title |  |
| _type |  |






# Hooks

## useClient

| Method | Description |
|--------|-------------|
| useClient(clientOptions): SanityClient | Returns an instance of SanityClient configured with the current project and dataset. Should be provided a configuration object specifying which API version to use for queries. Perspectives can be set by adding .withConfig({perspective: 'raw'}) to the client config. |


```javascript
import { useClient } from 'sanity'

export function MyComponent() {
  const [types, setTypes] = useState(undefined)
	const client = useClient({ apiVersion: '2023-01-01' }).withConfig({ perspective: 'raw'})
  
  useEffect(() => {
    async function fetchTypes() {
      const res = await client.fetch(`array::unique(*[]._type)`)
      setTypes(res)
    }
    if (!types) fetchTypes();
  }, [])

	return (
		<div>
			<h1>Types in project</h1>
				<ul>	
					{types && types.map(type => (
						<li key={type}>{type}</li>
					)}
				</ul>
		</div>
	)
}
```

## useDataset

| Method | Description |
|--------|-------------|
| useDataset(): string | Returns the name of the current dataset |


```javascript
import { useDataset } from 'sanity'

export function MyComponent() {
	const dataset = useDataset()
	return (
		{dataset === 'production' ? <ProductionComponent /> : <StagingComponent />}
	)
}
```

## useProjectId

| Method | Description |
|--------|-------------|
| useProjectId(): string | Returns the current project ID |


```javascript
import { useProjectId } from 'sanity'

export function MyComponent() {
	const pid = useProjectId()
	return (
			<h1>Project ID: {pid}</h1>
	)
}
```

## useFormValue

| Method | Description |
|--------|-------------|
| useFormValue(path): unknown | Returns the value of the field specified by path. Paths are built using array notation with segments that can be either strings representing field names, index integers for arrays with simple values, or objects with a _key for arrays containing objects. |


```javascript
import { useFormValue } from 'sanity'

export function MyComponent() {
	// ⬇ get value of field 'name' in object 'author'
  const authorName = useFormValue(['author', 'name'])
	// ⬇ get value of the second item in array 'tags' of type 'string'
	const secondTag = useFormValue(['tags', 1])
	// ⬇ get value of the reference with the matching key in an array of references
	const specificBook = useFormValue([ 'bibliography', {_key: '<key>'} ])

  return (
		<div>Author: {authorName}</div>
	)
}
```

## useSchema

| Method | Description |
|--------|-------------|
| useSchema(): Schema | Returns the schema registry for the current project |


```javascript
import { useSchema } from 'sanity'

export function MyComponent() {
  const [selectedSchema, setSelectedSchema] = useState(undefined)
	
	// ⬇ the returned value contains the complete catalog of schemas in
	// the project, as well as some neat methods for interacting with them
  const schema = useSchema()

  // ⬇ returns an array of all type names in project
  const types = schema.getTypeNames()

  const handleSelect = (type) => {
		// ⬇ contrived example to show usage of 
		// both schema.has() and schema.get()
		if(schema.has(type)) {
			setSelectedSchema(schema.get(type))
		} else {
			setSelectedSchema(undefined)
		}
  }
  // ⬇ list all types in project and display schema for selected type
  return (
    <Container>
      <Card>
        {types.map((type) => (
          <button key={type} onClick={() => handleSelect(type)}>
            {type}
          </button>
        ))}
      </Card>
      <Card>
				{selectedSchema && (
					<pre>{JSON.stringify(selectedSchema, null, 2)}</pre>
				)}
			</Card>
    </Container>
  )
}
```

## useTemplates

| Method | Description |
|--------|-------------|
| useTemplates(): Template[] | Returns an array of initial value templates available in the project. Note that all document types have an initial value template associated that sets the value of _type, even if no templates have been configured by user. |


```javascript
import { useTemplates } from 'sanity'

export function MyComponent() {
  const templates = useTemplates()

  return (
    <ul>
      {templates.map((template) => (
        <li key={template.id}>
          <h1>{template.title}</h1>
          <h2>Type: {template.schemaType}</h2>
        </li>
      ))}
    </ul>
  )
}
```

## useTools

| Method | Description |
|--------|-------------|
| useTools():  Tool[] | Returns an array listing all installed tools |


```javascript
import { useTools } from 'sanity'

export function MyComponent() {
  const tools = useTools();
  
	return (
		<div>
			<h1>Studio Tools</h1>
		  <ul>
				{tools.map(tool => <li key={tool.name}>{tool.title}</li>)}
			</ul>
		</div>
		)
}
```

## useWorkspace

| Method | Description |
|--------|-------------|
| useWorkspace(): Workspace | Returns the current workspace configuration |


```javascript
import { useWorkspace } from 'sanity'
			
export function MyComponent() {
  const { currentUser: { name }, dataset } = useWorkspace();
	return (
		 <h1>Hello, {name}! You are currently working in {dataset}!</h1>
	)
}
```

## useDocumentStore

| Method | Description |
|--------|-------------|
| useDocumentStore(): DocumentStore | Returns an observable that can be used to listen for changes and events to documents in the current project. |


```javascript
import {useMemoObservable} from 'react-rx'

export function MyComponent() {
  const docId = useFormValue(['_id'])
  const documentStore = useDocumentStore();
  const results = useMemoObservable(() => {
    return documentStore.listenQuery(
      `*[_type == 'article' && references($currentDoc) && !(_id in path("drafts.**"))]`,
      {currentDoc: docId},
      {}
    )
  }, [documentStore, docId])
  

	return (
	  /** Render component */
	)
}
```





# Structure tool

The Structure Tool is a top-level view within Sanity Studio where editors can drill down to specific documents to edit them. You can configure your studio's Structure tool(s) with the Structure Tool API.

## Properties

#### Properties

| Property | Description |
|----------|-------------|
| name | The name you want this structure to have (among other places, this name is used in routing, if name is set to “structure”, it is shown on /structure). Usually lowercase or camelcase by convention. Defaults to structure. |
| title | The title that will be displayed for the tool. Defaults to Structure |
| icon | React icon component for the tool, used in navigation bar. Defaults to MasterDetailIcon from @sanity/icons |
| structure | A structure resolver function. Receives two arguments:

S - an instance of the structure builder, that can be used to build the lists/items/panes for the structure tool 

context - an object holding various context that may be used to customize the structure, for instance the current user. 

Defaults to (S) => S.defaults() |
| defaultDocumentNode | A resolver function used to return the default document node used when editing documents. Receives two arguments:

S - an instance of the structure builder, that can be used to build the document node (S.document()) 

context - an object holding various context that may be used to customize the document node |


## Minimal Example

The `sanity/structure` package exports a `structureTool`, which is a plugin that installs a structure tool. You can add it to your studio by passing it as part of the `plugins` array.



```typescript
// sanity.config.ts
import { defineConfig } from 'sanity'
import { structureTool } from 'sanity/structure'

export default defineConfig((
	// ...
  plugins: [
    structureTool() // use defaults
  ]
})
```

To customize your `structure` tool, pass an object in with the settings you want to customize. For instance, if you want a custom structure tool called “cars” that shows in the toolbar as “Cars” and has an icon from `react-icons` and tweaks both the `structure` and `defaultDocumentNode`:

```typescript
// sanity.config.ts
import { defineConfig } from 'sanity'
import { structureTool } from 'sanity/structure'
import { FaCar } from 'react-icons'

export default defineConfig((
	// ...
  plugins: [
    structureTool({
      name: 'cars',
      title: 'Cars',
      icon: FaCar,
      structure: (S) => S.documentTypeList('car'),
      defaultDocumentNode: (S) =>
        S.document().views([
          S.view.form(),
          S.view.component(Preview).title('Preview')
        ])
    })
  ]
})
```



# Studio Components Reference

The top-level configuration property `studio` enables customization of several parts of the studio's user interface. Its sole `components` key accepts an object with overrides for the layout, navigation bar, and tool menu:

```javascript
// ./sanity.config.tsx|jsx

import {defineConfig} from 'sanity'
import {MyActiveToolLayout, MyLayout, MyNavbar, MyToolMenu} from './components/studio'

export default defineConfig({
  // rest of config ...
  studio: {
    components: {
      activeToolLayout: MyActiveToolLayout,
      layout: MyLayout,
      navbar: MyNavbar,
      toolMenu: MyToolMenu,
    },
  },
})
```

## Layout

The layout is the root UI component for the studio. You probably never want to replace this component entirely with a custom layout component, but you might want to render the default layout component inside, say, a React context provider. This would allow all components inside the studio to retrieve the values from your provider. 

### Properties

#### Properties

| Property | Description |
|----------|-------------|
| renderDefault | A callback function that renders the default layout component. The function takes the component's properties as an argument, and these properties can be modified. |


### Example

```typescript
// ./sanity.config.tsx|jsx

import {defineConfig, LayoutProps} from 'sanity'
import {MyProvider} from '../path/to/my-provider'

function CustomLayout(props: LayoutProps) {
  return (
    <MyProvider>
      {props.renderDefault(props)}
    </MyProvider>
  )
}

export default defineConfig({
  // rest of config ...

  studio: {
    components: {
      layout: CustomLayout,
    }
  }
})
```

## Active Tool Layout

Similar to `layout`, but wraps only the currently active tool. You probably never want to replace this component entirely with a custom layout component, but you might want to render the default layout component inside, say, a React context provider. This would allow all components inside the currently active tool to retrieve the values from your provider. 
Properties

#### Properties

| Property | Description |
|----------|-------------|
| renderDefault | A callback function that renders the default layout component. The function takes the component's properties as an argument, and these properties can be modified. |
| activeTool | The currently active studio tool. |


### Example

```typescript
// ./sanity.config.tsx|jsx

import {defineConfig, LayoutProps} from 'sanity'
import {MyProvider} from '../path/to/my-provider'

function CustomActiveToolLayout(props: ActiveToolLayoutProps) {
  return (
    <MyProvider>
      {props.renderDefault(props)}
    </MyProvider>
  )
}

export default defineConfig({
  // rest of config ...

  studio: {
    components: {
      activeToolLayout: CustomActiveToolLayout,
    }
  }
})
```

## Navbar

You can override and insert extra components in the Studio's navbar. This can be useful if you want to customize the navbar to be visually distinct in certain environments (e.g. development vs. production). Or control what's displayed based on user roles or other contextual factors.

### Properties

#### Properties

| Property | Description |
|----------|-------------|
| renderDefault | A callback function that renders the default navbar component. The function takes the component's properties as an argument, and these properties can be modified. |


### Example

```typescript
// ./sanity.config.tsx|jsx

import {defineConfig, NavbarProps, useWorkspace} from 'sanity'
import {Card, Stack, Text} from '@sanity/ui'

function CustomNavbar(props: NavbarProps) {
  const {dataset} = useWorkspace()

  return (
    <Stack>
      <Card padding={3} tone="primary">
        <Text size={1}>
          Using the <b>{dataset}</b> dataset
        </Text>
      </Card>

			
      {props.renderDefault(props)} {/* Render the default navbar */}
    </Stack>
  )
}

export default defineConfig({
  // rest of config ...
	
  studio: {
    components: {
      navbar: CustomNavbar,
    }
  }
})
```

## Tool menu

The tool menu appears in the navbar and lists all your Studio tools. The tool menu is displayed in two places depending on the width of the screen size. On wide screens, it appears in the top bar, while on narrow screens, it appears inside the sidebar.

### Properties

#### Properties

| Property | Description |
|----------|-------------|
| activeToolName | The active tool name |
| closeSidebar | A function that closes the sidebar |
| context | A string that informs about the "context" in which the tool menu is rendered. This value is useful when you want to make two different customizations depending on whether the tool menu is in the top bar or in the side bar. |
| tools | An array of the tools in the studio |
| renderDefault | A callback function that renders the default tool menu component. The function takes the component's properties as an argument, and these properties can be modified. |


### Example

```typescript
// ./components/custom-toolmenu.tsx|jsx

import {defineConfig, ToolMenuProps, ToolLink} from 'sanity'
import {Button, Flex} from '@sanity/ui'

function CustomToolMenu(props: ToolMenuProps) {
  const {activeToolName, context, tools} = props
  const isSidebar = context === 'sidebar'

	// Change flex direction depending on context
	const direction = isSidebar ? 'column' : 'row'

  return (
    <Flex gap={1} direction={direction}>
      {tools.map((tool) => (
        <Button
          as={ToolLink}
          icon={tool.icon || PlugIcon}
          key={tool.name}
          name={tool.name}
          padding={3}
          selected={tool.name === activeToolName}
          text={tool.title || tool.name}
          tone="primary"
        />
      ))}
    </Flex>
  )
}
```



# Tools



[Introduction to tools](/docs/studio/studio-tools)

[Tools cheat sheet](/docs/studio/tools-cheat-sheet)



The most commonly familiar tool is the Structure tool (formerly called "Desk tool"), which lets you browse and edit documents. You can install tools with plugins or create your own. Tools also control the top-level Studio routing.

The `tools` config property accepts an array of appropriately shaped objects (Tool) or a callback function returning the same. The callback function receives an array of existing tools and a context object as arguments.

### Tool Properties

#### Properties

| Property | Description |
|----------|-------------|
| name * | Unique identifier for the tool |
| title * | Title for the tool. This is what will show up in the navbar. |
| component | The root component for your tool. This is what shows up in the main work area of your studio. |
| router | Router for the tool. See Router in the API explorer. |
| options | Optional configuration object. Passed as arguments to the tool when invoked. |
| getIntentState | Gets the state for the given intent. |
| canHandleIntent |  |


### Tool Context Properties

These are the properties received in the second argument of the callback function.

#### Properties

| Property | Description |
|----------|-------------|
| dataset | Name of the current dataset |
| projectId | Unique ID for the project |
| schema | The schema registry of your project. Use `schema.get("schemaTypeName") to retrieve any schema by name. |
| currentUser | An object with info about the currently logged in user. |
| getClient | Callback function that returns a configured client |


### Example

```javascript
// in dev-tool.tsx
import { ToolIcon } from '@sanity/icons'
import { Card, Text } from '@sanity/ui'

const MyCoolComponent = (props) => {
  return (
    <Card padding={4} tone="positive">
      <Text>I am a very useful tool.</Text>
    </Card>
  )
}

export const devTool = (config?: any) => ({
  name: 'dev-tool',
  title: 'Dev Tool',
  component: MyCoolComponent,
  ...config,
})

// in sanity.config.ts
import { defineConfig } from 'sanity'
import { devTool } from './dev-tool'
//... more setup

export default defineConfig({
  projectId: '<projectId>',
  dataset: 'production',
  tools: [
    myTool(
       // overrides the default tool title
      {title: 'My better title'}
      ),
    ],
  // ... more config
})
```





# Initial Value Templates

> [!NOTE]
> To learn more about templates and how to assign them default values to prepopulate a document, see the introduction to initial value templates.
> 
> 

defaults(): array

Returns an array of all the default templates - one for each document type defined in the schema. Use this to combine your own templates with the default ones.

template(config): function

Creates a new initial value template with the given configuration. Returns a template builder function which can be used to customize the template.

### Parameters

#### Properties

| Property | Description |
|----------|-------------|
| id * | An id used to identify the template. You will often refer to this ID when configuring which initial value templates are available in a given context. Must be unique. |
| title * | The title of the template, used for display purposes. |
| description | An optional description, used to clarify the purpose of the template. |
| schemaType * | The name of the schema type the template applies to. |
| value * | The actual initial value to use, or a function that resolves to one.

The function receives an object of any defined parameters as the first argument and should return either a plain object value or a promise which resolves to one. |
| parameters | An array of parameters the template expects to receive. Follows the same format as fields within a schema type.

Note that only the property name is currently used - validation is not performed, nor is type checking. Parameters should still define the type for future compatiblity. |
| icon | An optional react component to use as the icon for this template |




# Help and troubleshooting



# Studio Performance Issues Caused by legacy HTTP protocols

### Why Is This Happening?

Sanity Studio is designed to use modern web protocols (`HTTP/2` or `HTTP/3`) to provide the best performance. If your network, VPN, or security software is set up to use an older protocol — such as `HTTP/1.1` or `HTTP/1.0` — the Studio will be much slower and less reliable, and in some cases performance may be extremely degraded or fail to work altogether.

#### Common causes

• Work VPNs that don’t support `HTTP/2` or `HTTP/3`

• Corporate firewalls or proxies that force traffic to use `HTTP/1.1` or `HTTP/1.0` (often ZScaler or similar security software)

• Strict company network policies that restrict web protocols to older versions

### What Should You Do?

8. Try Another Network2. Turn off your VPN (if you’re using one), then reload Sanity Studio.
2. Try from your home network or a mobile hotspot. If Studio suddenly becomes much faster and responsive, the problem is likely with your company’s network.


8. Contact Your IT Department. Share the link to this article which explains the details in the following section.

### For IT and Network Administrators

Sanity Studio requires `HTTP/2 `or `HTTP/3` for normal operation.

Checklist for resolving these issues:

- **Allow HTTP/2 (and/or HTTP/3)**: Ensure your firewall, proxy, or VPN is not forcing outdated protocols (such as HTTP/1.1 or HTTP/1.0) for `*.sanity.io` domains. 
- **Update enterprise security tools**: ZScaler and similar appliances sometimes default to HTTP/1.1 or even HTTP/1.0 — update configurations to support modern protocols.
- **Review VPN settings**: Some VPNs disable HTTP/2/3 by default. Check documentation for enabling them for trusted domains.

If users report Sanity Studio being almost unusable, and it works fine on other networks, this is a strong indicator that outdated protocols are being forced by the network.

### How do I find out which my protocol version?

While newer Studios will warn users about older protocol versions, you can find which one is used in a Studio by checking the network tab in your browser:

Navigate to the Studio in the browser of your choice, [open the developer tools network tab](https://www.google.com/search?q=open+network+developer+tools&sourceid=chrome&ie=UTF-8) and reload the page. Make sure the Protocol column is also displayed to see which version is used. 

If you see `1.1` in the protocol column, the Studio is running in its slower, degraded mode.

![Screenshot with instructions to right click on the network request header bar and make sure protocol is selected](https://cdn.sanity.io/images/3do82whm/next/0f090458e9874c17534c0ed22a9e11a2b63bae0f-974x569.png)

### Still Having Trouble?

If none of the above steps work, please [reach out to the Support team in the help channel in our community](https://snty.link/community) (or to the Support channel if you are an enterprise customer) for further assistance.



# AVIF

Images that have the query parameter `auto` set to `format` ([see documentation](https://www.sanity.io/docs/image-urls#auto-777d41f23d56)) and are requested from a browser that supports the AVIF format will usually get an AVIF returned. There are a few exceptions/quirks:

- The *first few requests* for an AVIF *may* get the "second best option" (WebP if supported, otherwise PNG/JPG depending on the source image). Subsequent requests will *eventually* get an AVIF back. This is done to ensure a speedy response, since encoding AVIFs is a slow process.
- Image requests made prior to the rollout of the AVIF support may already be cached in our CDN and will not return an AVIF response until they expire/fall out of the cache. 

In other words: if you are not seeing AVIF images being returned, don't worry —  they should *eventually* return AVIF. You can use `curl` to verify the behavior:

```sh
# Replace the URL with an actual URL from your project.
# Remember to include `?auto=format`!
curl -sS -I \
  -H 'accept: image/avif,image/webp,image/*' \
  'https://cdn.sanity.io/images/:projectId/:dataset/:filename?auto=format' \
  | grep 'content-type:'
```

On the first request, you will likely see `image/webp` returned. After waiting 30 seconds, run the same command again, and you should see `image/avif`. If you don't, wait a little longer and retry. If you still do not see AVIF, ensure that the accept header includes `image/avif` (before other formats) and that the query parameters includes `auto=format`.

## Reporting issues

If you encounter any issues, send an email to [avif@sanity.io](mailto:avif@sanity.io?subject=AVIF%20issue) with comprehensive details on the issue.



# Experimental feature: Spaces

A feature request we often get is the ability to switch between datasets from within Sanity Studio. This has a number of usages:

- Managing multiple departments within the same organization, each with their own, separate datasets, but all sharing the same schema.
- Modifying an existing schema type and testing how it works using a *staging* dataset.
- Using the same studio across multiple *projects* with separate sets of both documents and users.

To facilitate this, we are proposing "*spaces*". A *space* is simply a mapping from a name/title to an API config. Spaces can be configured in your studio's `sanity.json` (under the `__experimental_spaces` key). Here's an example:

```javascript
{
  "project": {
    "name": "Studio with spaces"
  },
  "__experimental_spaces": [
    {
      "name": "production",
      "title": "Prod",
      "default": true,
      "api": {
        "projectId": "ppsg7ml5",
        "dataset": "production"
      }
    },
    {
      "name": "staging",
      "title": "Staging",
      "api": {
        "projectId": "ppsg7ml5",
        "dataset": "staging"
      }
    }
  ],
  "//...": "..."
}
```

This will render a dropdown to switch between **spaces** in the studio:

![](https://cdn.sanity.io/images/3do82whm/next/2035acbd6ab83d1315abe556844be3a46bbd1040-2120x1512.gif)

Switching to another space will connect to the dataset configured for that space.

### Disclaimer: Experimental feature

Did we mention that this is an experimental feature? That means it may or may not become a feature in its current form. But we acknowledge the need for a feature that supports the above use cases.

#### Found bugs / have feedback?

Please. Do let us know, either by filing an issue on [GitHub](https://github.com/sanity-io/sanity/issues) or ping us on [Gitter](https://gitter.im/sanity-io/sanity).

List of things we'd love to get feedback on in particular:

- Naming things is hard. Is *spaces* a good word the feature? Maybe *configuration* is better? Suggestions?
- Would you expect to be able to search for content *across* spaces? Currently thats not possible.
- UX considerations: Would you expect to use one client to query for data across *spaces*, or would you typically configure one frontend per *space.*
- Other issues?





# Client API CDN configuration

Sanity provides a CDN-distributed, cached API that is faster and cheaper to use if you are exposing the API to end-users. If you are building static sites you should use the live API to ensure you always get the freshest version.

A full explanation of the differences between these APIs is outlined in the [API CDN documentation](/docs/content-lake/api-cdn).

The [Sanity JavaScript client](/docs/js-client) can be configured to use either the API CDN or the API by setting the `useCdn` option to `true` or `false`, respectively, when configuring the client:

```javascript
import sanityClient from '@sanity/client'

const client = sanityClient({
  projectId: 'your-project-id',
  dataset: 'your-dataset',
  apiVersion: '2022-08-12',
  useCdn: true
})

```

> [!TIP]
> Protip
> In most cases, we recommend setting your apiVersion to today's date. This ensures you get the most recent bugfixes and improvements, and if it works today it will continue to work tomorrow.

Note that the client will automatically fall back to using the live API in the following scenarios:

- When a mutation is performed (create/edit/delete).
- When listeners are used (subscribing to changes).

> [!WARNING]
> Gotcha
> Prior to v3.0.0 of the JavaScript Client, the API CDN could not be used in combination with a token (i.e., on private datasets or to query documents on a non-root path)—setting both would ignore useCdn: true and fall back on the live API.
> 
> Versions 3.0.0 and later of the JavaScript Client remove this limitation, allowing you to specify a token and useCdn: true together to make authenticated queries to the API CDN.



# Total attribute count exceeds limit

<p>Everything about the attribute limit: what it is, how to avoid it, and what to do if you hit the limit on one of your projects.</p>## What is the attribute limit?

The attribute limit determines how many unique combinations of attribute and datatype you can have in your dataset. Depending on what plan your project is on, your limit is one of the following:

- Standard: 2k attributes
- Advanced: 4k attributes
- Enterprise: 10k attributes

> [!WARNING]
> Gotcha
> The attribute limit is a hard technical limit right now. For this reason, we do not currently offer a pay-as-you-go option for extra attributes.

## What counts as an attribute?

As shown above, an attribute is officially defined as *a unique combination of attribute and datatype*. An alternative way to think about them is as the different paths through your content.

Let's take a basic data structure:

```json
{
  "foo": [
    {
      "bar":…,
      "baz":…
    },
    {
      "bar":…,
      "baz":…
    },
    {
      "bat": {
        "bar":…
      }
    }
  ]
}
```

This structure contains six unique paths or attributes:

11. foo -> an array
11. foo[] -> an object
11. foo[].bar -> a string
11. foo[].baz -> a string
11. foo[].bat -> an object
11. foo[].bat.bar -> a string

Paths only count towards your attribute limit when they hold actual content. Solely changing your schema definitions will not affect the attribute count. Schema definitions define the structure of your content, a bit like a blueprint defines the structure of a building. Until you add or remove content using the Sanity Studio or the HTTP API, your attribute count will remain unchanged.

Each unique path is counted once, no matter how often it is used. Removing a path from your attribute count requires deleting every piece of content on that path across all documents.

In short, your attribute count:

- goes up when you first add content on a path
- goes down when a path no longer holds any content
- stays the same regardless of whether a path is used once or many times

## Best practices

When structuring your content, there are a few pitfalls to keep in mind to avoid hitting the attribute limit. Although this is not an exhaustive list, following the best practices below should go a long way in keeping your attribute count in check.

### Create reusable data structures

Let's say you have an e-commerce site and want to use Sanity to enrich your product information. You decide  string fields are perfect for this purpose and set up the following structure:

### Use arrays for page building

A common use case for Sanity is using structured content for [page building](https://www.sanity.io/guides/how-to-use-structured-content-for-page-building). In setting up a page builder, it may be tempting to use the block content type as the editor gives a lot of flexibility and allows adding any number of custom objects that can then be used inline.

However, a block content field has quite an extensive data structure by default:
• a `blockContent` array, with inside of it:
• `blocks` objects, with inside of them:
• `markDefs` and `children` arrays, with inside of them:
• `span` types, with inside of them:
• a `marks` array and a `text` field

This nested structure is further extended by any custom types you add to it, all with their own unique paths. A block content field with many custom objects may therefore lead to a hefty amount of attributes.

Another issue with this approach is that people sometimes want to use block content fields *inside* of custom objects. This is likely to lead to even more attributes as a result of now having the above structure embedded in the same structure. Moreover, when the exact same block content component is used, allowing this type of nesting basically gives editors the freedom to nest to an arbitrarily deep level, which can then drag a project over the attribute limit.

To avoid any of these challenges and keep the attribute count as low as possible, we recommend using arrays for page building. In addition to fewer attributes, greater control over the exact content structure, and reduced risk of getting into nesting situations, this approach has the added advantage of not having to deal with serializers for complex custom objects. 

### Avoid excessive nesting and recursive data structures

Things get worse when subsequently the same block content configuration is used for any block content fields inside the custom objects, so editors can endlessly nest the entire page builder inside itself.

### Focus on meaning, not presentation

Before responsive web made its entrance and people started optimizing for different devices, it was customary to mix content with presentation. A headline could be blue, have font size 24px, line-height 30px, and a bottom padding of 10px. Although it may still be tempting today to offer that same level of control to editors, there are several downsides to this approach. For one, whenever you want to change your front-end's design, editors will have to review all relevant content.

Most importantly for this guide, adding all these presentational attributes is likely to boost your attribute count significantly as they would exist for nearly every piece of content.

Instead of mimicking CSS properties in your schema definitions, we recommend a separation of concerns. Leave the presentational aspects to wherever you implement your content and instead stick to semantics in your content structure - in other words, focus on the *meaning* of your content.

### Beware of multipliers in translation/localization

There is a variety of i18n/l10n approaches out there, some of which have a greater impact on your attribute count than others. For example, one approach suggests wrapping all your fields inside a language object, so you get the following structure:

```json
{
 "de": {
  ...
 }
 "en": {
  ...
 }
}
```

This basically multiplies the number of attributes by the number of languages added, as all fields get duplicated on a language path. Adding more than a few languages this way means trouble.

Instead of duplicating the fields inside a document, thereby creating all these extra paths, a more frugal approach is to duplicate the *document* instead. To differentiate between the different languages and more easily query for them, you can consider adding a (hidden) internationalization field to your document type and/or add the language to the document ID. As you will be reusing the same fields across different documents, adding an extra language no longer affects your attribute count at all.

## What to do if you hit the limit?

If you inadvertently hit the attribute limit on one of your datasets, you will see the following error when opening your Sanity Studio: `Total attribute count exceeds limit`.



### Export your data

Before deleting any content or changing your data structure, we highly recommend running a full export of your dataset to prevent any unintended data loss. To do so, you can run the [dataset export](https://www.sanity.io/docs/migrating-data#c4665bde1f66) command in your terminal. For example:

`sanity dataset export production production.tar.gz`

### Get unblocked

The first step after exporting your data is to get unblocked so you and other users on your project can work in the studio again. In other words, the challenge is to get back below the attribute limit.

Perhaps there is a heavily nested structure with block content *and* translations that could be optimised. Or maybe you have singletons for different pages that could be folded into a single page type instead to further reduce the number of unique paths.

A final note is that it also helps to remove any unused content from schema revisions. For example, if you used to have a particular document type with a bunch of documents, but later removed that type, or even some fields within a type, make sure to clean up the content so there are no leftovers in the datastore that will count towards the attribute limit.

### Restructure your content

How to restructure your content depends on your content model and is therefore different per project. However, there are a bunch of examples to get you started. Please note that in all cases, it is highly recommended to run a full dataset export *before *

### Track your progress

To keep an eye on your attribute limit while restructuring your content, you can use this URL: `https://<projectId>.api.sanity.io/v1/data/stats/<datasetName>`

The attribute count is the value of `fields.count.value` and the limit is inside `fields.count.limit`.

## Closing remarks

Although this guide was specifically about the attribute limit, the principles outlined above are best practices that are likely to lead to a more solid, flexible, and future-proof content model in any situation.



# Desk is now Structure

The version [3.20.0](https://github.com/sanity-io/sanity/releases/tag/v3.20.0) update to Sanity Studio introduced a notable change: the tool previously known as "Desk" has been renamed to "Structure".

You may notice this renaming in the toolbar menu of your studio, as well as in the path segment of your studio URLs.

![Comparison of the studio toolbar and browser address field before and after the change](https://cdn.sanity.io/images/3do82whm/next/ff0863973edd5c2d65c1ed8cf3f106ecdc83cb1f-1024x227.png)

## **Why the Change?**

The "Desk" name suggested a singular, one-size-fits-all approach to content management. As Sanity Studio has grown, so have its capabilities. With features like [Presentation](/blog/introducing-presentation) broadening your content interaction options, **Structure** is a more appropriate and descriptive name that reflects its status as one of the many diverse ways you can shape and organize your content models.

## **For Studio Users**

This update brings a minor yet significant change to your workspace:

- **Toolbar Update**: The studio toolbar label has changed from "Desk" to "Structure". Rest assured, this change is purely cosmetic, with no impact on the functionality you're familiar with.
- **URL Path Update**: The initial path segment of your studio URLs has changed from **/desk** to **/structure**. Existing bookmarks will automatically redirect, so there's no immediate need to update them.

## **For Studio Maintainers**

As of version [3.24.1](https://www.sanity.io/changelog/5784e03f-504d-4f74-a6be-443ad1fd96b6), the `deskTool` has been renamed `structureTool` and is found in `sanity/structure`. In other words, where you'd previously do this:

```typescript
import {defineConfig} from 'sanity'
import {deskTool} from 'sanity/desk'

export default defineConfig({
  // ...rest of config
  plugins: [
    deskTool(),
  ]
})
```

You should now update your code as follows:

```typescript
import {defineConfig} from 'sanity'
import {structureTool} from 'sanity/structure'

export default defineConfig({
  // ...rest of config
  plugins: [
    structureTool(),
  ]
})
```

No rush! Everything will still work as before since we are keeping the previous naming around as valid aliases, but going forth you should get used to the `structureTool`.



> [!TIP]
> Protip
> The Sanity Command Line Interface has a handy codemod to help you update your code with almost no effort! Run the following command in your studio root directory: 
> 
> 
> npx @sanity/cli codemod deskRename
> 
> 
> 
> Be sure to check in any local changes to version control before running the codemod in case it should fail!

## **Moving Forward**

This renaming is a step towards a more adaptable and intuitive Sanity Studio. We appreciate your flexibility and dedication as we evolve the platform to better meet your needs.

The 'Desk' name suggested a singular, one-size-fits-all approach to content management. As Sanity Studio has grown, so have its capabilities. With features like 'Presentation' broadening your content interaction options, 'Structure' is a more appropriate and descriptive name that reflects its status as one of the many diverse ways you can shape and organize your content models.

```typescript
import defineConfig from 'sanity'
import structureTool from 'sanity/structure'

export default defineConfig( 
  // ...rest of config
  plugins: 
    structureTool(),

 )
```

> [!TIP]
> Protip
> The Sanity Command Line Interface has a handy codemod to help you update your code with almost no effort! Run the following command in your studio root directory:
> 
> npx @sanity/cli codemod deskRename



# Invalid configuration for cross dataset reference

The `crossDatasetReference` type can be configured with the following options:

- `projectId`: The id of the project to create references from. Must be a valid project id.
- `dataset`: The name of the dataset in `<projectId>` to create references from. Must be a valid dataset name.
- `tokenId` (Optional) The ID of the access token stored in the current dataset that grants read access to documents in the specified dataset in the specified project. The tokenId must be a string made up of at least 2 characters in the `a-zA-Z0-9_-` range and cannot start with a - (dash) character

See the getting started guide for [Cross Dataset References](/docs/studio/cross-dataset-references) for more details

Or check out the documentation for the [Cross Dataset Reference Schema type](/docs/cross-dataset-reference-type)



# Missing or duplicate context error

The following is an explainer to why you may be seeing duplicate or missing context errors within the Sanity Studio.

## Why is this error occuring?

This error occurs when there are multiple versions of the `sanity` package installed locally. This causes issues because of how React context works. Having multiple versions of a React library that exports a React context can cause issues due to the singleton nature of React contexts.

React contexts are designed to ensure that there is a single provider for a given context that supplies data to multiple consumers within the component tree. When different versions of the same library are used, each version creates its own isolated context instance. This results in consumers and providers from different versions being incompatible with each other, leading to inconsistent data sharing and state management issues across the application.

## Why are multiple version of `sanity` being installed?

There are a few reasons why multiple versions of the `sanity` package may be installed in a project:

- **Transitive peer dependencies**: If your project depends on other libraries that also depend on `sanity`, (such as sanity plugins) but they require different versions, package managers may install multiple versions to satisfy all the dependencies. This includes `peerDependencies`. If you're using pnpm, [pnpm may even install the same exact version of sanity twice](https://pnpm.io/how-peers-are-resolved) to satisfy different sets of peer dependencies.
- **Incorrect version specification**: If you specify a version of `sanity` in your project's `package.json` file that doesn't match the version used by other dependencies, it could lead to multiple versions being installed.
- **Lock file state**: If you have updated the `sanity` dependency in your `package.json` file but haven't regenerated the lock file (`pnpm-lock.yaml`, `package-lock.json`, or `yarn.lock`), the old version might still be installed based on the lock file. There are certain scenarios where the state of your lockfile may result in more than one versions of sanity being installed and simply deleting it and reinstalling may fix the issue.

## What can be done to fix this issue?

There are 2 possible solutions for to this.

### Solution 1: Clean up dependencies

The first solution to resolve missing context errors is to clean up your project's dependencies. There are lots of scenarios where transitive dependencies (aka the dependencies of your dependencies) can cause a mismatched version.

Because `sanity` is React-based, you'll want to make sure that versions of `sanity`, `react`, `react-dom`, and similar react libraries are all compatible and consistent. In particular, this means that you should see just one version of `sanity` installed and the same version of `react` and `react-dom`.

At the time of writing, this will mean a sanity `^3` version (e.g. `3.50.0`) and a `react` `^18` version, and a `react-dom^18` version (e.g. `18.3.1`).

**Some quick rules of thumb:**

15. Ensure you have no deprecated sanity packages installed (e.g. `@sanity/base`, `@sanity/react-hooks`, `@sanity/desk-tool`) and ensure the rest of any `@sanity/` package or plugin is at the latest version. The latest version of any package can be determined by running `npm show <package-name> version`.
15. Ensure you don't see any warnings regarding unmet peer dependencies. Everything should be on the same version of react and react-dom.
15. Ensure you aren't using any outdated react libraries that don't support at least React 18. sanity has a peer dependency on at least react `^18`.

In my experience, there is no better way to inspect your dependencies than grepping your lockfile ( `pnpm-lock.yaml`, `package-lock.json`, or `yarn.lock`).

#### pnpm-lock.yaml

pnpm handles peer dependencies somewhat differently than npm. It's not as strict as npm and it tries its best to satisfy all peer dependencies on its own.

Run `pnpm install --resolution-only` to see any potential issues with peer dependencies. Fixing these issues may result in de-duping sanity.

> [!TIP]
> Protip
> You can also try running pnpm dedupe!

![](https://cdn.sanity.io/images/3do82whm/next/cca8ae090aeb0c433996adc355885cb798276c8f-1380x1046.png)

If you've seen warnings with unmet peer dependencies, this may result in more than one `sanity` being installed at once.

When grepping your `pnpm-lock.yaml`, search for `sanity@3`. For every set of peer dependencies, pnpm will include `sanity` and other packages in that set of peer dependencies in a single line.

![](https://cdn.sanity.io/images/3do82whm/next/72747523738b53f18cfe67f42f3aced45b51cdcd-1914x536.png)

Notice anything odd? There's an incompatible version of `react-dom` (`17.0.2`). The fix here is to search the lockfile for dependencies that are causing `react-dom` 17 to be installed and used.

You heard that right, the presence of `react-dom` 17 can even result in the *same* version of `sanity` to be installed more than once.

After searching for `react-dom:`, the culprit was found: An outdated dependency to `@reach/auto-id` which did not allow a peer dependency of `react-dom` 18.

![](https://cdn.sanity.io/images/3do82whm/next/f6b2255fda05393fb604ff0fdf8932fd2bb52f1b-686x264.png)

Further following this trail lead to finding an old version of `@sanity/desk-tool` was installed.

![](https://cdn.sanity.io/images/3do82whm/next/afcaad4ee00114b2614bb112e6525807a12978e6-750x286.png)

This led to the discovery of other legacy packages being installed. After removing `@sanity/desk-tool` and `@sanity/react-hooks` from all package.jsons the context issue was resolved.

#### package-lock.json

npm's package-lock.json `lockfileVersion` 3 file is relatively straightforward lockfile.

![](https://cdn.sanity.io/images/3do82whm/next/ef13c0c3fd2639e3ed899c0a29d275063815c3a8-1198x992.png)

Top-level is the `lockfileVersion` followed by a `packages` key that contains a flat list of all the dependencies installed in project. Note for monorepos that this lockfile should contain all dependencies for the root package and all of the subpackages as well.

In this lockfile, search for `node_modules/sanity"`, `react`, and `react-dom`.

![](https://cdn.sanity.io/images/3do82whm/next/5a2922bb8a5ca718af1c8c2465bdbb4b1f8e54ec-1570x488.png)

Ideally you'd see just one of each. If there are more than one and one of those versions mismatch, look for a package that would depend on the mismatched version.

#### yarn.lock

The yarn lock file is similar npm's native package-lock.json expect it's in yaml. See [this article](https://www.arahansen.com/the-ultimate-guide-to-yarn-lock-lockfiles/) for more information on yarn lock files.

### Solution 2: Override the `sanity` dependency

If cleaning up dependencies manually doesn't resolve the issue, you can force your package manager to use a specific version of `sanity` across all dependencies. You may want to do this anyway in order to prevent this issue from occuring in the future.

Doing this ensures that only one version is installed, even if different dependencies specify conflicting versions.

#### npm overrides

NPM supports an overrides key in the top-level of package.json.Wherever you see the `sanity` dependency in your package.json, add the following to your package.json.

```json
{
  "name": "your-studio",
  "version": "1.0.0",
  "description": "...",
  "dependencies": {
    "sanity": "^3.50.0"
  },
  "overrides": { "sanity": "$sanity" }
}
```

The `$sanity` value makes it reference the version listed above in your `package.json`.

If working in a monorepo, we recommend installing `sanity` at the root workspace.

#### pnpm overrides

pnpm has the same thing as NPM overrides but under a `pnpm` key.

```json
{
  "name": "your-studio",
  "version": "1.0.0",
  "description": "...",
  "dependencies": {
    "sanity": "^3.50.0"
  },
  "pnpm": {
    "overrides": { "sanity": "$sanity" }
  }
}
```

#### Yarn resolutions

Yarn allows you to force a particular dependency through `resolutions`.

```json
{
  "name": "your-studio",
  "version": "1.0.0",
  "description": "...",
  "dependencies": {
    "sanity": "^3.50.0"
  },
  "resolutions": {
    "sanity": "^3.50.0"
  }
}
```

## Appendix: What's a Lockfile?

A lockfile is a file that records the exact versions of all packages installed in a project, including transitive dependencies. It serves as a snapshot of the project's dependency tree at a given point in time.

Lockfiles are generated automatically by package managers like pnpm, npm, and yarn when you run the install command. They ensure reproducible builds by locking the versions of all dependencies, so that everyone working on the project uses the same versions.



# React Compiler and Sanity

Sanity Studio v3.65.0 introduced support for the [React Compiler](https://react.dev/learn/react-compiler). The compiler improves performance by automatically optimizing component rendering. This reduces the amount of manual memoization developers have to do through APIs such as `useMemo` and `useCallback`.

If you use @sanity/pkg-utils and/or @sanity/plugin-kit to distribute custom plugins and tools on npm then it's also possible to use the compiler there.

## Sanity Studio

Install the babel and ESLint plugins for the compiler

```sh
npm install --save-dev babel-plugin-react-compiler@beta eslint-plugin-react-compiler@beta
```

Setup your ESLint config to include the compiler

```json
{
  "parser": "@typescript-eslint/parser",
  "plugins": ["react-hooks", "react-compiler"],
  "rules": {
    "react-compiler/react-compiler": "warn",
    "react-hooks/rules-of-hooks": "error",
    "react-hooks/exhaustive-deps": "error"
  }
}
```

You don't need to fix all the warnings before you can start using the compiler, you can incrementally adopt it.

It's also recommended that you have [strictNullChecks](https://react.dev/learn/react-compiler#:~:text=example%2C%20by%20enabling-,strictNullChecks,-if%20using%20TypeScript) enabled.

### React 18

Install the [react-compiler-runtime package as a direct dependency](https://react.dev/learn/react-compiler#using-react-compiler-with-react-17-or-18)

```sh
npm install react-compiler-runtime@beta
```

And add `reactCompiler` to your `sanity.cli.ts` configuration, and set `target` to `'18'`

```typescript
import {defineCliConfig} from 'sanity/cli'

export default defineCliConfig({
   api: {
      projectId: 'abc123',
      dataset: 'production',
   },
   reactStrictMode: true,
   reactCompiler: {target: '18'},
})
```

### React 19

And add `reactCompiler` to your `sanity.cli.ts` configuration, and set `target` to `'19'`

```typescript
import {defineCliConfig} from 'sanity/cli'

export default defineCliConfig({
   api: {
      projectId: 'abc123',
      dataset: 'production',
   },
   reactStrictMode: true,
   reactCompiler: {target: '19'},
})
```

Since React 19 has the compiler runtime built in there's no need to install `react-compiler-runtime`.

### Embedded Studios

If your studio is hosted inside something like a Next.js, Remix app or otherwise not using `sanity build` and `sanity dev` commands?

If so you'll have to enable the compiler through one of the methods documented [here](https://react.dev/learn/react-compiler#installation).

## Publishing Sanity plugins and tools

[Since the compiler needs to run on the original source code it's not possible for Sanity Studio's to compile the libraries they use.](https://react.dev/learn/react-compiler#using-the-compiler-on-libraries) Instead, library authors need to ship compiled code to npm.

At Sanity we use @sanity/pkg-utils to build our libraries. It handles ESM, CJS, and even the React Compiler. Libraries like react-rx, @sanity/ui and @portabletext/editor, sanity, and @sanity/vision, are already shipping compiled code this way.

Start off by installing the ESLint and Babel plugins

```sh
npm install --save-dev --save-exact babel-plugin-react-compiler@beta eslint-plugin-react-compiler@beta
```

Your ESLint config should enable the compiler, as well as strict `eslint-plugin-react-hooks` rules

```json
{
  "parser": "@typescript-eslint/parser",
  "plugins": ["react-hooks", "react-compiler"],
  "rules": {
    "react-compiler/react-compiler": "warn",
    "react-hooks/rules-of-hooks": "error",
    "react-hooks/exhaustive-deps": "error"
  }
}
```

You don't need to fix all the warnings before you can start using the compiler, you can incrementally adopt it.

It's also recommended that you have [strictNullChecks](https://react.dev/learn/react-compiler#:~:text=example%2C%20by%20enabling-,strictNullChecks,-if%20using%20TypeScript) enabled.

You also need to install the react-compiler-runtime if you support react 18

```sh
npm install --save-exact react-compiler-runtime@beta
```

It's very important that the direct dependency you use, use an exact version number.

Next, you add two lines of code to your `package.config.ts`

```typescript
import {defineConfig} from '@sanity/pkg-utils'

export default defineConfig({
  // ... other stuff
  babel: {reactCompiler: true},
  reactCompilerOptions: {target: '18'}, // matches the minimum `react` major your library requires
})

```

## Troubleshooting

[Follow the official troubleshooting docs in case you run into problems.](https://react.dev/learn/react-compiler#troubleshooting) In our experience it's incredibly rare for the compiler to create a regression, it typically choses to skip over optimizing components it deems unsafe, or too complex to safely memoize.

Should a rare problem occur it's often enough to add `'use no memo'` at the top of the affected file, to buy you time and find the fix. And then use ESLint with `eslint-plugin-react-compiler` and `eslint-plugin-react-hooks` to find issues that could be the root cause. Running React Strict Mode is also incredibly helpful to uncover root issues.



# Sanity client: CDN + authorization token

As detailed in the [API CDN](/docs/content-lake/api-cdn) documentation, requests that include an authorization token cannot be cached. When you configure the Sanity client with a token and perform a query, we automatically route your request to the live API instead of the API CDN. Since this is probably not what you want, you will usually want to set the `setCdn` option to `false`, or remove the token. 

Here are a couple of recipes to help you decide the right course of action:

#### Query a private dataset

If you include a token in a so called "single page application" which is public facing, your data is, in principle, already public. In this case, setting the dataset as private doesn't necessarily make sense. If, on the other hand, you actually want your data to be private - make sure that your application do not publicly disclose the token, and set the `useCdn` setting to `false`, since it will never use the API CDN anyway.

#### Do queries and mutations with the same client

In this case, instantiate two clients instead of one. One for write operations, and one for queries:

```javascript
import sanityClient from '@sanity/client'

export const writeClient = sanityClient({
  projectId: 'some-project-id',
  dataset: 'my-dataset',
  token: process.env.SANITY_AUTH_TOKEN,
  useCdn: false
})

export const readClient = sanityClient({
  projectId: 'some-project-id',
  dataset: 'my-dataset',
  useCdn: true
})

```



# Specify API version for studio client

In a previous version of the Sanity content studio, you could import a global, preconfigured Sanity client instance by importing `part:@sanity/base/client`.

Having a global client use a single API version is both restrictive and prevents utilizing the latest and greatest features of the Sanity API. This is why we have now deprecated using the global studio client without explicitly defining an API version to use.

## Old usage:

```javascript
import client from 'part:@sanity/base/client'

client.fetch('*[_type == "author"][0...10]')

```

## New usage:

```javascript
import sanityClient from 'part:@sanity/base/client'

const client = sanityClient.withConfig({apiVersion: '2021-06-07'})

client.fetch('*[_type == "author"][0...10]')

```

Details about getting your versioned client set up can be found under [API Versioning](/docs/content-lake/api-versioning) and the [JavaScript Client](https://www.sanity.io/docs/js-client#api).



# Why give schema types a title?

We recommend to always give your schema types and fields a descriptive title. The title is used in different UI contexts, e.g on buttons and menus.  

The title must be a string.



# Array type has a invalid value for property "of"

All array types must define what kind of items they may contain. The "`of`" property must be an array of objects that describes the type of a valid item. Each entry in `of`, must have a `type`-property which must be the name of a valid schema type.

```javascript
{
  type: 'array',
  name: 'items',
  of: [ // The "of"-property must be set, and it must be an array
    {
      type: 'author', // type is required
      title: 'Author'
    },
    {
      type: 'book',
      title: 'Book'
    }
  ]
}
```

Types must be unique, or named

In order to know which type description an array item belongs to, you can not add multiple entries to of with the same name, unless giving them a name to tell them apart. This is therefore not allowed: 

```javascript
{
  type: 'array',
  name: 'items',
  of: [
    {
      type: 'author',
      title: 'Author'
    },
    {
      type: 'author', // 💥 ERROR will not be able to tell array items apart
      title: 'Another author'
    }
  ]
}
```

Instead, you can give items of the same type another name. This will work:

```javascript
{
  type: 'array',
  name: 'items',
  of: [
    {
      type: 'author',
      title: 'Author'
    },
    {
      type: 'author',
      name: 'anotherAuthor', // all good
      title: 'Another author'
    }
  ]
}
```

Items in this array will have their `_type` set to either `author` or `anotherAuthor`, depending on which of the types were selected when the item was added  e.g.:

```json
[
  {"_type": "author", "name": "Camilla Collett"},
  {"_type": "anotherAuthor", "name": "Henrik Ibsen"}
]
```



# React 19 and Sanity

Support for [React 19](https://react.dev/blog/2024/04/25/react-19) in Sanity Studio and most official plugins are completed. Active development of Sanity Studio itself is tested and deployed on React 19. Support in the wider ecosystem is being worked on. Progress can be tracked on [this page](https://arewereact19yet.sanity.build/). Most internal projects at Sanity are also deployed to React 19 in production.



## When is it safe to use React 19 in production?

It depends on the plugins you use. Our release strategy is to:

5. Update upstream libraries that Sanity Studio depends on with React 19 support (`@sanity/ui`, `react-rx`, and others).
5. Release a new version of Sanity Studio that adds React 19 to its `peerDependencies` and [passes the build test](https://arewereact19yet.sanity.build/package/sanity).
5. Release new versions of official plugins that adds React 19 to its `peerDependencies.`
5. Update `@sanity/plugin-kit` with support for React 19, and provide docs for plugin authors for how to test, verify and release a new version.
5. Help third party plugin authors with upgrading their plugins to add React 19.

Step 1 and 2 is done, it means it's generally safe to use React 19 in production, and if a library is marked as passing [on the tracker](https://arewereact19yet.sanity.build/) it means we've manually verified it works. Step 3 is in progress.

## How can I prepare for React 19 today?

Make sure you’ve upgraded to **React 18.3** or later, and that you don’t have any deprecation warnings logged to your console when running `sanity dev` .

Also ensure you have `reactStrictMode` [enabled](https://www.sanity.io/docs/cli-reference#reactStrictMode-d6dd5ed608de) and don’t have strict mode related warnings either.

## How can I test React 19?

Depending on your package manager you may get peer dependency errors or warnings that prevent you from being able to test 19. Here’s how you can configure your package manager to let you test 19 today:

### npm

Version v10 is recommended.

Start by adding these overrides to your `package.json` :

```json
{
  "dependencies": {
    "react": "^18.3.1",
    "react-dom": "^18.3.1",
    "sanity": "^3.43.0",
  },
  "packageManager": "npm@10.5.2",
  "overrides": {
    "react": "$react",
    "react-dom": "$react-dom"
  }
}

```

Run `npm install` and you shouldn’t see any errors. Next, upgrade `react` and `react-dom` :

```sh
npm i react@latest react-dom@latest --save-exact

```

### pnpm

Version v9 is recommended.

Start by adding an `overrides` field that ensures you don't get both v18 and v19 of react installed:

```json
{
  "dependencies": {
    "react": "^18.3.1",
    "react-dom": "^18.3.1",
    "sanity": "^3.43.0",
  },
  "packageManager": "pnpm@9.0.4",
  "pnpm": {
    "pnpm": {
      "peerDependencyRules": {
        "allowAny": [
          "react",
          "react-dom"
        ]
      }
    }
  }
}

```

Then run:

```sh
pnpm up react@latest react-dom@latest

```

### Bun

v1 or later is recommended.

```sh
bun add react@latest react-dom@latest

```

### Yarn

v1 is recommended.

```sh
yarn add react@latest react-dom@latest

```





# Schema: Lift anonymous object types

A common pattern is to have objects embedded within your document, which allows for a logical grouping of fields. For instance, a `person` might have an `address` which consists of many fields (street name, zip, country etc).

One way of doing this is to create an inline object for the field:

```javascript
export default {
  name: 'person',
  title: 'Person',
  type: 'object',
  fields: [
    {
      name: 'name',
      title: 'Name',
      type: 'string'
    },
    {
      name: 'address',
      title: 'Address',
      type: 'object',
      fields: [
        {name: 'street', type: 'string', title: 'Street name'},
        {name: 'zip', type: 'string', title: 'Zip code'}
      ]
    }
  ]
}

```

While this works for most cases, it can often be smart (and sometimes necessary) to "lift" this type up and define it as a global schema type.

Doing so can often lead to a more thought-out and future proof data model, since you will often rethink the fields in a global context - *"how can I define this type so it can be reused for both businesses and person records?"*

It can also help you reason about the same records when consuming the API from an application - perhaps you'll want to mirror the schema in TypeScript definitions or Go structs, or create a GraphQL API.

To lift a type, simply create a new type for it in the same way you would with a *person* type and import it into your schema:

```javascript
export default {
  name: 'address',
  title: 'Address',
  type: 'object',
  fields: [
    {name: 'street', type: 'string', title: 'Street name'},
    {name: 'zip', type: 'string', title: 'Zip code'}
  ]
}

```

Then, in your person model, set `address` as the *type* for the address field:

```javascript
export default {
  name: 'person',
  title: 'Person',
  type: 'object',
  fields: [
    {
      name: 'name',
      title: 'Name',
      type: 'string'
    },
    {
      name: 'address',
      title: 'Address',
      type: 'address'
    }
  ]
}

```





# Reference type has a invalid value for property "to"



All reference types must define what type of documents they may refer *to*. The "`to`" property must be an array of objects that describes the type of a valid reference. Each entry in `to`, must have a `type`-property which must be the name of a valid schema type.

```javascript
{
  type: 'reference',
  name: 'references',
  to: [ // The "to"-property must be set, and it must be an array of at least one type
    {
      type: 'author', // type is required
      title: 'Author'
    },
    {
      type: 'book',
      title: 'Book'
    }
  ]
}
```

### To-types must be unique, or named

In order to know which type description a reference value belongs to, you can not add multiple entries to of with the same type, unless you also give them a *name* to tell them apart. This is therefore not allowed: 

```javascript
{
  type: 'reference',
  name: 'authorReference',
  to: [
    {
      type: 'author',
      title: 'Author'
    },
    {
      type: 'author', // 💥 ERROR will not be able to tell reference values apart
      title: 'Another author'
    }
  ]
}
```

Instead, you can *name* the reference type. This will work:

```javascript
{
  type: 'reference',
  name: 'authorReference',
  to: [
    {
      type: 'author',
      title: 'Author'
    },
    {
      type: 'author',
      name: 'anotherAuthorReference', // all good
      title: 'Another author'
    }
  ]
}
```

The value of this definition will have its `_type` set to either `author` or `anotherAuthor`, depending on which of the type were selected when the value was set  e.g.:

```json
{"_type": "reference", "_ref": "329e893ewi"}
```

Or: 

```json
{"_type": "anotherAuthorReference", "_ref": "293e90iok3elwq213er"}
```



# Incorrect location for reference options

The reference field allows you to define options for the input component. These options should be defined under the `options` key.

If you are encountering this error, it usually means that you've placed options on the root of the reference type instead of in the `options` object:

```javascript
export default {
  name: 'blogPost',
  type: 'document',
  fields: [
    // ... your other schema fields ...
    {
      name: 'author',
      type: 'reference',
      to: [{type: 'person'}],
      
      // INCORRECT:
      filter: 'age > 30',
      
      // CORRECT:
      options: {
        filter: 'age > 30'
      }
    }
  ]
}
```



# Invalid part syntax

How parts are defined also defines how they behave.

An **implementable part** is defined by setting a `name` and a `description`. It should *not* have a path set. If you find yourself wanting to set a path, you probably want to do the following:

```json
[
  {
    "name": "part:foo/bar",
    "description": "Some really good description"
  },
  {
    "implements": "part:foo/bar",
    "path": "./some/part.js"
  }
]

```

A **non-overridable** part can be defined by setting a `name` and a `path` in the same declaration:



```json
{
  "name": "part:@sanity/base/schema",
  "path": "./some/schema.js"
}
```



# Asset metadata field

An asset field (*image* or *file*) may have a `metadata` option. This option defines which metadata the server should attempt to extract when receiving the uploaded file and populate into the assets metadata document. This field must be an array of strings:

```javascript
{
  name: 'coverPhoto',
  title: 'Cover photo',
  type: 'image',
  options: {
    metadata: ['location', 'palette']
  }
}
```

The above example will attempt to extract palette and location data for any image uploaded to this field, and populate the metadata on the *asset*. For instance, if this field is part of a document of type `article`, you could run a query such as the following:

```text
*[_type == "article"] {
  ...,
  "coverPhoto": coverPhoto.asset->{
    url,
    metadata {
      location,
      palette {
        dominant {
          background,
          foreground
        }
      }
    }
  }
}
```

Which might return something along the lines of:

```json
[
  {
    "_id": "some-blog-post",
    "_type": "article",
    "title": "Some blog post",
    "coverPhoto": {
      "url": "https://cdn.sanity.io/images/foobar/test/aa1N73Zv14r7pYsbUdXl-4288x2848.jpg",
      "metadata": {
        "location": {
          "lat": 59.924104,
          "lon": 10.758437
        },
        "palette": {
          "dominant": {
            "background": "#99b8cd",
            "foreground": "#000"
          }
        }
      }
    }
  }
]
```



# Warning: userStore.currentUser is deprecated

The `userStore.currentUser` method has been deprecated in favor of `userStore.me` which is an observable stream of the current logged in user or `null` if the user is logged out.

Where the `userStore.currentUser` observable stream emitted a "snapshot" event object with the user object at the `user` property every time user state changed, the `userStore.me` emits the user object (or null if logged out) as you would see it from the `/users/me` API endpoint.

Example of how to migrate existing code currently using `userStore.currentUser` to instead use `userStore.me`:

## Before

```javascript
userStore.currentUser.subscribe(event => {
  console.log('Current user is:', event.user)
})
```

## After

```javascript
userStore.me.subscribe(user => {
  console.log('Current user is:', user)
})

```



# CLI errors

You may run into errors while using the CLI. Listed below are some explanations and common solutions for these errors. 


> [!WARNING]
> Gotcha
> Some error explanations may be missing. If you cannot find the error you are looking for, please use the feedback form to let us know or make a post in our Slack Community.

## Common errors while installing the CLI

### `Error: EACCES: permission denied, access '/usr/local/lib/node_modules'`

This error often occurs when you do not have the correct permission to install packages with `npm`.

You can fix this by changing the owner of the global `node_modules` folder using the following command:

```sh
sudo chown -R $USER /usr/local/lib/node_modules

```

Another option to fix this issue is managing your node version(s) with a version manager like [nvm](https://github.com/nvm-sh/nvm) or [asdf](https://asdf-vm.com/).

### `Error: spawn cmd ENOENT` on Windows machines

If you're using a Windows computer and running into an error that resembles the one above while attempting to install (or use) the Sanity CLI, it's likely that there is an issue with the `$PATH` environment variable of your operating system.

To fix this, ensure the variable is correctly set before rerunning the CLI. More information on troubleshooting can be found in [this thread on Stack Overflow](https://stackoverflow.com/questions/57054403/problem-with-npm-start-error-spawn-cmd-enoent).



## Common errors while using the CLI

### Port 3333 is already in use

This error often occurs when there is another process or service running on the Sanity studio's default port (3333).

To fix this, try stopping or closing other applications running on that port.

If you're on Mac, you can quickly kill the process running on port 3333 using the following command:

```sh
lsof -ti :3333 | xargs kill -9

```

If you'd instead like to change which process the Studio runs on, you can do so by passing an alternative value to the [--port option](https://www.sanity.io/docs/start) of the `sanity start` and `sanity preview` commands.

Just be sure to add this new development URL and port to your [Allowed CORS Origins](https://www.sanity.io/docs/cors#5a355ee47b66) list or the Studio will not be able query data from your project.

### Command `(start|dev|deploy|...)` is not available outside of a Sanity project context

If you're seeing this error, it means that the CLI can not identify your Sanity project context.

To fix this, try the following:

24. Ensure that you're running the command within the correct directory. Your Sanity project directory should have a `package.json` and a `sanity.json` file.
24. Ensure that you have all of the necessary dependencies installed. Do this by running a `npm install` or `yarn install` if you're managing your dependencies with yarn.
24. Delete your `node_modules` folder, reinstall the project's dependencies, and try running the command again.

If these solutions don't solve the issue, please get in contact with us either through the feedback form at the bottom of the page or our [Slack community](http://slack.sanity.io/).

### Command failed with exit code 1 (EPERM): `npm install next-sanity@7`

If you're seeing this error, it's likely that you're on a Windows computer with insufficient permissions for installing dependencies with `npm`.

To fix this, try:

29. Running your command line program as an Administrator
29. Run `npm cache clean --force` and `npm cache verify`
29. Uninstall and reinstall Node.js

If these solutions don't solve the issue, please get in contact with us either through the feedback form at the bottom of the page or our [Slack community](http://slack.sanity.io/).

### `sanity.cli.(js|ts)` does not contain a project identifier

If you're seeing this error, first ensure that your `sanity.cli.(js|ts)` file contains a `projectId` and `dataset` in the `defineConfig` function. If that's not the case, add those details and retry the command you were attempting to execute.

If your `sanity.cli.(js|ts)` file looks correctly setup with those attributes present, try rerunning the CLI command with `npx`. For example, if you were previously trying to run `sanity deploy`, try running `npx sanity deploy`.

If the command executes without error, it's likely that your local CLI version is out of date. Try upgrading with `npm i -g @sanity/cli`.

If these solutions don't solve the issue, please get in contact with us either through the feedback form at the bottom of the page or our [Slack community](http://slack.sanity.io/).

### Unauthorized - You do not have access to the project with ID <projectID>

This error occurs when you run a command without the appropriate permissions. Common causes can be:

- Incorrect or misspelled project ID in your `sanity.json`.
- You don't have the rights to deploy a project. Need to be an Administrator or have a deploy token to do this.
For example: running `sanity graphql deploy` with Write or Read+Write access only will give you this error.

### Unauthorized - User is missing required grant sanity.project/deployStudio to perform this operation

This error occurs on `sanity deploy` when you have access to the studio but without the required permissions to deploy.

To fix this, ensure that you are logged into the CLI with the correct credentials for your project. You can easily do this by logging out of the CLI with the following command:



```sh
npx sanity logout
```

And logging back in again with the following command:

```sh
npx sanity login
```

### Unauthorized - Session not found

This can be one of several issues:

- A temporary issue, please try to run your command again.
- You have specified an invalid token with the `SANITY_AUTH_TOKEN` env variable.
- The session timed out. Try to log out and log in again with the `sanity logout` and `sanity login` CLI commands.
- There was an issue with your logged in user. Try to logout and login again.







# Renamed plugin sanity-plugin-vision

The plugin `sanity-plugin-vision` has been renamed to `@sanity/vision`.

### What should I do?

3. Install the `@sanity/vision` plugin with `sanity install @sanity/vision`
3. Remove `"vision"` from the `plugins` array in your `sanity.json`



# Part name format

A *part name* must start with `part:` and be followed by a prefix that matches the plugin that defines it, as well as an identifier for this particular part.

Ergo: `part:my-plugin/part-identifier`





# Array member type name is the same as a global type

This warning means you have an array type that has a member that is given the same name as one of your global schema types.

When defining an array type in your schema you have the option to quickly declare several "inline" object types and give each one of them their own name to be able to distinguish between them. For example, the following could be used to define an array that can hold different variations of contact info without having to declare `address` and `phone` as separate schema types.

This allows for "locally scoped", inline types that you don't want to re-use across other schema types.

```javascript
{
  name: 'contactInfo',
  type: 'array',
  of: [
    {
      type: 'object',
      name: 'address'
      title: 'Address',
      fields: [{name: 'street', type: 'string'}, /* … */]
    },
    {
      type: 'object',
      name: 'phone'
      title: 'Phone',
      fields: [{name: 'number', type: 'string', /* … */}]
    }
  ]
}
```

For inline object types we recommend not giving them the same name as an existing schema type, but either choose a different name, or refer to the globally defined schema type instead.



# Changes in block schema customization properties

As of sanity v3.1 decorators, annotations and styles will accept a `component` property that will handle any custom rendering of these types in the Portable Text input. This new property replaces the `blockEditor.render` property.

The `icon` property can be put directly on the root type as with all other schema types. This is replacing the `blockEditor.icon` property.

So if you previously did this in your block type schema:

```javascript
decorators: [
  {
    title: 'Highlight',
    value: 'highlight',
    blockEditor: {
      icon: MarkerIcon,
      render: highlightRender,
    },
  },
],
```

You should now do this:

```javascript
decorators: [
  {
    title: 'Highlight',
    value: 'highlight',
    icon: MarkerIcon,
    component: Highlight,
  },
],
```

Read more about customizing the [Portable Text Editor](/docs/studio/customizing-the-portable-text-editor)



# How to migrate from date to richDate

We'll soon rename Sanity's internal `date` type to `richDate`. If you're *not* using `date`, don't worry about any of the below.

This is unfortunately a breaking change. These are the three actions required of you:

### 1. Make your front-end(s) tolerate both the old and the new type

In a transition period, front-ends which consume Sanity documents and do conditional checks on `_type === 'date'`, should be updated to handle both the old `date` type and the new `richDate` type. If you don't do any checks on `_type === 'date'`, you can skip this step entirely.

### Before:

```javascript
if (value._type === 'date') {
  // ... doing something with the date value
}
```

### After:

```javascript
if (value._type === 'date' || value._type === 'richDate') {
  // ... doing something with the richDate value
}
```

Note: when all your date values are migrated (see pt. 3 below), you can safely remove the `value._type === 'date'` check.

Also note that only the `_type` attribute will change, so accessing the other attributes of your date object will work as before.

```javascript
moment(person.bornOn.utc).fromNow() // <-- this will still work
```

### 2. Modify your schema

All `date` fields must be changed.

### From this:

```javascript
{
  title: 'Birthday'
  name: 'bornOn',
  type: 'date'
}
```

### To this:

```javascript
{
  title: 'Birthday'
  name: 'bornOn',
  type: 'richDate'
}
```

Any options, as described in the [date documentation](/docs/studio/date-type), stay the same.

### 3. Migrate your data

All documents containing one or more `date` fields must be changed to `richDate` . We have written a [script](https://github.com/sanity-io/migrations/blob/master/date-to-richdate.js) that does this automatically for you. This can be executed with the command line tool `npx` , which has been shipping with `npm` from version `5.2.0`:

```markdown
cd <your sanity studio project folder>
npx -p sanity-io/migrations date-to-richdate
```

### Yes, but why?

We're of the opinion that the type named `date` should be represented as a pure string (e.g. `'2017-02-08T01:30:00+01:00'` or `'2017-02-08T00:30:00Z'`) instead of an object. Both because this the least surprising behavior and because it conforms with the `_createdAt` and `_updatedAt` fields which are automatically maintained by the data backend.

Shortly after all `date` --> `richDate` migrations are complete, we'll release a new version of Sanity which offers two distinct date types: `date` (a string representation) and `richDate` (an object representation).

Thanks for the patience!

Want to read the developer discussion? The issue is [over here](https://github.com/sanity-io/sanity/issues/79).



# Invalid shape of predefined choices

As a general rule, the list of possible choices for array types must only contain values of valid item types for the array.

```javascript
{
  type: 'array',
  name: 'colors'
  of: [
    {
      type: 'object',
      name: 'webColor',
      fields: [
        {name: 'name', type: 'string'},
        {name: 'hex', type: 'string'}
      ]
    },
    {
      type: 'object',
      name: 'rgbaColor',
      fields: [
        {name: 'name', type: 'string'},
        {name: 'r', type: 'number'},
        {name: 'g', type: 'number'},
        {name: 'b', type: 'number'},
        {name: 'a', type: 'number'},
      ]
    },
  ],

  options: {
    list: [
      // valid
      {_type: 'webColor', hex: '438D80', name: 'Sea Turtle Green'},

      // valid
      {_type: 'rgbaColor', r: 161, g: 201, b: 53, name: 'Salad Green'},


      // invalid (as of v3), object values can not be given a title
      {title: 'Sea Turtle Green', value: {_type: 'webColor', hex: 'C88141', name: 'Tiger Orange'}},

      // invalid, missing _type
      {hex: '438D80', name: 'Sea Turtle Green'},

      // invalid: hslaColor objects not valid for array
      {_type: 'hslaColor' h: 0.02, s: 0.93, l: 0.71, name: 'Salmon'},
      
    ]
  }
}
```

A notable exception here is choices for primitive values, which can be given a display title by providing an object with `title` and `value`, where value is of a valid item type:

```javascript
{
  type: 'array',
  name: 'numbersAndAnimals'
  of: [{type: 'string'}, {type: 'number'}],

  options: {
    list: [
      // valid: this array can contain strings
      'sheep',

      // valid: this array can contain numbers
      44,

      // valid: array can contain strings and primitive values can be given a display-title
      {title: 'Cat', value: 'cat'},

      // valid: array can contain numbers and primitive values can be given a display-title
      {title: 'Hundred', value: 100}
      
      // invalid: array can't contain booleans
      true,

    ]
  }
}
```





# JS Client: Promise Polyfill

The Sanity JavaScript client uses `Promises` to handle asyncronous requests. Some browsers (such as Internet Explorer) does not support this interface by default, and thus requires a so called *polyfill* for it to work.

There are many polyfills to choose from - we recommend something that comforms to the [Promises/A+ specification](https://promisesaplus.com/), such as the [es6-promise](https://www.npmjs.com/package/es6-promise) module. The readme for that module includes various ways to use it - the most important thing to note is that you should make sure the polyfill is applied before the Sanity client is instantiated. 

Here's an example if you are using a bundler such as webpack, browserify or parcel:

```javascript
require('es6-promise/auto')
const sanityClient = require('@sanity/client')

module.exports = sanityClient({
  projectId: '...',
  dataset: '...',
  useCdn: true
})

```

(Make sure es6-promise is added as a dependency for your project.)



# Introducing the document type

In version [0.118.0](https://github.com/sanity-io/sanity/releases/tag/v0.118.0) we introduced a new type `document`. This is the type for any object that you would like to store as documents in the datastore. Previously, any object type defined in your schema could be turned into a document, but now you must define these as documents instead. Only document types will appear in the desk tool sidebar.

NOTE: You should still use `type: 'object'` for the schema types that is reused on fields in your schema types (e.g. things like `localeString` and other types that you would never create standalone documents of)

## What should I do?

This is not a breaking change, so everything will continue to work as before. That is, **until** the moment you decide to use the `document` type. If you add a document type to your schema, you should also have to change the type of all the top-level object types in your schema. E.g. if your schema was:

```javascript
export createSchema({
  name: 'mySchema',
  types: [
    {
      name: 'book',
      type: 'object',
      fields: [
        {name: 'title', type: 'string'}
      ]
    }
    //...
  ]
})
```

You should change this to:

```javascript
export createSchema({
  name: 'mySchema',
  types: [
    {
      name: 'book',
      type: 'document',
      fields: [
        {name: 'title', type: 'string'}
      ]
    }
    //...
  ]
})
```

And do this for all types in your schema that you would like to be stored as documents. Note: you may still want re-usable object types at top-level in your schema, but these should should stay with type objects. In that case they will not be listed in the sidebar.

Note: The introduction of the document type makes the `hiddenTypes` config in the  `config/@sanity/data-aspects.json` config file obsolete, and you should remove it entirely.



# Third party login

Enterprise customers have the ability to support studio users to log in with their own authentication servers.

This requires the *thirdPartyLogin* feature to be set on the project.

If you are interested in enabling this for your project, [contact us](https://www.sanity.io/contact/).



# Unable to get a ref to an input component

This happens when the editor is unable to create a [ref](https://reactjs.org/docs/glossary.html#refs) to an input component. This is likely because of one of the following reasons:

- The input component is wrapped in a *higher order component *(HOC), which does not delegate a `focus()` method to the component it wraps. [See this guide on how to forward a ref inside a higher order component](https://reactjs.org/docs/forwarding-refs.html#forwarding-refs-in-higher-order-components).
- The input component is a [function component](https://reactjs.org/docs/components-and-props.html#function-and-class-components). Since function components cannot be given refs, the input component must be wrapped using [React.forwardRef](https://reactjs.org/docs/react-api.html#reactforwardref) in order to specify which element should receive focus. Note: keep in mind that the forwarded ref must be attached to an element that actually exposes a `.focus()` method.



# Authenticating the CLI when running remotely

As the authentication process for the CLI relies on you interacting with the API through a browser, this can be a problem when running the CLI on a remote server. However if you follow this recipe, you will be able to successfully authenticate the remote installed CLI.

2. Go to `https://api.sanity.io/v1/auth/login/github?type=token&origin=http://localhost
`
Exchange `github` for `google` if you prefer that. You don't need anything actually running on localhost.

2. Pick out the value of the `sid` parameter in the return url trying to redirect to localhost.

2. Load `https://api.sanity.io/v1/auth/fetch?sid=xxxx` using the `sid` value from above.

2. Get the `token` property from the result.

2. Create the file `~/.config/sanity/config.json` on the remote server in the home directory for the user running the CLI.

Set the file contents to the following,  and replace "yyyy" with the token value from above:

```json
{
  "authToken": "yyyy",
  "authType": "normal"
}
```





# Outdated modules

Some of the modules in your Sanity studio is on a version that we no longer support.

Usually, this is related to APIs that have changed and will no longer function when paired with the modules in question. In these cases, things might actually stop working. In other cases, the modules are simply so old that they might stop working when paired with other plugins and functionality in Sanity.

Either way, the upgrade process should be fairly simple:

4. Open up your terminal and find your Sanity studio folder
4. Run `sanity upgrade`. This will check for the latest versions of the modules you have installed, download them and set up any configuration files that might have been added.
4. Run your studio locally with `sanity start` and ensure that things seem to be working as it should. 

If you are having trouble upgrading, you can try to ask for help [on Gitter](https://gitter.im/sanity-io/sanity).



# Upgrade studio packages

From time to time, versions of packages Sanity Studio depends on needs to be upgraded. This can be done either by manually entering a new version of the dependency in your studio folder's `package.json`, or by running a command from your command line.

Currently, the Studio requires the following packages and versions:

- `react@^18.2.x`
- `react-dom@^18.2.x`
- `styled-components@^6.x`

#### Sanity UI

Additionally, if your Studio includes customizations using `@sanity/ui`, the version of `@sanity/ui` required by the current version of the Sanity Studio is **v2.1.0** or later. (We recommend always upgrading to the latest version.)

### Upgrading from styled-components v5 to v6

To upgrade from styled-components v5 to v6, you can either edit package.json directly and run `npm|pnpm|yarn install` after. In addition to upgrading styled-components, you might also remove the `react-is` dependency that used to be a required peer dependency of styled-components v5, unless your studio code directly depends on it.

#### npm

`npm install "styled-components@^6"`

`npm rm @types/styled-components react-is`

#### pnpm

`pnpm install "styled-components@^6"`

`pnpm rm @types/styled-components react-is`

#### yarn

`yarn add "styled-components@^6"`

`yarn rm @types/styled-components react-is`

Note: if you have local customizations in your Sanity Studio that's using `styled-components` you may need to  do some adjustments to your Studio code as well. Please consult the [styled-components migration guide](https://styled-components.com/docs/faqs) for more details.

### Upgrading React

#### npm

`npm install "react@^18.3" "react-dom@^18.3"`

#### pnpm

`pnpm install "react@^18.3" "react-dom@^18.3"`

#### yarn

`yarn add "react@^18.3" "react-dom@^18.3"`

Note: if you have local customizations in your Sanity Studio and are upgrading between major versions of React (e.g. going from version 17 to 18) you may need to  do some adjustments to your React components as well. Please consult the [React changelog](https://reactjs.org/versions/) for details on how to migrate to the latest version.



# Block Content rendering: Image materializing

The image type holds a set of user-defined fields as well as an `asset` field which is a reference to the actual [asset document](/docs/asset-pipeline). Quite often you will need to get ahold of the asset document in order to make decisions based on the size, name, type, or metadata of the image, or to get the full URL to the image.

## Joining the asset document using GROQ

You can join these references when you fetch the document(s) containing a Portable Text field. Let's say you have a document type named `article` which has a `body` field containing an array of blocks. The following query would expand all the `asset` fields within the array:

```json
*[_type == "article"]{
  body[]{
    ..., 
    asset->{
      ...,
      "_key": _id
    }
  }}[0...5]
```

Let's break it down:

- Fetch all documents of type `article`: `*[_type == "article"]`
- For each item in the `body` array: `body[]`
- Return all the properties: `...`
- Make a property called `asset` and let the value be the materialized value of the `asset` property: `"asset" asset->`
- Only return the 5 first documents matched: `[0...5]`

## More information on querying data

Looking to get started working with data from your Sanity data store? Find out [how GROQ queries work](/docs/content-lake/how-queries-work) or dive in with [Sanity's GraphQL interface](/docs/content-lake/graphql).





# Structure: Document schema type required

Certain nodes within the desk structure requires a document schema type to be defined in order to operate. 

Setting a schema type can be done by calling the `schemaType()` method:



```javascript
S.document()
  .id('car-editor')
  .schemaType('car')
  .documentId('am-db9')

```





# Parts: Declare vs implement

How parts are defined also defines how they behave.

An **implementable part** is defined by setting a `name` and a `description`. It should *not* have a path set. If you find yourself wanting to set a path, you probably want to do the following:

```json
[
  {
    "name": "part:foo/bar",
    "description": "Some really good description"
  },
  {
    "implements": "part:foo/bar",
    "path": "./some/part.js"
  }
]

```

A **non-overridable** part can be defined by setting a `name` and a `path` in the same declaration:



```json
{
  "name": "part:@sanity/base/schema",
  "path": "./some/schema.js"
}
```



# Incorrect options declaration in reference

The reference field allows you to define options for the input component. These options should be defined under the `options` key and be an object.

If you are encountering this error, it usually means that you've defined an options key which is not an object:

```javascript
export default {
  name: 'blogPost',
  type: 'document',
  fields: [
    // ... your other schema fields ...
    {
      name: 'author',
      type: 'reference',
      to: [{type: 'person'}],
      
      // INCORRECT
      options: [{some: 'option'}],
      
      // CORRECT
      options: {
        some: 'option'
      }
    }
  ]
}
```



# Block type cannot be used outside of array

Sanity Studio *currently* only supports using the block type in an array, not as a standalone field.

This will **not** work:

```javascript
{
  name: 'myField',
  title: 'My field',
  type: 'block'
}
```


But **this** will:

```javascript
{
  name: 'myField',
  title: 'My field',
  type: 'array',
  of: [{type: 'block'}]
}
```



# Structure: Node ID required

All nodes within the desk structure has an ID assigned to it. Normally, the ID is assigned automatically based on the title of the item, but some items require a manually assigned ID.

One example of this is the document list item - its ID refers to a specific document ID, and as such it needs to be manually assigned.

Setting an ID can be done by calling the `id()` method:



```javascript
S.documentListItem()
  .id('website-featured-articles')
  .schemaType('article-set')
  .title('Site config')
```





# Structure: List items must be an array

The desk structure list takes an *array* of items. A common mistake is to passing a list of items as arguments instead of an array:



```javascript
// Incorrect:
S.list()
  .title('Content')
  .items(
    S.listItem().title('Foo'),
    S.listItem().title('Bar')
  )

// Correct:
S.list()
  .title('Content')
  .items([
    S.listItem().title('Foo'),
    S.listItem().title('Bar')
  ])

```



# Installing Node.js

Node.js can be described as a tool to develop and run web-servers written in JavaScript ([read more](https://nodejs.org/en/about/)). In order to develop Sanity studios, you must have Node installed on your computer.

The Node people maintain a [comprehensive guide](https://nodejs.org/en/download/package-manager/#macos) to installing Node on whatever OS you may be running.

**Notes on installing Node on macOS**: The guide above mentions several different ways to install Node on macOS. It is our opinion that you should install Node via Homebrew. Follow [this guide](https://brew.sh/) to get Homebrew, then just `brew install node`.

All set? Let's [get started](/docs/archive/create-a-sanity-project)!



# Structure: Action or intent required

Menu items needs to know what to do when they are selected. This can done by specifying one of to parameters:

- `action` - which is a function called with the parameters set for this menu item
- `intent` - an object containing a `name` and an optional bag of `params`

Certain nodes within the desk structure requires a document ID to operate on. 

Setting an action or intent can be done by calling the `action()` or `intent()` methods, respectively:

```javascript
new MenuItemBuilder()
  .title('Open in website')
  .icon(OpenIcon)
  .params({breed: 'schnauzer'})
  .action(params => {
    window.open(`https://mywebsite/breeds/${params.breed}`)
  })

```





# Object type has a invalid value for fields

Documents or object types must define which fields they have. The fields property must be an array of field definitions, where both `name` and `type` are required, and each field having a unique `name`.

Additionally, field names must start with a letter from A-Z, and can can only include A-Z, numbers and underscore. [We recommend using camel case convention for field names](/docs/apis-and-sdks/naming-things).

```javascript
{
  type: 'object',
  name: 'myObject',
  fields: [ // fields must be defined, and it must be an array
    {
      name: 'myField', // field name is required and must be unique
      type: 'string' // field type is required
    },
    // ... 
    {
      name: 'myField', // 💥 ERROR a field named "myField" is already defined on this object
      type: 'string'
    }
  ]
}
```



# `studioHost` and `externalStudioHost` properties deprecated

Your projects are no longer tied to a single Sanity Studio application - you can deploy multiple different studios which could all talk to one or more datasets and projects.

In the past, there were two properties attached to project information APIs that recorded information about the studio for that project, `studioHost` and `metadata.externalStudioHost`. These are no longer guaranteed to be present and does not reflect the full truth of deployed studios/applications for those that deploy multiple studios.

If you have a need to programmatically access a list of your studio deployments, let us know!





# Schema type is ES Module but imported through require

This happens when you have a schema type definition in a file like the following:

```javascript
export default {
  // ... type definition here...
}
```

...but import it using CommonJS `require`:

```javascript
import createSchema from 'part:@sanity/base/schema-creator'

export default createSchema({
  name: 'test-examples',
  types: schemaTypes.concat([
    // ... your types ...
    require('./someTypeDef.js')
  ])
})

```

The best solution is to use an import statement instead of require:

```javascript
import createSchema from 'part:@sanity/base/schema-creator'
import someTypeDef from './someTypeDef'

export default createSchema({
  name: 'test-examples',
  types: schemaTypes.concat([
    // ... your types ...
    someTypeDef
  ])
})

```



# Structure: Invalid list item

A desk structure list takes an array of list items to display. If you encounter an error saying a list item is invalid, common causes can be:

- Passing a promise or an observable instead of an actual list item. If you actually need to resolve a list item asynchronously, resolve the items before you resolve the list definition.
- You passed an array of items within the list. For instance, you might have called the `documentTypeListItems()` method, but did not use the spread operator to flatten the returned items into the array: `...documentTypeListItems()`





# Structure: Query provided where filter is expected

Certain nodes within the desk structure requires a filter. A filter is the part of a [GROQ-query](/docs/content-lake/how-queries-work) which specifies which documents should be matched - the constraints of a query, if you will.

While a full GROQ-query could look like this:


```text
*[_type == "movie" && releaseDate > $afterDate] {
  _id, titlex, releaseDate
} [0...20]
```

The *filter* of the query is simply:



```text
_type == "movie" && releaseDate > $afterDate
```





# Structure: List item IDs must be unique

Within a single list, there can be no duplicate IDs. The IDs are used to resolve which child to render as the next item.

If you are not manually assigning IDs, it probably means that the title of two or more of your list items are the same, since the ID is inferred from that if not specified.

When this is the case, you can solve it by calling `id('someOtherId')` on the list items that conflict.



# Given type name is a reserved type

If you get this error, it means you most likely have tried to add a type to your schema that clashes with one of the builtin types. Currently the reserved types are:

`any`, `array`, `block`, `boolean`, `date`, `datetime`, `document`, `email`, `file`, `geopoint`, `image`, `number`, `object`, `reference`, `slug`, `string`, `telephone`, `text`, `time`, `type` and `url`.

If you got a type with one of these names in your schema, you'll have to give it another name or remove it.



# Structure: Schema type not found

This error occurs when the desk structure tries to find a schema type but did not find a match. Usually this is caused by a typo in the type name, or forgetting to import and include the document type in the studio schema definition.

First, check for any typos (obviously).

Secondly, check your schema definition (usually `<your-studio>/schemas/schema.js`) and ensure that you have both imported and included the document type in the call to `createSchema()`:

```javascript
import createSchema from 'part:@sanity/base/schema-creator'
import schemaTypes from 'all:part:@sanity/base/schema-type'

// Make sure you import the document type
import someDocumentType from './someDocumentType'

export default createSchema({
  name: 'default',
  types: schemaTypes.concat([
    // Make sure you include the type in this array:
    someDocumentType
  ])
})

```





# API versioning

Looking for information on API Versioning? View [the official documentation](/docs/content-lake/api-versioning) or visit [the changelog](https://sanity.io/changelog) to see what has changed in various versions.



# Migrating the legacy webhook behavior to GROQ-powered Webhooks

If you need to recreate the previous webhook behavior – triggering on all changes, and on a dataset level rather than document-level – you can do so by following these steps:

2. Create a webhook set to trigger on **create**, **update** and **delete**
2. Leave the **Filter** field empty
2. Add the following to the **Projection** field

```groq
// webhook projection
{
  "transactionId": "Not supported",
  "projectId": sanity::projectId(),
  "dataset": sanity::dataset(),
  "ids": {
    "created": [
    	select(before() == null && after() != null => _id)
    ],
    "deleted": [
      select(before() != null && after() == null => _id)
    ],
    "updated": [
      select(before() != null && after() != null => _id)
    ],
    "all": [
      _id
    ]
  }
}
```

You can also [click this link to get a template with the settings described above](https://www.sanity.io/manage/webhooks/share?name=Legacy+webhook&description=Recreation+of+legacy+webhooks&url=&on=create&on=delete&on=update&filter=&projection=%7B%0A++%22transactionId%22%3A+_rev%2C%0A++%22projectId%22%3A+sanity%3A%3AprojectId%28%29%2C%0A++%22dataset%22%3A+sanity%3A%3Adataset%28%29%2C%0A++%22ids%22%3A+%7B%0A++++%22created%22%3A+%5B%0A++++%09select%28before%28%29+%3D%3D+null+%26%26+after%28%29+%21%3D+null+%3D%3E+_id%29%0A++++%5D%2C%0A++++%22deleted%22%3A+%5B%0A++++++select%28before%28%29+%21%3D+null+%26%26+after%28%29+%3D%3D+null+%3D%3E+_id%29%0A++++%5D%2C%0A++++%22updated%22%3A+%5B%0A++++++select%28before%28%29+%21%3D+null+%26%26+after%28%29+%21%3D+null+%3D%3E+_id%29%0A++++%5D%2C%0A++++%22all%22%3A+%5B%0A++++++_id%0A++++%5D%0A++%7D%0A%7D&httpMethod=POST&apiVersion=v2021-03-25&includeDrafts=).



# Schema type is invalid

The type defined in your schema is not a valid schema type. Common culprits:

- The type declaration is imported through `import`/`require` from a different file, but the import declaration either references an incorrect name or the imported file does not have any export declaration
- Something is returning `undefined`, `null` or `false` instead of the schema type declaration



Double check the `types` array of your schema declaration at the specified index to figure out where the error stems from.



# Input component is missing a required prop

All input components should be passed a `onFocus` and `onBlur` prop.

Read more about [Custom input widgets](/docs/archive/custom-input-widgets)



# Structure: Title is required

Certain nodes within the desk structure requires a title. The title is used mainly for presentation concerns, but will also be used to generate a node ID if one is not given.

Setting a title can be done by calling the `title()` method:



```javascript
S.documentList()
  .id('cars')
  .title('Cars')
  .filter('_type == $type')
  .params({type: 'car'})
```





# Structure: Filter is required

Certain nodes within the desk structure requires a filter. A filter is the part of a [GROQ-query](/docs/content-lake/how-queries-work) which specifies which documents should be matched - the constraints of a query, if you will.

Let's imagine you want to run a query to find all documents that do not currently have a slug set (in a field called `slug`). While the full GROQ-query would look like this:


```text
*[!defined(slug.current)]
```

The *filter* of the query is simply:



```text
!defined(slug.current)
```

Setting a filter can be done by calling the `filter()` method:



```javascript
S.documentList()
  .title('Missing slug')
  .filter('!defined(slug.current)')

```





# Import: Asset file does not exist

This error usually occurs when you are importing documents using `sanity dataset import ...` and images or files (also known as assets) aren't found.

This typically happens if a file isn't at the given path on your local system or the asset URL returns 404.

The solution is to ensure that each path and URL actually points to a file. Note that local file paths must be absolute, not relative:

**Correct**: `image@file:///local/path/to/rogue-one-poster.jpg`

**Wrong**: `image@file://../../local/path/to/rogue-one-poster.jpg`

Sometimes it's ok if not all assets are imported successfully. E.g. you're fetching tons of cat gifs off the Internet and some of them are bound to not exist. If you can live with that, use the `--allow-failing-assets` flag when running your import command.

You can read more about [importing data here](/docs/content-lake/importing-data).



# Input component is missing a required method

All input components should implement a .focus() method.

Read more about [Custom input widgets](/docs/archive/custom-input-widgets)



# Implementing non-overridable part

Some parts are defined as non-overridable. Simply put, they should only be defined once. An example of this is the schema part - `part:@sanity/base/schema`, usually defined as the first thing in your studios `sanity.json`.

It doesn't make sense for other plugins to override this part, but by definining it as a part allows us to access the schema from anywhere without knowing the specific path to where it is located on disk. Another use case would be to provide the actual schema through a plugin instead of through the studio.

If you are encountering this error, it usually means that you have tried to implement a part that should not be overriden. If you think that the part in question is something you should be allowed to override, [reach out to us](https://www.sanity.io/contact).





# Structure: Item returned no child

In most cases, you will want to return a child when a list item is clicked. If you are receiving this warning, your list has probably not defined a child/child resolver, or the child resolver is returning `undefined`.

You usually want to specify a child for an item:

```javascript
S.listItem()
  .title('George R. R. Martin')
  .child(
    S.documentList()
      .title('GRRM books')
      .filter('_type == "book" && author._ref == "grrm"')
  )
```

If you intentionally don't want to return any child, define the child or child resolver to be `null` instead of `undefined`:



```javascript
S.documentListItem()
  .id('grrm')
  .title('George R. R. Martin')
  .schemaType('author')
  .child(null)
```



# How to migrate your block text schema for the new definition of inline objects

Previously inline objects in your text blocks were defined like this via the `options.inline` property:

```
{
  name: 'body',
  type: 'array',
  title: 'Content',
  of: [
    {type: 'block'},
    {
      name: 'author',
      title: 'Author',
      type: 'reference',
      to: {type: 'author'},
      options: {inline: true}
    }
}

```

We didn’t get the syntax totally right here, as inline nodes are children of the block type and therefore shouldn’t be defined on the same level as the other blocks in your block array.

So we needed to change it to make it work with validations – the new way to do this is via the block type’s of property:

```
{
  name: 'body',
  type: 'array',
  title: 'Content',
  of: [
    {
      type: 'block',
      of: [
        {
          name: 'author',
          title: 'Author',
          type: 'reference',
          to: {type: 'author'}
        }
      ]
    }
}

```

If you would like to support embedding an author object both inline *and* as a block, you would do it like this:

```
{
  name: 'body',
  type: 'array',
  title: 'Content',
  of: [
    {
      type: 'block',
      of: [
        {
          name: 'author',
          title: 'Author',
          type: 'reference',
          to: {type: 'author'}
        }
      ]
    },
    {
      name: 'author',
      title: 'Author',
      type: 'reference',
      to: {type: 'author'}
    }
}

```





# Structure: Schema type is required

Certain nodes within the desk structure requires knowledge of which schema type a document or a list of documents operates on.

Setting a schema typecan be done by calling the `schemaType()` method:



```javascript
S.editor()
  .id('car-editor')
  .schemaType('car')
  .documentId('am-db9')

```





# How to migrate from blocks spans to block children

We are resolving a few issues with the way block content is structured. Unfortunately, this is a breaking change and will thus require a migration. There are a couple of steps involved:

### Update your frontend modules to support both the new and the old format

There are a couple of official ways of rendering block content - if you have a Javascript app and are using [block-content-to-react](https://github.com/sanity-io/block-content-to-react) or[ block-content-to-html](https://github.com/sanity-io/block-content-to-html-js), please upgrade those to the latest versions. If you are using the [PHP client](https://github.com/sanity-io/sanity-php), update that to the latest version.

### Update schema

Unless you have customized the [block](/docs/block-type) type within your schema, you shouldn't have to do anything. However if you have customized it with allowed marks, or special rules for the span, you must ensure that the block type schema definition has the following new structure:

![](https://cdn.sanity.io/images/3do82whm/next/97c5becfc9eb575178ee3679fdf0ac70d9d88c00-2264x644.png)

See the updated [block](/docs/block-type) and [span](/docs/span-type) documentation for more information.

### Migrate your data

All documents containing one or more `block` fields must be migrated to the new structure. We have written a [script](https://github.com/sanity-io/migrations/blob/master/block-spans-to-children.js) that does this automatically for you. This can be executed with the command line tool `npx` , which has been shipping with `npm` from version `5.2.0`:

```markdown
cd <your sanity studio project folder>
npx -p sanity-io/migrations block-spans-to-children
```

### Update your studio

Run sanity upgrade in your studio to get the latest modules which includes a new version of the block editor. Running this new version without first migrating your data will yield a warning telling you that data needs migration.

### Technical details

Note: This isn't mandatory reading unless you want to understand the changes to the data structure.

Previously, blocks had a key named `spans` which we're renaming to `children`. This is more in line with how most people think of nodes within  their data model. We're also changing the way rich marks are represented. Instead of simply being attributes on each child, they are now pointers to a mark definition on a per-block basis. This helps to prevent duplication of data and also makes it easier to nest nodes in trees.

Old structure:

```json
{
  "_key": "d3bf4f7519f3",
  "_type": "block",
  "style": "normal",
  "spans": [
    {"_type": "span", "marks": ["em"], "text": "Headless CMS?"},
    {"_type": "span", "marks": [], "text": " Check out "},
    {"_type": "span", "marks": [], "text": "Sanity", "link": {"href": "https://sanity.io"}},
    {"_type": "span", "marks": [], "text": ", it's pretty neat"}
  ]
}

```

New structure:

```json
{
  "_key": "d3bf4f7519f3",
  "_type": "block",
  "style": "normal",
  "markDefs": [{
    "_type": "link",
    "_key": "someKey",
    "href": "https://sanity.io"
  }],
  "children": [
    {"_type": "span", "marks": ["em"], "text": "Headless CMS?"},
    {"_type": "span", "marks": [], "text": " Check out "},
    {"_type": "span", "marks": ["someKey"], "text": "Sanity"},
    {"_type": "span", "marks": [], "text": ", it's pretty neat"}
  ]
}

```





# Array type cannot contain array member

All array types must define what kind of items they may contain. The "`of`" property must be an array of objects that describes the type of a valid item. Each entry in `of`, must have a `type`-property which must be the name of a valid schema type that is *not* an array - Sanity currently does not support arrays inside of arrays, also known as multidimensional arrays.

A common use case for multidimensional arrays is when you want to represent rows and columns. One possible solution in this example is to wrap each row in an object type:

```javascript
export default createSchema({
  name: 'default',
  types: schemaTypes.concat([
    {
      name: 'row',
      title: 'Row',
      type: 'object',
      fields: [
        {
          name: 'columns',
          title: 'Columns',
          type: 'array',
          of: [{type: 'string'}]
        }
      ]
    },
    {
      name: 'someDocumentType',
      title: 'Some document type',
      type: 'document',
      fields: [
        {
          name: 'rows',
          title: 'Rows',
          type: 'array',
          of: [{type: 'row'}]
        }
      ]
    }
  ])
})
      
```



# Using tokens in the browser

Using a Sanity token in JavaScript that is consumed by a browser is usually a really bad idea unless you take extra precautions to protect it from unauthorized users.

Please make sure to read about [how to keep your data safe](/docs/content-lake/keeping-your-data-safe).

If you are **absolutely** sure you know what you are doing and have taken steps to protect the access token from leaking, you can disable the warning in the sanity client by setting the option `ignoreBrowserTokenWarning` to `true`, for example:

```javascript
const sanityClient = require('@sanity/client');

const client = sanityClient({
  dataset: '<dataset name>',
  token: '<secret token>',
  ignoreBrowserTokenWarning: true
});
```



# GraphQL

GraphQL is now out of beta. [Go to documentation](/docs/content-lake/graphql).



# Array member type name conflicts with built-in type

This error means you have an array type that has a member that is given the same name as one of the built-in types.

When defining an array type in your schema you have the option to quickly declare several "inline" object types and give each one of them their own name to be able to distinguish between them. For example, the following could be used to define an array that can hold different variations of contact info without having to declare `address` and `phone` as separate schema types.

This allows for "locally scoped", inline types that you don't want to re-use across other schema types.

```javascript
{
  name: 'contactInfo',
  type: 'array',
  of: [
    {
      type: 'object',
      name: 'address'
      title: 'Address',
      fields: [{name: 'street', type: 'string'}, /* … */]
    },
    {
      type: 'object',
      name: 'phone'
      title: 'Phone',
      fields: [{name: 'number', type: 'string', /* … */}]
    }
  ]
}
```



For inline object types we require their names to not conflict with the built-in types Sanity already ships with.

This means that naming your inline type things like "string", "reference" or "image" will cause an error. Consider this example:

```javascript
{
  name: 'contactInfo',
  type: 'array',
  of: [
    {
      type: 'object',
      name: 'address'
      fields: [{name: 'street', type: 'string'}, /* … */]
    },
    {
      type: 'object',
      name: 'reference' // <-- This will error because "reference" is a built-in type
      fields: [{name: 'caption', type: 'string', /* … */}]
    }
  ]
}
```

Here, using "reference" as the name for the second object will cause an error because "reference" is the name of a built-in type. The fix here would be to pick another name, e.g. `contactReference`

The list of built-in types can be found [here](https://github.com/sanity-io/sanity/blob/current/packages/%40sanity/schema/src/sanity/coreTypes.ts). 

> [!WARNING]
> Gotcha
> To avoid having multiple similar-typed values with different shapes it might be a good idea to consider "hoisting" inline types to your schema. This will help to make sure every object that share the same _type has the same shape throughout your application.



# Source vs. compiled paths

What just happened? The CLI command `sanity check` is running in production mode, and got this error.

The reason may be that you have defined a `compiled` path in the `sanity.json` file in your Studio. This tells Sanity to look for the files in a different location when running in production mode.

Another reason may be that `sanity check` has found a Studio plugin which is published on npm with files that are not compiled.

The `paths` propery in a `sanity.json` file tells Sanity where to look for both compiled and uncompiled code files. Given the following `sanity.json` config:

```json
{
  "paths": {
    "source": "./src",
    "compiled": "./lib"
  },
  "parts": [
    {
      "implements": "part:@sanity/base/tool",
      "path": "my-tool/index.js"
    }
  ]
}
```

Sanity will look for source files in `./src` (relative to the location of the `sanity.json` file) and compiled files in `./lib`. In the particular case above, the tool source should be in `./src/my-tool/index.js` and the compiled version will end up in `./lib/my-tool/index.js`.

If a plugin doesn't require any Babel compilation, the `sanity.json` for that plugin doesn't need a declaration of the `paths` property.

You can read more about [sanity.json](/docs/archive/sanity-json) and [parts](/docs/archive/parts).



# Import: Asset has different target than source

This error usually occurs when you have exported a dataset using `sanity dataset export` using the `--raw` flag, and then importing to a different project ID or dataset name.

The reason why this is failing is because the imported documents would refer to assets outside of it's own dataset, which is usually not what you want. If you delete an asset from the source dataset, it would create a "loose" asset document in the target dataset, which points to a file that no longer exists.

The solution is to not use `--raw` when exporting, which will also export all the assets from the source dataset. This will make sure the assets are also present in the target dataset when importing.

In *very* rare cases, you may want to allow the assets to reference URLs from a different dataset, in which case you can use the `--allow-assets-in-different-dataset` flag when importing.



# Using global studio client without specifying API version

In a previous version of the Sanity content studio, you could import a global, preconfigured Sanity client from `part:@sanity/base/client`. From version 2.7.0 and onwards, you should now specify which API version you want to use:

```javascript
import sanityClient from 'part:@sanity/base/client'

const client = sanityClient.withConfig({
  apiVersion: 'v2021-03-25'
})

client.fetch('/* ... */')
```

To explain why this is necessary, consider the following scenario:

- Plugin A is released in 2020, and contains queries and API calls that are written for API version `v1`.
- Plugin B is released in 2021, and contains queries and API calls that are written for API version `v2020-03-25`.
- If both plugins had to use the same API version, you would either have to wait for the plugin authors to align on a single version, or have the risk of the plugins breaking.

By allowing each plugin to declare which API version they want to use, we can use multiple different API versions within the studio, without causing any issues.

## Backwards compatibility

Using the global client without specifying an API version will still work as before (using `v1` for API calls), but will give a warning message in the developer console telling you to specify an API version.



# Structure: Action and intent are mutually exclusive

A menu item cannot have both an intent *and* an action defined. Either use `action` (with a function) or `intent` (with an intent declaration).





# Upgrade React

The React version used in Sanity Studio can be upgraded from the command line.

#### Using yarn

`yarn add react@latest react-dom@latest prop-types@latest`

#### Using npm

`npm install react@latest react-dom@latest prop-types@latest`



# Plugin is missing a sanity.json file

You're probably here because you tried to run `sanity start`, but got:

```markdown
No "sanity.json" file found in plugin "my-plugin-name"
See https://docs.sanity.io/help/missing-plugin-sanity-json
```

This can be fixed by adding a `sanity.json` file to the root level of the plugin in question. Also, you might want to *define* and/or *implement* a `part`, e.g.:

```json
{
  "paths": {
    "source": "./src",
    "compiled": "./lib"
  },
  "parts": [
    {
      "name": "part:@sanity/base/components/unicorn-slider",
      "description": "React component which provides a slider input"
    },
    {
      "implements": "part:@sanity/base/components/unicorn-slider",
      "path": "components/Slider.js"
    }
  ]
}

```



# Structure: Document ID required

Certain nodes within the desk structure requires a document ID to operate on. 

Setting a document ID can be done by calling the `documentId()` method:



```javascript
S.document()
  .id('car-editor')
  .schemaType('car')
  .documentId('am-db9')

```





# Incompatible combination of params and filter

The reference field allows you to define options for the input component, namely a GROQ filter and a set of parameters for this filter. You can *either* define the filter statically, **OR** you can use a function in order to *derive* the filter based on the surrounding document.

If you are encountering this error, it usually means that you've defined a function for deriving the filter, but has also defined a set of static parameters. The solution is to either use static values, or just use the filter function and return an object containing both the filter and the parameters:

```javascript
export default {
  name: 'blogPost',
  type: 'document',
  fields: [
    // ... your other schema fields ...
    {
      name: 'author',
      type: 'reference',
      to: [{type: 'person'}],
      options: {
        filter: () => {
          return {
            filter: 'age > $age',
            params: {age: 30}
          }
        }
      }
    }
  ]
}
```



# Using listener with tokens is not supported in browsers

The browser implementation of EventSource does not allow for sending custom headers. Therefore, authenticating a listener request using a token will not work in browsers.

> [!WARNING]
> Gotcha
> Configuring the sanity client using a token in the browser has security implications, and should only be done after a careful consideration.
> 
> Read more about how to keep your data safe

Instead consider setting the visibility of your dataset to public or make sure users are logged in using cookies when accessing your frontend.



# Schema type is missing a required property

Every schema type needs both a `type` and a `name` property. The type specifies which type your schema type is (e.g. if its a `document`, an `object` or a `string`). The type name is the name of which you will refer to this type later on. E.g. if you want to add a field that is a reference to a value of one of your own schema types, you will typically do something like this:

```javascript
[
  { // defines the schematype "author"
    name: 'author',
    type: 'document',
    fields: [/*...*/]
  },
  
  //...
  
  { // defines the schema type "book"
    name: 'book',
    type: 'document',
    fields: [
      {name: 'title', type: 'string'},
      {
      name: 'author',
      type: 'reference',
      to: [
        {
          type: 'author' // <-- refers to the schema type "author" by its name
        }
      ]
    }
  }
]
```



# API versioning in Javascript Client

In order to promote incremental changes, [the Sanity API is versioned](/docs/content-lake/api-versioning) based on ISO dates (YYYY-MM-DD) in the UTC timezone.

> [!WARNING]
> Gotcha
> The apiVersion property of the JavaScript client is currently optional. If no value is provided, the client will issue a deprecation warning and default to using v1 of the API.

Unless you know of a specific API version you want to use, you'll want to set it to **today's UTC date**. By doing this, you'll get all the latest bug fixes and features, while preventing any timezone confusion and locking the API to prevent breaking changes.



> [!NOTE]
> What does the apiVersion date mean?
> Essentially, the date you enter for the apiVersion will use the API as it worked on that date. You can confidently use features that were added on or before that date, and any breaking changes implemented after that date will not affect your use of the API.

**Note**: While it's tempting to use a date that's been set dynamically as an API version, this can be a risky idea. Using a static (i.e., hard coded) date, you pin your project to a specific version of the API, which prevents any sudden changes that can break your implementation. If you hard code your API to `v2021-08-31`, and it works, you can be assured it will continue to work even as new API versions are released.

> [!TIP]
> Protip
> Recommended: apiVersion: '2021-08-31'
> 
> Not recommended: apiVersion: new Date().toISOString().slice(0, 10)

In future versions, specifying an API version will be required. For now, to maintain backward compatibility, not specifying a version will trigger a deprecation warning and fall back to using `v1`.

> [!WARNING]
> Gotcha
> When using the HTTP API, the version number is prefixed with the v character (v1, v2021-08-31, etc.). In the JavaScript client, no prefix is needed (apiVersion: '2021-08-31').

## Example usage

```javascript
import sanityClient from '@sanity/client'

const client = sanityClient({
  projectId: 'your-project-id',
  dataset: 'production',
  apiVersion: '2021-08-31', // use a UTC date string
  token: 'sanity-auth-token', // or leave blank for unauthenticated usage
  useCdn: true, // `false` if you want to ensure fresh data
})
```



# Upgrade version of studio package

The version of a package used in Sanity Studio can be upgraded from the command line.

#### Using yarn

`yarn add <package>@latest`

e.g. to upgrade the version of React: 

`yarn add react@latest`

#### Using npm

`npm install <package>@latest`

e.g. to upgrade to the latest version of React:

`npm install react@latest`



# Slug: `slugifyFn` renamed

The slug type has gotten a brush-up recently, and as part of this process the option `slugifyFn` has been renamed to the easier-to-write, easier-to-remember option `slugify`.

There are no changes to it's signature, but you can now return a promise should you want to generate the slug asynchronously:



```javascript
{
  title: 'Slug',
  name: 'mySlugField',
  type: 'slug',
  options: {
    source: 'title',
    slugify: value => someAsyncSlugGenerator(value)
  }
}
```

The old option will still work for a number of upcoming releases, but will be removed at some point in the future.



# Renamed plugin @sanity/date-input

The plugin `@sanity/date-input` has been renamed to `@sanity/rich-date-input` to better reflect its purpose.

### What should I do?

3. Install the `@sanity/rich-date-input` plugin with `sanity install @sanity/rich-date-input`
3. Remove the `@sanity/date-input` entry from the `plugins` array in your `sanity.json`
3. Add the `richDate` type definition from the plugin to your schema, e.g: 

```javascript
import richDate from 'part:@sanity/form-builder/input/rich-date/schema'
 
// ...
export default createSchema({
  name: 'mySchema',
  types: [
    //...
    richDate
  ]
})
```



# Specify API version when using custom document list filters

When specifying custom filters for document lists, we now require specifying an `apiVersion`. This can be set to the current date, e.g. `v2025-02-19`. See our [API Versioning](/docs/content-lake/api-versioning) docs for more details.

## Before:

```javascript
S.documentList()
  .title('Posts')
  .filter('_type == "post" && $authorId == author._ref')
  .params({ authorId })

```

## After:

```javascript

  S.documentList()
    .title('Posts')
    .apiVersion('v2025-02-19')
    .filter('_type == "post" && $authorId == author._ref')
    .params({ authorId })
)
```





# Platform introduction

The Sanity Content Operating System provides three interconnected layers: 

- [Content Lake](/docs/content-lake) (content database)
- [Compute and AI](/docs/compute-and-ai) (business logic)
- [APIs and SDKs](/docs/apis-and-sdks) (developer tools)

[The Sanity Dashboard](/docs/dashboard) for running your content operations apps, such as: 

- [Studio](/docs/studio) (customizable CMS)
- [Media Library](/docs/media-library) (asset management)
- [Canvas](/docs/canvas) (AI-powered content creation)
- [Your custom-built apps](undefined) (using [the App SDK](/docs/app-sdk))

You can use Sanity to:

- Build custom content workflows that match your specific business processes
- Create and manage structured content that can be reused across any digital channel
- Develop specialized content applications that give teams exactly the tools they need

Unlike traditional or headless CMSes, Sanity provides a foundation for your entire content lifecycle across all digital channels, with the flexibility to evolve as your needs change.

You can get started with Sanity in minutes. [Go here to explore the different ways](/docs/getting-started).



# Setting up your studio

## Create a new Studio with Sanity CLI

![Video](https://stream.mux.com/wIMs3CS7T4pP7hRArpQZsBZ01Be02vCjbK)

Run the command in your Terminal to initialize your project on your local computer.

See the documentation if you are [having issues with the CLI](/docs/help/cli-errors).

```sh
npm create sanity@latest -- --dataset production --template clean --typescript --output-path studio-hello-world
cd studio-hello-world
```

## Run Sanity Studio locally

Inside the directory of the Studio, start the development server by running the following command.

```sh
npm run dev
```

## Log in to the Studio

**Open** the Studio running locally in your browser from [http://localhost:3333](http://localhost:3333).

You should now see a screen prompting you to log in to the Studio. Use the same service (Google, GitHub, or email) that you used when you logged in to the CLI.



# Defining a schema

## Create a new document type

![Video](https://stream.mux.com/IfVfAwxfwOKN2khdGCQ3cs5IuF1rYte1)

Create a new file in your Studio’s `schemaTypes` folder called `postType.ts` with the code below which contains a set of fields for a new `post` document type.

```
import {defineField, defineType} from 'sanity'

export const postType = defineType({
  name: 'post',
  title: 'Post',
  type: 'document',
  fields: [
    defineField({
      name: 'title',
      type: 'string',
      validation: (rule) => rule.required(),
    }),
    defineField({
      name: 'slug',
      type: 'slug',
      options: {source: 'title'},
      validation: (rule) => rule.required(),
    }),
    defineField({
      name: 'publishedAt',
      type: 'datetime',
      initialValue: () => new Date().toISOString(),
      validation: (rule) => rule.required(),
    }),
    defineField({
      name: 'image',
      type: 'image',
    }),
    defineField({
      name: 'body',
      type: 'array',
      of: [{type: 'block'}],
    }),
  ],
})
```

## Register the `post` schema type to the Studio schema

Now you can import this document type into the `schemaTypes` array in the `index.ts` file in the same folder.

```
import {postType} from './postType'

export const schemaTypes = [postType]
```

## Publish your first document

When you save these two files, your Studio should automatically reload and show your first document type. Click the `+` symbol at the top left to create and publish a new `post` document.



# Querying content with GROQ

## Write your first GROQ query

![Video](https://stream.mux.com/Mc12Sdeu00ugrGuQyz00Du1G4AQZmT36UV)

Open **Vision** in your Studio's top nav bar and paste this query into the **Query** code block field.

```groq
*[_type == "post"]{
  _id,
  title,
  slug,
  publishedAt
}
```

- `*` represents all documents in a dataset as an array
- `[_type == "post"]` represents a **filter** to only return matching documents
- `{ _id, title, slug, publishedAt }` represents a **projection** which defines the attributes from those documents that you wish to include in the response.

## Run the query

Click **Fetch** to see the JSON output in **Results**. You should see the document you previously published in the results.

Queries run in Vision use your authenticated session, so you will see private documents – which have a `.` in the `_id` key, like `drafts.`. You will not see when queried from your front end in the next step.



# Displaying content in a front end

## Install a new React Router 7 (Remix) application

![Video](https://stream.mux.com/BImVH3jL01viMdWCMfBbfSdrD2Gg01oEB01)

If you have an *existing* application, skip this first step and adapt the rest of the lesson to install Sanity dependencies to fetch and render content.

**Run** the following in a new tab or window in your Terminal (keep the Studio running) to create a new React Router 7 application with Tailwind CSS and TypeScript.

```sh
# outside your studio directory
npx create-react-router@latest react-router-{{PROJECT_NAME_SLUGIFIED}} -y
cd react-router-{{PROJECT_NAME_SLUGIFIED}}
```

You should now have your Studio and React Router 7 application in two separate, adjacent folders:

```text
├─ /react-router-{{PROJECT_NAME_SLUGIFIED}}
└─ /studio-{{PROJECT_NAME_SLUGIFIED}}
```

## Install Sanity dependencies

**Run** the following inside the `react-router-hello-world` directory to install:

- [@sanity/client](https://reference.sanity.dev/_sanity/client/) for fetching content from Sanity
- [@sanity/image-url](https://github.com/sanity-io/image-url) helper functions to take image data from Sanity and create a URL
- [@portabletext/react](https://github.com/portabletext/react-portabletext) to render Portable Text as React components

```sh
npm install @sanity/client @sanity/image-url @portabletext/react
```

## Start the development server

**Run** the following command and open [http://localhost:5173](http://localhost:5173) in your browser.

```sh
npm run dev
```

## Configure the Sanity client

To fetch content from Sanity, you’ll first need to configure a Sanity Client.

**Create** a directory `react-router-hello-world/app/sanity` and within it create a `client.ts` file, with the following code:

```
import { createClient } from "@sanity/client";

export const client = createClient({
  projectId: "your-project-id",
  dataset: "production",
  apiVersion: "2024-01-01",
  useCdn: false,
});
```

## Display content on the home page

React Router uses a `loader` function exported from **routes** for server-side fetching of data. Routes are configured in the `app/routes.ts` file.

The default home page can be found at `app/routes/home.tsx`

**Update** it to render a list of posts fetched from your Sanity dataset using the code below.

```tsx
import { SanityDocument } from "@sanity/client";
import { Link } from "react-router";
import { client } from "~/sanity/client";
import { Route } from "./+types/home";

const POSTS_QUERY = `*[
  _type == "post"
  && defined(slug.current)
]|order(publishedAt desc)[0...12]{_id, title, slug, publishedAt}`;

export async function loader() {
  return { posts: await client.fetch<SanityDocument[]>(POSTS_QUERY) };
}

export default function IndexPage({ loaderData }: Route.ComponentProps) {
  const { posts } = loaderData;

  return (
    <main className="container mx-auto min-h-screen max-w-3xl p-8">
      <h1 className="text-4xl font-bold mb-8">Posts</h1>
      <ul className="flex flex-col gap-y-4">
        {posts.map((post) => (
          <li className="hover:underline" key={post._id}>
            <Link to={`/${post.slug.current}`}>
              <h2 className="text-xl font-semibold">{post.title}</h2>
              <p>{new Date(post.publishedAt).toLocaleDateString()}</p>
            </Link>
          </li>
        ))}
      </ul>
    </main>
  );
}
```

## Display individual posts

**Create** a new route for individual post pages.

The dynamic value of a slug when visiting `/:post` in the URL is used as a parameter in the GROQ query used by Sanity Client.

Notice that we’re using [Tailwind CSS Typography](https://github.com/tailwindlabs/tailwindcss-typography)’s `prose` class name to style the post’s `body` block content. Install it in your project following their documentation.

**Update** the `routes.ts` configuration file to load this route when individual post links are clicked.

```tsx
import { Link } from "react-router";
import imageUrlBuilder from "@sanity/image-url";
import { SanityDocument } from "@sanity/client";
import { SanityImageSource } from "@sanity/image-url/lib/types/types";
import { PortableText } from "@portabletext/react";
import { Route } from "../routes/+types/post";
import { client } from "~/sanity/client";

const { projectId, dataset } = client.config();
const urlFor = (source: SanityImageSource) =>
  projectId && dataset
    ? imageUrlBuilder({ projectId, dataset }).image(source)
    : null;

const POST_QUERY = `*[_type == "post" && slug.current == $slug][0]`;

export async function loader({ params }: Route.LoaderArgs) {
  return { post: await client.fetch<SanityDocument>(POST_QUERY, params) };
}

export default function Component({ loaderData }: Route.ComponentProps) {
  const { post } = loaderData;
  const postImageUrl = post.image
    ? urlFor(post.image)?.width(550).height(310).url()
    : null;

  return (
    <main className="container mx-auto min-h-screen max-w-3xl p-8 flex flex-col gap-4">
      <Link to="/" className="hover:underline">
        ← Back to posts
      </Link>
      {postImageUrl && (
        <img
          src={postImageUrl}
          alt={post.title}
          className="aspect-video rounded-xl"
          width="550"
          height="310"
        />
      )}
      <h1 className="text-4xl font-bold mb-8">{post.title}</h1>
      <div className="prose">
        <p>Published: {new Date(post.publishedAt).toLocaleDateString()}</p>
        {Array.isArray(post.body) && <PortableText value={post.body} />}
      </div>
    </main>
  );
}
```





# Deploying Studio and inviting editors

## Deploy your Studio with Sanity

![Video](https://stream.mux.com/CvYhCQr8e1oZt98NW202BZLLNv376VVKc)

In your Studio directory (`studio-hello-world`) run the following command to deploy your Sanity Studio.

```sh
npm run deploy
```

## Invite a collaborator

Now that you’ve deployed your Studio, you can optionally invite a collaborator to your project. Navigate to: `https://www.sanity.io/manage/project/{{PROJECT_ID}}/members`.

They will be able to access the deployed Studio, where you can collaborate together on creating content.





# Setting up your studio

## Create a new Studio with Sanity CLI

![Video](https://stream.mux.com/wIMs3CS7T4pP7hRArpQZsBZ01Be02vCjbK)

Run the command in your Terminal to initialize your project on your local computer.

See the documentation if you are [having issues with the CLI](/docs/help/cli-errors).

```sh
npm create sanity@latest -- --dataset production --template clean --typescript --output-path studio-hello-world
cd studio-hello-world
```

## Run Sanity Studio locally

Inside the directory of the Studio, start the development server by running the following command.

```sh
npm run dev
```

## Log in to the Studio

**Open** the Studio running locally in your browser from [http://localhost:3333](http://localhost:3333).

You should now see a screen prompting you to log in to the Studio. Use the same service (Google, GitHub, or email) that you used when you logged in to the CLI.



# Querying content with GROQ

## Write your first GROQ query

![Video](https://stream.mux.com/Mc12Sdeu00ugrGuQyz00Du1G4AQZmT36UV)

Open **Vision** in your Studio's top nav bar and paste this query into the **Query** code block field.

```groq
*[_type == "post"]{
  _id,
  title,
  slug,
  publishedAt
}
```

- `*` represents all documents in a dataset as an array
- `[_type == "post"]` represents a **filter** to only return matching documents
- `{ _id, title, slug, publishedAt }` represents a **projection** which defines the attributes from those documents that you wish to include in the response.

## Run the query

Click **Fetch** to see the JSON output in **Results**. You should see the document you previously published in the results.

Queries run in Vision use your authenticated session, so you will see private documents – which have a `.` in the `_id` key, like `drafts.`. You will not see when queried from your front end in the next step.



# Deploying Studio and inviting editors

## Deploy your Studio with Sanity

![Video](https://stream.mux.com/CvYhCQr8e1oZt98NW202BZLLNv376VVKc)

In your Studio directory (`studio-hello-world`) run the following command to deploy your Sanity Studio.

```sh
npm run deploy
```

## Invite a collaborator

Now that you’ve deployed your Studio, you can optionally invite a collaborator to your project. Navigate to: `https://www.sanity.io/manage/project/{{PROJECT_ID}}/members`.

They will be able to access the deployed Studio, where you can collaborate together on creating content.





# Media Library

#### Get started

[Media Library Introduction](/docs/media-library/introduction)

[Configure your library](/docs/media-library/configure-library)

[Configure Studio](/docs/media-library/configure-studio)



#### Next steps

[Create an aspect](/docs/media-library/create-aspect)

[Aspect patterns](/docs/media-library/aspect-patterns)

[Importing assets](/docs/media-library/importing-assets)



#### Dive deeper

[Media Library API](/docs/http-reference/media-library)

[Link assets to documents](/docs/media-library/link-media-assets)





# Introduction

Media Library is a Sanity app for managing your organization's assets.

Media Library allows you to:

- Centrally store assets for use across multiple applications and datasets.
- Create custom groupings, called aspects, to make managing assets easier.

[Configure your library](/docs/media-library/configure-library)

[Configure Studio](/docs/media-library/configure-studio)



## Requirements

- Dashboard
- Studio v3.82.0 or later is required to incorporate Media Library assets in the Studio.
- API v2024-06-24 or later is required for any Media Library API requests.

## Core Concepts

The Media Library introduces a few new concepts in addition to the image and asset workflows in the rest of the Sanity ecosystem.

### Assets

An asset is a digital file that your apps and Studio can use, like an image, video, or document.

Common examples include product photos, marketing videos, and downloadable PDFs. For now, all non-image assets are considered `file` types. We may provide additional support for audio and video-specific formats in the future. For now, Media Library treats these like other files.

Outside of Media Library, these assets live alongside your dataset. In Media Library, they live in a special dataset your organization shares.

> [!TIP]
> Your Sanity project still supplies the assets to your applications
> With Media Library, you can treat it as the source of truth for your assets, but your project is still the access point for rendering images and creating download links. All requests for Media Library assets should go through your project datasets.
> 
> Enable library access in your studios, then continue presenting images as if they were coming straight from the same dataset as the rest of your content. This could be by passing asset into a URL builder, or expanding the asset reference with asset -> {...} and building the URL yourself.

### The library

The library is the interface that your content teams use to manage assets. Users can upload, search, manage, and assign aspects to assets.

[Meet the library](/docs/media-library/interface)



### Aspects

Aspects are schema-style fields that apply to assets. They include additional, identifying information that helps asset managers search and organize assets. Some examples are usage licenses, references to products in your organization, and copyright details. This extra level of information is specific to the Media Library. For local metadata, you should create schemas in your Studio projects.

Developers define aspects that users can then apply to an asset. Depending on your plan, there are limits to the number of aspects each asset can have.

[Create an aspect](/docs/media-library/create-aspect)

[Aspect patterns](/docs/media-library/aspect-patterns)



### Collections

_This is a paid feature, available as an addon on the Enterprise plan._

Collections allow teams to group assets for better organization and sharing.

### Global document references

Media Library assets exist outside your projects and datasets, so you need a way to connect them. Global document references are a new reference type that allows you to target a reference in a different resource. Resources are currently limited to datasets and media libraries, and at this time you can only reference dataset documents from Media Library aspects. See the [common aspect patterns guide](/docs/media-library/aspect-patterns) for details on referencing documents from within aspects.

## Limitations

- Media Library is only available within [Dashboard](/docs/dashboard).
- For additional usage limits, see the [limits and usage document](/docs/media-library/limits-and-usage).



# Meet the library

## Media Library at a glance

Media Library is home to your organization's shared assets. It stores assets for use across your projects and datasets, and allows content teams to have a central source of truth for their media.

![a screenshot of a media library showing various images](https://cdn.sanity.io/images/3do82whm/next/cae386064a9678b739ff46b3370a3773a92c6c10-3136x1596.png)

Media Library is an organization-wide application. [You can access it from the dashboard](/docs/dashboard) by selecting the "Media" icon in the left navigation bar. Media Library requires the dashboard.

> [!NOTE]
> Where are my existing assets?
> If you've been using Sanity already, you may have images and other files that you're using in your studios. These files are saved within your datasets, and they are not automatically copied into the media library.
> 
> Soon, we will add the capability to migrate existing assets into the media library and preserve connections to those assets within your studios.



## The library interface

The library adapts based on the assets you have selected.

![a view of the three main panels in the media library](https://cdn.sanity.io/images/3do82whm/next/0771a14ab13fa57617b60a79fda2c3f416825d5b-3128x1596.png)

The core of the interface is split into three sections:

11. The asset list: View existing assets, filter the results, and upload new assets.
11.  The library menu: Narrow your view of the asset list, explore collections, and see recently uploaded assets.
11. The asset sidebar: Edit asset metadata, apply aspects, and view additional details about the asset. 

### Uploading assets

There are two ways to upload assets in the library interface:

14. Select the **Upload** button in the top right of the asset list to upload an asset.
14. Drag-and-drop one or more assets directly into the asset list to start an upload.

As your assets upload, you'll see a status screen showing the progress of each asset.



### Select multiple assets

You can select multiple assets at once by pressing the checkbox in the top-left corner of each asset.

![a screenshot of a media library showing a selection of images](https://cdn.sanity.io/images/3do82whm/next/a970d91d7470f28fd859237ffa75c2a7271705c9-3070x1596.png)

### Delete assets

To delete one or more assets, first select them in the asset list.

Next, select the vertical **"..."** icon from the popover at the bottom of the asset list.

![a screenshot of the popover that says delete 1 asset](https://cdn.sanity.io/images/3do82whm/next/1582b2b41d1a0b92c88cb6742a396be2af2da257-1306x826.png)

Select **"Delete 1 asset"** to delete the asset.

## Aspects

![a screenshot of a media library with an asset detail panel open](https://cdn.sanity.io/images/3do82whm/next/6bb17c72377a527f1b361a8ede94a1877f2360b1-3388x1910.png)

Aspects let you organize your assets with customly defined fields. Aspects are defined programatically with a schema-like syntax.

#### Developing aspects

[Create an aspect](/docs/media-library/create-aspect)

[Aspect patterns](/docs/media-library/aspect-patterns)



You can use aspects to sort and filter results in the asset list, or to store internal metadata.

> [!NOTE]
> Aspect limits
> You can have multiple aspects defined, but depending on your plan, there may be a limit on the number of aspects an asset can have attributed to it. 

### Add aspects to an asset, or edit an aspect

To add aspects to an asset, first select one or more assets in the asset list.

If adding an aspect for the first time, select **"Add aspect"**. If adding additional aspects, select the **"+"** button to the right of the *Aspects* heading.

Once you've made changes to an aspect, select the** "Publish"** button to publish the changes to the asset.

> [!TIP]
> Publishing changes
> Don't forget to publish changes whenever you add or remove aspects, or when you make updates to the asset title.

## Collections

![a screenshot of the media library showing a collection of landscapes](https://cdn.sanity.io/images/3do82whm/next/de4563243f810e5c856444146aa6c22544debc80-3070x1596.png)

_This is a paid feature, available as an addon on the Enterprise plan._

Collections allow further grouping of assets and are not limited to available aspects. You can create new collections while selecting an asset, or from the collection's screen.

### Add an asset to a collection

You can add an asset to a collection in two ways:

42. Navigate to the collection, then select **"Add"** in the top right, where the upload button normally is.
42. In any view, select the asset then, then select the vertical **"..."** icon, then select **"Add to existing collection"** from the popover menu.



# Studio configuration

The Media Library app allows content managers to share a central store of assets. You can enable Media Library integration with Studio to allow editors to select assets from the library for use in their documents, and upload new files and images to the library.

![The Studio image selector interface with Media Library included.](https://cdn.sanity.io/images/3do82whm/next/a4583e5572cb2473fcdac34aadce64df0c56358c-1369x486.png)

In this guide, you'll learn to configure Studio to support integration with Media Library.

Prerequisites

- Studio v3.82.0 or later. [Learn how to upgrade your Studios](/docs/studio/upgrade).

## Update the Studio configuration file

Enable Media Library support by setting `mediaLibrary.enabled` to true in your `studio.config.ts` file. 

```
import { defineConfig } from 'sanity'

export default defineConfig({
  projectId: '<your-project-id>',
  dataset: '<dataset>',
  mediaLibrary: {
    enabled: true,
  },
  plugins: [structureTool()],
  schema: {
    // ...
  }
})
```

 

## Optional: disable the default source

If you want to move completely to Media Library, you can disable the default media source. Use the `form.images.assetSources` configuration in your Studio config file (`studio.config.ts`) to filter the sources.

This example keeps Media Library and any custom sources, but removes the default media select option.

```
import { defineConfig } from 'sanity'

export default defineConfig({
  // ... rest of config
  
  mediaLibrary: {
    enabled: true,
  },
  form: {
    // Disable the default for image assets
    image: {
      assetSources: (sources) => sources.filter((source) => source.name !== 'sanity-default')
    },
    // Disable the default for file assets
    file: {
      assetSources: (sources) => sources.filter((source) => source.name !== 'sanity-default')
    }
  },

  // ... rest of config
})
```

If you'd like to be more explicit and target the Media Library source, you can do so by comparing the source name with `sanity-media-library`.



# Media Library configuration

Media Library is preconfigured for your organization, but to create and deploy aspects, you need to configure your CLI to connect to the library.

Prerequisites:

- `npm`, `pnpm`, or a similar package manager capable of installing and running the `sanity` CLI.
- Read/write access to your organization's Media Library.

## Obtain your `mediaLibraryId`

When you interact with the `sanity media` commands, you're prompted to select your library. If you only have a single organization, you should see a single library. If you're a member of multiple organizations, you can parse your organization and library ID using the URL.

Navigate to [sanity.io/welcome](https://sanity.io/welcome), select your organization, then select the Media Library app. The URL for your library includes the mediaLibraryId.

```text
https://www.sanity.io/@<organizationId>/media/<mediaLibraryId>/assets
```

## Configure the CLI

Tell `sanity` CLI where to define your aspects by updating `sanity.cli.ts`.

```
import {defineCliConfig} from 'sanity/cli'

export default defineCliConfig({
  api: {
    projectId: '<projectId>',
    dataset: '<dataset-name>'
  },
  mediaLibrary: {
    // set the path relative to the location of sanity.cli.ts.
    aspectsPath: 'aspects',
  },
  /**
   * Enable auto-updates for studios.
   * Learn more at https://www.sanity.io/docs/cli#auto-updates
   */
  autoUpdates: true,
})
```

With the aspects path defined, you can now use the `media` commands to [create and deploy aspects](/docs/media-library/create-aspect).



# Create an aspect

Aspects are sets of properties that describe an asset and are defined just like schemas. Asset managers can apply aspects to assets in the library, with mutations, or programmatically during upload. This information stored in aspects is specific to the Media Library. For local metadata, you should use fields in your Studio projects.

In this guide, you'll create a new aspect and deploy it to your Media Library.

Prerequisites:

- `sanity` v3.85.1 or higher

## Configure your aspect directory

In a project with a `sanity.cli.ts` file, edit the configuration to include a `mediaLibrary.aspectsPath`.

```
import {defineCliConfig} from 'sanity/cli'

export default defineCliConfig({
  api: {
    projectId: '3do82whm',
    dataset: 'production'
  },
  mediaLibrary: {
    aspectsPath: 'aspects',
  },
  autoUpdates: true,
})
```

In the example above, the `aspects` directory is relative to the location of the `sanity.cli.ts` file.

> [!NOTE]
> Where should you place aspect definitions?
> At this time, in order to deploy aspects you need a sanity.cli.ts configuration connected to a project and dataset. We're working to improve this experience in the future. For the time being, we recommend manually setting up a configuration file, or working directly in an existing Sanity Studio project.

## Define a new aspect

In the same directory, generate a new aspect with the Sanity CLI.

```sh
sanity media create-aspect
```

This command prompts you for a name and creates a new aspect in your project. Aspect names must be unique and contain only letters, numbers, and hyphens.

Aspects can be a single field or an object containing multiple fields. They can contain strings, objects, arrays, or nearly any [Studio schema type](/docs/studio/schema-types). Validation rules, image types, file types, and custom components are not allowed. 

The CLI creates an object-type aspect with a single string field similar to the example below. In this case, we set the name as `copyright`.

```
import {defineAssetAspect, defineField} from 'sanity'

export default defineAssetAspect({
  name: 'copyright',
  title: 'copyright',
  type: 'object',
  fields: [
    defineField({
      name: 'string',
      title: 'Plain String',
      type: 'string',
    }),
  ],
})

```

Modify the aspect with more fields. In this example, we'll update the existing string field and add a `date` type field.

```
import {defineAssetAspect, defineField} from 'sanity'

export default defineAssetAspect({
  name: 'copyright',
  title: 'copyright',
  type: 'object',
  fields: [
    defineField({
      name: 'copyrightHolder',
      title: 'Copyright Holder',
      type: 'string',
    }),
    defineField({
      name: 'copyrightDate',
      title: 'Date',
      type: 'date',
    }),
  ],
})

```

Once deployed, this will look like the following in your Media Library:

![a screenshot of a web page that says aspects copyright copyright holder and date .](https://cdn.sanity.io/images/3do82whm/next/49ac62823d932a41274fce626ea3b92e1e2e02eb-882x806.png)

You can see more aspect examples in the [aspect patterns cheat sheet](/docs/media-library/aspect-patterns).

## Deploy the aspect

With your aspect defined, it's time to deploy it to your Media Library.

Run the following to deploy a single aspect. Replace `copyright` with your aspect name.

```sh
sanity media deploy-aspect copyright
```

If you make additional changes to the aspect, you can update it by running the `deploy-aspect` command again.

### Delete an aspect

To delete an aspect from your library, run the following command, replacing `copyright` with the name of your aspect.

```sh
sanity media delete-aspect copyright
```

This deletes the aspect from your Library, but will not remove the local definition file.



# Add an aspect to an asset

This guide explores options for programmatically assigning aspects to Media Library assets.

Prerequisites:

- `mediaLibraryId`: The ID for your organization's Media Library.
- Read/write access to documents in Media Library.
- A [personal authentication token](/docs/content-lake/http-auth). At this time, robot tokens are not supported for Media Library.

## Mutate the asset

To add an aspect to an asset, you need to mutate the asset in Media Library. Rather than capture and rewrite the whole asset, [use a patch](/docs/content-lake/http-patches) to apply just the change in aspect to the asset document.

### Option 1: use the CLI

The `media import` CLI command can be used to set aspects on a single asset or set of assets. More details are available in our guide on [importing assets](/docs/media-library/importing-assets).

> [!TIP]
> Protip
> This option requires that you have a local copy of the file you're adding aspect information for. You can use media export as a way to generate a directory of the assets in your library alongside their existing aspect data.

### Option 2: use the HTTP API

Use the `media-libraries/<media-library-id>/mutate` endpoint to apply the mutation. 

In this example, we patch the value of a single field aspect with a name of `comment`.

```typescript
const mediaLibraryId = "<your-media-library-id>"
const url = `https://api.sanity.io/v2025-03-24/media-libraries/${mediaLibraryId}/mutate`

const mutation = JSON.stringify({
  mutations: [
    {
      patch: {
        id: assetId,
        setIfMissing: { aspects: {} }, // confirm the asset has an aspects property.
        set: {
          "aspects.comment": "Updated aspect details"
        }
      }
    }
  ]
})

await fetch(url, {
  method: 'POST',
  headers: {
    "Content-type": "application/json",
    "Authorization": "Bearer <your-token>"
  },
  body: mutation
})
```

This modifies the asset document to look something like this:

```json
{
  "title": "myImage.jpg"
  "assetType": "sanity.imageAsset",
  "_rev": "8397ffea-abf2-4eed-b6b7-d5e383171061",
  "_type": "sanity.asset",
  "aspects": {
    "comment": "updated aspect details"
  },
  "_createdAt": "2025-04-02T15:46:26Z",
}
```

For nested fields or more complex aspects, start with the outer-most name and work down to the individual field level.

### Option 3: use `@sanity/client`

We're actively working to make interacting with Media Library form the Sanity JavaScript client happen. This section will be updated once client support is available.

#### Additional resources

[Media Library API](/docs/http-reference/media-library)







# Query aspects

When you create and deploy an aspect, it's stored in the Media Library dataset alongside your asset documents. In this guide, you'll query your Media Library based on your aspects.

Prerequisites:

- API version v2025-02-19 or later 

The examples below use JavaScript's `fetch` to perform API calls, but the same principles apply regardless of the language or request library.

## List all aspects

Aspects are Sanity documents with a `_type` of `sanity.asset.aspect`. You can query them with GROQ using the Media Library's query endpoint.

```
const mediaLibraryId = '<your-library-id>'
const token = '<your-auth-token>'
const query = `*[_type == 'sanity.asset.aspect'] { ... }`

await fetch(`https://api.sanity.io/v2025-02-19/media-libraries/${mediaLibraryId}/query?query=${query}`, {
  method: 'GET',
  headers: {
    'Authorization': `Bearer ${token}`
  }
})
```

Like other Sanity queries, you'll receive a response containing the original query, a result, sync tags, and the response time. Here's an example response for a single field, boolean aspect:

```json
{
  "query": "*[_type == 'sanity.asset.aspect'] { ... }",
  "result": [
    {
      "_type": "sanity.asset.aspect",
      "definition": {
        "type": "boolean",
        "initialValue": false,
        "name": "placeholder",
        "description": "Set to true for temporary placeholder assets.",
        "title": "Placeholder"
      },
      "_id": "placeholder",
      "_updatedAt": "2025-04-15T23:21:59Z",
      "_system": {
        "createdBy": "gvRshKueQ"
      },
      "_createdAt": "2025-04-15T23:16:59Z",
      "_rev": "liBwLfU12KkZimf6bVlggr"
    }
  ],
  "syncTags": [
    "s1:W7DfKQ"
  ],
  "ms": 3
}
```

## Query assets by aspect details

Any asset document in your library that has an assigned aspect will include those aspect details in an `aspects` property. You can query for specific aspect information using GROQ and the Media Library's query endpoint.

As an example, if you want to query all assets that have the `placeholder` aspect set to `true`, you can perform the following query.

```
const mediaLibraryId = '<your-library-id>'
const token = '<your-auth-token>'

// You may need to encode your query to pass it as a query string.
const query = encodeURIComponent(`*[_type == 'sanity.asset' && (defined(aspects.placeholder) && true == aspects.placeholder)]`)

await fetch(`https://api.sanity.io/v2025-02-19/media-libraries/${mediaLibraryId}/query?query=${query}`, {
  method: 'GET',
  headers: {
    'Authorization': `Bearer ${token}`
  }
})
```

The above request returns any `sanity.asset` document with the `placeholder` aspect set to `true`. 

### `POST` instead of `GET`

You can also query with a `POST` request. Instead of using the `?query=` parameter, set the body to your stringified query, the method to `POST`, and the `Content-type` to `application/json`. Here's the same example above, but as a `POST`.

```
const mediaLibraryId = '<your-library-id>'
const token = '<your-auth-token>'
const query = `*[_type == 'sanity.asset' && (defined(aspects.placeholder) && true == aspects.placeholder)]`

await fetch(`https://api.sanity.io/v2025-02-19/media-libraries/${mediaLibraryId}/query`, {
  method: 'POST',
  body: JSON.stringify({query}),
  headers: {
    'Authorization': `Bearer ${token}`,
    'Content-Type': 'application/json'
  }
})
```





# Aspect patterns

In the [Create an aspect guide](/docs/media-library/create-aspect) you learned to define and deploy an aspect. This guide explores common patterns for defining aspects.

Prerequisites:

- `sanity` CLI v3.88.0 or newer.

## Single string field

```
import { defineAssetAspect } from 'sanity'

export default defineAssetAspect({
    name: 'copyright',
    title: 'Copyright',
    type: 'string',
    description: 'Enter the copyright value for this asset.',
  }),
})
```

## Single boolean field

```
import {defineAssetAspect} from 'sanity'

export default defineAssetAspect({
  name: 'placeholder',
  title: 'Placeholder',
  type: 'boolean',
  initialValue: false,
  description: 'Set to true for temporary placeholder assets.',
})

```

## Multiple fields

```
import {defineAssetAspect, defineField} from 'sanity'

export default defineAssetAspect({
  name: 'copyright',
  title: 'copyright',
  type: 'object',
  fields: [
    defineField({
      name: 'copyrightHolder',
      title: 'Copyright Holder',
      type: 'string',
    }),
    defineField({
      name: 'copyrightDate',
      title: 'Date',
      type: 'date',
    }),
  ],
})

```

## Global references

You can combine aspects with global document references. Use the `globalDocumentReference` type to target documents in another project and dataset. This example targets a `photographer` type in the `example` dataset of the `3do82whm` project.

```
import { defineAssetAspect } from 'sanity'

export default defineAssetAspect({
  name: 'photographer',
  title: 'Photographer',
  type: 'globalDocumentReference',
  description: 'Select the photographer.',
  resourceType: 'dataset',
  resourceId: '3do82whm.example',
  weak: true,
  to: [
    {
      type: 'photographer',
      preview: {
        select: {
          title: 'name'
        }
      }
    }
  ]
})
```

The `resourceId` value is the `projectId.datasetName`. As with normal references in Studio, you can use the `preview` property to select fields to display. In this case, it sets the preview `title` to the `photographer.name` field.

Learn more about [Global Document References](/docs/studio/global-document-reference-type).



# Importing assets (media + aspects)

When importing assets to the library, there are two primary goals:

2. Upload the media so it can be used in a studio or another application
2. Assign [aspect data](/docs/media-library/aspect-patterns) so that the asset is categorized and tagged correctly

There are two ways to import assets into your library to accomplish these goals together. The recommended way is to use the [Command Line Interface](/docs/apis-and-sdks/cli). You can run  `npx sanity media import --help` for a quick summary of syntax and options. Your other option is to use the HTTP API directly. 

## Import using the CLI

The `media import` command operates on a directory or archive that can have three components:

6. A `/images` directory for files that should be uploaded as an image
6. A `/files` directory for non-image files
6. A `data.ndjson` file that contains aspect data for any of the files contained in the `images` or `files` directories

```text
.
├── data.ndjson
├── images
│   └── ...all image files
└── files
    └── ...all other files
```

When the `import` command is run, every file contained within the `images` and `files` directories will be uploaded to the library. If an asset with a matching SHA-1 hash is identified within the library, then that file is skipped.

> [!NOTE]
> Each of the three components is optional. import will use the pieces that are provided.

#### `data.ndjson` format

`data.ndjson` is a file that can contain aspect information for any of the files that are being processed. It is a [newline-delimited JSON](https://github.com/ndjson/ndjson-spec) (NDJSON) file. Each line is a valid JSON object containing the information that should be associated with a single file.

Here's an example of a sample JSON object we would use to define information for a licensed photograph:

```json
{
  "filename": "images/my_example_photo_2.jpg",
  "aspects": {
    "licensedPhotograph": {
      "expiration": "2026-05-02T17:34:00.000Z",
      "photographer": {
        "_ref": "dataset:3do82whm.example:photographer-7926527",
        "_type": "globalDocumentReference",
        "_weak": true
      }
    }
  }
}
```

However, ndjson uses the newline character as delimiter (NDJSON == Newline Delimited JSON), therefore your ndjson file must be structured with one asset on each line, like this:

```json
{"filename": "images/my_example_photo_1.jpg","aspects":{"description":"A dog chasing a stick"}}
{"filename": "images/my_example_photo_2.jpg","aspects":{"licensedPhotograph":{"expiration": "2026-05-02T17:34:00.000Z","photographer": {"_ref":"dataset:3do82whm.example:photographer-7926527","_type":"globalDocumentReference","_weak": true}}}}
```

Each JSON object for an asset has two components:

17. `filename` - the relative path to the file that you're adding information for
17. `aspects` - the aspect data that should be saved on the asset associated with the designated file

Once you have prepared your files, you can run the import using the Sanity CLI.

> [!NOTE]
> What should I import?
> In some cases you will want to import your directory, such as when you've exported your library, made changes to the ndjson file, and are importing it back into the same library.
> 
> In other cases you will want to compress your assets into a tarball / tar file (.tar, .tar.gz, or .tgz), which includes the ndjson file and your assets.

```bash
npx sanity media import my-assets

// or

npx sanity media import my-assets.tar.gz
```

> [!TIP]
> Protip
> If a file matches an existing asset in the library, then that asset is not uploaded a second time. However any aspect data for that file in data.ndjson will still be processed.
> 
> This can be used to add aspect data for a set of assets that have already been imported into the library. If there's no aspect information yet, then the value within data.ndjson will be used. If there's existing aspect information, then the values in data.ndjson will be skipped unless the --replace-aspects option is used.

## Import using a client library

If you prefer not to use our CLI import tool, you may run the import yourself by using the HTTP API.

24. [Upload an asset](/docs/media-library/upload-assets)
24. [Add an aspect to an asset](/docs/media-library/assign-aspects)

There are some common pitfalls to keep in mind:

- *Concurrency*. While you may have thousands of assets to import, you shouldn't trigger thousands of requests in parallel. This is going to exceed API rate limits and might fail. We advise you to use a queue with a reasonably low concurrency.
Use a library to keep your import below our [API rate limit](/docs/content-lake/technical-limits):
- *API usage limits*. Importing large libraries can quickly cause a lot of requests, especially if you import a single asset per request. It is usually a good idea to send [multiple mutations within a single transaction](https://www.sanity.io/docs/js-client#multiple-mutations-in-a-transaction).
- *Mutation size limits*. While it's a good idea to do multiple mutations per transaction, you need to make sure that the size of the request is [within our limits](https://www.sanity.io/docs/technical-limits#c854fda72658), in terms of byte size.



# Upload an asset

Users can upload assets to Media Library directly from the app or from a configured Studio. Developers can upload assets with the CLI and the Media Library HTTP API. In this guide, you'll learn to upload an asset to your library programatically.

## Upload an asset

### Option 1: use the CLI

The Sanity CLI includes the `media import` command that standardizes the process for uploading batches of assets and simultaneously assigning aspect information. More detail is available in our guide for [importing assets](/docs/media-library/importing-assets).



### Option 2: use the HTTP API

Prerequisites:

- Node.js v21.0 or later to run the built-in `fetch` API, or a compatible request library.
- A [personal authentication token](/docs/content-lake/http-auth) with read/write access to your organization's Media Library.
- The `mediaLibraryId`.



Make a POST request to the `/media-libraries/<mediaLibraryId>/upload` endpoint. In the code below, replace `mediaLibraryId` with your library's ID, and `token` with your personal authentication token.

```
// Define your library ID.
const mediaLibraryId = '<your-library-id>'

// Define your personal auth token.
const token = '<personal-auth-token>'

// Read the contents of a file.
const asset = fs.readFileSync('assets/spring-launch-promo.jpg')

// POST a request to the HTTP API with the file as the body
await fetch(`https://api.sanity.io/v2025-02-19/media-libraries/${mediaLibraryId}/upload`, {
  method: 'POST',
  body: asset,
  headers: {
    'Authorization': `Bearer ${token}`,
  }
})
```

Uploading an asset without additional parameters will let Media Library generate the asset title and infer the filename. 

See the Media Library HTTP API reference for all available upload options and parameters.

[Media Library API](/docs/http-reference/media-library)





# Link assets to documents

When editors use Studio to add assets to documents, Studio and Media Library work together to link the project, document, and library together. To do this programatically, like when migrating a large number of assets to your library, you'll need to perform the steps manually.

> [!TIP]
> Rendering assets
> This guide is specifically about programatically linking assets from Media Library to documents in your datasets. For details on rendering assets, check out the Presenting Images guide.

In this guide, you'll learn how to link Media Library assets to documents in a project dataset. There are three key steps:

4. Upload the asset to Media Library, or query an existing asset in Media Library to obtain the necessary IDs.
4. Link the Media Library asset to your project dataset.
4. Patch any documents that use the asset with the correct references.

Prerequisites:

- An environment where you can make HTTP requests. The code examples use `fetch` in Node.js, which was made stable in v21.0. You can substitute any request library and any language.
- Project and library identifiers:- `mediaLibraryId` from your Media Library.
- `projectId` for the project you want to link the assets to.
- `dataset` for the project you want to link the assets to.


- A [personal authorization token](/docs/content-lake/http-auth) with permission to read/write from both the Media Library and your project.

## Gather the asset IDs

Linking a Media Library asset requires two identifiers:

- Asset ID: The primary identifier (`_id`) for an asset document in your Media Library. 
- Asset instance ID: The identifier for a versioned instance of the asset. Your assets likely have a single version, but in some future cases there may be multiple instances.

There are two ways to obtain these values.

### Get IDs during upload

After uploading an asset with the `media-libraries/upload` endpoint, the response contains `asset._id`* and *`assetInstance._id`. Refer to the guide below for details on using the API to upload assets.

[Upload assets](/docs/media-library/upload-assets)



### Get IDs by querying assets

If your assets are already in Media Library, you can query the Media Library API for asset documents. If you already know the Asset ID, you can query it directly with the `_id == '<asset-id>'` GROQ filter. If you don't know the ID(s), you can query all assets with `_type == 'sanity.asset'`.

For example, this query returns all asset documents in the library:

```
// Define your library ID.
const mediaLibraryId = '<your-library-id>'

// Define your personal auth token.
const token = '<personal-auth-token>'

// Define your query
const query = `*[_type == 'sanity.asset']`

// POST a request to the HTTP API with the file as the body
await fetch(`https://api.sanity.io/v2025-02-19/media-libraries/${mediaLibraryId}/query`, {
  method: 'POST',
  body: JSON.stringify({query}),
  headers: {
    'Authorization': `Bearer ${token}`,
    'Content-Type': 'application/json',
  }
})
```

Here is an example of the output with some values removed for clarity. 

```json
{
  "query": "*[_type == 'sanity.asset']",
  "result": [
    {
      "versions": [],
      "_type": "sanity.asset",
      "_id": "2w91UKgsNKEhWD6au6OzeNaOe7f",
      "_updatedAt": "2025-04-23T20:09:54Z",
      "_createdAt": "2025-04-23T20:09:49Z",
      "_rev": "LR2OfQiXk5TMHKZpxdBNWd",
      "aspects": {},
      "title": "Silhouette of a Woman in a Doorway Overlooking the Ocean",
      "currentVersion": {
        "_type": "reference",
        "_key": "",
        "_weak": false,
        "_ref": "image-11736fa2881515ae4fb5ba3db2fc247778ce8fab-4948x7422-jpg"
      },
      "assetType": "sanity.imageAsset",
      "cdnAccessPolicy": "public",
      "_system": {
        "createdBy": "gvRshKueQ"
      }
    }
  ],
  "syncTags": [],
  "ms": 5
}
```

The highlighted lines of the output show the IDs you need.

- Asset ID: The `id` of the document.
- Asset Instance ID: The `currentVersion._ref`.

## Link the asset to your project dataset

The next step is to link the asset to your project dataset with the [assets/media-library-link API](/docs/http-reference/media-library). This creates a local reference point that you can use throughout the dataset.

```
// Define your library ID, project ID, and dataset
const mediaLibraryId = '<your-library-id>'
const projectId = '<your-project-id>'
const dataset = 'production'

// Define your personal auth token.
const token = '<personal-auth-token>'

// Define the request body based on the Asset IDs from the previous step
// We've included the IDs from the earlier output as an example.
const requestBody = {
  mediaLibraryId: mediaLibraryId,
  assetInstanceId: "image-11736fa2881515ae4fb5ba3db2fc247778ce8fab-4948x7422-jpg",
  assetId: "2w91UKgsNKEhWD6au6OzeNaOe7f"
}

// POST a request to the assets API
fetch(`https://${projectId}.api.sanity.io/v2025-02-19/assets/media-library-link/${dataset}`, {
  method: 'POST',
  body: JSON.stringify(requestBody),
  headers: {
    'Authorization': `Bearer ${token}`,
    'Content-Type': 'application/json',
  }
})
```

Here's an example of the output. Note the `id` and the `media._ref` for the next step.

```json
{
  "document": {
    "_createdAt": "2025-04-23T22:01:54Z",
    "_id": "image-11736fa2881515ae4fb5ba3db2fc247778ce8fab-4948x7422-jpg",
    "_rev": "DkN0DBPn76SUp7SIvfkLE9",
    "_type": "sanity.imageAsset",
    "_updatedAt": "2025-04-23T22:01:54Z",
    "assetId": "11736fa2881515ae4fb5ba3db2fc247778ce8fab",
    "extension": "jpg",
    "media": {
      "_ref": "media-library:mlNBkjZ8wqSZ:image-11736fa2881515ae4fb5ba3db2fc247778ce8fab-4948x7422-jpg",
      "_type": "reference",
      "_weak": true
    },
    "metadata": {},
    "mimeType": "image/jpeg",
    "originalFilename": "11736fa2881515ae4fb5ba3db2fc247778ce8fab-4948x7422.jpg",
    "path": "images/y856rro4/production/11736fa2881515ae4fb5ba3db2fc247778ce8fab-4948x7422.jpg",
    "sha1hash": "11736fa2881515ae4fb5ba3db2fc247778ce8fab",
    "size": 4763977,
    "uploadId": "ml-link-Sg56SH3Sncz1on2HW9jvecXRdNC4mSWJ",
    "url": "https://cdn.sanity.io/images/y856rro4/production/11736fa2881515ae4fb5ba3db2fc247778ce8fab-4948x7422.jpg"
  }
}
```

## Patch the documents

Now that the asset is linked to your project and dataset, you can attach it to documents with a [document mutation](/docs/http-reference/mutation).

Use the document ID and the media reference from the previous step to attach the asset to documents in your dataset. As with local assets, you can patch linked Media Library assets to a document. The key difference is that you need to update the `asset` and `media` objects.

In this example, we're adding the asset to a `poster` field on our target document. Replace the path to the asset and media to match the shape of your content.

```
// Define your project ID, and dataset
const projectId = '<your-project-id>'
const dataset = 'production'

// Define your personal auth token.
const token = '<personal-auth-token>'

// Define the ID of the target document
const documentId = '<target-document-id>'

// Define the asset document ID and the media reference
// from the previous step
const assetDocumentId = 'image-11736fa2881515ae4fb5ba3db2fc247778ce8fab-4948x7422-jpg'
const mediaRef = 'media-library:mlNBkjZ8wqSZ:image-11736fa2881515ae4fb5ba3db2fc247778ce8fab-4948x7422-jpg'

// Define the mutation
const mutations = [{
  patch: {
    id: documentId,
    set: {
      poster: {
        asset: {
          _type: 'reference',
          _ref: assetDocumentId,
        },
        media: {
          _type: 'globalDocumentReference',
          _ref: mediaRef,
          _weak: true,
        }
      }
    }
  }
}]

// POST a request to the mutate API
fetch(`https://${projectId}.api.sanity.io/v2025-02-19/data/mutate/${dataset}`, {
  method: 'POST',
  body: JSON.stringify({mutations}),
  headers: {
    'Authorization': `Bearer ${token}`,
    'Content-Type': 'application/json',
  }
})
```

We captured the `media._ref` in the previous step, but you can also build it by combining the resource type (`media-library`) with your library ID and the asset ID.

These same steps apply for linking Media Library assets to any documents. 

#### Additional resources

[Media Library API](/docs/http-reference/media-library)

[Assets API](/docs/http-reference/assets)







# HTTP API reference



# CLI reference

Interact with Media Library with the `npx sanity media` command.

```text
usage: npx sanity media [--default] [-v|--version] [-d|--debug] [-h|--help] <command> [<args>]

Commands:
   create-aspect  Create a new aspect definition file.
   delete-aspect  Undeploy an aspect.
   deploy-aspect  Deploy an aspect.
   export         Export an archive of all assets and aspect data from the target media library.
   import         Import a set of assets to the target media library.

See 'npx sanity help media <command>' for specific information on a subcommand.
```



The `media` command must be run from within a directory that contains a valid `santy.cli.ts` configuration file. We recommend running it from within an existing Sanity project. [Learn more about configuring Media Library](/docs/media-library/configure-library).

## Commands

### `create-aspect`

Usage details are available in the [aspects guide](/docs/media-library/create-aspect).

```text
usage: npx sanity media create-aspect 

   Create a new aspect definition file.

Examples
  # Create a new aspect definition file.
  sanity media create-aspect
```

### `delete-aspect`

```text
usage: npx sanity media delete-aspect [ASPECT_NAME]

   Undeploy an aspect.

Options
  --media-library-id The id of the target media library.

Examples
  # Delete the aspect named "someAspect".
  sanity media delete-aspect someAspect
```

### `deploy-aspect`

```text
usage: npx sanity media deploy-aspect [ASPECT_NAME]

   Deploy an aspect.

Options
  --media-library-id The id of the target media library.
  --all              Deploy all aspects.

Examples
  # Deploy the aspect named "someAspect".
  sanity media deploy-aspect someAspect

  # Deploy all aspects.
  sanity media deploy-aspect --all
```

### `export`

```text
usage: npx sanity media export [FILE]

   Export an archive of all assets and aspect data from the target media library.

Options
  --media-library-id The id of the target media library.

Examples
  # Export all assets and aspects.
  sanity media export
```

### `import`

```text
usage: npx sanity media import [FILE | FOLDER]

   Import a set of assets to the target media library.

Options
  --media-library-id The id of the target media library.
  --replace-aspects  Replace existing aspect data. All versions will be replaced (e.g. published and draft aspect data).

Examples
  # Import all assets from the "products" directory.
  sanity media import products

  # Import all assets from "gallery" archive.
  sanity media import gallery.tar.gz

  # Import all assets from the "products" directory and replace aspects.
  sanity media import products --replace-aspects
```





# Limits and usage

This article describes limits in the media library and discusses techniques for leveraging your project bandwidth when rendering media library assets.

> [!NOTE]
> The limits in the media library are consistent across the free, growth, and enterprise plans. An add-on is available to enterprise customers that modifies the limits detailed here.

## Media Library limits

- Maximum number of stored assets: 20,000
- Included assets storage: 30GB
- Monthly included usage:- API: 2,500 requests
- APICDN: 10,000 requests
- Bandwidth: 1GB



> [!NOTE]
> Free plan customers are limited at these levels. Customers on other plans will be invoiced at their standard usage-based rates for usage above these thresholds.

The Media Library APIs share the same [technical limits defined here](/docs/content-lake/technical-limits) for rate limiting and HTTP request details.

## Library and project usage

When using the Media Library, you'll interact with a few different types of APIs. Some APIs are specific to the library, others apply globally across your organization, and others are specific to a project where you're running your studio.

Storing assets in the library will count against the 30GB and 20k asset limits defined above. However, using the media library does not count against any document limits you have in your projects.

When you use an asset within a studio, the asset becomes available through the standard project APIs for [presenting the media](/docs/apis-and-sdks/presenting-images). When you present the media this way, your bandwidth usage is accounted for through your project. This means that rendering assets that you've attached to a dataset/studio apply to that project's bandwidth, **not** the bandwidth of the media library.

> [!TIP]
> When is library bandwidth used?
> The library has a 'Copy Media URL' option to generate a URL to render the selected asset. This URL isn't linked to any particular dataset or studio. If you render the media through this URL, then the usage will count against your media library bandwidth. You might do this if you're using the asset in a social media post or an email -- i.e., locations outside the applications you've built on top of your studios.



# Developer guides

#### Getting started

[Sanity Studio Quickstart](/docs/sanity-studio-quickstart)

[App SDK Quickstart Guide](/docs/app-sdk/sdk-quickstart)

[Connected Content](/docs/studio/connected-content)

[Platform introduction](/docs/getting-started/the-sanity-content-operating-system-an-introduction)

[Framework-specific starters](/docs/getting-started)

[The developer essentials track](https://www.sanity.io/learn/track/sanity-developer-essentials)



#### Content OS

[Canvas content mapping](/docs/canvas/configure-content-mapping)

[Configure Dashboard](/docs/dashboard/dashboard-configure)

[App SDK](/docs/app-sdk)





# Best practices

Because Sanity is an unabashedly all-code tool, you can use AI to rapidly configure a new Sanity Studio or integrate Sanity Client into your application faster than low-code alternatives requiring hundreds of in-browser clicks. 

However, without proper guidance, AI typically produces average code that relies on defaults—resulting in suboptimal user experiences for your content creators.

AI tools can dramatically accelerate Sanity development, but without proper guidance, they often produce generic code that fails to leverage Sanity's full capabilities. This guide bridges that gap, teaching you how to instruct AI to generate Sanity code that follows established best practices while maintaining the speed advantages of AI assistance. 

While most models understand all the APIs Sanity makes available, they are unlikely to follow [opinionated best practices](https://www.sanity.io/guides/an-opinionated-guide-to-sanity-studio), which we have published over the years in guides and courses on [Sanity Learn](https://www.sanity.io/learn).

This guide will help you:

- Set up AI tools to generate high-quality Sanity code 
- Avoid common pitfalls of AI-generated configurations 
- Implement best practices from Sanity Learn into your AI workflow 
- Create better experiences for your content teams



## Implementing AI Rules and documentation

You can add these rules to your project to ensure any generated code follows our published best practices. We will continue to iterate on these files over time.

Implementing rules for AI code generation is different for each IDE. We recommend that you also go to its documentation to learn how to exactly configure its context.

### Setting Up AI Rules

The [sanity-io/ai-rules](https://github.com/sanity-io/ai-rules) repository contains a deliberately short, manually curated `.mdc` file with most of the [opinionated best practices](https://www.sanity.io/guides/an-opinionated-guide-to-sanity-studio) for writing Sanity Studio configuration and GROQ queries.

We intend to keep it updated as we discover new rules, so it might be worth revisiting it from time to time—we also welcome PRs.

**Cursor**

- [Add the rules file](https://github.com/sanity-io/ai-rules/blob/main/sanity-opinionated.mdc) to your project at `.cursor/rules/sanity-opinionated.mdc`

**Others (VS Code, etc)**

- [Add the rules file](https://github.com/sanity-io/ai-rules/blob/main/sanity-opinionated.mdc) to your project as a `.txt` file and refer to it when prompting new Sanity code

### Leveraring documentation content

The Sanity documentation has several ways you can use AI to get the job done:

- There is a **Copy article **button on all articles that puts the markdown version of the content on your clipboard
- You can go to /docs/llms.txt and /docs/llms-full.txt to access all the links and the full corpus as markdown formatted content
- You can also add `.md` at the end of any article URL to get the markdown version

### Leveraging Sanity Learn content

All course and lesson material on [Sanity Learn](https://www.sanity.io/learn) is automatically kept updated and made available in an LLM-friendly format thanks to Portable Text being easily serialized to the `llms.txt` standard. You can read [how we made this](https://www.sanity.io/blog/improving-the-agent-experience-for-sanity-learn) on our blog.

We are working on making the documentation available as well. 

There are two different sizes you can import into your IDE:

- `/llms.txt` is an abbreviated index of all the content with links 
- `/llms-full.txt` is the complete content (sometimes optimized to fit within the context window limits)

**Cursor**

- Within Cursor chat, type "@Docs," select "Add new doc" and then paste in `https://www.sanity.io/learn/llms.txt` (or the filename of your choice above)
- You can also add `https://www.sanity.io/docs/` to the Cursor Settings -> Features -> Docs configuration. 

**Others (VS Code, etc)**

- At this time, there is no documented support for the `llms.txt` standard. We will update this page as it is made available.



# Paginating with GROQ

## What is pagination?

It's often necessary to display a large amount of content. For example, a shopping site may want to show thousands of products and let the user navigate it, page by page.

GROQ makes it easy to sort and slice your data, and it's tempting to simply use array slicing to select the data to display on a page. For example, if we think of a traditional web page with a **Next Page** link, the first page might show results 1–100, while the next page shows results 101–200, and so on. We can also use page numbers, and calculating the offset range from a page number is simple.

However, the most obvious way to do pagination isn't actually very performant. In this article we'll explore different ways to do it.

## The less efficient approach: Array slicing

One approach to pagination would be to use array slicing. For this example, we want to fetch articles sorted by their ID:

```groq
*[_type == "article"] | order(_id) [100...200] {
  _id, title, body
}
```

While GROQ makes this super easy, this is actually surprisingly inefficient. To understand why that is, we need to look at how the GROQ execution engine operates on your dataset's content.

In order to slice your dataset like this, GROQ needs to fetch all the content and then sort it (this is true even if you don't use `order()`, because results are always ordered in some way). It then needs to skip all the documents that aren't included in the slice range.

The sorting can usually happen while fetching, and we can apply some magic to limit the total number of results we need to look at. But the engine still has to skip all the unwanted results; it can't simply "teleport" to a specific position.

In this case, it will need to fetch 200 documents and then skip the first 100.

If you have just a few hundred documents in your dataset, the performance drop might not be that noticeable. The problem only becomes measurable once you reach thousands of results. For example, it's not unthinkable that a query such as `*[10000..10100]` could take several seconds to execute.

Generally, you can expect slicing performance to be roughly linear relative to the slice range. For example, if the range `100...200` takes 5ms to run, then you can expect `200...300` to take about 10ms, `300...400` to take 15ms, and so on.

## A better approach: Filtering

We can use GROQ's filtering capabilities in combination with sorting to quickly skip elements. This is much more efficient than slicing, because it allows the GROQ query engine to throw away lots of results very efficiently.

> [!NOTE]
> Make sure all parts of your pagination query are optimised! 
> At this moment, not all functions etc. are optimised to be used in filters for pagination.
> If you want to make sure that your pagination queries run smoothly, try to follow the article on High-performance GROQ in your queries!

The very first page is exactly the same as before; we fetch the first 100 results:

```groq
*[_type == "article"] | order(_id) [0...100] {
  _id, title, body
}
```

The difference in the approach becomes clear when we want to fetch the second page. To find the second page, we first need to know what the last document we looked at was, which we can find from the last document ID in the array of resuls. Once we have that information, we can plug it back into our next query as a query parameter:

```groq
*[_type == "article" && _id > $lastId] | order(_id) [0...100] {
  _id, title, body
}
```

The key part here is `_id > $lastId`. By adding such a filter, we are skipping past all the results that were present in the first batch of results. 

Getting the third page is exactly the same: After getting the second page, we need to look at the last ID and then continue from there.

To tie everything together, our code will end up looking something like this:

```javascript
let lastId = ''

async function fetchNextPage() {
  if (lastId === null) {
    return []
  }
  const {result} = await fetch(
    groq`*[_type == "article" && _id > $lastId] | order(_id) [0...100] {
      _id, title, body
    }`, {lastId})
  
  if (result.length > 0) {
    lastId = result[result.length - 1]._id
  } else {
    lastId = null // Reached the end
  }
  return result
}

```



## Sorting on other fields: Tiebreakers to the rescue

One thing we didn't tell you is that the above solution is a partial one: **It only works on fields whose values are unique in your dataset**. If you want to sort on something non-unique like a "published at" field, you need to a little bit more work.

The filter technique doesn't work properly for non-unique fields because the filter will skip duplicate values, even if the documents are different. For example, imagine the filter, once again, but for a `publishedAt` field:

```groq
publishedAt > $lastPublishedAt
```

If you have more than one document with the same `publishedAt` timestamp at the pagination boundary, this filter will actually skip documents by accident, since it will always skip distinct field values.

The solution is to use a tiebreaker, which is another field that takes priority if more than one document with the same `publishedAt` is encountered. As before, the first page is trivial:

```groq
*[_type == "article"] | order(publishedAt) [0...100] {
  _id, title, body, publishedAt
}
```

Notice how we are also asking for the `publishedAt` attribute. We'll need this for the next page.

Here's how we would fetch the next page:

```groq
*[_type == "post" && (
  publishedAt > $lastPublishedAt
  || (publishedAt == $lastPublishedAt && _id > $lastId)
)] | order(publishedAt) [0...100] {
  _id, title, body, publishedAt
}
```

As before, we are filtering. Our main filter is `publishedAt > $lastPublishedAt`, which allows us to continue from the last page. But we also include a tiebreaker: If we have a document that is the same as the last `publishedAt`, we instead ask that it be higher than the last *document ID*.

As before, our client code would look something like this:

```javascript
let lastPublishedAt = '' 
let lastId = ''

async function fetchNextPage() {
  if (lastId === null) {
    return []
  }

  const {result} = await fetch(
    groq`*[_type == "article" && (
      publishedAt > $lastPublishedAt
      || (publishedAt == $lastPublishedAt && _id > $lastId)
    )] | order(publishedAt) [0...100] {
      _id, title, body, publishedAt
    }`, {lastPublishedAt, lastId})
  
  if (result.length > 0) {
    lastPublishedAt = result[result.length - 1].publishedAt
    lastId = result[result.length - 1]._id
  } else {
    lastId = null  // Reached the end
  }
  return result
}

```

## What if the data changes during pagination?

One key difference between filter-based and slice-based pagination is with data changes.

Slicing will always operate on the entire dataset as a sequence. This means that the following can happen:

- The user views page 1 of the results, which are sorted by date.
- You publish 5 new documents.
- The user navigates to page 2.
- The user will now see a repetition of the last 5 documents they saw on page 1.

This cannot happen if you are using the filter approach.

## Finding the total number of documents

Something you may want is to display the total number of results somewhere. This can be accomplished by wrapping your query in a `count()`:

```groq
count(*[_type == "post"])
```

This query should be relatively fast, but watch out for large datasets, as the speed of counting is relative to the number of results.

## Using this technique in a real-world application

### Tracking navigation state

In order to provide forward and backward navigation in a real-world application, we'll need to store the navigation state about the last page we saw:

- In a React application, this can be as simple as using `useState()` to store the `lastId` information as state. If you want to support backward navigation, you will also have to store the previous one.
- In a traditional backend app (Rails, Django, etc.), you may want to encode this as a query parameter instead. For example, a URL might look like this: `https://example.com/products?lastId=4a3b2e84`.

### Page numbers

The main downside to filtering is that there's no way to easily jump to any page number — there's no *random access*. In order to render the right page, we need to have visited its preceding pages. But it's still possible to implement number-based navigation.

To show the **previous** page numbers (relative to the current page), you can simply track the current page number, which increases by one for each forward navigation. To enable quick navigation to a past page, you can also store some state about each `lastId` encountered during forward navigation (e.g. stored as an array of IDs).

To show the **next** page numbers, you need to do a `count()` pass that counts the total number of results that follows the last `lastId`. (See separate section on how to count the total number of results.) To determine the ID that each next page corresponds to, your application then needs to be able to randomly skip results. For example, if the user is on page 2 and wants to jump to page 10, you can find the next `lastId` with this query:

```groq
*[_type == "article" && _id > $lastId][$index]._id
```

The value for `$index` should be set to `($pageNumber - $currentPage) * $pageSize - 1`.

Note that this query can be slow, and you probably don't want to offer this type of navigation past a certain page number.

> [!TIP]
> Protip
> Do you need page numbers? It may be tempting to render a traditional navigation bar with links to each numbered page. But you may want to think about whether this UX pattern even makes sense for your application. What a user mostly wants, we would argue, is to go forwards and backwards.

## Batch processing

Everything above has been written with a front-end application in mind. But pagination is also relevant in a different scenario: When we want to execute a query that returns many results, perhaps even the entire dataset, and we want to process those results.

Some examples:

- A batch process that runs through the entire dataset and updates a field.
- A static site builder job that builds a web site from a Sanity dataset, using something like Next.js or Gatsby.
- A job that exports a big subset of the data to a file.

For such scenarios, we strongly recommend paginating by filtering, and by sorting the dataset by `_id`. Since all datasets are already physically sorted by ID, this is the most performant way to sort the dataset, and avoids the need for a tiebreaker.

You may also want to experiment with different batch sizes. Larger batch sizes are usually faster than small ones, but only up to a point.

> [!TIP]
> Protip
> In the general case, we recommend a batch size of no more than 5,000. If your documents are very large, a smaller batch size is better.





# High performance GROQ

<p>GROQ makes it really easy to query data from Sanity. Thanks to Sanity’s super fast CDN, it's possible to write large, complex queries that still return data quickly.</p>The majority of users will never need to think too much about optimizing their queries to improve response times. But occasionally one may come to discover that queries that were initially fast when the dataset was small, suddenly have become slower as the dataset has grown and the queries have become more complex and resource-heavy during development.

Unlocking optimization in the way you write queries can be helped by understanding the way in which a query is executed. In this document we'll explain how the query engine works, and how to avoid certain pitfalls that can make your queries slow.

## Understanding fetching, filtering, and sorting

The GROQ query engine is designed to deconstruct a query into a set of *pipelines* according to a query plan. A pipeline has the following broad structure:

![Query pipeline diagram](https://cdn.sanity.io/images/3do82whm/next/502007bb22c147e11f8a6803977f25bc5da4aa42-1780x670.jpg)

Most GROQ queries start by **fetching** data. The shortest query you can write looks like this, and will return all documents:

```groq
// Fetch all documents
*[]
```

Anything we write inside of a pair of square brackets will **filter** the results. Let’s update our query to filter for just `product` type documents:

```groq
// Only fetch documents where the '_type' field equals 'product'
*[_type == "product"]
```

This filter is made of three parts:

- `_type` is an **identifier**, referring to the attribute of a document
- `==` is an **operator**, “equals”
- `"product"` is a **literal**, in this instance a string

This query is fast because we know ahead of time what we want `_type` to equal. So the query can perform a **filtered fetch** against the dataset’s indexes for exact matches. Internally, the query engine uses special index structures to optimize such queries, which you may be familiar from traditional SQL databases.

Queries that can do filtered fetches are the fast type of query. But it’s also possible to write a query that is unable to do filtering efficiently, which will often make the query execute slowly.

> [!WARNING]
> Gotcha
> Inefficient filtering may not become apparent until your dataset reaches a certain size. Small datasets are inherently fast to query. A query that executes against a dataset of 5 documents may take a millisecond. But once your dataset grows to thousands of documents, performance issues may start to crop up.

Similarly to filtering, using **sorting** in a query can be unexpectedly slow. The `order()` function is designed to also make use of index structures, which means it generally only accepts simple attributes or attribute paths. For example, the following query cannot be optimized and must load the entire result set into memory to sort it:

```groq
// This cannot be optimized, because the order expression uses string concatenation
*[_type == "person"] {
  firstName, lastName
} | order(firstName + " " + lastName)
```

### Unfiltered over-fetches

It’s possible to write filters that cannot be optimized to make use of our internal index structures. One example is when they use a non-literal expression on both sides of the operator.

Imagine our product documents have both `salePrice` and `displayPrice` number fields. Since the Content Lake is schemaless and we know these fields only exist on product documents, let’s fetch all products that are currently discounted with the shortest possible query.

```groq
// Fetch all documents
// ...then filter down to those where salePrice is less than displayPrice
*[salePrice < displayPrice]
```

- `salePrice` is an **identifier**, assumed to be a numeric attribute that exists
- `<` is an **operator**, “less than”
- `displayPrice` is an **identifier**, assumed to be a numeric attribute that exists

This is not something that the query engine can optimize, because neither `salesPrice` nor `displayPrice` are known ahead of time. In order to satisfy the filter expression, we explicitly need to look at every single document.

So in this case, the GROQ query engine must **over-fetch** for all documents, loading every single document into memory and then filtering the results by the expression.

> [!TIP]
> Protip
> See reference below. For a complete list of optimizable filter expressions, see the reference section at the end of this document.

### The parent operator

The parent operator `^` is a notable exception to the rule above. Consider a document that contains a reference to a parent document, an additional sub-query will fetch all documents that also have the same parent reference:

```groq
// Direct-use of the parent operator is optimized
*[_type == "person"] {
  _id, parent,
  "siblings": *[_type == ^._type && parent._ref == ^._id] 
}
```

Note: This is currently only optimized where the expression satisfies the list in the previous section. Using an expression on `^` together with a function, string concatenation, etc. may not be optimized. For example:

```groq
// String concatenation of the operator is not optimized 
* { _id, "draft": *[_id == "drafts." + ^._id] }
```

### Counting aggregation

Similarly to filtering, a `count()` is only optimizable if its interior expression is completely optimizable. For example:

```groq
count(*[_type == "person" && isPublished])
```

This, on the other hand, is not optimizable:

```groq
// Not optimizable, because it uses == with a computed expression count
(*[_type == "person" && (firstName + " " + lastName) == "Ronald McDonald"])
```

### Sorting

As with filtering, sorting is only optimizable on certain types of expressions such as single attributes:

```groq
* | order(name)
```

### Deep array indexing and slicing

It's worth talking about what happens when you slice a query result:

```groq
// Deep index
*[_type == "article"][10000]

// Deep slice
*[_type == "article"][10000..10100]
```

All results returned by `*` always have a sort order; even if you don't specify an order(), the results will be ordered by `_id`. This means that to slice a subset of the results, the entire dataset needs to be sorted and the first 10,000 results "skipped" over in order to reach the slice range.

While this sorting and skipping is quite fast, the performance is directly related to the number of results that are skipped. For example, if getting `[1000]` takes 100ms, you can expect `[2000]` to take about 200ms.

### Query performance and dataset size

Dataset growth should have **no impact** on optimized, filtered queries that return the same amount of data. For example this query should return at the same speed no matter if there is 100 or 100,000 documents in the dataset.

```groq
*[slug.current == "discounted"]._id
```

## Tips and tricks

### Reduce search space by “stacking” filters

If your query cannot be written to avoid this comparison of non-literals, you can stack additional filters into your query to reduce the number of documents loaded into memory before filtering.

For example, we can be explicit about the `_type` and in this hypothetical we know that not all products even have a `salePrice` field.

```groq
// Only fetch documents where:
// the '_type' field equals 'product' and it has a 'salePrice' field
// ...then filter down to those where salePrice is less than displayPrice
*[_type == "product" && salePrice != null && salePrice < displayPrice]
```

This query can now run faster because it only needs to look at products where the `salePrice` exists as an attribute. But you are best to avoid comparisons of non-literals where possible.

### Avoid repeated resolving of references

Resolving references in GROQ is made easy with the reference access operator `->`. The simplicity of this syntax hides its functionality, however. This `->` operator is actually a subquery in disguise:

```groq
// Fetch all categories titles and parents:
*[_type == "category"] {
  title,
  parent->
}

// ...is the same as:
*[_type == "category"] {
  title,
  "parent": *[_id == ^.parent._ref][0] 
}
```

This is fine in the above instance where we are resolving the reference once. However, needlessly repeating the `->` operator will perform that subquery over and over.

```groq
// Slow, repeated subquery
*[_type == "category"] {
  title,
  "slug": slug.current,
  "parentTitle": parent->title, 
  "parentSlug": parent->slug.current
}
```

We can instead use the merge operator, `...`:

```groq
// Merge a single subquery into the root level of the result
*[_type == "category"] {
  title,
  "slug": slug.current,
  ...(parent-> {
    "parentTitle": title,
    "parentSlug": slug.current
  }) 
}
```

### Reduce the amount of data returned

The merge operator `...` is a convenient way to return all data. However, this may be returning significantly more data than required. This can impact the speed at which your data is returned.

For example, our query for discounted products is currently returning every field in each document. But we could scope this down to required parts once our front-end knows exactly what it needs.

```groq
// Return all fields and resolve all fields in all 'categories' references
*[_type == "product" && defined(salePrice) && salePrice < displayPrice]{
  ...,
  categories[]->
}

// Return just these required fields and the title of each 'categories' reference
*[_type == "product" && defined(salePrice) && salePrice < displayPrice]{
  title,
  salePrice,
  displayPrice,
  "categories": categories[]->title
}
```

### Avoid joins in filters

The `->` operator can be used to pull in related data. However, it is a comparatively expensive operation, and should be used with care.

It is particularly expensive to use `->` inside a filter expression:

```groq
*[_type == "post" && author->name == "Bob Woodward"]
```

Avoiding this can be difficult, but spending a little more time on the data modeling can be worth the effort. While “denormalizing” a data model is often considered a negative, a little denormalizing for frequently “core” fields can significant improve query performance.

Another common pitfall is using a related document’s field as an “identity” field, rather than relying on that document’s ID. This is a sign of poor data modeling:

```groq
*[_type == "post" && vertical->slug.current == "football"]
```

Consider giving the related document a better ID, and then using the ID itself:

```groq
*[_type == "post" && vertical._ref == "football"]
```

### Avoid resolving assets

It may be tempting to resolve an asset reference in order to get its URL. This URL is to the full size image and therefore not optimized. The rest of the metadata on the asset record may not be used by your front end and adds bloat to the amount of data returned.

```groq
*[_type == "product"]{
  // Resolves much more metadata that you probably need
  image->,

  // The url of a full size unoptimized image
  "imageUrl": image->asset.url,

  // Just get the image _ref and dynamically create the URL
  image
}
```

However, the `_id` assigned to an image once uploaded is deterministic, unique to that specific file and contains a lot of data about the image itself like its filetype and size.

So you can [dynamically create a URL](https://www.sanity.io/docs/image-url) using just the `projectId`, `dataset` and `_id` of the image.

### Avoid reusing projected values

A common mistake is to use a projection expression, followed by additional query expressions that filter or sort. For example:

```groq
// Sorting on a projected attribute
*[_type == "person"] {
  "name": firstName + " " + lastName,
  "isBoss": role->name == "boss",
} | order(isBoss, name)

// Filtering on a projected attribute
*[_type == "person"] {
  "isBoss": role->name == "boss",
}[isBoss == true]
```

In these two queries, the expressions we use — `order(isBoss, name)` and `isBoss == true` — look like they satisfy the constraints we explained earlier; after all, they are just using simple attributes. But they are in fact not optimizable, since the attributes in question are computed at query time.

> [!TIP]
> Protip
> The query engine is smart enough to know if an attribute is merely being renamed or moved around. For example, *{ "foo": bar.baz } | order(foo). Here, even though foo is a “made up” attribute, the query engine understands that it can simply sort on bar.baz.

### Avoid large queries

The bigger a GROQ query, the longer the engine takes to parse and plan it. It can only be slow for a web browser to send several hundred kilobytes of GROQ to the backend.

### Avoid slicing when "paginating"

It's tempting to use slicing to fetch results in pages:

```groq
*[_type == "article"] | order(_id)[1000..1020]
```

As described above, this is inefficient. You can make this faster by paginating by a field instead:

```groq
*[_type == "article" && _id > $lastId] | order(_id)[0..20]
```

For each page, you can note down the highest `_id` and use that as the next `$lastId`.

### Parallelize independent queries

A common technique for “page builder”-driven applications is to build a “superquery” that collects data for multiple components at once:

```groq
{
  "topPosts": *[_type == "post" && category == $category]
    | order(popularity desc)[0..30],
  "news": *[_type == "news"] | order(_createdAt desc)[0..10],
  "user": *[_type == "user" && _id == $id],
}
```

This could be broken up into three queries, which allows them to be parallelized. The `topPosts`, `news`, and `user` queries would each be fetched separately, as in the following example using the [JavaScript client](/docs/js-client): 

```javascript
const topPostsParams = {
  category: // ...
}
const userParams = {
  id: // ...
}

const topPosts = client.fetch(`*[_type == "post" && category == $category] | order(popularity desc)[0..30]`, topPostsParams)
const news = client.fetch(`*[_type == "news"] | order(_createdAt desc)[0..10]`)
const user = client.fetch(`*[_type == "user" && _id == $id]`, userParams)
```

## Explain mode

If you want to look at how your query is executed by the query engine, you can ask the API for a query plan. You do this by providing the `explain` parameter, which you can read more about in the [API reference](/docs/http-api).

> [!NOTE]
> The explain format
> The format used for the explain output is currently undocumented and can be difficult to understand if you are not familiar with the query engine.

## Reference: Filter expressions

The following filter expressions can always be optimized. **This is not an exhaustive list**.

> [!NOTE]
> Attributes
> In the examples below, attribute can also be a dotted path such as foo.bar.baz, but not more complex attribute expressions such as foo[0].bar, foo[1..2], etc.

### Binary

#### Expression

Where `op` is one of `==`, `!=`, `<`, `<=`, `>`, `>=`:

- `<attribute> <op> <literal>`
-  `<literal> <op> <attribute>`

#### Examples

- `category == "sport"`
- `age > 20`

### Boolean attribute

#### Expression

- `attribute`

#### Example

- `isPublished`

### Logical expressions

#### Expression

- `!optimizableExpression`
- `<optimizableExpression> && <optimizableExpression>`
- `<optimizableExpression> || <optimizableExpression>`

#### Examples

- `published && !visible`
- `!hidden || category == “post”`

### Arrays

#### Expression

- `<literal> in <attribute>`
- `<literal> in <arrayAttribute>`

#### Examples

- `"sport" in categories"`
- `user-123" in users[]._ref`

### `dateTime()` function

#### Expression

Where `op` is one of `==`, `!=`, `<`, `<=`, `>`, `>=`:

- `dateTime(<attribute>) <op> dateTime(<string>)`
- `dateTime(<string>) <op> dateTime(<attribute>)`

Note that `now()` is a special case; it evaluates to a static string at query time, and can be used here.

#### Examples

- `dateTime(publishedAt) <= dateTime(now())`

### defined() function

#### Expression

**Note**: This is shorthand for `<attribute> != null`.

- `defined(<attribute>)`

#### Examples

- `defined(publishedAt)`

### `string::lower()` function

**Note**: This also applies to `lower()` (with no namespace).

#### Expression

- `string::lower(<attribute>) == <string>`
- `<string> == string::lower(<attribute>)`

#### Examples

- `lower(tag) == “football”`

### `match` operator

#### Expression

- `<attribute> match <string>`
- `<attribute> match <array>`

#### Examples

- `title match "content*"`
- `title match ["structured", "content"]`

### pt::text() function

#### Expression

- `pt::text(<attribute>) match <string>`
- `pt::text(<attribute>) match <array>`

#### Examples

- `pt::text(content) match "structured content"`

### path() function

#### Expression

> [!WARNING]
> Gotcha
> This is only optimized for the _id attribute.

- `_id in path(<string>)`

#### Examples

- `_id in path("a.b.**")`

### references() function

#### Expression

- `references(<string>[, ...])`

#### Examples

- `references(^._id)`

### Geographic functions

#### Expression

- `geo::intersects(<attribute>, <geo>)`
- `geo::contains(<attribute>, <geo>)`
- `geo::disjoint(<attribute>, <geo>)`
- `geo::contained(<attribute>, <geo>)`

## Reference: Sort expressions

The following expressions are optimized for sorting.

### Plain attribute

#### Expression

- `order(<attribute>)`

#### Examples

- `* | order(lastName, firstName)`

### `string::lower()` function

**Note**: This also applies to `lower()` (with no namespace).

#### Expression

- `order(string::lower(<attribute>))`

#### Examples

- `* | order(string::lower(name))`

### `dateTime()` function

#### Expression

- `order(dateTime(<attribute>))`

#### Examples

- `* | order(dateTime(publishedAt))`



# Setting up Single Sign-On with SAML

## Introduction

[SAML](https://en.wikipedia.org/wiki/Security_Assertion_Markup_Language) [SSO](https://en.wikipedia.org/wiki/Single_sign-on) enables your organization to control access to Sanity projects by using a third-party identity provider, such as [Okta](https://www.okta.com/), [Google](https://support.google.com/a/answer/6087519?hl=en), or [Azure Active Directory](https://azure.microsoft.com/en-us/products/active-directory/). When you enable SAML (Security Assertion Markup Language) SSO (Single Sign-on), users who log into a project through studio or the management interface, will be authenticated through the organization’s identity provider. Once they log in, they will be assigned roles according to rules based on group membership from their user record in the identity provider.

## Prerequisites

- An [organization](https://www.sanity.io/docs/projects-organizations-and-billing) with a project on [Enterprise plan](https://www.sanity.io/pricing) or [Growth plan with the SAML SSO add-on](https://www.sanity.io/pricing#add-ons).
- An external Identity Provider which supports SAML authentication (e.g., [Okta](https://www.okta.com/), [Google](https://support.google.com/a/answer/6087519?hl=en), or [Azure AD](https://azure.microsoft.com/en-us/products/active-directory/)).
- Organization administrator permissions.



## Setting up Single Sign-On with SAML

### 1. Create a new SAML SSO configuration for your organization

Access your organization's settings by going to [sanity.io/manage](https://sanity.io/manage) and select the appropriate organization in the dropdown menu in the upper left corner. Then select the SAML SSO section in the left sidebar, and click the button to create a SAML SSO provider.

> [!WARNING]
> Gotcha
> SAML SSO is available only for Enterprise plan or Growth plan with SAML SSO add-on.
> 
> Read more about our plans and pricing options
> 
> 

A new dialog will appear informing you that SAML SSO has not yet been configured. Click the button labeled **Configure** to start setting up your provider. This will generate the details needed to connect your identity provider with your organization on [Sanity.io](http://sanity.io/).

![Dialog informing the user that SAML SSO has not yet been configured](https://cdn.sanity.io/images/3do82whm/next/af796cdb46bca2202072958c9e08f1d7c2cd7000-4020x2259.png)

### **2. Use the details presented to configure the external identity provider**

The process of configuring SAML SSO starts with Sanity providing you with the necessary details for setting up your external identity provider. You can use the copy buttons to put the correct strings on the clipboard. It's also possible to download the settings as a SAML XML file.

![Shows the Sanity provider details screen with 4 fields highlighted with sequential integers from 1 to 4.. These are labeled "Sanity callback URL", "Sanity entity ID", "NameID Format" and "Attributes"](https://cdn.sanity.io/images/3do82whm/next/2f76e590de33b08a16622d5da0ce8a5e854623a2-4020x3090.png)

Be aware that different providers may use different terminology. Shown here is the interface for entering these details if you're using [Okta.com](https://www.okta.com/). If you're using Azure AD, please also see the guide to [set up SSO authentication with SAML and Azure](https://www.sanity.io/guides/set-up-sso-authentication-with-saml-and-azure). Note that “callback URL” is called “Single sign on URL.”

![Shows the Okta SAML settings screen with 4 fields highlighted with sequential integers from 1 to 4. These are labeled "Single sign-on URL", "Audience URI (SP Entity ID", "Name ID Format" and "Attribute statements"](https://cdn.sanity.io/images/3do82whm/next/3121993fea41db265f77342257c3c47e213c0bf6-4020x3090.png)

Note the mapping of attributes for user accounts. Sanity requires `email`, `firstName`, and `lastName` to be mapped to corresponding values from the identity provider. `id` and `displayName` are optional.

> [!WARNING]
> Gotcha
> Make sure to set the groups in the external identity provider that should have access to the integration.

### 3. Configure the SAML service provider with the settings of the external identity provider

Having set up the external identity provider with the parameters you obtained from Sanity, it is now time to do the reverse. Scroll down to the next section with the heading **Your Identity Provider details.** Go ahead and fill in the appropriate values.

> [!TIP]
> Protip
> Many providers will let you download the required settings as an XML file. If you have such a file, click on the top right button labeled Upload new metadata to save yourself some tedious copy/pastes.

![](https://cdn.sanity.io/images/3do82whm/next/9819293029949cf9ea730c668c5e55881b8d2070-4021x3090.png)

### 4. Name your configuration and set options for role mapping

Scroll to the next section to give your configuration a meaningful name, and choose whether or not to automatically update roles whenever a user logs in. You may also set the length of user sessions to your liking before saving your configuration.

> [!NOTE]
> Automatically update roles
> If the option to automatically update roles is selected, the mapping of roles will happen every time a user logs in using the project-specific login url. This will also disallow manual management of roles.

![Show a section of the Sanity project management console labeled "General settings for SAML SSO across all projects". The following options can be set:  "Configuration name", "Auto update roles on login" and "Session TTL"](https://cdn.sanity.io/images/3do82whm/next/6d028886f556fd521c639de8ee893601ac7a79f1-4020x2259.png)

### 5. Set a slug for your organization

At the bottom of the SAML SSO configuration page you'll be able to define a unique slug that will identify your organization in certain SSO workflows, such as logging in via CLI or logging into the project management console. Note that this setting can also be accessed under **General settings**, and might therefore already have a value set.

> [!NOTE]
> Your organization slug must:
> Be globally unique 
> 
> Be between 1-20 characters long 
> 
> Start with an alphanumeric character 
> 
> Contain no other characters than a-z, 0-9 and -

![Shows the UI for specifying a slug for an organization](https://cdn.sanity.io/images/3do82whm/next/7f5b45a6f5bf426fec9b2ce7747ed2fe20b8f3d1-4020x2259.png)

```sh
# Example CLI login using the slug 'saml-docs'
sanity login --sso saml-docs
```

### 6. Enable SSO and configure role mapping for the desired project(s)

After saving your settings you are ready to move on and enable SSO for one or several of your projects. In the process, you’ll also configure role mapping for each project.

![Shows a list of projects belonging to an organization, both labeled as not having SAML SSO configured yet](https://cdn.sanity.io/images/3do82whm/next/d06f4bd99e65405a58c70dc1981e322a10d5043c-4020x2259.png)

_This is a paid feature, available as an addon on the Growth plan._

In the role mapping dialog, you’ll set a default fallback role which will be applied to users who don’t belong to any groups matching your mapping rules, as well as rules to map groups from your SSO provider to roles in this project. Role mapping rules are evaluated against the group membership attribute of the user identity on the identity provider, and they support [regular expression](https://github.com/google/re2/wiki/Syntax) syntax (observe back references and lookahead assertions are not supported). A few examples are listed below:

- `editors` will match *exactly* `editors`
- `.*-admin` will match `news-admin`, `sales-admin`, `-admin` and so forth
- `[aA]dmin` will match `admin` & `Admin`

![A role mapping dialog with a default fallback role and three group names mapped to Sanity roles](https://cdn.sanity.io/images/3do82whm/next/eeea636533b5c2bd061c7a0a8434e6ea5cad38e5-4020x3090.png)

### 7. Test your configuration by attempting to login

Before setting up your studio to use the new SSO setting, make sure everything is working as expected by visiting the project-specific link provided and log in to your project management console. Copy-paste the Sanity manage project-specific login URL into your browser's address bar. If correctly set up, this will log you out of your current account and into the user account given to you by your SSO identity provider. It might be convenient to test this in another browser.

![](https://cdn.sanity.io/images/3do82whm/next/6904066477e2b016f5feac0c8e39c9ff1bd19cf4-4020x2259.png)

### 8. Configure the studio to use the new SSO provider

Next, you'll want to update your Studio to show the login screen from your SSO identity provider by using the [Auth API](https://www.sanity.io/docs/auth-api-reference). Instructions for both v3 and v2 studios can be found by expanding the dropdown labeled **SAML SSO login for Sanity Studio**.

![Shows a dialog confirming that SAML SSO is activated and configured. Also shown is example code for setting up your studio for SAML SSO authentication.](https://cdn.sanity.io/images/3do82whm/next/bd5d9cfeaec98a0c2a7fdb973e62a28c167ef808-2632x2032.png)

> [!TIP]
> Protip
> By default the code snippet will show how to add SAML SSO to the list of login options. If you want to replace the list of options entirely, change the value of mode from "append" to "replace".

### 9. Verify by logging in with SSO

Finally, verify that the configuration work as expected by logging in as a user from your identity provider. Your login screen should list only the appropriate options.

![Shows the Sanity Studio login screen with the following alternative for logging in: "Google", "Github", "E-mail / password", and "SAML"](https://cdn.sanity.io/images/3do82whm/next/a6bde8176914f60b15bb1c0838371a7f196096d8-4020x2259.png)

After logging in at least once with SAML SSO, you may want to check your organization's members in the management interface. You'll see a visual indicator on each member's avatar indicating what sign-in-method they use, allowing you to quickly make any adjustments – such as deleting or demoting accounts outside your identity provider domain.

![Shows a list of users with different roles and different icons that reflect their chosen method of authentication](https://cdn.sanity.io/images/3do82whm/next/7860adfc3c9a109729f9188efc23f7c0e3eb395c-4020x2259.png)

> [!WARNING]
> Gotcha
> SAML SSO members will take up an additional seat towards your quota since they are considered individual users in Sanity. Checking your member list for duplicates is recommended after changing your identity provider.



# Third-Party Login (Deprecated)

_This is a paid feature, available on the Enterprise plan._

> [!NOTE]
> Looking for SAML SSO?
> This article discusses the deprecated, legacy method of implementing third-party login. Please use our new and improved implementation of SAML SSO. 

Users that have activated custom access control on their plan may replace a project's user database with their own custom login solution, e.g. to integrate with a single sign-on (SSO) system such as Active Directory or Kerberos. 

Sanity provides APIs to register users and permissions for datasets and create Sanity sessions, but it is up to the customer to actually implement the integration with their authentication system.

Implementing third-party login involves:

- Registering Sanity groups with appropriate permissions via the API, either manually or with code.
- Writing code to generate a Sanity session when a user logs in.
- Optionally modifying the Sanity Studio to use a separate login form.

We'll go through each of these steps in detail below. 

We also have a [sample Node.js application](https://github.com/sanity-io/3rd-party-auth-example) that shows how to go about this using Passport.js. It currently has support for authenticating with Google's OAuth and Okta's SAML API.

## Managing groups and users

Sanity uses groups to grant permissions to various users, as described in the [access control](/docs/archive/access-control-deprecated) section. You will first need to set up the groups and permissions that you need, as well as group memberships for users. Depending on your use case it may be sufficient to simply set up the groups manually, but we expect most users will require code to automatically keep groups in sync with their local database.

> [!WARNING]
> Gotcha
> Custom user IDs must adhere to the following rules:
> 
> Must begin with a lower case e
> 
> Followed by a string of any combination of upper or lowercase characters from the English alphabet, numbers, hyphens, and underscores. E.g. e-A3f-Lm_N6-eQrS-Xw9y
> 
> 
> This can also be expressed with the following regular expression: /^e[a-zA-Z0-9-_]+$/

> [!TIP]
> Protip
> There is no need to create Sanity users corresponding to your local users, it is sufficient to simply list the user IDs as group members.

> [!WARNING]
> Gotcha
> You cannot assign a third-party login user to a role created with the roles API. We recommend using SAML integration instead where possible. You can create roles for third-party login users with groups, described below.

Groups are stored as regular Sanity documents of type `system.group` under the `_.groups.` path, as outlined in the [access control](/docs/archive/access-control-deprecated) section. Use regular [mutations](/docs/http-reference/mutation) via the API to create and modify them. 

For example, let's say we would like to give the journalists `e-henrik` and `e-emma` in our Norway office full access to all articles in the `norway` edition, but only read access to other editions - we could create the following group for this:

```json
{
  _id: '_.groups.office-norway',
  _type: 'system.group',
  grants: [
    {
      filter: "_type == 'article' && edition._ref == 'norway'",
      permissions: ["create", "update", "read"]
    },
    {
      filter: "_type == 'article'",
      permissions: ["read"]
    }
  ],
  members: ["e-henrik", "e-emma"]
}
```

> [!TIP]
> Protip
> Sanity user IDs may be exposed in publicly available data (e.g. as the author of a document), so take care not to use any personally identifiable information when generating IDs. An arbitrary number or a hash is usually a good choice.

Once all of our groups and memberships are properly set up, we'll need to give users a Sanity token when they log in.

## Generating Sanity tokens

You will need some sort of login solution on your end, which authenticates users with your user database and then makes an API call to Sanity to generate a session claim. The details depend entirely on the specific authentication system you use, but we have a GitHub repo with a [complete example](https://github.com/sanity-io/3rd-party-auth-example).

Once you have authenticated the user you should make an HTTP `POST` request using a [robot token](/docs/content-lake/http-auth) that has the [create-session](/docs/archive/access-control-deprecated) permission to the following endpoint:

`POST https://<projectId>.api.sanity.io/v2021-06-07/auth/thirdParty/session`

The `POST` body should be JSON- or URL-encoded and contain the following fields:

- `userId`: the user's ID as listed in the group (see above).
- `userFullName`: the user's full name.
- `userEmail`: the user's email address.
- `userImage`: optional HTTPS URL to the user's profile image.
- `userRole`: If the user should be able to log into the Sanity Studio, role must be either `administrator` or `editor` 
- `sessionExpires`: ISO timestamp for when the session should expire.
- `sessionLabel`: optional label for the session.

The API call will return JSON with two fields; `token` and `endUserClaimUrl`. If the session is to be used for managing content in the Sanity Studio, the `endUserClaimUrl` contains a URL which the end user's browser can visit to obtain a Sanity session (set as a cookie). This URL is valid for a single use only. You can add a query parameter `origin` with a URL to redirect the user to after the session has been created - this URL must be listed as a valid [CORS origin](/docs/content-lake/cors) for the project. For other use cases, such as creating native applications, you'll want to store the `token` returned in a secure location and use it to authenticate requests against the Sanity API. 

#### User profiles

Every time you create a SSO session, the user info you post with it will be saved to a user profile model. It's attatched to the user id and project id. This information is only available for logged in users to your project. The model is needed to display user info even though the session is destroyed (user logging out or session expires). You are responsible for deleting these profiles when they should be deleted (according to your terms).

```text
DELETE https://api.sanity.io/v2021-06-07/projects/<project-id>/users/<e-user-id>/profile

Authorization: Bearer <your-create-session-token>

```

If you want to explicitly create or update a user profile, you can do so by sending a PUT request with the details:

```text
PUT https://api.sanity.io/v2021-06-07/projects/<project-id>/users/<e-user-id>/profile

Content-Type: application/json
Authorization: Bearer <your-create-session-token>

{
  "name": "Some username",
  "profileImage": "https://optional.user.img/url.jpg"
}

```

## Using external logins in the Sanity Studio

The Studio can be configured to use your own login solution rather than the standard ones by modifying the config file `config/@sanity/default-login.json` in the studio code.

```json
{
  "providers": {
    "mode": "replace",
    "redirectOnSingle": true,
    "entries": [
      {
        "name": "custom-login",
        "title": "Custom Login",
        "url": "https://mydomain.com/login",
        "logo": "static/custom-login.png"
      }
    ]
  }
}
```

The Studio redirects the user to the specified `url` to initiate the authentication. The `title` and `logo` are displayed to the user on the Studio login screen. You may add multiple entries if you need to support several authentication solutions.



# Add live content to your application

The Live Content API enables you to deliver live content experiences without the complexity and infrastructure requirements traditionally found in real-time apps.

The next-sanity library offers the most seamless integration with the API. The JavaScript client offers helper utilities to get you started, but you'll need to build additional functionality. 

## Next.js

Enable live content with only a few lines of code with next-sanity. 

### Prerequisites:

- A new or existing Sanity project.
- Add your front end or deployment target's  to the projects [CORS origins](/docs/content-lake/cors). This is found in the project's API section at [sanity.io/manage](https://sanity.io/manage). 
- A Next.js application built with the [app router architecture](https://nextjs.org/docs/app/building-your-application/routing#the-app-router). The Live Content features in `next-sanity` do not support app built with pages router.

### Install and configure the client

You can install, setup, and configure Sanity in your existing Next.js project with `init`. 

```sh
npx sanity@latest init
```

The `init` command will guide you through linking your project. 

Alternatively, install the package or update it to the latest version if you have an earlier version.

```sh
npm install next-sanity@latest
```

Next, confirm that you have an existing Sanity client configured.

```typescript
// src/sanity/lib/client.ts
import { createClient } from "next-sanity";

import { dataset, projectId } from "../env";

export const client = createClient({
  projectId,
  dataset,
  apiVersion: "v2021-03-25",
  useCdn: true
});
```

### Create the live utilities

Create a live utility file and configure the `sanityFetch` helper and `SanityLive` component by passing in your local Sanity client and a token. `defineLive` requires a browser and server token in order to fetch draft content when using Draft Mode. If you aren't using visual editing or draft previews, you can omit the token.

```typescript
// src/sanity/lib/live.ts

import { defineLive } from "next-sanity";
// import your local configured client
import { client } from "@/sanity/lib/client";

// set your viewer token
const token = process.env.SANITY_API_READ_TOKEN
if (!token) {
  throw new Error("Missing SANITY_API_READ_TOKEN")
}

// export the sanityFetch helper and the SanityLive component
export const { sanityFetch, SanityLive } = defineLive({
  client,
  serverToken: token,
  browserToken: token,
})
```

> [!NOTE]
> Tokens
> Tokens passed to defineLive need viewer access rights in order to fetch draft content.
> 
> The token for serverToken and browserToken can be the same. The browserToken is only used when Draft Mode is enabled and initiated by Presentation Tool or Vercel Toolbar. 

### Fetch your queries

Whenever you need to query data in your Sanity dataset, import the `sanityFetch` helper and call it as you would any Sanity client by passing in a GROQ query and any query parameters.

```typescript
import { sanityFetch } from "@/sanity/lib/live"
import { POST_QUERY } from "./queries.ts"

const {data: post} = await sanityFetch({query: POST_QUERY, params: {}})
```

In this example, the `data` response is destructured to `post` and `sanityFetch` receives a GROQ query and an optional `params` object.

### Enable the SanityLive component

The final step to enable the Live Content API is adding the `SanityLive` React component. It listens for changes in your data and works with your `sanityFetch` queries to efficiently update content . Include it in application so it renders on any page that needs live content. 

In this example, it lives just before the closing body tag in the `RootLayout` component.

```tsx
// app/layout.tsx

import { SanityLive } from "@/sanity/lib/live"

export default function RootLayout({
  children,
}: {
  children: React.ReactNode
}) {
  return (
    <html lang="en">
      <body>
        {children}
        <SanityLive />
      </body>
    </html>
  )
}
```

### Next steps

- For more on how to incorporate the Live Content API in your Next.js application, as well as how to set up Visual Editing and Draft Previews, check out the [next-sanity](https://github.com/sanity-io/next-sanity/blob/main/packages/next-sanity/README.md#live-content-api) readme.
- Level up with [Work-ready Next.js](https://www.sanity.io/learn/track/work-ready-next-js) on Sanity Learn.
- Dive into the [Sanity + Next.js example application](https://github.com/sanity-io/next.js/tree/canary/examples/cms-sanity).

Looking for even more ways to incorporate Sanity and Next.js? Explore the ecosystem of guides, plugins, and starter templates available on the [Exchange](https://sanity.io/exchange). 

## Create your own integration

If there isn't an official library for your framework that enables live content, you need to create your own integration to use the Live Content API. We've created a collection of examples in the Live Content API Examples repository on GitHub. It's a good starting point for working with custom implementations. 

[Live Content API Examples](https://github.com/sanity-io/lcapi-examples)



Below, you'll find a minimal example using the Sanity JS client.

### Prerequisites:

- A new or existing Sanity project.
- Add your front end or deployment target's  to the projects [CORS origins](/docs/content-lake/cors). This is found in the project's API section at [sanity.io/manage](https://sanity.io/manage). 

### Install and configure the client

First, install the latest version of the client.

```
npm install @sanity/client@latest
```

Configure your `@sanity/client` with your project settings and the latest API version:

```typescript
const client = createClient({ 
  projectId: "your-project-id", 
  dataset: "your-dataset", 
  apiVersion: "v2021-03-25",
  useCdn: true
})
```

### How it works

Here's a high-level overview of how the Live Content API works:

43. Every response from the content lake now includes *sync tags. *If you want to keep that content up to date in real time, you need to store the tags.
43. Then you can subscribe to a stream of live updates by calling the `client.live.events()` method. This will return an Observable that emits events whenever the content in the dataset changes.
43. Whenever you receive an event, you can check if any of the event tags match the sync tags from content you want to keep up to date.
43. If there is a match, you can refetch the content using the event ID as the `lastLiveEventId` argument in your `client.fetch` call. This ensures that you always get the latest version of the content from the CDN, avoiding any stale data.

### Minimal example

Here is a minimal example running in the console. It keeps a single, pre-defined, document in sync using sync tags:

```typescript
import { createClient } from "@sanity/client"

// Create the client instance
const client = createClient({ 
  projectId: "your-project-id", 
  dataset: "your-dataset", 
  apiVersion: "v2021-03-25", 
  useCdn: true
})

const query = "*[slug.current == $slug][0]"
const slug = "were-doing-it-live"

let syncTags = []

function render(lastLiveEventId?: string) {
  // Query the content lake
  client.fetch(
    query, 
    { slug }, 
    { filterResponse: false, lastLiveEventId }
  ).then(
    (res) => { 
      // 3. Store the syncTags and "render" the data 
      syncTags = res.syncTags 
      const data = res.result 
      console.log(data)
    })
}

// Kick off initial render
render()


// Subscribe to live updates
const subscription = client.live.events().subscribe(
  (event) => { 
    // Check if incoming tags match saved sync tags 
    if (event.type === "message" && event.tags.some((tag) => syncTags.includes(tag))) { 
      // Refetch with ID to get latest data
      render(event.id)
    }
    if (event.type === "restart") {
      // A restart event is sent when the `lastLiveEventId` we've been given earlier is no longer usable
      render()
    }
})

// Later, unsubscribe when no longer needed (such as on unmount)
// subscription.unsubscribe()
```

In this example:

48. We create a Sanity client instance with the necessary configuration.
48. We define a query to fetch posts and execute it, setting `filterResponse: false` to get the syncTags along with the result.
48. We store the returned syncTags and render the initial data.
48. We subscribe to live updates using `client.live.events()`.
48. Whenever an update event is received, we check if any of its tags match our stored syncTags.
48. If there's a match, we refetch the data, passing the event ID as `lastLiveEventId` to get the latest version.
48. We update the stored syncTags and re-render with the fresh data.
48. Finally, we unsubscribe from the live updates when no longer needed.

This pattern allows your application to efficiently keep its content in sync with the latest changes in your Sanity dataset. For additional examples, including listening for drafts, see the [JS client documentation](https://github.com/sanity-io/client?tab=readme-ov-file#listening-to-live-content-updates).



Learn more about the underpinnings of the Live Content API and sync tags by exploring the [Live reference docs](/docs/http-reference/live).



# Forms with Sanity

## The two types of form integration

Integrating a form service is no different than [integrating any external system with Sanity](https://www.sanity.io/guides/integrating-external-data). That being said, integrating an external form service with Sanity most commonly falls into two buckets, you either want to:

3. Author forms in an external service, then reference those forms by ID in your content (*”I want my-marketing-form from MailChimp to go here on my page”*), or…
3. Author forms inside Sanity, with the external service being used as a “bucket” for all the form submissions on your site (services like Netlify Forms or Formspree)

There are other form use cases Sanity can cover, you could even use Sanity to *collect* form submissions and manage user generated content (like we do on this site and [https://www.sanity.io/learn](https://www.sanity.io/learn)), but this guide will cover the above two most common use cases.

## [@sanity/form-toolkit](https://www.npmjs.com/package/@sanity/form-toolkit)

The plugin @sanity/form-toolkit offers pre-built tooling for both types of form integrations if you’re looking to have something “out of the box”. This guide will dive into a more general look at how form-toolkit goes about creating these integrations.

form-toolkit currently has integrations for these services:

- MailChimp
- HubSpot

form-toolkit also exposes the `formSchema` plugin and `FormRendering` React component, which provides a pre-built form schema for your Studio and a component to render those forms respectively.

## Syncing external forms with Sanity

Syncing external forms with Sanity typically assumes that the service you’re syncing has some way to embed forms on your front-end that expects an ID to know which form to render. In such cases [@sanity/sanity-plugin-async-list](https://www.npmjs.com/package/@sanity/sanity-plugin-async-list) allows you to add a string field to your Sanity documents that fetches data from a remote source. Our [guide on syncing external data sources](https://www.sanity.io/guides/integrating-external-data) includes a section outlining this approach in detail.

## Authoring forms in Sanity

> [!TIP]
> Protip
> @sanity/form-toolkit includes a package, formSchema for building and rendering forms from your Sanity Studio

For basic form authoring, it may be preferred to author the form structure and fields in Sanity, pass that data to a component to render the form, and then use a ‘catch-all’ service for form submissions like Formspree or Netlify forms.

If you’d prefer to build your own implementation instead of using form-toolkit, the process is the following:

16. Model a typical form in Sanity schemas, including 2. A `form` type containing an array of `formField` objects
2. A `formField` type with various props for the HTML `input` element like `type` , `placeholder` , `name`, `required` , `label` and others based on your needs
2. Additional properties on `form` or `formField` based on your needs, perhaps your forms need multiple sections or you want to control the form action from the CMS, all can be built into your schema


16. Create a component that renders your form in your front-end framework of choice 2. Take the `form` from a GROQ query, render a `<form>` element, passing the relevant props from your data
2. Take the `fields` array, and return an appropriate input for each provided field and its `type`
2. Optionally, use a form package like [TanStack Form](https://tanstack.com/form/latest/docs/overview) or [react-hook-form](https://react-hook-form.com/) for better error and state management


16. Create logic for how your form handles and sends submissions.2. With some platforms like Netlify forms this means adding data attributes to the `<form>` element
2. In other cases like Formspree its adding their URL as the `action` attribute on your form





# Displaying Sanity content in Shopify

By default, the Sanity Connect application will sync Products, Product Variants, and Collections from Shopify into a Sanity dataset. This guide outlines how to sync additional data from Sanity into Shopify. This allows you to power your storefront with a range of content, provided through Shopify's metaobject and metafield APIs.

## Visual Walkthrough

This video walks through how you can leverage synced metafields and metaobjects within Shopify's native collection and theme tooling.

![Walkthrough of displaying Sanity content within Shopify](https://youtu.be/Obu3ea6J-8k)

## Configuring Synced Objects and Fields

To sync Sanity content:

7. On the Sanity Connect dashboard, enable the option 'Sync content from Sanity to Shopify'.
7. Save the configuration, and you will prompted to grant new access permissions to Sanity Connect. Allow the application to edit metaobjects within your store.
7. After confirming permissions, you will return to the Sanity Connect dashboard. Visit the ‘Metaobject’ tab listed at the top of your page. You will see a list of all document types available within your linked Sanity dataset. Select the document types that you want available within Shopify. These will sync alongside the native Product, Product Variant, and Collection objects.
7. Return to the Sanity Connect dashboard and trigger a full sync.

Only published documents will be synced to Shopify. Drafts are not processed.

## Updating the list of synced fields

After the initial configuration, you can review your synced resources from the 'Metafields' and 'Metaobjects' tabs within the Sanity Connect application.

### Metafields

The Metafields tab displays data for the standard Product, Product Variant, and Collection objects. Within Sanity, these entities sync over with the [native fields from the Shopify API](https://shopify.dev/docs/api/storefront/latest/objects/Product). However, you can extend these documents with custom fields. Your custom fields are available within Shopify as metafields.

On the Metafields tab you can review which custom fields are configured to sync, review the inferred data type for each field, select whether to [pin the field](https://help.shopify.com/en/manual/custom-data/metafields/pinning-metafield-definitions) within your Shopify storefront configuration, or remove the synced metafield definition.

Removing the metafield definition is a temporary action. This feature will be used if you're troubleshooting syncing issues or updating the inferred data type of the field. A removed metafield definition will be reset during the next sync event.

### Metaobjects

The Metaobjects tab displays data for all custom document types within Sanity. Each object is displayed along with its fields, similar to the Metafield tab.

Unique to metaobjects, there is an option to control whether the document type will sync to Shopify.

If you deselect a previously-synced metaobject, we will remove the synced metaobjects and its definition from your Shopify store once you save the configuration.

## Inferring Field Types

When data is synced from Sanity to Shopify, we create each metafield with a static data type. Data is mapped to Shopify using the below table:

| Sanity Schema Types | Shopify Metafield Types |
| --- | --- |
| Date | Date |
| Datetime | Date time |
| Number | Number (either decimal or integer) |
| String | String |
| Url | Url |
| Slug | String |
| Reference | Reference (Product, Variant, Collection) |
| Array | List |
| Block | JSON |
| Span | JSON |
| Text | Multiline text |
| Image  | Shopify File Reference (plugin-enabled) |
| File | Shopify File Reference (plugin-enabled) |
| Geopoint | JSON |
| Object | JSON |


Portable Text is serialized as JSON and can be integrated into Liquid storefronts using [portable-text-to-liquid](https://github.com/portabletext/portable-text-to-liquid).

Images are available in Shopify as a file reference when they are added to Sanity using the [Shopify Assets plugin](https://github.com/sanity-io/sanity-plugin-shopify-assets). Examples rendering these assets are available in the [portable-text-to-liquid](https://github.com/portabletext/portable-text-to-liquid) repository. Images and files added without the Shopify Assets plugin will sync to Shopify as JSON, referencing the asset hosted on Sanity’s CDN.

These data types are inferred from the values available during the first sync. If Sanity Connect encounters a value that doesn't match the expected type, then that field will be skipped. Unaffected fields will continue to sync.

## Accessing Your Data Within Shopify

Metaobjects are available via the [metaobject API](https://shopify.dev/docs/api/storefront/latest/objects/metaobject) and within your Shopify Admin at **Settings > Custom data**.

Metafields on the native Shopify objects are visible on each resource in a dedicated **Metafields** section.

## Collections

You can create a Dynamic Collection referencing metafields on your products. Any of your synced metafields will be available when specifying the conditions to match products.

You will not be able to delete a metafield definition (for example, to reset the type inference) if that field is being used by a Dynamic Collection.

## Pages

You can [create custom pages](https://help.shopify.com/en/manual/custom-data/metaobjects/webpages) in Shopify based on your metaobjects. You will select the metaobject definition to use and then create a template for displaying your content.

When you host pages on Shopify, the page URL is derived from your metaobject's handle. To customize this handle, use the 'Use slug as handle' setting, available on the Metaobjects tab in Sanity Connect.

## Visual Editor

In the [visual theme editor](https://shopify.com/admin/themes/current/editor), you can select metafields to serve as a dynamic source for an element. Compatible elements will have a ‘Connect Dynamic Source’ option available. This option will list available metafields whose type definitions match the inputs required for the component.

![](https://cdn.sanity.io/images/3do82whm/next/fc41048e7df839993c3e4b403906195a339b69f0-739x321.png)

## Liquid

### Native object metafields

For Products, Product Variants, and Collections, you'll access metafields using the `app--6007307--sanity-fields` namespace.

```javascript
<div>Spiciness Level: {{ product.metafields['app--6007307--sanity-fields'].spicinessLevel }}</div>
<div>Season: {{ product.metafields['app--6007307--sanity-fields'].season }}</div>
<div>
  <h2>This product pairs well with</h2>
  <ul>
    {%- for p in product.metafields['app--6007307--sanity-fields'].pairsWellWith.value -%}
      <li>{{ p.title }}</li>
    {%- endfor -%}
  </ul>
</div>
```

### Metaobjects

Metaobjects are accessed by their Type ID, which is a concatenation of:

43. `app--6007307`: The Sanity Connect app ID
43. `sanity-documents`: The namespace where Sanity metaobjects are stored
43. `your-document-type-name`: The name of your synced metaobject

For example, this Liquid would list the `name` of all of our `recipe` documents:

```javascript
{% for o in shop.metaobjects['app--6007307--sanity-documents-recipe'].values -%}
	<li>{{ o.name }}</li>
{%- endfor -%}
```

To reference an individual document, you use the metaobject's handle. There are two possible values for the handle depending on your configuration.

1. The default handle is your document's Sanity ID. However you must transform the dashes in the ID to underscores. For example, this Liquid would display the picture associated a specific document:

```javascript
{{
  shop.metaobjects['app--6007307--sanity-documents-recipe'].ea53f398_e42b_4f2c_9495_e750a00eafaf.picture
  | image_url: width: 300
  | image_tag
}}
```

2. On the Metaobjects tab in Sanity Connect, you can enable a setting to 'Use slug as handle.' When enabled, your metaobject's handle will be the value you have set in a `slug`-type field that you have configured on the document. The first slug field found on the document is used. If there is no value for a slug field, then the handle defaults back to the Sanity document ID.

Handle names must follow a set of rules [documented in Shopify's platform](https://shopify.dev/docs/api/liquid/basics#handles). When a slug field is used as the handle, Shopify automatically transforms the field value. So a slug field of `exampleSlug` in Sanity would be `exampleslug` as a Shopify handle.

> [!NOTE]
> Gotcha
> Updating the slug in Sanity will update the API handle for the object. Beware of hardcoded references to your objects.

## Storefront API

You would use [Metafields](https://shopify.dev/docs/api/storefront/latest/objects/Product#field-product-metafield) and [Metaobjects](https://shopify.dev/docs/api/storefront/latest/objects/Metaobject) within the Storefront API.

- Native Object Metafields- Namespace: `app--6007307--sanity-fields`
- Key: your metafield name


- Metaobjects- Handle: `app--6007307--sanity-documents-foo`, with `foo` replaced with the name of your metaobject



## Troubleshooting

### Stale Data

If you have automatic syncing enabled for Sanity Connect, updates to your documents should sync to Shopify within a few seconds. Most changes should appear right away in your storefronts. Shopify provides different caches for managing the content on your storefronts. Some destinations could take up to five minutes to update.

If you have stale data, first check the 'Logs' tab within Sanity Connect. That will report any sync failures.

Then check your Shopify Admin. You can navigate to your custom metaobjects and metafields to see if the new values have synced.

### Field Types

You may encounter issues syncing if:

62. You change your schema within Sanity and transform the type of data returned by a field
62. The values of your field could be interpreted as multiple data types

For the first situation, you should be able to remove the metafield definition and trigger a new sync. Your next sync should capture the new values and infer your new data type.

For the second situation, you may need to review your data within Sanity.

### String Types

Strings could be evaluated as four different data types in Shopify. They are evaluated in this order:

67. If `YYYY-MM-DD`, then we consider it a `date`.
67. If `YYYY-MM-DD[T]HH:MM:SS`, then we consider it a `date_time`.
67. If the string contains a newline, then we consider it a `multi_line_text_field`.
67. Otherwise, we consider it a `single_line_text_field`.

### Number Types

Sanity has a single [number data type](https://www.sanity.io/docs/number-type) which can represent integers or decimal types. Shopify treats these as two different data types.

During sync, numbers are evaluated in this order:

71. If the number contains a decimal, it is `number_decimal`.
71. Otherwise, it is `number_integer`.

### Setting Your Desired Type

If the sync process fails due to a type mismatch, you’ll need to make updates within the Sanity Connect application to complete your sync.

Consider the following scenario:

> You have a `pageDescription` field that supports multi-line text. The first document that Sanity attempts to sync is a placeholder page you published with the value `"Placeholder description"`. Sanity Connect doesn’t see your schema, so this gets interpreted as a `single_line_text` field in Shopify. The next document that syncs has a longer description value that requires a `multi_line_text` field. Sanity Connect fails to sync this document.

Here you would take the following steps:

77. In Sanity Connect, navigate to the Metaobjects page. Navigate to the document type that failed.
77. Find the `pageDescription` field and select 'Edit Type'. Confirm the update.

We provide automatic type updates for string fields (between `single_line_text_field` and `multi_line_text_field`) and number fields (between `number_integer` and `number_decimal`).

### Missing Document Types

In general, we make any document types within Sanity available to sync. In some situations, a document type may not be available:

81. The document type name must only contain letters, numbers, and underscores (`_`).
81. Document type names must be unique when case-insensitive. For example, `aboutPage` and `aboutpage` will not sync as two metaobjects within Shopify.



# Sanity Connect for Shopify

The [Sanity Connect application for Shopify](https://apps.shopify.com/sanity-connect) is used to synchronize content between a Sanity dataset and your Shopify store. This gives you flexibility to use the tools that are right for your needs. You can take a headless approach using Shopify's Hydrogen framework and Next.js, or you can sync data into Shopify's platform and use Liquid or the Storefront API.

## Requirements

To take advantage of Sanity Connect you will need:

4. A Shopify store
4. A Sanity project and dataset

If you are starting with a new Sanity dataset, you can create the dataset and a pre-configured Studio instance using:

```sh
npm create sanity@latest -- --template shopify --create-project "Shopify Store" --dataset production --typescript --output-path shopify-store
```

## Installation

To install Sanity Connect in your Shopify store and connect it to a project:

- Find [Sanity Connect on the Shopify app store](https://apps.shopify.com/sanity-connect) and push the Add App button
- If you have multiple Shopify accounts, you need to choose the one that contains the store you want to add the app to
- After choosing the store, Shopify will show you the permissions Sanity Connect needs to work and its data policies. You can push the Install app button to continue.
- The app will ask you to connect to your Sanity account. If you don't have one, you can choose to **Create new account**.
- When you're logged in, you will need to connect your shop with a project on Sanity. You can choose between existing projects or create a new one (for free).
- Select organization to list out projects under it, and then the project and dataset you want to sync to.
- You are now ready to configure the app.

> [!WARNING]
> Gotcha
> Once you chose Start synchronizing now, the app will add product documents to your content lake. It can be wise to test it against a non-production dataset if you haven't tried it before.

You might also want to consider using our [Shopify asset plugin](https://github.com/sanity-io/sanity-plugin-shopify-assets), which allows you to select assets from your Shopify store in the context of your Sanity Studio, allowing you to serve assets from the Shopify CDN in your front ends.

## **Settings**

You can configure how and when Sanity Connect should synchronize products to your content lake, and whether content should be synchronized back to your Shopify store. You can change these options at any time.

![Settings panel with synchronization settings](https://cdn.sanity.io/images/3do82whm/next/8c633519b6b003dd7a95026d8e8c13df9df5b809-1274x1346.png)

![Settings panel with synchronization settings](https://cdn.sanity.io/images/3do82whm/next/f8a87a923c694905c6c8293814534ea436df29fc-1282x1438.png)

### Sync content from Sanity to Shopify

This setting allows you to sync any custom fields and document types you've created in Sanity back into Shopify. Your custom content will sync as Shopify metafields and metaobjects.

For a deeper dive, review our documentation on [displaying Sanity content within Shopify](/docs/developer-guides/displaying-sanity-content-in-shopify).

### How to synchronize

Sanity Connect offers two ways to synchronize content from Shopify into your content lake - direct sync and custom sync.

**Direct Sync**

This will synchronize all products, product variants and collections as documents to your content lake. You can check the [reference](/docs/apis-and-sdks/sanity-connect-for-shopify-reference) to preview the data model for these documents.

> [!TIP]
> Protip
> Synced documents created by Sanity Connect will count towards your Sanity document usage limit. One document will be created for every product, product variant and collection in your storefront.

**Custom Sync**

This option will let you enter an endpoint that receives updates from Shopify and syncs data to your content lake. Typically that will be a serverless function handler where you can reshape the data and do other business logic as part of the sync.

You may, for example, want to reduce document usage by syncing products but not variants, or sync variants as objects on a product document rather than individual variant documents.

We have further documentation on [custom sync handlers](/docs/developer-guides/custom-sync-handlers-for-sanity-connect) including an example serverless function.

### When to synchronize

**Sync data automatically:** Automatically sync whenever you save products. Note: The sync will update the Shopify information for both published and draft documents. An update is typically available in your content lake after a couple of seconds.

**Sync manually:** There will no automatic sync, and you'll have to go into the Sanity Connect settings to trigger a synchronization manually.

> [!WARNING]
> Gotcha
> Sanity Connect will do an initial synchronization once you choose one of these options.

### Sync collections

The Sanity Connect app can optionally sync collections data. This will sync data and properties about your collection, but it will not sync the product membership of your collections.

## **Set up your Studio**

You can install a production-ready reference studio that's set up with a great editor experience by running this command in your local shell. Replace the `PROJECT_ID` and `DATASET_NAME` placeholders with the actual values from the project your Shopify store is connected to:

```sh
npx @sanity/cli init --template shopify --project PROJECT_ID --dataset DATASET_NAME
```

You'll find comprehensive documentation for this studio in its `README.md`.

![Screenshot of Shopify reference studio](https://cdn.sanity.io/images/3do82whm/next/58ebc2e9801b90061c4184d22ff0d267f534a25e-720x427.png)

### Integrate with an existing studio

If you've already set up a studio instance, you can follow the patterns exposed in [sanity-shopify-studio](https://github.com/sanity-io/sanity-shopify-studio). This repository showcases the same studio customizations that are implemented when creating a new studio with the `shopify` template.

## Further reading

[Sanity Studio for Shopify](https://github.com/sanity-io/sanity-shopify-studio)

[Shopify asset selection for Sanity Studio](https://github.com/sanity-io/sanity-plugin-shopify-assets)





# Custom sync handlers for Sanity Connect

A custom sync handler allows you to provide an endpoint which receives updates from Shopify and passes data into your content lake. Typically, this will be a serverless function where you can reshape the data from Shopify and apply business logic before it is passed to your content lake.

## When to use a custom sync handler

There are a number of scenarios where you may choose to implement a custom sync handler - common examples include:

- Where you need to apply additional logic to the data - for example, querying additional APIs to retrieve more data (e.g. the Shopify API to get additional metafields)
- You may want to reduce your document usage on Sanity by only syncing selected products, or syncing variants as an object on product documents rather than variant documents.
- Where you want to amend the default manner in which Sanity Connect handles a product being deleted on Shopify - by setting `isDeleted` to `true` - to fully delete the document from your content lake.

## How custom sync handlers work

When enabled, the custom sync handler will send a payload on every update from Shopify as a POST request. You can write your custom business logic in your endpoint and [update](https://www.sanity.io/docs/transactions) your content lake accordingly in the function, or respond with a set of documents which Sanity Connect will update for you.

Sanity Connect expects a response header with `content-type: application/json` and will regard a `200` status code as a success. Any other status code will be considered a failure.

You can find the [shape of the payload your handler](/docs/apis-and-sdks/sanity-connect-for-shopify-reference) will receive in our Sanity Connect reference.

> [!WARNING]
> Gotcha
> The request has a 10s timeout and your handler needs to reply before that. Any failed requests will be retried up to 10 times.
> 
> If your handler needs more time to complete updates (for example if it calls a third party API), a common pattern would be to store the payload in a queue for background processing, and respond 200 OK immediately to acknowledge receipt of the payload.

> [!WARNING]
> Gotcha
> This operation will in batched when manually syncing, especially when dealing with larger catalogs.

> [!WARNING]
> Gotcha
> Changes in product inventory (through sales) will also trigger updates to your custom handler.
> 
> Make sure to tailor your custom handler to account for how our API CDN invalidates cache on writes to non-draft documents, especially if operating on a high traffic stores with fast moving content.
> 
> 

## Example custom sync handler function

Below is an example of a barebones custom function that will:

- Create/update/delete products (including drafts) in the Content Lake on Shopify product operations
- Only deal with products (variants are included as objects within products)
- Manual sync will create and update products on your dataset, but will not delete products that have since been removed.

For a more complete example, refer to [this gist](https://gist.github.com/snorrees/1ca7c3191d62ede6b9b5d0a1822d7103#file-requirements-md).

```javascript
import {createClient} from "@sanity/client";

// Document type for all incoming synced Shopify products
const SHOPIFY_PRODUCT_DOCUMENT_TYPE = "shopify.product";

// Prefix added to all Sanity product document ids
const SHOPIFY_PRODUCT_DOCUMENT_ID_PREFIX = "product-";

// Enter your Sanity studio details here.
// You will also need to provide an API token with write access in order for this
// handler to be able to create documents on your behalf.
// Read more on auth, tokens and securing them: https://www.sanity.io/docs/http-auth
const sanityClient = createClient({
  apiVersion: "2021-10-21",
  dataset: process.env.SANITY_DATASET,
  projectId: process.env.SANITY_PROJECT_ID,
  token: process.env.SANITY_ADMIN_AUTH_TOKEN,
  useCdn: false,
});

/**
 * Sanity Connect sends POST requests and expects both:
 * - a 200 status code
 * - a response header with `content-type: application/json`
 * 
 * Remember that this may be run in batches when manually syncing.
 */
export default async function handler(req, res) {
  // Next.js will automatically parse `req.body` with requests of `content-type: application/json`,
  // so manually parsing with `JSON.parse` is unnecessary.
  const { body, method } = req;

  // Ignore non-POST requests
  if (method !== "POST") {
    return res.status(405).json({ error: "Method not allowed" });
  }

  try {
    const transaction = sanityClient.transaction();
    switch (body.action) {
      case "create":
      case "update":
      case "sync":
        await createOrUpdateProducts(transaction, body.products);
        break;
      case "delete":
        const documentIds = body.productIds.map((id) =>
          getDocumentProductId(id)
        );
        await deleteProducts(transaction, documentIds);
        break;
    }
    await transaction.commit();
  } catch (err) {
    console.error("Transaction failed: ", err.message);
  }

  res.status(200).json({ message: "OK" });
}

/**
 * Creates (or updates if already existing) Sanity documents of type `shopify.product`.
 * Patches existing drafts too, if present.
 *
 * All products will be created with a deterministic _id in the format `product-${SHOPIFY_ID}`
 */
async function createOrUpdateProducts(transaction, products) {
  // Extract draft document IDs from current update
  const draftDocumentIds = products.map((product) => {
    const productId = extractIdFromGid(product.id);
    return `drafts.${getDocumentProductId(productId)}`;
  });

  // Determine if drafts exist for any updated products
  const existingDrafts = await sanityClient.fetch(`*[_id in $ids]._id`, {
    ids: draftDocumentIds,
  });

  products.forEach((product) => {
    // Build Sanity product document
    const document = buildProductDocument(product);
    const draftId = `drafts.${document._id}`;

    // Create (or update) existing published document
    transaction
      .createIfNotExists(document)
      .patch(document._id, (patch) => patch.set(document));

    // Check if this product has a corresponding draft and if so, update that too.
    if (existingDrafts.includes(draftId)) {
      transaction.patch(draftId, (patch) =>
        patch.set({
          ...document,
          _id: draftId,
        })
      );
    }
  });
}

/**
 * Delete corresponding Sanity documents of type `shopify.product`.
 * Published and draft documents will be deleted.
 */
async function deleteProducts(transaction, documentIds) {
  documentIds.forEach((id) => {
    transaction.delete(id).delete(`drafts.${id}`);
  });
}

/**
 * Build Sanity document from product payload
 */
function buildProductDocument(product) {
  const {
    featuredImage,
    id,
    options,
    productType,
    priceRange,
    status,
    title,
    variants,
  } = product;
  const productId = extractIdFromGid(id);
  return {
    _id: getDocumentProductId(productId),
    _type: SHOPIFY_PRODUCT_DOCUMENT_TYPE,
    image: featuredImage?.src,
    options: options?.map((option, index) => ({
      _key: String(index),
      name: option.name,
      position: option.position,
      values: option.values,
    })),
    priceRange,
    productType,
    status,
    title,
    variants: variants?.map((variant, index) => {
      const variantId = extractIdFromGid(variant.id);
      return {
        _key: String(index),
        compareAtPrice: Number(variant.compareAtPrice || 0),
        id: variantId,
        inStock: !!variant.inventoryManagement
          ? variant.inventoryPolicy === "continue" ||
            variant.inventoryQuantity > 0
          : true,
        inventoryManagement: variant.inventoryManagement,
        inventoryPolicy: variant.inventoryPolicy,
        option1: variant?.selectedOptions?.[0]?.value,
        option2: variant?.selectedOptions?.[1]?.value,
        option3: variant?.selectedOptions?.[2]?.value,
        price: Number(variant.price || 0),
        sku: variant.sku,
        title: variant.title,
      };
    }),
  };
}

/**
 * Extract ID from Shopify GID string (all values after the last slash)
 * e.g. gid://shopify/Product/12345 => 12345
 */
function extractIdFromGid(gid) {
  return gid?.match(/[^\/]+$/i)[0];
}

/**
 * Map Shopify product ID number to a corresponding Sanity document ID string
 * e.g. 12345 => product-12345
 */
function getDocumentProductId(productId) {
  return `${SHOPIFY_PRODUCT_DOCUMENT_ID_PREFIX}${productId}`;
}
```



# How to pitch Sanity.io to your team

Sanity.io is the platform for [structured content](/structured-content-platform). It comes with an [open-source headless CMS called Sanity Studio](/studio) that’s built with React, and that you can **customize**. You also get a [hosted real-time datastore](/developer-experience) with powerful APIs. There are also [libraries and tools](/docs/plugins-overview) that **make it easier** to use structured content in the products and services that you’re building. And not the least, there’s a growing [friendly community of developers](https://slack.sanity.io) that will gladly help and learn with you.

## Sanity.io gives your team:

- **Ultra-portable structured content. **Your content is stored as plain JSON documents. That’s it. You can export all your documents from the backend with [one API request](/docs/http-reference/export) or [CLI command](/docs/archive/getting-started-with-sanity-cli). And if you need to move them out of Sanity, it’s much easier to import these documents into another system, compared with some specific XML-export from a CMS littered with plugin-specific junk (looking at you WordPress). After all, portability is the hallmark of structured content.
- **A customizable editor environment**. With Sanity.io, you get a CMS that’s open-source and customizable with JavaScript and React. You only need a `name` and a `type` to make a new field, and when you’re ready for it, you can extend with custom [JavaScript validations](/docs/studio/validation), [custom input components](/docs/studio/intro-to-custom-studio-components), and [previews](/docs/studio/studio-components) with React, [CSS-variable overrides](/guides/how-to-brand-your-studio), and you can install [plugins and tools](/docs/studio/installing-and-configuring-plugins) or make your own. You have access to all the APIs that the Studio uses.
- **Something that’s easy to set up**. You are probably way faster on a keyboard compared to dragging and dropping fields with your mouse. [Creating a field](/docs/archive/content-modelling) in Sanity Studio is as easy as writing `{ name: ‘title’, type: ‘string’ } `and hitting “save”. With content models in code, you can create your own snippets, you can bootstrap config, commit them to git, or even publish on npm.
- **The joy of rapid iteration with GROQ**. Sanity.io offer [GROQ (Graph-Relation Object Queries)](/docs/groq-reference) as a way to filter your dataset’s documents, join them, and project the data structures that you need for your project. Like GraphQL it gives you one endpoint for all your content, but it’s way more versatile in the way you can shape and wrangle your data. After a couple of minutes, you can learn enough GROQ to be productive. With GROQ there is no need to loop over your data on the client-side after querying, you can shape it how you want it right in the query. This saves both bandwidth and processing time. [GROQ is open source](/blog/we-re-open-sourcing-groq-a-query-language-for-json-documents) and can be used elsewhere as well.
- **Great APIs. **In addition to GROQ, you can query your content with [GraphQL](/docs/content-lake/graphql). If you want to change a deeply nested value or change running text, you can do so with the powerful [mutations API](/docs/http-reference/mutation). The [listener API](/docs/content-lake/realtime-updates) lets your apps subscribe to changes happening in your content in real-time. With the [Asset pipeline](/docs/asset-pipeline), you can get on-demand image transforms. With the [History API](/docs/http-reference/history), you can browse document revisions and see who did what. [Webhooks](/docs/compute-and-ai/webhooks) lets you integrate with other services.
- **The calm of no-ops**. We offer you a scalable backend, both in terms of the amount of data, but also traffic, security, and availability. [CDNs for assets and content delivery](/docs/content-lake/api-cdn). Sanity Studio, the CMS, is a Single Page Application. We can host the HTML and the JavaScript file for you, or you can put it pretty much on any host. You can even deploy different studios connected to the same datastore if you want to build specialized editor experiences.
- **Flexible, transparent pricing**. You won't be forced to change tiers because of traffic or usage. [All tiers are pay-as-you-go with modestly priced overages](https://www.sanity.io/pricing). You can also add more datasets and users on all plans. The tiers differ on SLAs, support, and advanced features. There’s no hidden schemes or gotchas, it’s all on the website for you to scrutinize. We let you upgrade and downgrade whenever you want, and will prorate you for what you haven’t used if you downgrade before the month has ended. You don’t *have to* talk to sales ever (but we sure love to if you want).
- **Privacy and GDPR. **Sanity.io host your data in the heart of GDPR land: Brussels. Sanity.io is designed with GDPR in mind so that it is easy for you to stay compliant. None of your content is shared with third-party services (not even your images). We also offer custom edit history retention if your business requires that. If this isn’t enough, we also [blogged about how to run a GDPR compliant SaaS](https://www.sanity.io/blog/a-rough-guide-to-running-a-gdpr-compliant-saas-business)
- **A content platform that has been in production since 2015. **Although Sanity.io is a relatively new product on the market, it has been used in production by companies such as the renowned architecture firm [OMA](https://www.sanity.io/case-studies/oma), and one of Norway’s largest media companies, Amedia. Publicly launched in 2017. Sanity.io is now used by thousands of developers and companies including [Cornerstone OnDemand](https://www.sanity.io/case-studies/cornerstone-ondemand), [Eurostar](https://www.eurostar.com), [Condé Nast](https://www.thelovemagazine.co.uk), and [micro:bit](https://microbit.org/).
- **A tool for modern content strategy and design processes**. If you look at the conversations happening within content strategy, you’ll quickly find *structured content* as a frequent topic. No wonder, since it’s a pattern that prevents duplicated content and tries to connect your text and media to the goals of your team and users. Sanity.io also makes *content-first* approaches to design easier with rapid content modeling and having the content available instantly. This is perfect when you’re building component-based design systems. Which you should be doing!



# Not-profit plan

## The plan

The non-profit plan mirrors the [Growth plan](/pricing), but we offer it for free (no credit card required) as long as you stay within the quotas. Additionally, we've added the following features to the plan:

- 25 users included free of charge, with $15 per additional user without limit
- 3 datasets (+1 from Growth plan)
- [SAML SSO add-on](/docs/growth-plan-add-ons#ec44fec5f1cd) included

Note that additional [add-ons](/docs/platform-management/growth-plan-add-ons) are not available, and you need to add a credit card to pay for additional overages and users.

## Who's eligible?

We offer the non-profit plan to:

- Small and mid-sized organizations that are “organized and operated for a collective, public or social benefit” and where the revenue exceeding expenses goes back into the cause
- Educational and academic institutions of smaller sizes and budgets
- Open-source projects that are based on sponsorships or voluntary effort (so not monetized)

## Who's *not *eligible?

- Organizations that qualify for our [Enterprise plan](/pricing), including large non-profit organizations like global humanitarian operations, universities, etc.
- Organizations that can’t comply with our [Terms of Service](https://www.sanity.io/legal/tos).

## How to apply?

[Fill out the application form](https://forms.gle/xkQstGLFrujT2me39) and you'll hear back from us within 14 business days. Please note:

- If you don't provide a valid Sanity project ID, your application will be ignored.
- You'll receive an email when a decision has been made, but we're not able to provide technical support over email after this. Please join our community on Discord to get help.



# Presenting Portable Text

When you query your Sanity project’s API your rich text content is returned as Portable Text. If you are accustomed to traditional or other [headless CMSes ](https://www.sanity.io/headless-cms)you are probably used to dealing with HTML or Markdown out of the box. Portable Text is designed to be used in pretty much any format or markup where you want to render rich text content. 

You render Portable Text by serializing the arrays that contain your content into the format you need it. There is tooling for generic markup and programming languages and for popular frameworks, that makes it easier to serialize Portable Text and lets you decide how custom content types should be handled.

## Serialization tooling

We have helpers for different languages and platforms.

> [!TIP]
> Protip
> You may notice some mentions of block text, including in the tool names. This was the nomenclature we used before open sourcing and publishing the specification for Portable Text. You can explore the specification on www.portabletext.org.

### Portable Text to HTML

[https://github.com/portabletext/to-html](https://github.com/portabletext/to-html)

### Portable Text to React

[https://github.com/portabletext/react-portabletext](https://github.com/portabletext/react-portabletext)

### Portable Text to Vue

[https://github.com/portabletext/vue-portabletext](https://github.com/portabletext/vue-portabletext)

### Portable Text to Svelte

[https://github.com/portabletext/svelte-portabletext/](https://github.com/portabletext/svelte-portabletext/)

### Portable Text to Astro

[https://github.com/theisel/astro-portabletext](https://github.com/theisel/astro-portabletext)

### Portable Text to Hyperscript

[https://github.com/sanity-io/block-content-to-hyperscript](https://github.com/sanity-io/block-content-to-hyperscript)

### Portable Text to Markdown

[https://github.com/sanity-io/block-content-to-markdown](https://github.com/sanity-io/block-content-to-markdown)

### Portable Text in .NET

[https://github.com/oslofjord/sanity-linq#9-rendering-block-content](https://github.com/oslofjord/sanity-linq#9-rendering-block-content)

### Portable Text in Python

[https://github.com/otovo/python-portabletext-html](https://github.com/otovo/python-portabletext-html)

### Portable Text in PHP

[https://github.com/sanity-io/sanity-php#rendering-block-content](https://github.com/sanity-io/sanity-php#rendering-block-content)

Need to serialize to something not listed here in a language we don't cover? Create an issue on the repo for [Portable Text](https://www.portabletext.org), or [join us on Slack](https://slack.sanity.io) and let us know.

## Plain text serialization

Serializing Portable Text to plain text can be useful when you need it previews or similar. It also helps demystify what goes into serializing Portable Text. Here's a function written in JavaScript that takes a Portable Text array as an argument, and returns it as paragraphs in plain text:

```javascript
function toPlainText(blocks = []) {
  return blocks
    // loop through each block
    .map(block => {
      // if it's not a text block with children, 
      // return nothing
      if (block._type !== 'block' || !block.children) {
        return ''
      }
      // loop through the children spans, and join the
      // text strings
      return block.children.map(child => child.text).join('')
    })
    // join the paragraphs leaving split by two linebreaks
    .join('\n\n')
}

```

## Rendering Portable Text in React

A common use case is to render rich text content from Sanity in the popular web framework [React](https://reactjs.org/). We have made tooling that deals with the defaults out of the box, and lets you add components for controlling how custom content type should be rendered in the front end. Let's look at an example of how to set it up:

```jsx
import React from 'react'
import * as ReactDOM from 'react-dom'
import {PortableText} from '@portabletext/react'
import {createClient} from '@sanity/client'

const client = sanityClient({
  projectId: '<your project id>',
  dataset: '<your dataset>',
  apiVersion: '2022-05-05',
  useCdn: true
})

const components = {
  types: {
    code: (props) => {
      const {language, code} = props.value
      return (
        <pre data-language={language}>
          <code>{code}</code>
        </pre>
      )
    }
  }
}

client.fetch('*[_type == "article"][0]')
  .then(article => {
    ReactDOM.render(
      <PortableText value={article.body} components={components} />,
      document.getElementById('root')
    )
  })

```

Additional details are available [in our documentation](https://www.sanity.io/docs/block-content) or on the [@portabletext/react README](https://github.com/portabletext/react-portabletext).

> [!NOTE]
> Portable Text to React
> In 2022, the @sanity/block-content-to-react package was deprecated in favour of @portabletext/react. The example above uses components and values from the new package instead of serializers and blocks, but we offer a migration guide and will continue to answer questions about @sanity/block-content-to-react in the Slack community.

## Join references

If you have references to other documents such as internal links, files or images in your Portable Text, you typically want to join the data from those documents into your Portable Text. Here's how:

Say you have a Portable Text with internal links to other articles using [mark annotations](https://www.sanity.io/docs/customization#annotations-f924645007e1) like so:

```json
// portableText.js
export default {
  name: 'portableText',
  type: 'array',
  title: 'Content',
  of: [
    {
      type: 'block',
      marks: {
        annotations: [
          {
            name: 'internalLink',
            type: 'object',
            title: 'Internal link',
            fields: [
              {
                name: 'reference',
                type: 'reference',
                title: 'Reference',
                to: [
                  { type: 'article' },
                  // other types you may want to link to
                ]
              }
            ]
          }
        ]
      }
    }
  ]
}
```

In order to get the slug of the linked article you need to use [GROQ](/docs/groq-reference) to query your document and use the [join syntax](https://www.sanity.io/docs/query-cheat-sheet#joins-e82ab8c0925b) to fetch the referenced article like so: 

```groq
*[_type == "post"]{
  ...,
  body[]{
    ...,
    markDefs[]{
      ...,
      _type == "internalLink" => {
        "slug": @.reference->slug
      }
    }
  }
}
```

[Read the full guide on including links in Portable Text](/guides/portable-text-internal-and-external-links).

## 
Deserialization

If you need to convert existing markup to Portable Text you can use the JavaScript library [Sanity Block Tools](https://github.com/sanity-io/sanity/blob/next/packages/%40sanity/block-tools/README.md). You can use it both in a browser and in a node.js environment. It also lets you make custom rules to deserialize parts of your HTML into custom content types etc.

Complete example of deserialization of HTML into Portable Text blocks in a browser environment:

```javascript
import Schema from '@sanity/schema'
import blockTools from '@portabletext/block-tools'


// Start with compiling a schema we can work against
const defaultSchema = Schema.compile({
  name: 'myBlog',
  types: [
    {
      type: 'object',
      name: 'blogPost',
      fields: [
        {
          title: 'Title',
          type: 'string',
          name: 'title'
        },
        {
          title: 'Body',
          name: 'body',
          type: 'array',
          of: [{type: 'block'}]
        }
      ]
    }
  ]
})

// The compiled schema type for the content type that holds the block array
const blockContentType = defaultSchema.get('blogPost')
  .fields.find(field => field.name === 'body').type


// Convert HTML to block array
const blocks = blockTools.htmlToBlocks(
  '<html><body><h1>Hello world!</h1><body></html>',
  blockContentType
)
// Outputs
//
//  {
//    _type: 'block',
//    style: 'h1'
//    children: [
//      {
//        _type: 'span'
//        text: 'Hello world!'
//      }
//    ]
//  }


// Get the feature-set of a blockContentType
const features = blockTools.getBlockContentFeatures(blockContentType)
```





# Create your own Sanity template

#### Ready to submit your template?
Click the button below to submit your template for review. Don't forget to join the #template-creators Slack channel to connect with others and share your work.
[Submit your template](https://community.sanity.tools/intent/create/type=contribution.starter;template=contribution.starter/)

[Sanity templates](https://www.sanity.io/templates) are reusable, pre-configured projects that come with an integrated, customizable front-end. They're a great way to streamline your development process, ensuring consistency across different projects, and reducing the time required to get a new project off the ground.

Creating a template for the Sanity community is a great way to share your expertise with other developers and make starting future Sanity projects a breeze.

This guide will walk you through how to create and submit a new Sanity template.

## 1) Clone the [template-kit](https://github.com/sanity-io/scaffolding-template) repository

The easiest way to get started creating a Sanity template is by using the official [Sanity Template Kit repository](https://github.com/sanity-io/scaffolding-template) as a starting point.

This repo provides all the necessary boilerplate for creating a template; including a generic studio configuration, a folder to generate your front-end framework of choice, and the `@sanity/template-validator` [GitHub action](https://docs.github.com/en/actions/writing-workflows/quickstart) to ensure your template meets our technical requirements.

Click the "Use this template" button in Github to create a new repository with the Sanity Template Kit as a base.



![](https://cdn.sanity.io/images/3do82whm/next/875c3209d26a5a7563aab9eae7ba1b3dd2347085-910x300.png)

## 2) Initialize your frontend

After cloning the [template-kit repository](https://github.com/sanity-io/scaffolding-template), initialize your preferred front-end inside of a directory called `frontend`.

The following command will initialize a new app with Next.js, but you can use any front-end framework to build out your template.

```sh
npx create-next-app@latest frontend
```

> [!TIP]
> Protip
> If you're prompted to initialize a new git repository for your project, say 'No' since you're initializing the front-end inside of an existing git repository.

## 3) Build out your template

Now, the fun part! After initializing your project, it's time to build out your template.

We *highly* recommend starting this process by familiarizing yourself with our [Opinionated Guide to Sanity Studio](https://www.sanity.io/guides/an-opinionated-guide-to-sanity-studio).

This guide provides best practices for:

- File organization for schemas and plugins
- Defining reusable and extensible content schemas
- Formatting GROQ queries for readability and performance

For advice on how to build out a front-end that works well with Sanity, we recommend taking a look at our official, framework-specific templates for [Next.js](https://www.sanity.io/templates/nextjs-sanity-clean), [Astro](https://www.sanity.work/templates/astro-sanity-clean), [Nuxt](https://www.sanity.io/templates/nuxt-sanity-clean), [Angular](https://www.sanity.io/templates/angular-sanity-clean), [SvelteKit](https://www.sanity.io/templates/sveltekit-sanity-clean), and [React Router](https://www.sanity.io/templates/remix-sanity-clean).

These templates show how to use advanced Sanity features like the [Presentation tool](/docs/visual-editing/configuring-the-presentation-tool) and the [Live Content API](/docs/content-lake/live-content-api) to build powerful experiences into your template like:

- [Visual editing](/docs/visual-editing/introduction-to-visual-editing)
- [Drag-and-drop page building](/docs/visual-editing/enabling-drag-and-drop)
- [Real-time content updates](/docs/developer-guides/live-content-guide)

If you need help or inspiration, join the `#template-creators` channel in our [Slack community](https://slack.sanity.io/). We're here to help you build great stuff with Sanity!

## 4) Validate your template

Once you're ready to submit your Sanity template, you can validate it using the `@sanity/template-validator` package by running:

```sh
npm run validate
```

This script ensures your template complies with our technical requirements. You can find a full [list of validation rules here](https://github.com/sanity-io/template-validator#validation-rules), but in general the script checks for:

- A fully configured Sanity Studio.
- A functioning front-end project (e.g., Next.js, Astro, React Router, etc) with valid configuration files and TypeScript support
- A `README.md` file at the root of the project with an H2 titled "Getting Started" that includes a step-by-step setup guide for users.

See the [current list of featured templates](https://www.sanity.io/templates) for examples of well structured template projects.

#### Common Errors

```sh
Environment template in ${packageName} contains invalid environment variable syntax. Please see https://dotenvx.com/docs/env-file for proper formatting.
```

This error is usually do to having whitespace before or after and `=` in your `.env.[example | template | local]` file.



```sh
Invalid package.json file in ${packageName}
```

This error usually means you don't have any Sanity related dependencies in your `package.json` like `sanity`, `next-sanity`, or `@sanity/client`.

## 5) Submit your template

Once your template passes the validator, [submit your template for review](https://community.sanity.tools/intent/create/type=contribution.starter;template=contribution.starter/) in the Community Studio.

You'll need to provide:

- A title and description of your template
- A link to your template's Github repository (make sure it's public!)
- A 1200px x 750px image or screenshot of your template

Although optional, we **strongly suggest** you also provide a link to a deployed example application showing your template in action.

This will allow users to preview your template and see if it's a good fit for their use case.



![](https://cdn.sanity.io/images/3do82whm/next/fab4594a9cb536682f2487bf05886cc4e27eec40-3248x2112.png)



The Sanity team will review your submission to ensure it meets our quality standards and provides value to the community. For reference, a good template is:

- **Purposeful**: Clearly communicates the use case (e.g., an e-commerce store with Sanity + Shopify or a documentation site with search powered by Algolia).
- **Configurable**: Includes sensible defaults but allows for customization.
- **Documented**: Provides clear setup instructions and a `README.md` with a correctly formatted "Getting Started" section. The template kit repository provides this for you out of the box.

If changes are needed, we will reach out to you with feedback on how to improve your template.

## Next steps

Congratulations! Once your template is approved, it will be listed in the official [Sanity templates gallery](https://www.sanity.io/templates) in Sanity Exchange. After approval, be sure to:

- **Promote your template**: Share your template with the Sanity community on [LinkedIn](https://www.linkedin.com/company/sanity-io/posts/?feedView=all), [X](https://x.com/sanity_io?lang=en), and [BlueSky](https://bsky.app/profile/sanity.io) and tag us @sanity.
- **Iterate and improve your template**: Address user feedback by checking in on your template in Sanity Exchange. Keep your template updated as dependencies and best practices evolve.
- **Contribute other templates**: Don't stop with just one! Build additional templates or join discussions in the `#template-creators` channel [on Slack](https://slack.sanity.io/).

By sharing your work, you’re empowering the community to create amazing projects with Sanity.

We can’t wait to see what you build!



# Community Code of Conduct

## Welcome to the Sanity Community

All participants in the Sanity.io community must comply with the Sanity.io code of conduct. This includes discussions and contributions to GitHub repositories, our Discord community, meetups, events, and other venues hosted by Sanity.io. We’re all on the same team and responsible for maintaining a welcoming community.

## The Community Discord

If you join the Sanity Discord, you will be welcomed with this message:

This space is community-driven, so feel free to suggest new channels, emojis, integrations, or whatever you think will make this place better and more useful for everyone (#feedback). We welcome everyone to help each other out!

**Read this before asking for help:**

- Keep the noise down by only posting a question in one appropriate channel (we will delete duplicates without notice)
- State your problem/question in the main message, and provide useful context in threads (error messages, steps to reproduce, etc.)
- Make code examples easy to read by using the code block formatting or linking to gists on Github, etc.
- Do not tag Sanity team members in threads or send them Direct Messages, even though you are frustrated and stuck. This is incredibly distracting for the team and undermines the purpose of a community where your question can be helpful to others. The Sanity team is told to ignore unsolicited DMs.
- If you use ChatGPT to help people, check the quality of the answer first. ChatGPT often gets it wrong.

## The short version

Everyone at Sanity is dedicated to providing an inclusive, safe, and harassment-free environment for all participants regardless of age, disability, sexual orientation, gender, gender identity, physical appearance, body size, race, ethnicity, nationality, or religion (or lack thereof). This includes memes, emojis, and GIFs. And remember, we’re not only “guys” – including us at Sanity HQ – (we are people, folks, friends, y’all, etc.).

> [!TIP]
> Protip
> The community Slack workspace features automatic moderation that triggers on use of words and phrases recognized as non-inclusive for under-represented groups in tech. The purpose of this bot is to mitigate language that prevents people from feeling belonging and to educate how we all can act more inclusively in a space with many different people coming from different backgrounds and experiences.

All attendees, speakers, sponsors, and volunteers must agree with this code of conduct at events and in our community spaces. We expect cooperation from everyone to help ensure a safe environment for everybody.

If someone makes you or anyone else feel unsafe or unwelcome, please report it as soon as possible.

**You can make a report by:**

- Contacting a member of Sanity HQ as denoted in their community chat username
- Emailing: [community@sanity.io](mailto:community@sanity.io)

## **The long version**

First of all, thank you for reading this! We want you to come to our community and feel that you can be your authentic self.

The Sanity community is made of people with a common agenda, cause, and interests, who collaborate by sharing ideas, information, and resources. It is important that each and every person attending our events and spaces has a positive and rewarding experience and to that end, we are committed to providing a safe, productive, and welcoming environment for all participants, speakers, and staff.

### **Expected behavior**

- We expect everyone to:    - Follow this code of conduct; it’s important for us!
- Use inclusive language
- Let people finish speaking and give each other space to share their thoughts
- Treat each other with consideration and curiosity, and celebrate that we’re all stoked about technology, structured content, and the Sanity community (we hope!)
- Be mindful that everyone comes with their own struggles and experiences
- Keep all conversations in public channels unless you have explicit consent from the receiver to have direct interaction or direct message (DM)
- Celebrate and cheer for each other!
- Follow health guidelines from local health authorities and the venue (wearing masks, hygiene, etc.)
- Make content that’s accessible, not limited to adding alternative text to slides and images, making transcriptions for audio, and adding closed captioning to videos



### **Unacceptable behavior**

- Harassment is not tolerated. That includes, but is not limited to:      - Comments that are unwelcomed or offensive relating to gender, gender identity, and expression, sexual orientation, disability, physical appearance, body size, race, age, religion
- Sexual images, GIFs, and memes in public spaces
- Spamming messages or advertisements
- Deliberate intimidation, stalking, or following
- Purposeful misgendering of any person
- Harassing photography or recording
- Disruption of talks or other events
- Inappropriate physical contact
- Invasion of personal space
- Unwelcome sexual attention
- Bring any kind of weapon
- Advocating for or encouraging any of the above behavior
- Portray any of the above behavior in any medium (chat, email, social media, webinars, etc.)
- Make false reports or exploit the code of conduct to exclude people or for purposes of retaliation


- Physical violence or threatening behavior is not tolerated. That includes, but is not limited to, hitting, punching, pushing, kicking, pinching, biting, shouting, and spitting.

### **Consequences of unacceptable behavior**

- Engaging in unacceptable behavior will result in the following: - A host contacting you to figure out what happened
- You’ll be given instructions for expected behavior going forward, and you are expected to comply immediately
- Removal from the event or space
- Lose of access to Sanity’s community platforms and future events



### **Reporting**

If someone makes you or anyone else feel unsafe or unwelcome, please report it as soon as possible.

**You can make a report by:**

- Contacting a member of Sanity HQ as denoted in their community chat username
- Emailing: [community@sanity.io](mailto:community@sanity.io)



### Emergency contact information

For life-threatening emergencies, call 911.



**Mental health and crisis phone numbers:**

##### Emergency contact phone numbers

| San Francisco Crisis Line | 1-415-781-0500 |
| U.S. National Suicide & Veteran Crisis Hotline | 1-800-273-8255 |
| TTY - U.S. National Suicide Line | Use your preferred relay service or dial 711 then 1-800-273-8255 |




**Mental health and crisis chat services:**

[U.S. National Suicide Prevention](https://suicidepreventionlifeline.org/chat/)

[Veteran Crisis Chat](https://www.veteranscrisisline.net/get-help-now/chat/)

[ASL - U.S. National Suicide Chat](https://vibrant.aslnow.io/app/8/10004)









# Migrating plugins to support Content Releases

The introduction of [Content Releases](/docs/studio/content-releases-configuration) into Sanity Studio introduces some new core concepts available through the Sanity SDK.

To interface with the studio global perspective, `usePerspective` is available. This hook must be used within Studio. For example:

```tsx
import { usePerspective } from 'sanity'

function MyComponent() {
  const { perspectiveStack } = usePerspective()
  // ...
}
```

`usePerspective` returns:

```tsx
  /* The selected perspective name, it could be a release or Published */
  selectedPerspectiveName: 'published' | ReleaseId | undefined
  /**
   * The releaseId as `r<string>``; it will be undefined 
   * if the selected perspective is `published` or `drafts`
   */
  selectedReleaseId: ReleaseId | undefined
  /* Return the current global release */
  selectedPerspective: SelectedPerspective
  /**
   * The stacked array of releases ids ordered chronologically 
   * to represent the state of documents at the given point in time.
   */
  perspectiveStack: PerspectiveStack
  /* The excluded perspectives */
  excludedPerspectives: string[]
```

Further, a `ReleaseId` may be used to query document versions within a release as noted [here](/docs/apis-and-sdks/content-releases-api).

## Custom Input Component plugins

There are particular concerns relevant for plugins which make custom input components available via custom input types. Prior to Content Releases, a document form may have made its form inputs read only in instances where data was loading, being re-synced, or in a transient state. Now, Studio Perspectives allow users to view the document form of the published document version. This form is read only in all instances, besides liveEdit. In these cases it is imperative that plugins pass the `readOnly` prop available when rendering custom components:

```tsx
const PluginComponent = ({ readOnly }: InputProps) => {
	... spread `readOnly` if using Sanity UI, or use `readOnly` to control disabling input ...
}

export const plugin =  defineType({
	...,
	fields: [
		...,
		{
			...,
			components: {
			input: PluginComponent
		}
	]
})
```





# Getting started with Sanity

#### Quickstart guides

[Next.js](/docs/next-js-quickstart)

[Nuxt.js](/docs/nuxt-js-quickstart)

[Astro](/docs/astro-quickstart)

[React Router (Remix)](/docs/react-router-quickstart)

[Sanity Studio](/docs/sanity-studio-quickstart)



#### Other ways of getting started

[A very short introduction](/docs/getting-started/the-sanity-content-operating-system-an-introduction)

[The developer essentials track](https://www.sanity.io/learn/track/sanity-developer-essentials)

[Templates](https://www.sanity.io/templates)





# Platform introduction

The Sanity Content Operating System provides three interconnected layers: 

- [Content Lake](/docs/content-lake) (content database)
- [Compute and AI](/docs/compute-and-ai) (business logic)
- [APIs and SDKs](/docs/apis-and-sdks) (developer tools)

[The Sanity Dashboard](/docs/dashboard) for running your content operations apps, such as: 

- [Studio](/docs/studio) (customizable CMS)
- [Media Library](/docs/media-library) (asset management)
- [Canvas](/docs/canvas) (AI-powered content creation)
- [Your custom-built apps](undefined) (using [the App SDK](/docs/app-sdk))

You can use Sanity to:

- Build custom content workflows that match your specific business processes
- Create and manage structured content that can be reused across any digital channel
- Develop specialized content applications that give teams exactly the tools they need

Unlike traditional or headless CMSes, Sanity provides a foundation for your entire content lifecycle across all digital channels, with the flexibility to evolve as your needs change.

You can get started with Sanity in minutes. [Go here to explore the different ways](/docs/getting-started).



# What is content operations?

The Sanity Content Operating System goes beyond storage and delivery of content. It provides the computational infrastructure and developer tools to build content workflows and applications supporting your business processes. That's a fancy way to say "content operations."

In the real world, content operations are not linear processes. They are cyclical, with each stage informing and feeding back into the others. For example, analytics from content distribution might influence your governance policies, or AI-powered creation tools might change how you approach content planning. 

You can think of these content operation processes in the following five stages.

## 1. Planning and governing

Defining your content model, setting up workflows, establishing [roles and permissions](/docs/user-guides/roles), and creating governance policies that ensure content quality and consistency. This stage is about building the foundation for how your organization thinks about and structures content.

## 2. Content creation and management

The actual content production, from writing and editing to asset creation and organization. This includes collaborative workflows, version control, and the tools that make content creation efficient. Sanity provides structured editing environments like [Studio](/docs/studio) and [Canvas](/docs/canvas) that adapt to your content model while maintaining the integrity of your structured data.

## 3. Distributions and automation

Getting your content to where it needs to go, when it needs to be there. This includes publishing workflows, content delivery networks, and automated processes that ensure content reaches its intended audience across all channels. The Content Operating System provides [APIs, SDKs](/docs/apis-and-sdks), and [serverless functions](/docs/compute-and-ai) that automate distribution while maintaining content integrity. This stage is where your structured content proves its value. Flowing seamlessly to websites, apps, digital displays, and other touchpoints.



## 4. Analysis and optimization

Understanding how your content performs across channels and using those insights to improve. This includes analytics, A/B testing, and performance monitoring that help you make data-driven decisions about your content strategy. The Content Operating System provides tools to track content performance and integrate with analytics platforms, giving you visibility into how your audience engages with your content and its impact on your business goals.

## 5. Extending and integrations

Connecting your content system with other tools and platforms in your tech stack. This includes building custom integrations, extending functionality with plugins, and creating specialized applications that address specific business needs. The Content Operating System provides APIs, SDKs, and serverless functions that make it easy to connect with other systems while maintaining the integrity of your content model. This stage is where the programmable nature of Sanity truly shines, allowing you to build exactly the content ecosystem your organization needs.



